# Performance Optimization Guide
## Archivist Magika v2.0 - Batch Processing

**–î–∞—Ç–∞:** 2025-11-05
**–í–µ—Ä—Å–∏—è:** 1.0

---

## –¢–µ–∫—É—â–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

### Baseline (Sequential Processing)
- **–ú–µ—Ç–æ–¥:** –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (1 –∫–Ω–∏–≥–∞ –∑–∞ —Ä–∞–∑)
- **–°–∫–æ—Ä–æ—Å—Ç—å:** ~9-15 –º–∏–Ω—É—Ç –Ω–∞ –∫–Ω–∏–≥—É (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ä–∞–∑–º–µ—Ä–∞)
- **–í—Ä–µ–º—è –¥–ª—è 20 –∫–Ω–∏–≥:** ~3-4 —á–∞—Å–∞
- **–†–µ—Å—É—Ä—Å—ã:** 1 CPU core + Ollama API

### –°–∏—Å—Ç–µ–º–∞
- **CPU:** 16 cores / 32 threads
- **RAM:** 32 GB
- **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:** ~5% CPU, ~2 GB RAM (sequential mode)

---

## –£–∑–∫–∏–µ –º–µ—Å—Ç–∞ (Bottlenecks)

### 1. LM Extraction (Extended Stage)
**–°–∞–º–æ–µ –º–µ–¥–ª–µ–Ω–Ω–æ–µ:** 60-70% –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏

- Ollama API –≤—ã–∑–æ–≤—ã –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
- Model: `qwen2.5:7b`
- ~120 –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –∫–Ω–∏–≥—É (–ø–æ 1 –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É)
- ~2-3 —Å–µ–∫—É–Ω–¥—ã –Ω–∞ –∑–∞–ø—Ä–æ—Å

**–†–∞—Å—á–µ—Ç:**
```
120 pages √ó 2.5 sec = 300 sec = 5 min (—Ç–æ–ª—å–∫–æ LM)
+ 4 min (–æ—Å—Ç–∞–ª—å–Ω—ã–µ stages) = 9 min total
```

### 2. PDF Extraction (Structural Stage)
**–í—Ç–æ—Ä–æ–µ –ø–æ –º–µ–¥–ª–µ–Ω–Ω–æ—Å—Ç–∏:** 15-20% –≤—Ä–µ–º–µ–Ω–∏

- pdfminer.six –¥–ª—è —Ç–µ–∫—Å—Ç–∞
- OCR —á–µ—Ä–µ–∑ Tesseract (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
- –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–∞–±–ª–∏—Ü

### 3. Embedding (Embed Stage)
**–¢—Ä–µ—Ç—å–µ:** 10-15% –≤—Ä–µ–º–µ–Ω–∏

- BGE-M3 —á–µ—Ä–µ–∑ Ollama
- Batch encoding chunks
- FAISS index creation

---

## –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

### ‚ö° –£—Ä–æ–≤–µ–Ω—å 1: –ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è –∫–Ω–∏–≥ (Easy, 2-3x speedup)

**–ò–¥–µ—è:** –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å 2-3 –∫–Ω–∏–≥–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ

**–ü–ª—é—Å—ã:**
- ‚úÖ –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è (–≥–æ—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç: `batch_parallel.sh`)
- ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –±–æ–ª—å—à–µ CPU cores
- ‚úÖ 2-3x —É—Å–∫–æ—Ä–µ–Ω–∏–µ (3-4 —á–∞—Å–∞ ‚Üí 1-1.5 —á–∞—Å–∞)

**–ú–∏–Ω—É—Å—ã:**
- ‚ö†Ô∏è Ollama –º–æ–∂–µ—Ç —Å—Ç–∞—Ç—å bottleneck (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç concurrent requests)
- ‚ö†Ô∏è –£–≤–µ–ª–∏—á–µ–Ω–∏–µ RAM usage (~6 GB)

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –ó–∞–ø—É—Å–∫–∞—Ç—å 2-3 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–∞

**–ö–æ–º–∞–Ω–¥–∞:**
```bash
# –í batch_parallel.sh —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å:
PARALLEL_JOBS=3

# –ó–∞–ø—É—Å–∫:
./batch_parallel.sh
```

---

### ‚ö°‚ö° –£—Ä–æ–≤–µ–Ω—å 2: Batch LM requests (Medium, 1.5-2x speedup –Ω–∞ stage)

**–ò–¥–µ—è:** –û—Ç–ø—Ä–∞–≤–ª—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ Ollama –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ

**–ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ –∫–æ–¥–µ:**
```python
# am_extended.py - –¥–æ–±–∞–≤–∏—Ç—å async batch processing

import asyncio
import aiohttp

async def extract_batch(self, pages: List[dict]) -> List[dict]:
    """Extract extended fields for batch of pages."""
    tasks = [self._extract_single_page(page) for page in pages]
    return await asyncio.gather(*tasks)

# Usage:
batch_size = 5  # 5 pages at once
for i in range(0, len(pages), batch_size):
    batch = pages[i:i+batch_size]
    results = await extract_batch(batch)
```

**–ü–ª—é—Å—ã:**
- ‚úÖ –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ LM stage (5x faster)
- ‚úÖ –õ—É—á—à–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Ollama capacity

**–ú–∏–Ω—É—Å—ã:**
- ‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ –∫–æ–¥–∞ (async/await)
- ‚ö†Ô∏è Ollama –¥–æ–ª–∂–µ–Ω –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å concurrent requests

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –î–ª—è production use –ø–æ—Å–ª–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

---

### ‚ö°‚ö°‚ö° –£—Ä–æ–≤–µ–Ω—å 3: GPU acceleration (Hard, 5-10x speedup)

**–ò–¥–µ—è:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU –¥–ª—è inference –≤–º–µ—Å—Ç–æ CPU

**–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:**
- NVIDIA GPU —Å CUDA
- Ollama —Å GPU support
- VRAM: 8+ GB –¥–ª—è qwen2.5:7b

**–ò–∑–º–µ–Ω–µ–Ω–∏—è:**
```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å Ollama —Å GPU support
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å:
ollama run qwen2.5:7b --verbose

# –í –∫–æ–Ω—Ñ–∏–≥–µ:
extended:
  lm_provider: "ollama"
  lm_model: "qwen2.5:7b"
  device: "cuda"  # –î–æ–±–∞–≤–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫—É
```

**–ü–ª—é—Å—ã:**
- ‚úÖ –û–≥—Ä–æ–º–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ LM inference (10x+)
- ‚úÖ –ú–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å batch_size

**–ú–∏–Ω—É—Å—ã:**
- ‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç GPU
- ‚ö†Ô∏è –°–ª–æ–∂–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –ï—Å–ª–∏ –µ—Å—Ç—å GPU - –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

---

### üîß –£—Ä–æ–≤–µ–Ω—å 4: Optimization tweaks (Small gains)

#### 4.1 Reduce logging level
```bash
# –í config/batch_full.yaml:
logging:
  level: "WARNING"  # –í–º–µ—Å—Ç–æ INFO
  console: false
```

**–≠–∫–æ–Ω–æ–º–∏—è:** ~5% –≤—Ä–µ–º–µ–Ω–∏ (–º–µ–Ω—å—à–µ I/O)

#### 4.2 Skip duplicate detection
```yaml
# –ï—Å–ª–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω—ã:
extended:
  skip_duplicate_detection: true
```

**–≠–∫–æ–Ω–æ–º–∏—è:** ~2-3% –≤—Ä–µ–º–µ–Ω–∏

#### 4.3 Disable quality tracking
```yaml
quality:
  enabled: false  # –î–ª—è batch runs
```

**–≠–∫–æ–Ω–æ–º–∏—è:** ~1-2% –≤—Ä–µ–º–µ–Ω–∏

#### 4.4 Smaller embedding model
```yaml
embedding:
  model: "bge-small-en-v1.5"  # –í–º–µ—Å—Ç–æ bge-m3
  dimensions: 384 ‚Üí 256
```

**–≠–∫–æ–Ω–æ–º–∏—è:** ~20% –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ embed stage
**–ú–∏–Ω—É—Å:** –ß—É—Ç—å —Ö—É–∂–µ –∫–∞—á–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–æ–≤

---

## –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

### –î–ª—è –≤–∞—à–µ–π —Å–∏—Å—Ç–µ–º—ã (16 cores, 32 GB RAM):

**–ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (Best performance):**
```bash
# 1. –ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è: 3 –∫–Ω–∏–≥–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
PARALLEL_JOBS=3

# 2. Reduce logging
logging:
  level: "WARNING"
  console: false

# 3. Skip non-critical checks
extended:
  skip_duplicate_detection: true

quality:
  enabled: false
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**
- **–í—Ä–µ–º—è:** 1-1.5 —á–∞—Å–∞ (–≤–º–µ—Å—Ç–æ 3-4)
- **RAM usage:** ~6-8 GB
- **CPU usage:** ~30-40%

---

### –î–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ (Best quality):

```bash
# 1. Sequential processing (—Ç–µ–∫—É—â–∏–π —Ä–µ–∂–∏–º)
PARALLEL_JOBS=1

# 2. Full logging
logging:
  level: "INFO"
  console: true

# 3. All checks enabled
extended:
  skip_duplicate_detection: false

quality:
  enabled: true
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**
- **–í—Ä–µ–º—è:** 3-4 —á–∞—Å–∞
- **Quality:** Maximum
- **Logs:** –ü–æ–ª–Ω—ã–µ –¥–µ—Ç–∞–ª—å–Ω—ã–µ –ª–æ–≥–∏

---

## –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã

### –ü—Ä–∏–º–µ—Ä 1: Quick reprocessing (–ø–æ—Å–ª–µ bugfix)

```bash
# –ò—Å–ø–æ–ª—å–∑—É–µ–º parallel processing
cp batch_parallel.sh batch_quick.sh

# Edit batch_quick.sh:
PARALLEL_JOBS=4
CONFIG="config/batch_quick.yaml"

# config/batch_quick.yaml:
logging:
  level: "WARNING"

extended:
  skip_duplicate_detection: true

quality:
  enabled: false

# Run:
./batch_quick.sh
```

**–í—Ä–µ–º—è:** ~45-60 –º–∏–Ω—É—Ç –¥–ª—è 20 –∫–Ω–∏–≥

---

### –ü—Ä–∏–º–µ—Ä 2: Production processing (–ø–µ—Ä–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)

```bash
# –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–∏–π sequential script
./batch_reprocess_lm.sh

# –° –ø–æ–ª–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞
```

**–í—Ä–µ–º—è:** ~3-4 —á–∞—Å–∞ –¥–ª—è 20 –∫–Ω–∏–≥
**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ + –ø–æ–ª–Ω—ã–µ –ª–æ–≥–∏

---

## –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—É—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤:

```powershell
# CPU usage
Get-Process python | Select-Object CPU,ProcessName

# RAM usage
Get-Process python | Select-Object WS,ProcessName

# Ollama status
curl http://localhost:11434/api/tags
```

### Real-time monitoring:

```bash
# –í –æ—Ç–¥–µ–ª—å–Ω–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ:
watch -n 5 'ps aux | grep python | grep -v grep'

# –ò–ª–∏ —á–µ—Ä–µ–∑ monitor script:
./monitor_batch.sh
```

---

## Benchmarks (–í–∞—à–∞ —Å–∏—Å—Ç–µ–º–∞)

### Sequential (Current)
- **1 –∫–Ω–∏–≥–∞:** 9 –º–∏–Ω—É—Ç (DORA report, 120 pages)
- **20 –∫–Ω–∏–≥:** ~180 –º–∏–Ω—É—Ç (–æ—Ü–µ–Ω–∫–∞)
- **CPU:** 5-10% usage
- **RAM:** ~2 GB

### Parallel (x3 jobs)
- **3 –∫–Ω–∏–≥–∏:** ~12-15 –º–∏–Ω—É—Ç (concurrent)
- **20 –∫–Ω–∏–≥:** ~60-80 –º–∏–Ω—É—Ç (–æ—Ü–µ–Ω–∫–∞)
- **CPU:** 30-40% usage
- **RAM:** ~6 GB

### GPU-accelerated (—Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏)
- **1 –∫–Ω–∏–≥–∞:** ~3-4 –º–∏–Ω—É—Ç—ã
- **20 –∫–Ω–∏–≥:** ~60 –º–∏–Ω—É—Ç (sequential)
- **GPU:** 80-90% usage
- **VRAM:** ~6 GB

---

## Troubleshooting

### –ü—Ä–æ–±–ª–µ–º–∞: Ollama becomes bottleneck

**–°–∏–º–ø—Ç–æ–º—ã:**
- Multiple processes waiting for Ollama
- API timeouts
- High Ollama CPU usage

**–†–µ—à–µ–Ω–∏–µ:**
```bash
# –£–≤–µ–ª–∏—á–∏—Ç—å concurrent requests –≤ Ollama:
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_MAX_LOADED_MODELS=2

# Restart Ollama:
ollama serve
```

---

### –ü—Ä–æ–±–ª–µ–º–∞: Out of memory

**–°–∏–º–ø—Ç–æ–º—ã:**
- Python crashes
- System slowdown

**–†–µ—à–µ–Ω–∏–µ:**
```bash
# Reduce parallel jobs:
PARALLEL_JOBS=2  # –í–º–µ—Å—Ç–æ 3-4

# –ò–ª–∏ —É–≤–µ–ª–∏—á–∏—Ç—å swap:
# (Windows: Settings ‚Üí System ‚Üí About ‚Üí Advanced system settings ‚Üí Performance ‚Üí Virtual memory)
```

---

### –ü—Ä–æ–±–ª–µ–º–∞: Disk I/O bottleneck

**–°–∏–º–ø—Ç–æ–º—ã:**
- Slow FAISS writes
- Log files growing slowly

**–†–µ—à–µ–Ω–∏–µ:**
```bash
# Use SSD for processing (if available)
# Reduce logging:
logging:
  level: "ERROR"

# Disable quality JSON writes:
quality:
  enabled: false
```

---

## Future improvements

### Short term (1-2 weeks)
1. ‚úÖ Implement batch_parallel.sh
2. üî≤ Add async LM requests (asyncio)
3. üî≤ Add progress bar (tqdm)
4. üî≤ Add time estimates per book

### Medium term (1 month)
1. üî≤ GPU support for Ollama
2. üî≤ Distributed processing (multiple machines)
3. üî≤ Cache LM results (avoid reprocessing)
4. üî≤ Incremental updates (process only changed pages)

### Long term (3+ months)
1. üî≤ Cloud processing (AWS Lambda, Azure Functions)
2. üî≤ Real-time dashboard (web UI)
3. üî≤ Auto-scaling based on load
4. üî≤ ML model optimization (quantization, pruning)

---

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

**–¢–µ–∫—É—â–∏–π —Ä–µ–∂–∏–º (sequential):**
- ‚úÖ –°—Ç–∞–±–∏–ª—å–Ω—ã–π –∏ –Ω–∞–¥–µ–∂–Ω—ã–π
- ‚úÖ –ü–æ–ª–Ω—ã–µ –ª–æ–≥–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–æ
- ‚è±Ô∏è ~3-4 —á–∞—Å–∞ –¥–ª—è 20 –∫–Ω–∏–≥

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –¥–ª—è –±—É–¥—É—â–µ–≥–æ:**
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `batch_parallel.sh` —Å `PARALLEL_JOBS=3`
- –û–∂–∏–¥–∞–µ–º–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ: 2-3x
- –í—Ä–µ–º—è: ~1-1.5 —á–∞—Å–∞ –¥–ª—è 20 –∫–Ω–∏–≥

**–ï—Å–ª–∏ –µ—Å—Ç—å GPU:**
- –ù–∞—Å—Ç—Ä–æ–∏—Ç—å Ollama —Å GPU support
- –û–∂–∏–¥–∞–µ–º–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ: 5-10x
- –í—Ä–µ–º—è: ~30-60 –º–∏–Ω—É—Ç –¥–ª—è 20 –∫–Ω–∏–≥

---

**–ö–æ–Ω–µ—Ü –¥–æ–∫—É–º–µ–Ω—Ç–∞**

*–í–µ—Ä—Å–∏—è 1.0*
*–î–∞—Ç–∞: 2025-11-05*
*–ê–≤—Ç–æ—Ä: Claude Code*

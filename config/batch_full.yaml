# ============================================
# Archivist Magika - BATCH FULL Configuration
# ============================================
# Version: 2.0.0 - BATCH PROCESSING MODE
# Created: 2025-10-31
# Purpose: Maximum quality batch processing for entire library
#
# Features:
# ✅ OCR enabled (Tesseract with preprocessing)
# ✅ LM extraction enabled (qwen2.5:7b)
# ✅ Table extraction (pdfplumber)
# ✅ Structure detection (chapters/sections)
# ✅ Deduplication (exact + fuzzy)
# ✅ Summarization (L1 + L2)
# ✅ Quality tracking (all metrics)
# ✅ MAXIMUM LOGGING (DEBUG level)
# ✅ Error recovery (continue on failure)
# ============================================

project:
  name: "Archivist Magika - Batch Processing"
  version: "2.0.0"
  product: "archivist magika v4.3.8"
  permissions: "public"
  mode: "batch"  # Batch processing mode

# ============================================
# PIPELINE STAGES (for validation)
# ============================================
pipeline:
  stages:
    - structural
    - structure_detect
    - summarize
    - extended
    - finalize
    - chunk
    - embed

# ============================================
# PATHS CONFIGURATION
# ============================================
paths:
  source_pdfs: "data/sources/"  # Batch input directory

  datasets:
    structural: "data/datasets/structural/"
    structured: "data/datasets/structured/"
    summarized: "data/datasets/summarized/"
    extended: "data/datasets/extended/"
    final: "data/datasets/final/"
    chunks: "data/datasets/chunks/"

  indexes:
    faiss: "data/indexes/faiss/"
    metadata: "data/indexes/metadata/"

  tables: "data/tables/"
  logs: "logs/"
  cache: "data/cache/"

# ============================================
# BATCH PROCESSING CONFIGURATION
# ============================================
batch:
  enabled: true

  # Error handling
  continue_on_error: true              # Continue processing even if one book fails
  save_partial_results: true           # Save intermediate results on failure
  max_retries: 2                       # Retry failed books up to 2 times
  retry_delay: 5                       # Seconds between retries

  # Logging
  per_book_logs: true                  # Create separate log file per book
  log_dir: "logs/batch/"               # Batch-specific logs

  # Progress tracking
  checkpoint_interval: 1               # Save checkpoint after each book
  checkpoint_file: "data/batch_checkpoint.json"

  # Multi-book index
  create_unified_index: true           # Create single FAISS index for all books
  unified_index_name: "library_unified"

  # Book exclusions
  exclude_files: ["Ядро 2025-08-29.pdf"]  # Process separately

# ============================================
# PIPELINE CONFIGURATION
# ============================================

# Stage 1: Structural extraction (MAXIMUM MODE)
structural:
  max_pages: 5000
  empty_streak_threshold: 5            # More lenient for technical books
  backend: "pdfminer"

  # Multi-extractor fallback chain
  extractors:
    - "pdfminer"                       # Primary
    - "pdfplumber"                     # Fallback 1
    - "pypdf2"                         # Fallback 2

  # ✅ OCR SUPPORT - FULL ENABLED
  ocr:
    enabled: true
    trigger: "auto"                    # Trigger on low text yield
    threshold_chars: 50                # OCR if page has <50 chars

    engine: "tesseract"
    languages: ["eng", "rus"]          # English + Russian

    # Preprocessing (for better OCR quality)
    preprocess:
      enabled: true
      deskew: true                     # Fix skewed scans
      denoise: true                    # Remove noise/artifacts
      binarize: true                   # Convert to black & white
      resize: false                    # Keep original size

    # Quality thresholds
    min_confidence: 0.60               # Accept OCR with 60%+ confidence
    dpi: 300                           # High DPI for quality

    # Performance
    cache_results: true                # Cache OCR per page
    parallel: false                    # Sequential (safer for batch)
    timeout_per_page: 30               # 30 sec max per page

  # ✅ TABLE EXTRACTION - FULL ENABLED
  tables:
    enabled: true
    engine: "pdfplumber"               # Best for technical books

    # Detection thresholds
    min_rows: 2
    min_cols: 2

    # Extraction settings
    detect_headers: true               # Identify header rows
    merge_cells: true                  # Handle merged cells
    snap_tolerance: 3                  # Cell boundary tolerance

    # Output format
    format: "markdown"                 # Human-readable
    storage: "inline"                  # Embed in segment

    # Fallback vector detection
    vector_detection:
      line_threshold: 8
      rect_threshold: 6
      total_threshold: 10

  # PDF Metadata
  extract_metadata:
    enabled: true
    fields: ["title", "author", "subject", "keywords", "creator", "creation_date"]
    fallback_from_first_page: true

  toc_mode: "strict_per_page"

# Stage 2: Structure Detection (ENHANCED)
structure_detection:
  enabled: true

  # Chapter detection patterns
  chapters:
    patterns:
      - regex: "^CHAPTER\\s+(\\d+|[IVX]+)[:\\.]?\\s*(.+)"
        level: 1
        priority: 1
      - regex: "^Chapter\\s+(\\d+)[:\\.]?\\s*(.+)"
        level: 1
        priority: 2
      - regex: "^PART\\s+([IVX]+|\\d+)[:\\.]?\\s*(.+)"
        level: 0
        priority: 3
      - regex: "^ГЛАВА\\s+(\\d+)[:\\.]?\\s*(.+)"  # Russian support
        level: 1
        priority: 4

    # Heuristics (fallback)
    heuristics:
      enabled: true
      font_size_change: true
      all_caps: true
      short_line: true
      followed_by_empty: true
      position_top: true
      max_line_length: 100
      min_font_size_ratio: 1.5

  # Section detection
  sections:
    patterns:
      - regex: "^(\\d+\\.\\d+)\\s+(.+)"      # 5.1 Title
        level: 2
        priority: 1
      - regex: "^([A-Z][a-z]+(?:\\s+[A-Z][a-z]+){1,5})$"  # Title Case
        level: 2
        priority: 2

    heuristics:
      enabled: true
      numbered: true
      bold_text: false

  # Output
  update_cards:
    add_structure_field: true
    update_section_path: true

  create_toc:
    enabled: true
    format: "nested"

# Stage 3: Summarization (FULL ENABLED)
summarize:
  enabled: true                        # ENABLED for batch
  engine: "extractive"                 # Fast extractive summarization
  policy: "minimal"

  extractive:
    level1_max_chars: 300              # Short summary
    level2_max_chars: 900              # Long summary
    top_sentences: 3
    preserve_order: true
    algorithm: "textrank"              # TextRank for extraction

  # LM-based summarization (optional, slow)
  lm:
    enabled: false                     # Disabled for speed
    provider: "ollama"
    model: "qwen2.5:7b"
    temperature: 0.0
    top_p: 1.0
    use_cache: true

  filters:
    min_length: 50                     # Skip very short segments
    max_length: null
    pages: null
    every_n: null
    regex: null

  write_policy: "fill_missing"

# Stage 4: Extended pass (FULL LM EXTRACTION)
extended:
  # Merge short pages
  merge_short:
    enabled: true
    short_threshold: 30
    max_merge_span: 3

  # Continuity validation
  continuity:
    enabled: true
    method: "jaccard"
    overlap_threshold: 0.7
    window_size: 50

  # Visual detection
  visuals:
    enabled: true
    detect_mentions: true

  # ✅ DEDUPLICATION - FULL ENABLED
  deduplication:
    enabled: true

    methods:
      - "exact"                        # Exact match
      - "fuzzy"                        # High similarity

    fuzzy_threshold: 0.95              # 95% = duplicate

    action: "mark"                     # Mark, don't remove
    keep_first: true

    compare_within_book: true
    compare_across_books: false        # Within book only

    compare_fields: ["segment"]
    ignore_whitespace: true
    ignore_case: false

  # ✅ LM EXTRACTION - FULL ENABLED
  extended_fields:
    enabled: true                      # ENABLED for metadata

    # Use LM for extraction
    use_lm: true
    lm_provider: "ollama"
    lm_model: "qwen2.5:7b"             # 7B model (balance speed/quality)
    lm_temperature: 0.0
    lm_timeout: 30                     # 30 sec timeout per segment

    # Fallback to heuristics on LM failure
    fallback_to_heuristics: true

    # Fields to extract
    fields:
      content_type:
        enabled: true
        values: ["theory", "practice", "case_study", "example", "exercise", "reference"]

      domain:
        enabled: true
        values: ["devops", "architecture", "security", "testing", "monitoring", "cloud", "general"]

      complexity:
        enabled: true
        values: ["beginner", "intermediate", "advanced", "expert"]

      technical_features:
        enabled: true
        detect: ["has_code", "has_formulas", "has_diagram", "has_citations"]

      entities:
        enabled: true
        extract: ["companies", "products", "technologies", "people"]

      metrics:
        enabled: true
        terms: ["deployment frequency", "lead time", "MTTR", "change failure rate",
                "cycle time", "throughput", "WIP", "flow efficiency"]

      frameworks:
        enabled: true
        terms: ["DORA", "CALMS", "DevOps", "Lean", "Agile", "Scrum", "Kanban",
                "SAFe", "Theory of Constraints", "Little's Law"]

      methods:
        enabled: true
        terms: ["continuous delivery", "continuous integration", "trunk-based development",
                "feature flags", "blue-green deployment", "canary release", "A/B testing"]

      tools:
        enabled: true
        terms: ["Jenkins", "GitLab", "GitHub Actions", "Kubernetes", "Docker", "Terraform",
                "Ansible", "Prometheus", "Grafana", "Jira", "Trello"]

      cases:
        enabled: true
        patterns: ["Case Study:", "Example:", "At [A-Z][a-z]+"]
        extract_company_names: true

# Stage 5: Finalize (LENIENT)
finalize:
  require_l1: false                    # Don't require summaries
  require_l2: false
  require_l3: true

  stop_on:
    continuity_gap: false              # Don't stop on gaps
    missing_visuals: false
    missing_extended: false

  report:
    enabled: true
    path: "data/datasets/final/report.json"
    include_issues: true

# Stage 6: Chunking (STRUCTURE-AWARE)
chunking:
  # Token limits
  target_tokens: 512
  max_tokens: 600
  min_tokens: 100
  overlap_tokens: 50

  # Context addition
  add_context: true
  context_template: "[BOOK: {book}]{chapter_context}{section_context} | pp.{pages}\n\n{text}"
  context_parts:
    chapter: " | CHAPTER: {chapter}"
    section: " | SECTION: {section}"

  # Preservation rules
  preserve_boundaries:
    case_study: true
    table: true                        # Keep tables intact
    code_block: true
    list: true
    chapter: true                      # Don't split chapters
    section: false

  # Smart chunking
  respect_paragraph_breaks: true
  avoid_mid_sentence_split: true
  merge_short_segments: true

  # Metadata enrichment
  extract_metadata:
    from_extended_fields: true
    from_summary: true
    from_structure: true
    detect_content_type: true

# Stage 7: Embedding (BGE-M3)
embedding:
  model: "BAAI/bge-m3"
  provider: "ollama"                   # Use Ollama for embeddings
  ollama_model: "bge-m3"               # Ollama model name

  device: "cpu"
  batch_size: 16                       # Smaller batch for stability

  dimension: 1024
  normalize: true

  # FAISS index
  faiss:
    index_type: "Flat"                 # Simple flat index (best quality)
    metric: "cosine"                   # Cosine similarity

    # HNSW params (if needed later)
    hnsw_m: 32
    hnsw_ef_construction: 200
    hnsw_ef_search: 100

  # Error handling
  retry_on_failure: true
  max_retries: 3
  skip_on_persistent_failure: true     # Skip chunk if embedding fails 3x

# ============================================
# RAG SEARCH CONFIGURATION
# ============================================
rag:
  search:
    top_k: 10                          # More results for batch
    similarity_threshold: 0.65         # Slightly lower threshold

    filters:
      by_book: true
      by_chapter: true
      by_section: true
      by_content_type: true
      by_domain: true
      by_complexity: true
      by_metrics: true
      by_frameworks: true
      by_tools: true
      by_has_table: true
      by_has_code: true

    rerank:
      enabled: false                   # Disabled for speed

# ============================================
# LOGGING CONFIGURATION
# ============================================
logging:
  level: "INFO"                        # INFO level (not DEBUG - too verbose!)
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

  file:
    enabled: true
    path: "logs/batch_full.log"
    max_bytes: 52428800                # 50 MB per log file
    backup_count: 10                   # Keep 10 backups

  console:
    enabled: true
    colored: true
    level: "INFO"                      # Less verbose on console

  # Per-stage logging
  stage_logs:
    enabled: true
    directory: "logs/stages/"

  # Error aggregation
  error_log:
    enabled: true
    path: "logs/errors.log"
    level: "ERROR"

# ============================================
# QUALITY METRICS (FULL TRACKING)
# ============================================
quality_metrics:
  enabled: true
  output_dir: "data/quality/"

  # Per-book reports
  per_book_reports: true
  per_book_dir: "data/quality/books/"

  # Aggregate report
  aggregate_report: true
  aggregate_path: "data/quality/batch_aggregate.json"

  # Thresholds (lenient for batch)
  thresholds:
    structural:
      empty_pages_ratio: 0.15          # Max 15% empty (more lenient)
      avg_chars_per_page: 400          # Min 400 chars
      ocr_avg_confidence: 0.70         # Min 70% OCR confidence
      unicode_errors: 100              # Max 100 unicode errors
      success_ratio: 0.90              # Min 90% pages extracted

    structure_detect:
      structure_coverage: 0.60         # Min 60% coverage
      chapters_detected: 1             # At least 1 chapter

    summarize:
      summary_coverage: 0.80           # Min 80% coverage
      avg_l1_length: 50

    extended:
      extraction_coverage: 0.70        # Min 70% LM extraction
      duplicate_ratio: 0.05            # Max 5% duplicates
      continuity_gaps_ratio: 0.10

    chunk:
      avg_chunk_tokens: 100
      max_chunk_tokens: 650
      context_completeness: 0.90       # Min 90% with context

    embed:
      embedding_failures_ratio: 0.05   # Max 5% failures allowed

  # Reporting
  summary_report:
    enabled: true
    path: "data/quality/batch_summary.json"
    log_to_console: true

  # Track processing time
  track_performance: true
  performance_report: "data/quality/performance.json"

# ============================================
# VALIDATION (LENIENT FOR BATCH)
# ============================================
validation:
  check_manifest: true
  check_required_fields: true

  # Stop on critical errors only
  stop_on_validation_error: false
  log_validation_warnings: true

  required_fields:
    structural:
      - "segment_id"
      - "segment"
      - "summary.level3"
      - "flags.ocr_used"
      - "tables"

    structured:
      - "segment_id"
      - "structure"
      - "section_path"

    summarized:
      - "segment_id"
      - "summary.level1"

    extended:
      - "segment_id"
      - "extended_fields"
      - "flags.is_duplicate"

    final:
      - "segment_id"
      - "summary.level3"
      - "__audit__.finalized_at"

    chunks:
      - "chunk_id"
      - "text"
      - "metadata"
      - "source_segments"

# ============================================
# PERFORMANCE (BATCH OPTIMIZED)
# ============================================
performance:
  use_multiprocessing: false           # Sequential for stability
  num_workers: 1

  # Caching
  cache_ocr: true                      # Cache all OCR results
  cache_tables: true                   # Cache table extractions
  cache_embeddings: true               # Cache embeddings
  cache_lm_responses: true             # Cache LM API responses

  # Progress bars
  show_progress: true
  progress_bar_style: "tqdm"

  # Memory management
  clear_cache_interval: 5              # Clear cache every 5 books
  gc_interval: 1                       # Run garbage collection per book

# ============================================
# ERROR RECOVERY
# ============================================
error_recovery:
  enabled: true

  # Retry strategy
  max_retries_per_stage: 2
  retry_backoff_factor: 2              # 5s, 10s, 20s...

  # Partial results
  save_on_error: true
  continue_to_next_book: true

  # Error categorization
  categorize_errors: true
  error_categories:
    - "ocr_failure"
    - "extraction_failure"
    - "lm_timeout"
    - "embedding_failure"
    - "validation_error"
    - "file_io_error"

  # Error reporting
  error_report_path: "logs/error_report.json"

# ============================================
# DATASET FORMAT
# ============================================
dataset_format:
  version: "v4.3.8"

  card_required:
    - "segment_id"
    - "section_path"
    - "segment"
    - "summary"
    - "structure"

  card_optional:
    - "flags"
    - "visuals"
    - "image_count"
    - "table_line_count"
    - "table_rect_count"
    - "table_score"
    - "tables"
    - "extended_fields"
    - "role"
    - "ocr_confidence"
    - "duplicate_of"

# ============================================
# NOTES
# ============================================
# Batch Full Configuration - Design Decisions:
#
# 1. ERROR TOLERANCE: Continue on errors, save partial results
# 2. LOGGING: Maximum verbosity (DEBUG) for troubleshooting
# 3. OCR: Full enabled with preprocessing for scanned PDFs
# 4. LM: Enabled for metadata extraction (qwen2.5:7b)
# 5. SUMMARIZATION: Extractive only (fast)
# 6. QUALITY: Lenient thresholds for diverse books
# 7. PERFORMANCE: Sequential processing for stability
# 8. CACHING: All caches enabled to avoid re-computation
# 9. MULTI-BOOK: Unified index creation supported
# 10. EXCLUSIONS: Ядро 2025-08-29.pdf processed separately
#
# Expected runtime: 30-120 minutes for 20 books
# Expected storage: 5-10 GB (datasets + indexes + logs)

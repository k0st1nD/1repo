===DATASET_BEGIN===
{"segment_id": "__header__", "source": {"title": "Agile Metrics in Action", "author": "Christopher W. H. Davis", "file_name": "Agile-Metrics-in-Action.pdf", "file_size": 20224710, "pages": 270}, "book": "agile-metrics-in-action", "total_cards": 270, "segment_ids": ["00001", "00002", "00003", "00004", "00005", "00006", "00007", "00008", "00009", "00010", "00011", "00012", "00013", "00014", "00015", "00016", "00017", "00018", "00019", "00020", "00021", "00022", "00023", "00024", "00025", "00026", "00027", "00028", "00029", "00030", "00031", "00032", "00033", "00034", "00035", "00036", "00037", "00038", "00039", "00040", "00041", "00042", "00043", "00044", "00045", "00046", "00047", "00048", "00049", "00050", "00051", "00052", "00053", "00054", "00055", "00056", "00057", "00058", "00059", "00060", "00061", "00062", "00063", "00064", "00065", "00066", "00067", "00068", "00069", "00070", "00071", "00072", "00073", "00074", "00075", "00076", "00077", "00078", "00079", "00080", "00081", "00082", "00083", "00084", "00085", "00086", "00087", "00088", "00089", "00090", "00091", "00092", "00093", "00094", "00095", "00096", "00097", "00098", "00099", "00100", "00101", "00102", "00103", "00104", "00105", "00106", "00107", "00108", "00109", "00110", "00111", "00112", "00113", "00114", "00115", "00116", "00117", "00118", "00119", "00120", "00121", "00122", "00123", "00124", "00125", "00126", "00127", "00128", "00129", "00130", "00131", "00132", "00133", "00134", "00135", "00136", "00137", "00138", "00139", "00140", "00141", "00142", "00143", "00144", "00145", "00146", "00147", "00148", "00149", "00150", "00151", "00152", "00153", "00154", "00155", "00156", "00157", "00158", "00159", "00160", "00161", "00162", "00163", "00164", "00165", "00166", "00167", "00168", "00169", "00170", "00171", "00172", "00173", "00174", "00175", "00176", "00177", "00178", "00179", "00180", "00181", "00182", "00183", "00184", "00185", "00186", "00187", "00188", "00189", "00190", "00191", "00192", "00193", "00194", "00195", "00196", "00197", "00198", "00199", "00200", "00201", "00202", "00203", "00204", "00205", "00206", "00207", "00208", "00209", "00210", "00211", "00212", "00213", "00214", "00215", "00216", "00217", "00218", "00219", "00220", "00221", "00222", "00223", "00224", "00225", "00226", "00227", "00228", "00229", "00230", "00231", "00232", "00233", "00234", "00235", "00236", "00237", "00238", "00239", "00240", "00241", "00242", "00243", "00244", "00245", "00246", "00247", "00248", "00249", "00250", "00251", "00252", "00253", "00254", "00255", "00256", "00257", "00258", "00259", "00260", "00261", "00262", "00263", "00264", "00265", "00266", "00267", "00268", "00269", "00270"], "dataset_created_at": "2025-11-05T10:38:19Z", "pdf_sha256": "15091110fbd330cf7ca5afd9e3c5719905cef128fa8eadc62dfc6be974c4f1ac", "version": "2.0.0", "product": "archivist magika 2.0.0", "created_at": "2025-11-05T10:38:19Z", "stage": "final", "structure_detected_at": "2025-11-05T10:38:19Z", "chapters": 53, "sections": 14, "toc": {"found": true, "start_page": 6, "end_page": 6, "page_count": 1}, "summarized_at": "2025-11-05T10:38:20Z", "summary_engine": "extractive-v2", "generate_l2": true, "extended_at": "2025-11-05T10:50:53Z", "finalized_at": "2025-11-05T10:50:53Z", "validation_passed": true}
{"segment_id": "00001", "page_num": 1, "segment": "How to measure and improve team performance\n\nIN ACTION\n\nChristopher W. H. Davis\nFOREWORD BY Olivier Gaudin\n\nM A N N I N G", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "l1_summary": "How to measure and improve team performance Davis\nFOREWORD BY Olivier Gaudin", "l2_summary": "How to measure and improve team performance Davis\nFOREWORD BY Olivier Gaudin", "next_page": {"page_num": 2, "segment_id": "00002"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": [], "content_type": "tutorial", "domain": "management", "complexity": "intermediate", "companies": [], "people": ["Christopher W. H. Davis", "Olivier Gaudin"], "products": [], "technologies": [], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["team performance"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["measuring team performance", "improving team performance"], "key_concepts": ["performance metrics", "best practices"], "problem_statement": "How to measure and improve the performance of a team", "solution_approach": "Theoretical guidance on measuring and improving team performance", "extraction_method": "lm"}}
{"segment_id": "00002", "page_num": 2, "segment": "Agile Metrics in Action\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "l1_summary": "Agile Metrics in Action Licensed to Mark Watson <nordickan@gmail.com>", "l2_summary": "Agile Metrics in Action Licensed to Mark Watson <nordickan@gmail.com>", "prev_page": {"page_num": 1, "segment_id": "00001"}, "next_page": {"page_num": 3, "segment_id": "00003"}, "flags": {"continuity_gap": true}}
{"segment_id": "00003", "page_num": 3, "segment": "ii\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "prev_page": {"page_num": 2, "segment_id": "00002"}, "next_page": {"page_num": 4, "segment_id": "00004"}}
{"segment_id": "00004", "page_num": 4, "segment": "Agile Metrics in Action\n\nHow to measure and improve team performance\n\nCHRISTOPHER W. H. DAVIS\n\nM A N N I N G\nSHELTER ISLAND\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "l1_summary": "Agile Metrics in Action How to measure and improve team performance M A N N I N G\nSHELTER ISLAND", "l2_summary": "Agile Metrics in Action How to measure and improve team performance M A N N I N G\nSHELTER ISLAND Licensed to Mark Watson <nordickan@gmail.com>", "prev_page": {"page_num": 3, "segment_id": "00003"}, "next_page": {"page_num": 5, "segment_id": "00005"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": [], "content_type": "tutorial", "domain": "management|programming", "complexity": "intermediate", "companies": [], "people": ["CHRISTOPHER W. H. DAVIS"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["team performance"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Agile Metrics", "Improving Team Performance"], "key_concepts": ["Agile Metrics", "Team Performance Measurement"], "problem_statement": "How to measure and improve team performance in an Agile environment", "solution_approach": "Using metrics and practices specific to Agile methodologies", "extraction_method": "lm"}}
{"segment_id": "00005", "page_num": 5, "segment": "iv\n\nFor online information and ordering of this and other Manning books, please visit. The publisher offers discounts on this book when ordered in quantity.\nFor more information, please contact\n\nSpecial Sales Department\nManning Publications Co.\n20 Baldwin Road\nPO Box 761\nShelter Island, NY 11964\nEmail: orders@manning.com\n\n©2015 by Manning Publications Co. All rights reserved.\n\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in\nany form or by means electronic, mechanical, photocopying, or otherwise, without prior written\npermission of the publisher.\n\nMany of the designations used by manufacturers and sellers to distinguish their products are\nclaimed as trademarks. Where those designations appear in the book, and Manning\nPublications was aware of a trademark claim, the designations have been printed in initial caps\nor all caps.\n\nRecognizing the importance of preserving what has been written, it is Manning’s policy to have\nthe books we publish printed on acid-free paper, and we exert our best efforts to that end.\nRecognizing also our responsibility to conserve the resources of our planet, Manning books are\nprinted on paper that is at least 15 percent recycled and processed without elemental chlorine.\n\nManning Publications Co.\n20 Baldwin Road\nPO Box 761\nShelter Island, NY 11964\n\nDevelopment editor: Dan Maharry\n\nTechnical development editor: Michael Smolyak\n\nCopyeditor: Linda Recktenwald\nProofreader: Elizabeth Martin\n\nTechnical proofreader: David Pombal\nTypesetter: Marija Tudor\nCover designer: Marija Tudor\n\nISBN: 9781617292484\nPrinted in the United States of America\n1 2 3 4 5 6 7 8 9 10 -- EBM -- 20 19 18 17 16 15\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "l1_summary": "Special Sales Department\nManning Publications Co. ©2015 by Manning Publications Co. Manning Publications Co.", "l2_summary": "For online information and ordering of this and other Manning books, please visit. Special Sales Department\nManning Publications Co. 20 Baldwin Road\nPO Box 761\nShelter Island, NY 11964\nEmail: orders@manning.com ©2015 by Manning Publications Co. Manning Publications Co. 20 Baldwin Road\nPO Box 761\nShelter Island, NY 11964", "prev_page": {"page_num": 4, "segment_id": "00004"}, "next_page": {"page_num": 6, "segment_id": "00006"}, "flags": {"continuity_gap": true}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["manning", "publications", "printed", "books", "designations", "information", "please", "publisher", "book", "baldwin"], "content_type": "reference", "domain": "programming", "complexity": "intermediate", "companies": ["Manning Publications Co."], "people": [], "products": [], "technologies": [], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": false, "has_antipatterns": false, "has_instructions": false, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Microsoft Word"], "topics": ["book publishing", "order information", "copyright notice"], "key_concepts": ["publishing company details", "ordering process", "copyright information"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00006", "page_num": 6, "segment": "brief contents\n\nPART 1 MEASURING AGILE TEAMS ............................................ 1\n\n1 ■ Measuring agile performance 3\n2 ■ Observing a live project 20\n\nPART 2 COLLECTING AND ANALYZING YOUR TEAM’S DATA ....... 35\n\n3 ■ Trends and data from project-tracking systems 37\n4 ■ Trends and data from source control 62\n5 ■ Trends and data from CI and deployment servers 84\n6 ■ Data from your production systems 107\n\nPART 3 APPLYING METRICS TO YOUR TEAMS, PROCESSES,\n AND SOFTWARE ....................................................... 125\n\n7 ■ Working with the data you’re collecting: the sum\n\nof the parts 127\n\n8 ■ Measuring the technical quality of your software 154\n9 ■ Publishing metrics 177\n\n10 ■ Measuring your team against the agile principles 201\n\nv\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "l1_summary": "PART 1 MEASURING AGILE TEAMS ............................................ PART 2 COLLECTING AND ANALYZING YOUR TEAM’S DATA ....... 7 ■ Working with the data you’re collecting: the sum", "l2_summary": "PART 1 MEASURING AGILE TEAMS ............................................ 1 ■ Measuring agile performance 3\n2 ■ Observing a live project 20 PART 2 COLLECTING AND ANALYZING YOUR TEAM’S DATA ....... 3 ■ Trends and data from project-tracking systems 37\n4 ■ Trends and data from source control 62\n5 ■ Trends and data from CI and deployment servers 84\n6 ■ Data from your production systems 107 7 ■ Working with the data you’re collecting: the sum 10 ■ Measuring your team against the agile principles 201", "prev_page": {"page_num": 5, "segment_id": "00005"}, "next_page": {"page_num": 7, "segment_id": "00007"}, "flags": {"continuity_gap": true}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["data", "measuring", "part", "agile", "trends", "teams", "project", "collecting", "team", "systems"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["project-tracking systems", "source control", "CI and deployment servers", "production systems"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["agile performance", "technical quality of software"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["project-tracking systems", "source control", "CI and deployment servers", "production systems"], "topics": ["Measuring Agile Teams", "Collecting and Analyzing Data", "Applying Metrics to Teams, Processes, and Software"], "key_concepts": ["agile performance measurement", "data collection from various sources", "technical quality metrics", "publishing metrics", "aligning with agile principles"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00007", "page_num": 7, "segment": "vi\n\nBRIEF CONTENTS\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "l1_summary": "Licensed to Mark Watson <nordickan@gmail.com>", "l2_summary": "Licensed to Mark Watson <nordickan@gmail.com>", "prev_page": {"page_num": 6, "segment_id": "00006"}, "next_page": {"page_num": 8, "segment_id": "00008"}}
{"segment_id": "00008", "page_num": 8, "segment": "contents\n\nforeword xiii\npreface\nacknowledgments\nabout this book\n\nxv\n\nxvii\n\nxix\n\nPART 1 MEASURING AGILE TEAMS ................................. 1\n\n1 Measuring agile performance\n\n3\n\n1.1 Collect, measure, react, repeat---the feedback loop 4\n\nWhat are metrics? 5\n\n1.2 Why agile teams struggle with measurement 5\n\nProblem: agile definitions of measurement are not\nstraightforward 6 ■ Problem: agile focuses on a product,\nnot a project 7 ■ Problem: data is all over the place without a\nunified view 8\n\n1.3 What questions can metrics answer, and where do I get the\n\ndata to answer them? 9\n\nProject tracking 10 ■ Source control 11 ■ The build\nsystem 11 ■ System monitoring 12\n\n1.4 Analyzing what you have and what to do with the data 13\n\nFiguring out what matters 14 ■ Visualizing your data 14\n\nvii\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "l1_summary": "PART 1 MEASURING AGILE TEAMS ................................. 1 Measuring agile performance 1.4 Analyzing what you have and what to do with the data 13", "l2_summary": "PART 1 MEASURING AGILE TEAMS ................................. 1 Measuring agile performance 1.2 Why agile teams struggle with measurement 5 Problem: agile definitions of measurement are not\nstraightforward 6 ■ Problem: agile focuses on a product,\nnot a project 7 ■ Problem: data is all over the place without a\nunified view 8 1.4 Analyzing what you have and what to do with the data 13 Figuring out what matters 14 ■ Visualizing your data 14", "prev_page": {"page_num": 7, "segment_id": "00007"}, "next_page": {"page_num": 9, "segment_id": "00009"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": true, "has_citations": false, "key_terms": ["agile", "what", "data", "problem", "measuring", "teams", "metrics", "measurement", "project", "answer"], "content_type": "tutorial", "domain": "devops|architecture", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["agile", "metrics", "project tracking", "source control", "build system", "system monitoring"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": true, "has_metrics": true, "metrics": ["agile performance", "project tracking", "source control", "build system", "system monitoring"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["measuring agile teams", "metrics and measurement in agile", "data collection and analysis"], "key_concepts": ["feedback loop", "agile definitions of measurement", "project tracking", "source control", "build system", "system monitoring"], "problem_statement": "agile teams struggle with measurement", "solution_approach": "analyzing metrics, data sources, and visualization", "extraction_method": "lm"}}
{"segment_id": "00009", "page_num": 9, "segment": "viii\n\nCONTENTS\n\n1.5 Applying lessons learned 16\n\n1.6 Taking ownership and measuring your team 16\n\nGetting buy-in 17 ■ Metric naysayers 18\n\n1.7\n\nSummary 19\n\n2 Observing a live project\n\n2.1 A typical agile project 20\n\n20\n\nHow Blastamo Music used agile 21\n\n2.2 A problem arises 21\n\n2.3 Determining the right solution 22\n\n2.4 Analyzing and presenting the data 26\n\nSolving the problems 27 ■ Visualizing the final product for\nleadership 28\n\n2.5 Building on the system and improving their processes 31\n\nUsing data to improve what they do every day 32\n\n2.6\n\nSummary 33\n\nPART 2\n\nCOLLECTING AND ANALYZING YOUR\nTEAM’S DATA .................................................. 35\n\n3 Trends and data from project-tracking systems\n\n37\n3.1 Typical agile measurements using PTS data 39\n\nBurn down 39 ■ Velocity 40 ■ Cumulative flow 41\nLead time 42 ■ Bug counts 42\n\n3.2 Prepare for analysis; generate the richest set of data you\n\ncan 44\n\nTip 1: Make sure everyone uses your PTS 45 ■ Tip 2: Tag tasks\nwith as much data as possible 46 ■ Tip 3: Estimate how long\nyou think your tasks will take 47 ■ Tip 4: Clearly define when\ntasks are done 49 ■ Tip 5: Clearly define when tasks are\ncompleted in a good way 50\n\n3.3 Key project management metrics; spotting trends in data 52\n\nTask volume 52 ■ Bugs 53 ■ Measuring task movement;\nrecidivism and workflow 54 ■ Sorting with tags and labels 55\n\n3.4 Case study: identifying tech debt trending with project\n\ntracking data 57\n\n3.5\n\nSummary 60\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "section_num": "1.5", "section_title": "Applying lessons learned 16", "l1_summary": "2.1 A typical agile project 20 2.4 Analyzing and presenting the data 26 3 Trends and data from project-tracking systems", "l2_summary": "2 Observing a live project 2.1 A typical agile project 20 2.4 Analyzing and presenting the data 26 COLLECTING AND ANALYZING YOUR\nTEAM’S DATA .................................................. 3 Trends and data from project-tracking systems 37\n3.1 Typical agile measurements using PTS data 39", "prev_page": {"page_num": 8, "segment_id": "00008"}, "next_page": {"page_num": 10, "segment_id": "00010"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": true, "has_citations": false, "key_terms": ["data", "project", "tasks", "summary", "agile", "measuring", "team", "typical", "analyzing", "using"], "content_type": "tutorial", "domain": "management|data_science", "complexity": "intermediate", "companies": ["Blastamo Music"], "people": [], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": true, "has_metrics": true, "metrics": ["Burn down", "Velocity", "Cumulative flow", "Lead time", "Bug counts", "Task volume", "Bugs", "Measuring task movement; recidivism and workflow"], "has_case_study": true, "case_study_company": "Blastamo Music", "tools_mentioned": ["Project-tracking systems (PTS)"], "topics": ["Agile project management", "Data analysis in Agile", "Case study of Blastamo Music"], "key_concepts": ["Applying lessons learned", "Taking ownership and measuring the team", "Getting buy-in", "Metric naysayers", "Collecting and analyzing data", "Trends and metrics from PTS", "Improving processes using data"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00010", "page_num": 10, "segment": "CONTENTS\n\nix\n\n4 Trends and data from source control\n\n4.1 What is source control management? 63\n\n62\n\n4.2 Preparing for analysis: generate the richest set of data\n\nyou can 64\n\nTip 1: Use distributed version control and pull requests 65\n\n4.3 The data you’ll be working with; what you can get\n\nfrom SCM 68\n\nThe data you can get from a DVCS 68 ■ Data you can get from\ncentralized SCM 71 ■ What you can tell from SCM alone 71\n\n4.4 Key SCM metrics: spotting trends in your data 77\n\nCharting SCM activity 78\n\n4.5 Case study: moving to the pull request workflow and\n\nincorporating quality engineering 79\n\n4.6\n\nSummary 82\n\n5 Trends and data from CI and deployment servers\n\n5.1 What is continuous development? 86\n\n84\n\nContinuous integration 86 ■ Continuous delivery 88\nContinuous testing 89\n\n5.2 Preparing for analysis: generate the richest set of data\n\nyou can 90\n\nSet up a delivery pipeline 90\n\n5.3 The data you’ll be working with: what you can get from\n\nyour CI APIs 91\n\nThe data you can get from your CI server 91 ■ What you can tell\nfrom CI alone 96\n\n5.4 Key CI metrics: spotting trends in your data 97\n\nGetting CI data and adding it to your charts 97\n\n5.5 Case study: measuring benefits of process change through\n\nCI data 101\n\n5.6\n\nSummary 105\n\n6 Data from your production systems\n\n107\n\n6.1 Preparing for analysis: generating the richest set of data\n\nyou can 109\n\nAdding arbitrary metrics to your development\ncycle 110 ■ Utilizing the features of your application\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "section_num": "4.1", "section_title": "What is source control management? 63", "l1_summary": "4.3 The data you’ll be working with; what you can get The data you can get from a DVCS 68 ■ Data you can get from\ncentralized SCM 71 ■ What you can tell from SCM alone 71 5.3 The data you’ll be working with: what you can get from", "l2_summary": "4 Trends and data from source control 4.2 Preparing for analysis: generate the richest set of data 4.3 The data you’ll be working with; what you can get The data you can get from a DVCS 68 ■ Data you can get from\ncentralized SCM 71 ■ What you can tell from SCM alone 71 5.3 The data you’ll be working with: what you can get from The data you can get from your CI server 91 ■ What you can tell\nfrom CI alone 96", "prev_page": {"page_num": 9, "segment_id": "00009"}, "next_page": {"page_num": 11, "segment_id": "00011"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": true, "has_citations": false, "key_terms": ["data", "what", "trends", "continuous", "control", "preparing", "analysis", "richest", "metrics", "source"], "content_type": "tutorial", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["source control management", "distributed version control", "pull requests", "continuous integration", "deployment servers"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["SCM metrics", "CI metrics"], "has_case_study": true, "case_study_company": null, "tools_mentioned": ["source control management", "distributed version control", "pull requests", "continuous integration", "deployment servers"], "topics": ["source control management", "continuous integration and deployment", "data from production systems"], "key_concepts": ["source control metrics", "CI metrics", "data collection for analysis", "process improvement through data"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00011", "page_num": 11, "segment": "x\n\nCONTENTS\n\nperformance monitoring system 113 ■ Using logging best\npractices 115 ■ Using social network interaction to connect\nwith your consumers 116\n\n6.2 The data you’ll be working with: what you can get from\n\nyour APM systems 118\n\nServer health statistics 118 ■ Consumer usage 120\nSemantic logging analysis 120 ■ Tools used to collect\nproduction system data 121\n\n6.3 Case study: a team moves to DevOps and continuous\n\ndelivery 122\n\n6.4\n\nSummary 124\n\nPART 3\n\nAPPLYING METRICS TO YOUR TEAMS,\nPROCESSES, AND SOFTWARE .......................... 125\n\n7 Working with the data you’re collecting: the sum of the parts\n\n7.1 Combining data points to create metrics 127\n\n127\n\n7.2 Using your data to define “good” 129\n\nTurning subjectivity into objectivity 130 ■ Working backward\nfrom good releases 132\n\n7.3 How to create metrics 135\n\nStep 1: explore your data 136 ■ Step 2: break it down---\ndetermine what to track 139 ■ Step 3: create formulas around\nmultiple data points to create metrics 140\n\n7.4 Case study: creating and using a new metric to measure\n\ncontinuous release quality 144\n\n7.5\n\nSummary 153\n\n8 Measuring the technical quality of your software\n\n154\n8.1 Preparing for analysis: setting up to measure your\n\ncode 155\n\n8.2 Measuring the NFRs through the code “ilities” 156\n\n8.3 Measuring maintainability 158\n\nMTTR and lead time 158 ■ Adding SCM and build\ndata 162 ■ Code coverage 164 ■ Adding static code\nanalysis 165 ■ Adding more PTS data 167\n\n8.4 Measuring usability 168\n\nReliability and availability 169 ■ Security 171\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "section_num": "6.2", "section_title": "The data you’ll be working with: what you can get from", "l1_summary": "6.2 The data you’ll be working with: what you can get from 7.1 Combining data points to create metrics 127 7.2 Using your data to define “good” 129", "l2_summary": "6.2 The data you’ll be working with: what you can get from 7 Working with the data you’re collecting: the sum of the parts 7.1 Combining data points to create metrics 127 7.2 Using your data to define “good” 129 7.3 How to create metrics 135 Step 1: explore your data 136 ■ Step 2: break it down---\ndetermine what to track 139 ■ Step 3: create formulas around\nmultiple data points to create metrics 140", "prev_page": {"page_num": 10, "segment_id": "00010"}, "next_page": {"page_num": 12, "segment_id": "00012"}, "extended_fields": {"has_code": false, "has_formulas": true, "has_diagram": false, "has_list": true, "has_citations": false, "key_terms": ["data", "using", "metrics", "create", "measuring", "code", "working", "analysis", "step", "adding"], "content_type": "tutorial", "domain": "devops|architecture", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["APM systems", "DevOps", "continuous delivery", "metrics", "code coverage", "static code analysis"], "frameworks": [], "methodologies": ["DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["Server health statistics", "Consumer usage", "Semantic logging analysis", "MTTR and lead time", "Code coverage"], "has_case_study": true, "case_study_company": null, "tools_mentioned": ["APM systems", "SCM", "build data", "static code analysis"], "topics": ["Performance monitoring", "Data collection and analysis", "DevOps practices", "Metrics creation", "Technical quality measurement"], "key_concepts": ["logging best practices", "social network interaction", "metrics creation process", "code 'ilities'", "continuous delivery"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00012", "page_num": 12, "segment": "CONTENTS\n\nxi\n\n8.5 Case study: finding anomalies in lead time 173\n\n8.6\n\nSummary 176\n\n9 Publishing metrics\n\n177\n\n9.1 The right data for the right audience 178\n\nWhat to use on your team 180 ■ What managers want to\nsee 184 ■ What executives care about 188 ■ Using metrics to\nprove a point or effect change 189\n\n9.2 Different publishing methods 191\n\nBuilding dashboards 192 ■ Using email 193\n\n9.3 Case study: driving visibility toward a strategic goal 194\n\n9.4\n\nSummary 199\n\n10 Measuring your team against the agile principles\n\n10.1 Breaking the agile principles into measurable\n\n201\n\ncomponents 202\n\nAligning the principles with the delivery lifecycle 204\n\n10.2 Three principles for effective software 205\n\nMeasuring effective software 206\n\n10.3\n\nFour principles for effective process 207\n\nMeasuring effective processes 208\n\n10.4\n\nFour principles for an effective team 210\n\nMeasuring an effective development team 210\n\n10.5 One principle for effective requirements 213\n\nMeasuring effective requirements 213\n\n10.6 Case study: a new agile team 215\n\n10.7\n\nSummary 219\n\nappendix A DIY analytics using ELK 221\nappendix B Collecting data from source systems with Grails 229\n\nindex 319\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "10 Measuring your team against the agile principles Four principles for an effective team 210 Measuring an effective development team 210", "l2_summary": "10 Measuring your team against the agile principles Measuring effective software 206 Measuring effective processes 208 Four principles for an effective team 210 Measuring an effective development team 210 Measuring effective requirements 213", "prev_page": {"page_num": 11, "segment_id": "00011"}, "next_page": {"page_num": 13, "segment_id": "00013"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": true, "has_citations": false, "key_terms": ["effective", "principles", "team", "measuring", "case", "study", "summary", "what", "using", "agile"], "content_type": "tutorial", "domain": "management|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["lead time", "publishing metrics"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["Case studies", "Publishing metrics", "Agile principles"], "key_concepts": ["Anomalies in lead time", "Different publishing methods", "Measuring agile principles"], "problem_statement": "Improving team performance and visibility through the use of metrics and case studies", "solution_approach": "Implementing specific methodologies, analyzing data, and creating dashboards", "extraction_method": "lm"}}
{"segment_id": "00013", "page_num": 13, "segment": "xii\n\nCONTENTS\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "Licensed to Mark Watson <nordickan@gmail.com>", "l2_summary": "Licensed to Mark Watson <nordickan@gmail.com>", "prev_page": {"page_num": 12, "segment_id": "00012"}, "next_page": {"page_num": 14, "segment_id": "00014"}, "flags": {"continuity_gap": true}}
{"segment_id": "00014", "page_num": 14, "segment": "foreword\n\nAlthough it is still fairly young, the software development industry has matured considerably in the last 15 years and gone through several major transformations:\n\n■\n\nJust a short while ago, it seems, the waterfall lifecycle was pretty much the only\noption for software development projects. Today, agile methodology is also frequently used.\n\n■ New development engineering practices have entered the game, such as SCM,\nissue tracking, build standardization, continuous integration, continuous\ninspection, and so on. In most organizations, it is now the norm to have a complete software factory.\n\n■ Although they started out minimalistic, modern IDEs have become a widely\n\nadopted productivity tool for developers.\n\nThis is all great news; and what’s more, there is strong traction to continue these\nefforts and make the software development world even better. It is amazing how many\ndevelopment teams are seeking a common Holy Grail: continuous delivery. In other\nwords, teams want a software development process that is predictable and repeatable,\nand that enables a shift to production at any time in a controlled manner.\n\n Despite all the good things that have happened within the industry in recent years,\nthere is a general consensus that we are not quite there yet. Software development is\nnot yet a fully mastered science, and delivery generally still is not reliable. Projects are\noften delivered late, with a reduced scope of features, generating frustration at all levels in organizations and justifying their reputation for being both expensive and\nunpredictable.\n\nxiii\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "Although it is still fairly young, the software development industry has matured considerably in the last 15 years and gone through several major transformations: In most organizations, it is now the norm to have a complete software factory. Software development is\nnot yet a fully mastered science...", "l2_summary": "Although it is still fairly young, the software development industry has matured considerably in the last 15 years and gone through several major transformations: Just a short while ago, it seems, the waterfall lifecycle was pretty much the only\noption for software development projects. In most organizations, it is now the norm to have a complete software factory. This is all great news; and what’s more, there is strong traction to continue these\nefforts and make the software development world even better. It is amazing how many\ndevelopment teams are seeking a common Holy Grail: continuous delivery. Software development is\nnot yet a fully mastered science, and delivery generally still is not reliable.", "prev_page": {"page_num": 13, "segment_id": "00013"}, "next_page": {"page_num": 15, "segment_id": "00015"}, "flags": {"continuity_gap": true}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["development", "software", "have", "continuous", "there", "although", "still", "industry", "years", "projects"], "content_type": "theory", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["SCM", "issue tracking", "build standardization", "continuous integration", "continuous inspection", "IDEs"], "frameworks": [], "methodologies": ["waterfall lifecycle", "agile methodology"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["SCM", "issue tracking", "IDEs"], "topics": ["software development lifecycle evolution", "agile methodology adoption", "development engineering practices", "continuous delivery"], "key_concepts": ["waterfall lifecycle", "agile methodology", "development engineering practices", "continuous integration", "continuous inspection", "IDEs", "common Holy Grail: continuous delivery"], "problem_statement": "Software development projects often delivered late, with reduced scope of features, generating frustration at all levels in organizations and justifying their reputation for being both expensive and unpredictable.", "solution_approach": "Adopting agile methodology, implementing new development engineering practices like SCM, issue tracking, build standardization, continuous integration, and continuous inspection, and striving towards a common goal of continuous delivery.", "extraction_method": "lm"}}
{"segment_id": "00015", "page_num": 15, "segment": "xiv\n\nFOREWORD\n\n One aspect that is missing from the recent improvements in our industry is measurement: measurement of what we produce, of course, but also measurement of the\nimpact of the changes we make to improve delivery. We should be able to answer questions such as, “Did this change improve the process?” and “Are we delivering better\nnow?” In many cases, these questions are not even asked, because doing so is not part\nof the company culture or because we know they are difficult to answer. If we, as an\nindustry, want to reach the next level of maturity, we need to both ask and answer\nthese questions. Many companies have realized this and have begun to move into the\nmeasurement area.\n\n This is the journey that Chris will take you on in this book. It will be your steadfast\ncompanion on your expedition into measurement. Whether you are just starting out\nor are already an advanced “measurer,” Agile Metrics in Action will provide you with a\n360-degree guide: from theory to practice; from defining what you should be measuring, in which area and at which frequency, to who should be targeted with which indicators; and from how to gather the numbers and which tool to use to consolidate\nthem, to how to take action on them. The book focuses mostly on agile teams, but\nmuch of it can also be applied in other contexts. All this is done using existing tools,\nmost of them open source and widely used.\n\n But that’s not all! For each area of measurement, Chris presents a case study that\nmakes it concrete and applicable, based on his own experiences. Whatever your current maturity level with regard to measuring your development process, you will learn\nfrom this book. Enjoy!\n\nOLIVIER GAUDIN\nCEO AND COFOUNDER\nSONARSOURCE\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "Many companies have realized this and have begun to move into the\nmeasurement area. This is the journey that Chris will take you on in this book. It will be your steadfast\ncompanion on your expedition into measurement.", "l2_summary": "One aspect that is missing from the recent improvements in our industry is measurement: measurement of what we produce, of course, but also measurement of the\nimpact of the changes we make to improve delivery. If we, as an\nindustry, want to reach the next level of maturity, we need to both ask and answer\nthese questions. Many companies have realized this and have begun to move into the\nmeasurement area. This is the journey that Chris will take you on in this book. It will be your steadfast\ncompanion on your expedition into measurement. Whether you are just starting out\nor are already an advanced “measurer,” Agile Metrics in Action will provide you with a\n360-degree guide: from theory to practice; from defining what you should be measuring, in which area and at which frequency, to who should be targeted with which indicators; and from how to gather the numbers and which tool to use to...", "prev_page": {"page_num": 14, "segment_id": "00014"}, "next_page": {"page_num": 16, "segment_id": "00016"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["measurement", "will", "which", "should", "answer", "questions", "area", "book", "industry", "what"], "content_type": "tutorial", "domain": "programming|devops", "complexity": "intermediate", "companies": ["Sonarsource"], "people": ["Chris", "Olivier Gaudin"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["process improvement", "delivery quality"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["Agile Metrics", "Measurement in Agile Teams", "Process Improvement"], "key_concepts": ["measurement of process and delivery impact", "agile metrics implementation"], "problem_statement": "Lack of measurement in the industry", "solution_approach": "Implementing agile metrics for process improvement", "extraction_method": "lm"}}
{"segment_id": "00016", "page_num": 16, "segment": "preface\n\n At regular intervals, the team reflects on how to become more effective, then tunes and\nadjusts its behavior accordingly.\n\n---agilemanifesto.org/principles.html\n\nDevelopment teams adopt agile practices differently based on team members, time\ncommitments, the type of project being developed, and the software available, to\nname only a few factors. As quoted from the Agile Manifesto, teams should have regular check and adjust periods where they can reflect on how well they’re working and\nhow they can improve. This book demonstrates how to gather performance data to\nmeasure an agile team, interpret it, and react to it at check and adjust intervals so the\nteam can reach their full potential.\n\n After years of working on agile teams, I’ve noticed that many times teams check\nand adjust based on gut feelings or the latest blog post someone read. Many times\nteams don’t use real data to determine what direction to go in or to rate their team or\ntheir process. You don’t have to go far to find the data with development, tracking,\nand monitoring tools used today. Applications have very sophisticated performancemonitoring systems; tracking systems are used to manage tasks; and build systems are\nflexible, simple, and powerful. Combine all of this with modern deployment methodologies and teams shipping code to production multiple times a day in an automated\nfashion, and you have a wealth of data you can use to measure your team in order to\nadjust your process.\n\n I’ve used the techniques in this book over the years, and it has been a game\nchanger in how my teams think about their work. Retrospectives that start with\n\nxv\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "As quoted from the Agile Manifesto, teams should have regular check and adjust periods where they can reflect on how well they’re working and\nhow they can improve. This book demonstrates how to gather performance data to\nmeasure an agile team, interpret it, and react to it at check and adjust...", "l2_summary": "At regular intervals, the team reflects on how to become more effective, then tunes and\nadjusts its behavior accordingly. As quoted from the Agile Manifesto, teams should have regular check and adjust periods where they can reflect on how well they’re working and\nhow they can improve. This book demonstrates how to gather performance data to\nmeasure an agile team, interpret it, and react to it at check and adjust intervals so the\nteam can reach their full potential. After years of working on agile teams, I’ve noticed that many times teams check\nand adjust based on gut feelings or the latest blog post someone read. Many times\nteams don’t use real data to determine what direction to go in or to rate their team or\ntheir process. You don’t have to go far to find the data with development, tracking,\nand monitoring tools used today.", "prev_page": {"page_num": 15, "segment_id": "00015"}, "next_page": {"page_num": 17, "segment_id": "00017"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["teams", "team", "agile", "have", "adjust", "data", "check", "times", "used", "systems"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["performance monitoring systems", "tracking systems", "build systems", "deployment methodologies"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["performance data", "team's full potential"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["development, tracking, and monitoring tools", "sophisticated performance-monitoring systems", "tracking systems", "build systems"], "topics": ["Agile practices", "data-driven retrospectives", "performance measurement in agile teams"], "key_concepts": ["regular check and adjust intervals", "data-driven decision making", "retrospectives for improvement"], "problem_statement": "Teams often rely on gut feelings or blog posts instead of real data to measure their performance and make adjustments.", "solution_approach": "Using modern tools and techniques to gather, interpret, and react to performance data during retrospectives.", "extraction_method": "lm"}}
{"segment_id": "00017", "page_num": 17, "segment": "xvi\n\nPREFACE\n\nconversations around data end up being much more productive and bring to light\nreal issues to work on instead of going off of guesswork or opinion. Being able to set\nmetrics with a team and using them in Scrums, retrospectives, or anywhere else\nthroughout the development process helps the team focus on issues and filter out\nnoise or celebrate parts of the process that are working well.\n\n Finally, having this data at their fingertips typically makes managers and leadership\nteams happy because it gives them real insight into how the teams they’re sponsoring\nand responsible for are really performing. They can see how their initiatives affect\ntheir teams and ultimately the bottom line.\n\n I started using these techniques as a developer who wanted to report to leadership\nthe true picture of the performance of my team. As I transitioned into management, I\nstarted to look at this data from another angle and encouraged my team to do the\nsame, adding data they thought was important that reflected their day-to-day work. As\nI transitioned into a more senior management position, I’ve been able to look at this\ndata from yet another perspective to see how strategies, initiatives, and investments\naffect cross-team efforts, how to bring operating efficiencies from one team to\nanother, and how to track success on a larger scale. No matter what your role is on an\nagile development team, I’m sure you’ll be able to apply these techniques with success\nin your organization.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "They can see how their initiatives affect\ntheir teams and ultimately the bottom line. As I transitioned into management, I\nstarted to look at this data from another angle and encouraged my team to do the\nsame, adding data they thought was important that reflected their day-to-day work. As\nI...", "l2_summary": "conversations around data end up being much more productive and bring to light\nreal issues to work on instead of going off of guesswork or opinion. Being able to set\nmetrics with a team and using them in Scrums, retrospectives, or anywhere else\nthroughout the development process helps the team focus on issues and filter out\nnoise or celebrate parts of the process that are working well. They can see how their initiatives affect\ntheir teams and ultimately the bottom line. I started using these techniques as a developer who wanted to report to leadership\nthe true picture of the performance of my team. As I transitioned into management, I\nstarted to look at this data from another angle and encouraged my team to do the\nsame, adding data they thought was important that reflected their day-to-day work. As\nI transitioned into a more senior management position, I’ve been able to look at...", "prev_page": {"page_num": 16, "segment_id": "00016"}, "next_page": {"page_num": 18, "segment_id": "00018"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["team", "data", "able", "teams", "another", "more", "bring", "real", "issues", "work"], "content_type": "theory", "domain": "management|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "Scrum"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["team performance", "initiatives impact"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["data-driven decision making", "metric setting in agile teams", "management perspectives on data"], "key_concepts": ["productive conversations around data", "setting metrics with a team", "managers' insights into team performance"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00018", "page_num": 18, "segment": "acknowledgments\n\nAnyone who writes a book will tell you it’s a lot of work, and they’re right. It’s been a\njourney just to get to a point where I could write a book on anything, and writing itself\nhas been an extremely rewarding experience. You wouldn’t be reading this if it\nweren’t for the love and support of many people throughout my life who have encouraged me to try new things, picked me up when I’ve stumbled, and given me the confidence to keep innovating.\n\n To start, there have been my teachers through the years who noticed my love of\nwriting and encouraged me to keep at it: my fifth-grade teacher, Mr. Rosati, who first\nnoticed my love of writing; my seventh-grade English teacher and tennis coach, Mr.\nNolan, who gave me the opportunity to continue working on my creative skills; and\nmy tenth-grade English teacher, Ms. Kirchner, who encouraged me to publish my\nwork. My college professors Sheila Silver, Christa Erickson, Perry Goldstein, and\nDaniel Weymouth all encouraged my creativity and put me on a path that combined\nmy technical and creative abilities.\n\n A special thank you goes out to my parents, Ward and Irene Davis, who have always\nstood by me and encouraged me to be myself. They gave me the freedom to grow and\nencouraged my efforts no matter how crazy they have been.\n\n I’m grateful also to my lovely wife, Heather, who tolerated my long nights and\n\nweekends of typing and gave me the encouragement to keep going.\n\n Thanks also to Grandma Davis, who taught me about the long line of inventors\n\nand writers in our family tree, which has always been a source of inspiration.\n\n Thanks to all of the great people at Manning Publications who have helped along\nthe way: Dan Maharry for being a great editor and Michael Stephens, Candace\n\nxvii\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "To start, there have been my teachers through the years who noticed my love of\nwriting and encouraged me to keep at it: my fifth-grade teacher, Mr. Rosati, who first\nnoticed my love of writing; my seventh-grade English teacher and tennis coach, Mr. Kirchner, who encouraged me to publish my\nwork.", "l2_summary": "Anyone who writes a book will tell you it’s a lot of work, and they’re right. To start, there have been my teachers through the years who noticed my love of\nwriting and encouraged me to keep at it: my fifth-grade teacher, Mr. Rosati, who first\nnoticed my love of writing; my seventh-grade English teacher and tennis coach, Mr. Nolan, who gave me the opportunity to continue working on my creative skills; and\nmy tenth-grade English teacher, Ms. Kirchner, who encouraged me to publish my\nwork. A special thank you goes out to my parents, Ward and Irene Davis, who have always\nstood by me and encouraged me to be myself.", "prev_page": {"page_num": 17, "segment_id": "00017"}, "next_page": {"page_num": 19, "segment_id": "00019"}, "flags": {"continuity_gap": true}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["encouraged", "have", "writing", "love", "keep", "grade", "teacher", "gave", "book", "work"], "content_type": "reference", "domain": "programming", "complexity": "beginner", "companies": ["Manning Publications"], "people": ["Mr. Rosati", "Mr. Nolan", "Ms. Kirchner", "Sheila Silver", "Christa Erickson", "Perry Goldstein", "Daniel Weymouth", "Ward Davis", "Irene Davis", "Heather", "Grandma Davis"], "products": [], "technologies": [], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": false, "has_antipatterns": false, "has_instructions": false, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Manning Publications"], "topics": ["Acknowledgments", "Inspirations and Support"], "key_concepts": ["love and support", "innovation", "writing journey"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00019", "page_num": 19, "segment": "xviii\n\nACKNOWLEDGMENTS\n\nGillhoolley, and Marjan Bace for their suggestions and direction during the writing of\nthis book. Thanks also to the production team and everyone else at Manning who\nworked behind the scenes.\n\n I’d like to express my gratitude to the MEAP readers and the reviewers who took\ntime to read my manuscript at various stages during its development and who provided invaluable feedback, especially Boyd Meier, Chris Heneghan, Enzo Matera, Ezra\nSimeloff, Francesco Bianchi, Hamideh Iraj, James Matlock, John Tyler, Ken Fricklas,\nLuca Campobasso, Marcelo Lopez, Max Hemingway, Noreen Dertinger, Steven Parr,\nand Sune Lomholt.\n\n Special thanks to my technical proofreader, David Pombal, who checked the code\nand read the chapters one last time shortly before the book went into production, and\nto Olivier Gaudin for contributing the foreword to my book.\n\n I’d also like to thank everyone who has driven me crazy by not measuring things\nover the years; they ultimately pushed me into exploring and mastering this topic\nthroughout my career. Conversely, I’d like to thank everyone who has found value in\nthese techniques or has worked on my teams that have used them, because they have\nhelped me hone them into useful and practical ways of creating great agile teams.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "Gillhoolley, and Marjan Bace for their suggestions and direction during the writing of\nthis book. Thanks also to the production team and everyone else at Manning who\nworked behind the scenes. I’d also like to thank everyone who has driven me crazy by not measuring things\nover the years; they...", "l2_summary": "Gillhoolley, and Marjan Bace for their suggestions and direction during the writing of\nthis book. Thanks also to the production team and everyone else at Manning who\nworked behind the scenes. I’d like to express my gratitude to the MEAP readers and the reviewers who took\ntime to read my manuscript at various stages during its development and who provided invaluable feedback, especially Boyd Meier, Chris Heneghan, Enzo Matera, Ezra\nSimeloff, Francesco Bianchi, Hamideh Iraj, James Matlock, John Tyler, Ken Fricklas,\nLuca Campobasso, Marcelo Lopez, Max Hemingway, Noreen Dertinger, Steven Parr,\nand Sune Lomholt. Special thanks to my technical proofreader, David Pombal, who checked the code\nand read the chapters one last time shortly before the book went into production, and\nto Olivier Gaudin for contributing the foreword to my book. I’d also like to thank everyone who has driven me crazy by...", "prev_page": {"page_num": 18, "segment_id": "00018"}, "next_page": {"page_num": 20, "segment_id": "00020"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["book", "everyone", "like", "thanks", "also", "production", "worked", "time", "read", "thank"], "content_type": "tutorial", "domain": "programming|management", "complexity": "intermediate", "companies": ["Manning"], "people": ["Gillhoolley", "Marjan Bace", "Boyd Meier", "Chris Heneghan", "Enzo Matera", "Ezra Simeloff", "Francesco Bianchi", "Hamideh Iraj", "James Matlock", "John Tyler", "Ken Fricklas", "Luca Campobasso", "Marcelo Lopez", "Max Hemingway", "Noreen Dertinger", "Steven Parr", "Sune Lomholt", "David Pombal", "Olivier Gaudin"], "products": [], "technologies": ["agile teams"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["MEAP readers"], "topics": ["book acknowledgments", "technical proofreading", "foreword contribution"], "key_concepts": ["agile teams", "feedback from reviewers"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00020", "page_num": 20, "segment": "about this book\n\nIn this book I hope to show you how to use the data you're already generating to make\nyour teams, processes, and products better. The goal of the book is to teach your agile\nteam which metrics it can use to objectively measure performance. You'll learn what\ndata really counts, along with where to find it, how to get it, and how to analyze it.\nBecause meaningful data may be gathered or used by any part of an agile team, you'll\nlearn how all team members can publish their own metrics through dashboards and\nradiators, taking charge of communicating performance and individual accountability. Along the way, I hope you'll pick up practical data analysis techniques, including a\nfew emerging Big Data practices.\n\nRoadmap\n\nThis book is broken into three parts: “Measuring agile performance,” “Collecting and\nanalyzing your team’s data,” and “Applying metrics to your teams, processes, and software.”\n\n The first part introduces the concepts of data-driven agile teams: how to measure your\nprocesses and how to apply it to your team. Chapter 2 is an extended case study that takes\nthe concepts from the first chapter and shows them in action on a fictional team.\n\n The second part of this book is made up of four chapters, each focusing on a specific type of data, how to use it on your team, and what that data tells you by itself. We\nstart off with project tracking system (PTS) data in chapter 3, move on to source control management (SCM) data in chapter 4, explore data from continuous integration\n(CI) and deployment systems in chapter 5, and finally in chapter 6 look at data you\n\nxix\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "In this book I hope to show you how to use the data you're already generating to make\nyour teams, processes, and products better. You'll learn what\ndata really counts, along with where to find it, how to get it, and how to analyze it. The first part introduces the concepts of data-driven agile...", "l2_summary": "In this book I hope to show you how to use the data you're already generating to make\nyour teams, processes, and products better. The goal of the book is to teach your agile\nteam which metrics it can use to objectively measure performance. You'll learn what\ndata really counts, along with where to find it, how to get it, and how to analyze it. This book is broken into three parts: “Measuring agile performance,” “Collecting and\nanalyzing your team’s data,” and “Applying metrics to your teams, processes, and software.” The first part introduces the concepts of data-driven agile teams: how to measure your\nprocesses and how to apply it to your team. We\nstart off with project tracking system (PTS) data in chapter 3, move on to source control management (SCM) data in chapter 4, explore data from continuous integration\n(CI) and deployment systems in chapter 5, and finally in chapter 6 look at...", "prev_page": {"page_num": 19, "segment_id": "00019"}, "next_page": {"page_num": 21, "segment_id": "00021"}, "flags": {"continuity_gap": true}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["data", "team", "chapter", "book", "agile", "teams", "processes", "metrics", "performance", "part"], "content_type": "tutorial", "domain": "data_science|devops|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Big Data", "agile team", "dashboards", "radiators", "project tracking system (PTS)", "source control management (SCM)", "continuous integration (CI) and deployment systems"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["performance metrics", "team performance metrics"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["measuring agile performance", "collecting and analyzing team data", "applying metrics to teams, processes, and software"], "key_concepts": ["data-driven agile teams", "metrics for objective measurement", "dashboards and radiators", "Big Data practices"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00021", "page_num": 21, "segment": "xx\n\nABOUT THIS BOOK\n\ncan get from application performance monitoring (APM) tools. Each chapter in this\nsection ends in a case study that shows you how the data and metrics from the chapter\ncan be applied to your team from the team’s point of view.\n\n The third part of this book shows you what you can do with the data you’ve learned\nabout in the first two parts. Chapter 7 shows you how to combine different types of\ndata to create complex metrics. Chapter 8 shows you how to measure good software\nand uses a variety of data and techniques to monitor your code throughout its lifecycle. Chapter 9 shows you how to report on your metrics, diving into dashboards and\nreports and how to use them across your organization. The final chapter in this book\nshows you how to measure your team against the agile principles to see how agile your\nteam really is.\n\n Throughout the book I use primarily open source tools to demonstrate these practices. The appendixes walk you through the code for a data-collection system called\nmeasurementor based on Elasticsearch, Kibana, Mongo, and Grails that I’ve used to\ncollect, aggregate, and display data from multiple systems.\n\nCode conventions and downloads\n\nAll the source code in the book, whether in code listings or snippets, is in a fixedwidth font like this, which sets it off from the surrounding text. In some listings,\nthe code is annotated to point out key concepts, and numbered bullets are sometimes\nused in the text to provide additional information about the code. The code is formatted so that it fits within the available page space in the book by adding line breaks and\nusing indentation carefully.\n\n The code for this book is available for download from the publisher’s website at/AgileMetricsinAction and is also posted on GitHub at github.com\n/cwhd/measurementor.\n\n Feel free to contribute to the project, fork it, or use the concepts to roll your own\nversion in your language of choice. I tried to make it as easy as possible to use by\nemploying open source tools for the bulk of the functionality. There’s a Puppet script\nthat will install everything you need and a Vagrant file so you can get up and running\nin a virtual machine pretty quickly.\n\n In appendix A, I detail the architecture of the system used throughout the book.\n\nAuthor Online\n\nPurchase of Agile Metrics in Action includes free access to a private web forum run by\nManning Publications, where you can make comments about the book, ask technical\nquestions, and receive help from the author and from the community. To access the\nforum and subscribe to it, go to/AgileMetricsinAction. This page\nprovides information on how to get on the forum once you’re registered, what kind of\nhelp is available, and the rules of conduct on the forum.\n\n Manning’s commitment to our readers is to provide a venue where a meaningful\ndialog between individual readers and between readers and the author can take place.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "Chapter 7 shows you how to combine different types of\ndata to create complex metrics. Chapter 9 shows you how to report on your metrics, diving into dashboards and\nreports and how to use them across your organization. The final chapter in this book\nshows you how to measure your team against the...", "l2_summary": "Each chapter in this\nsection ends in a case study that shows you how the data and metrics from the chapter\ncan be applied to your team from the team’s point of view. Chapter 7 shows you how to combine different types of\ndata to create complex metrics. Chapter 8 shows you how to measure good software\nand uses a variety of data and techniques to monitor your code throughout its lifecycle. Chapter 9 shows you how to report on your metrics, diving into dashboards and\nreports and how to use them across your organization. The final chapter in this book\nshows you how to measure your team against the agile principles to see how agile your\nteam really is. Code conventions and downloads", "prev_page": {"page_num": 20, "segment_id": "00020"}, "next_page": {"page_num": 22, "segment_id": "00022"}, "extended_fields": {"has_code": true, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["book", "code", "chapter", "shows", "data", "metrics", "team", "forum", "tools", "throughout"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": ["Manning Publications"], "people": ["cwhd"], "products": [], "technologies": ["Elasticsearch", "Kibana", "Mongo", "Grails", "Puppet", "Vagrant"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": ["Groovy"], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["application performance metrics", "complex metrics", "software quality metrics"], "has_case_study": true, "case_study_company": null, "tools_mentioned": ["measurementor", "Elasticsearch", "Kibana", "Mongo", "Grails", "Puppet", "Vagrant"], "topics": ["application performance monitoring", "data collection and analysis", "metric creation and reporting", "agile metrics"], "key_concepts": ["APM tools", "case studies", "complex metrics", "software quality measurement", "dashboard reporting", "Agile principles"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00022", "page_num": 22, "segment": "ABOUT THIS BOOK\n\nxxi\n\nIt’s not a commitment to any specific amount of participation on the part of the\nauthor, whose contribution to the forum remains voluntary (and unpaid). We suggest\nyou try asking the author some challenging questions lest his interest stray!\n\n The Author Online forum and the archives of previous discussions will be accessible from the publisher’s website as long as the book is in print.\n\nAbout the author\n\nChristopher W. H. Davis has been leading and working on development teams since\nthe latter part of the twentieth century. Working in the travel, finance, healthcare,\ntelecommunications, and manufacturing industries, he’s led diverse teams in several\ndifferent environments around the world.\n\n An avid runner, Chris enjoys the beautiful and majestic Pacific Northwest in Portland, Oregon, with his wife and two children.\n\nAbout the cover illustration\n\nThe figure on the cover of Agile Metrics in Action is captioned “Montagnard du Nord\nde l’Ecosse,” which means an inhabitant of the mountainous regions in the north of\nScotland. The mountaineer is shown wearing a long blue robe and a red hat, and is\nholding an older version of a traditional Scottish bagpipe.\n\n The illustration is taken from a nineteenth-century edition of Sylvain Maréchal’s\nfour-volume compendium of regional dress customs published in France. Each illustration is finely drawn and colored by hand. The rich variety of Maréchal’s collection\nreminds us vividly of how culturally apart the world’s towns and regions were just 200\nyears ago. Isolated from each other, people spoke different dialects and languages. In\nthe streets or in the countryside, it was easy to identify where they lived and what their\ntrade or station in life was just by their dress.\n\n Dress codes have changed since then and the diversity by region and country, so\nrich at the time, has faded away. It is now hard to tell apart the inhabitants of different\ncontinents, let alone different towns, regions, or nations. Perhaps we have traded cultural diversity for a more varied personal life---certainly for a more varied and fastpaced technological life.\n\n At a time when it is hard to tell one computer book from another, Manning celebrates the inventiveness and initiative of the computer business with book covers\nbased on the rich diversity of regional life of two centuries ago, brought back to life by\nMaréchal’s pictures.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "About the cover illustration The rich variety of Maréchal’s collection\nreminds us vividly of how culturally apart the world’s towns and regions were just 200\nyears ago. It is now hard to tell apart the inhabitants of different\ncontinents, let alone different towns, regions, or nations.", "l2_summary": "The Author Online forum and the archives of previous discussions will be accessible from the publisher’s website as long as the book is in print. Davis has been leading and working on development teams since\nthe latter part of the twentieth century. About the cover illustration The rich variety of Maréchal’s collection\nreminds us vividly of how culturally apart the world’s towns and regions were just 200\nyears ago. It is now hard to tell apart the inhabitants of different\ncontinents, let alone different towns, regions, or nations. Perhaps we have traded cultural diversity for a more varied personal life---certainly for a more varied and fastpaced technological life.", "prev_page": {"page_num": 21, "segment_id": "00021"}, "next_page": {"page_num": 23, "segment_id": "00023"}, "flags": {"continuity_gap": true}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["life", "book", "author", "different", "illustration", "regions", "chal", "dress", "rich", "diversity"], "content_type": "reference", "domain": "programming", "complexity": "intermediate", "companies": [], "people": ["Christopher W. H. Davis"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": false, "has_antipatterns": false, "has_instructions": false, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Author Online forum"], "topics": ["Agile Metrics", "Author Bio", "Cover Illustration"], "key_concepts": ["voluntary author contribution", "diverse development teams", "cultural diversity in dress codes"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00023", "page_num": 23, "segment": "xxii\n\nABOUT THIS BOOK\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "Licensed to Mark Watson <nordickan@gmail.com>", "l2_summary": "Licensed to Mark Watson <nordickan@gmail.com>", "prev_page": {"page_num": 22, "segment_id": "00022"}, "next_page": {"page_num": 24, "segment_id": "00024"}, "flags": {"continuity_gap": true}}
{"segment_id": "00024", "page_num": 24, "segment": "Part 1\n\nMeasuring agile teams\n\nA gile development has guidelines instead of hard-and-fast rules. Many\n\nteams that practice agile struggle with measuring their processes and their\nteams, despite having all the data they need to do the measurement.\n\n Chapter 1 navigates through the challenges of agile measurement. You’ll\nlearn where you can get data to measure your team, how to break down problems into measureable units, and how to incorporate better agile measurement\non your team.\n\n Chapter 2 puts what you learned in chapter 1 into action through a case\nstudy where a team uses several open source technologies to incorporate better\nagile measurement. They identify key metrics, use the tools to collect and analyze data, and check and adjust based on what they find.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "Measuring agile teams teams that practice agile struggle with measuring their processes and their\nteams, despite having all the data they need to do the measurement. Chapter 1 navigates through the challenges of agile measurement.", "l2_summary": "Measuring agile teams A gile development has guidelines instead of hard-and-fast rules. teams that practice agile struggle with measuring their processes and their\nteams, despite having all the data they need to do the measurement. Chapter 1 navigates through the challenges of agile measurement. You’ll\nlearn where you can get data to measure your team, how to break down problems into measureable units, and how to incorporate better agile measurement\non your team. Chapter 2 puts what you learned in chapter 1 into action through a case\nstudy where a team uses several open source technologies to incorporate better\nagile measurement.", "prev_page": {"page_num": 23, "segment_id": "00023"}, "next_page": {"page_num": 25, "segment_id": "00025"}, "flags": {"continuity_gap": true}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["agile", "measurement", "teams", "data", "chapter", "team", "measuring", "where", "incorporate", "better"], "content_type": "tutorial", "domain": "programming|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["data to measure team performance", "key metrics"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["measuring agile teams", "agile measurement techniques", "case study on implementing agile measurement"], "key_concepts": ["agile development", "measurement challenges", "data collection and analysis"], "problem_statement": "Measuring processes and team performance in agile development", "solution_approach": "Navigating through the challenges of agile measurement, breaking down problems into measurable units, incorporating better agile measurement on teams", "extraction_method": "lm"}}
{"segment_id": "00025", "page_num": 25, "segment": "2\n\nCHAPTER\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "Licensed to Mark Watson <nordickan@gmail.com>", "l2_summary": "Licensed to Mark Watson <nordickan@gmail.com>", "prev_page": {"page_num": 24, "segment_id": "00024"}, "next_page": {"page_num": 26, "segment_id": "00026"}}
{"segment_id": "00026", "page_num": 26, "segment": "Measuring agile\nperformance\n\nThis chapter covers\n\n■ Struggling with agile performance\n\nmeasurement\n\n■ Finding objective data for measuring agile\n\nperformance\n\n■ Answering performance questions with data\n\nyou’re generating\n\n■ Adopting agile performance measurement\n\nThere isn’t a silver-bullet metric that will tell you if your agile teams are performing\nas well as they can. Performance improvement is made possible by incorporating\nwhat you learn about your team’s performance into how your team operates at regular intervals. Collecting and analyzing data in the form of metrics is an objective\nway to learn more about your team and a way to measure any adjustments you\ndecide to make to your team’s behavior.\n\n3\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "Measuring agile\nperformance ■ Struggling with agile performance ■ Adopting agile performance measurement", "l2_summary": "Measuring agile\nperformance ■ Struggling with agile performance ■ Finding objective data for measuring agile ■ Answering performance questions with data ■ Adopting agile performance measurement Performance improvement is made possible by incorporating\nwhat you learn about your team’s performance into how your team operates at regular intervals.", "prev_page": {"page_num": 25, "segment_id": "00025"}, "next_page": {"page_num": 27, "segment_id": "00027"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["performance", "agile", "team", "data", "measuring", "measurement", "objective", "learn"], "content_type": "tutorial", "domain": "management|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["agile performance metrics"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Struggling with agile performance measurement", "Finding objective data for measuring agile performance", "Answering performance questions with data you're generating", "Adopting agile performance measurement"], "key_concepts": ["agile performance metrics", "performance improvement", "data-driven decision making"], "problem_statement": "Struggling with agile performance measurement", "solution_approach": "Incorporating learned team performance into regular operations and using data to make adjustments", "extraction_method": "lm"}}
{"segment_id": "00027", "page_num": 27, "segment": "4\n\n1.1\n\nCHAPTER 1 Measuring agile performance\n\nCollect, measure, react, repeat---the feedback loop\nWorking with metrics in a feedback loop in parallel with your development cycle will\nallow you to make smarter adjustments to your team and help improve communication across your organization. Here are the steps in the feedback loop:\n\n■ Collect---Gather all the data you can about your team and performance. Understand where you are before you change anything.\n\n■ Measure---Analyze your data.\n\n■ Look for trends and relationships between data points.\n■ Formulate questions about your team, workflow, or process.\n■ Determine how to adjust based on your analysis.\n■ React---Apply the adjustments based on your analysis.\n■ Repeat---Keep tabs on the data you’ve determined should be affected so you can\n\ncontinuously analyze and adjust your team.\n\nThe feedback loop depicted in figure 1.1 naturally fits into the operations of agile\nteams. As you’re developing, you’re generating and collecting data; when you pause\nto check and adjust, you’re doing your analysis; and when you start again, you’re\napplying lessons learned and generating more data.\n\nContinuous delivery and continuous improvement\n\nThe word continuous is everywhere in agile terminology: continuous integration, continuous delivery, continuous improvement, continuous testing, continuous (choose\nyour noun). No matter if you’re doing Scrum, Kanban, extreme programming (XP), or\nsome custom form of agile, keeping your stream of metrics as continuous as your\ncheck-and-adjust period is key.\n\n4 Repeat.\n\n3 React (apply):\n\nAdjust your team based\non your findings.\n\n1 Collect data:\n\nGet as much data\nas you can from your\napplication lifecycle.\n\n2 Measure (analyze):\nAsk questions, find\ntrends, make\nhypotheses.\n\nFigure 1.1 The feedback loop: collecting data from your process, asking questions, and\ntweaking your process\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "1", "chapter_title": "Measuring agile performance", "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "■ Measure---Analyze your data. Continuous delivery and continuous improvement The word continuous is everywhere in agile terminology: continuous integration, continuous delivery, continuous improvement, continuous testing, continuous (choose\nyour noun).", "l2_summary": "Here are the steps in the feedback loop: ■ Collect---Gather all the data you can about your team and performance. ■ Measure---Analyze your data. Continuous delivery and continuous improvement The word continuous is everywhere in agile terminology: continuous integration, continuous delivery, continuous improvement, continuous testing, continuous (choose\nyour noun). Figure 1.1 The feedback loop: collecting data from your process, asking questions, and\ntweaking your process", "prev_page": {"page_num": 26, "segment_id": "00026"}, "next_page": {"page_num": 28, "segment_id": "00028"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "continuous", "feedback", "loop", "team", "adjust", "agile", "collect", "measure", "react"], "content_type": "theory", "domain": "management|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "Scrum"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["team performance", "communication across organization"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["feedback loop", "continuous improvement", "agile performance measurement"], "key_concepts": ["collect, measure, react, repeat", "data collection and analysis", "team adjustment based on metrics"], "problem_statement": "Improving agile team performance through continuous feedback and data-driven adjustments", "solution_approach": "Implementing a feedback loop with steps of collecting, measuring, reacting, and repeating to continuously improve team performance and communication", "extraction_method": "lm"}}
{"segment_id": "00028", "page_num": 28, "segment": "Why agile teams struggle with measurement\n\n5\n\nTo begin you need to know where you stand. You’re probably already tracking something in your development process, like what was accomplished, how much code is\nchanging, and how your software is performing.\n\n Your questions will drive the analysis phase by providing a lens with which to view\nthis data. Through this lens you can identify data points and metrics that help answer\nyour questions. These data points then become the indicators of progress as you\nadjust your process to get to an ideal operating model for your team. Once you have\nquestions you want to answer, then you can start identifying data points and metrics\nthat represent them. At that point you can adjust how your team operates and track\nthe metrics you’ve identified.\n\n1.1.1 What are metrics?\n\n“A method of measuring something, or the results obtained from this.”\n\n---metrics defined by Google\n\nIn the scope of this book metrics will represent the data you can get from your application lifecycle as it applies to the performance of software development teams. A\nmetric can come from a single data source or it can be a combination of data from\nmultiple data sources. Any data point that you track eventually becomes a metric that\nyou can use to measure your team’s performance. Examples of common metrics are:\n\n■ Velocity---The relative performance of your team over time\n■ Changed lines of code (CLOC)---The number of lines of code that were changed\n\nover time\n\nMetrics can be used to measure anything you think is relevant, which can be a powerful tool when used to facilitate better communication and collaboration. These metrics in effect become key performance indicators (KPIs) that help measure what’s\nimportant to your team and your business.\n\n Using KPIs and data trends to show how certain data points affect behaviors and\nprogress, you can tweak the behavior of your team and watch how the changes you\nmake affect data that’s important to it.\n\n1.2 Why agile teams struggle with measurement\n\nAs you drive down the road, the gauges on your dashboard are the same as the gauges\nin the cars around you. There are highway signs that tell you how fast you should go\nand what you should do. Everyone on the road has gone through the same driving\ntests to get a driver’s license and knows the same basic stuff about driving.\n\n Agile development is nothing like this. The people involved in delivering a software product have different roles and different backgrounds. Their idea of what good\nmeans can vary substantially.\n\n■ A developer might think that good means a well-engineered piece of software.\n■ A product owner might define good as more features delivered.\n■ A project manager may think good means it was done on time and within budget.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "1", "chapter_title": "Measuring agile performance", "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "Through this lens you can identify data points and metrics that help answer\nyour questions. 1.1.1 What are metrics? A\nmetric can come from a single data source or it can be a combination of data from\nmultiple data sources.", "l2_summary": "Through this lens you can identify data points and metrics that help answer\nyour questions. Once you have\nquestions you want to answer, then you can start identifying data points and metrics\nthat represent them. At that point you can adjust how your team operates and track\nthe metrics you’ve identified. 1.1.1 What are metrics? A\nmetric can come from a single data source or it can be a combination of data from\nmultiple data sources. Examples of common metrics are:", "prev_page": {"page_num": 27, "segment_id": "00027"}, "next_page": {"page_num": 29, "segment_id": "00029"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["data", "metrics", "team", "what", "software", "points", "performance", "good", "agile", "teams"], "content_type": "theory", "domain": "programming|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["Velocity", "Changed lines of code (CLOC)"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Agile development", "Measurement in software development", "Key Performance Indicators (KPIs)"], "key_concepts": ["metrics", "data points", "agile teams", "velocity", "changed lines of code (CLOC)", "key performance indicators (KPIs)"], "problem_statement": "Why agile teams struggle with measurement", "solution_approach": "Identifying metrics and KPIs to measure team performance", "extraction_method": "lm"}}
{"segment_id": "00029", "page_num": 29, "segment": "6\n\nCHAPTER 1 Measuring agile performance\n\nEven though everyone is doing something different, they’re all headed down the\nsame road.\n\n So now picture a bunch of people driving down the same road in different vehicles\nwith totally different gauges. They all need to get to the same place, yet they’re all\nusing different information to get there. They can follow each other down the road,\nbut when they pull over to figure out how the trip is going, each has different ideas of\nwhat the gauges in their vehicle are telling them.\n\n Agile is a partnership between product owners and product creators. To make the\nmost out of that partnership you need to smooth out the communication differences\nby turning the data you’re already generating in your development process into\nagreed-upon metrics that tell you how your team is doing.\n\n Let’s look at some universal problems that end up getting in the way of a common\n\nunderstanding of agile metrics:\n\n■ Agile definitions of measurement are not straightforward.\n■ Agile deviates from textbook project management.\n■ Data is generated throughout the entire development process without a unified\n\nview.\n\nAll of these are common problems that deserve exploring.\n\n1.2.1\n\nProblem: agile definitions of measurement are not straightforward\n\nThere are a few commonly accepted tenets about measuring agile that tend to be\nrather confusing. Let’s start with common agile principles:\n\n■ Working software is the primary measure of progress. That statement is so ambiguous\nand open to interpretation that it makes it very hard for teams to pinpoint\nexactly how to measure progress. Essentially the point is you are performing\nwell if you’re delivering products to your consumers. The problem is the subjective nature of the term working software. Are you delivering something that works\nper the original requirements but has massive security holes that put your consumer’s data in jeopardy? Are you delivering something that is so non-performant that your consumers stop using it? If you answered yes to either question,\nthen you’re probably not progressing. There’s a lot more to measuring progress\nthan delivering working software.\n\n■ Any measurement you’re currently using has to be cheap. So what’s included in the cost\nassociated with gathering metrics? Are licenses to software included? Are you\nlooking at the hours spent by the people collecting measures? This statement\nbelittles the value of measuring performance. When you start measuring something, the better thing to keep in mind is if the value you get from the improvement associated with the metric outweighs the cost of collecting it. This open\nstatement is a good tenet, but like our first statement, it’s pretty ambiguous.\n■ Measure only what matters. This is a bad tenet. How do you know what matters?\nWhen do you start tracking new things and when do you stop tracking others?\nBecause these are hard questions, metrics end up getting thrown by the wayside\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "1", "chapter_title": "Measuring agile performance", "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "CHAPTER 1 Measuring agile performance understanding of agile metrics: ■ Agile definitions of measurement are not straightforward.", "l2_summary": "CHAPTER 1 Measuring agile performance They all need to get to the same place, yet they’re all\nusing different information to get there. understanding of agile metrics: ■ Agile definitions of measurement are not straightforward. Problem: agile definitions of measurement are not straightforward Let’s start with common agile principles:", "prev_page": {"page_num": 28, "segment_id": "00028"}, "next_page": {"page_num": 30, "segment_id": "00030"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["agile", "measuring", "different", "something", "when", "what", "metrics", "software", "statement", "delivering"], "content_type": "theory", "domain": "management|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["progress", "team performance"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Agile metrics", "measurement challenges", "communication in Agile teams"], "key_concepts": ["working software as progress measure", "cheap data collection", "measure only what matters"], "problem_statement": "agile definitions of measurement are not straightforward", "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00030", "page_num": 30, "segment": "Why agile teams struggle with measurement\n\n7\n\nwhen they could be providing value. A better wording would be “measure everything and figure out why metrics change unexpectedly.”\n\n1.2.2\n\nProblem: agile focuses on a product, not a project\n\nOne of the strengths of agile development methods is the idea that you are delivering\na living product, not completing a project. A project is a defined set of time within which\nsomething is developed and tracked; a product is a living thing that continues to\nchange to meet the needs of the consumer. This is shown in figure 1.2.\n\n A good example of a project would be building a bridge. It gets designed, built to\nspec, and then it stands likely unchanged for a long time. Before you start the project\nyou design exactly what you need so you can understand the total cost, and then you\ntrack the progress against the plan to make sure you stay on schedule and budget.\nThis process is the art of project management.\n\n A good example of a product is a mobile application geared toward runners that\nshows their paths on a map and aggregates their total mileage. It’s a tool; you can run\nwith it and get lots of data about your workouts. There are several competing apps\nthat have the same functionality but different bells and whistles. To keep up with the\ncompetition, any app that competes in that space must constantly evolve to stay the\nmost relevant product its consumers can use. This evolution tends to happen in small\niterations that result in frequent feature releases which are immediately consumed\nand tracked for feedback that helps shape the direction of the next feature.\n\n Frequently, software products are built through the lens of a project, which ends\nup mixing project management techniques typically used for large predictive projects\n\nProject mentality\n\nDesign\n\nBuild and\ntrack\n\nDone!\n\nOne iteration\nis all you need.\n\nProduct mentality\n\nDesign\n\nBuild and\ntrack\n\nIncorporate\nfeedback\n\nContinue to iterate through\nthe life of the project.\n\nDone...\n\nFigure 1.2 Project vs. product mentality\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "1", "chapter_title": "Measuring agile performance", "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "Problem: agile focuses on a product, not a project This process is the art of project management. Figure 1.2 Project vs.", "l2_summary": "Problem: agile focuses on a product, not a project One of the strengths of agile development methods is the idea that you are delivering\na living product, not completing a project. A good example of a project would be building a bridge. This process is the art of project management. Continue to iterate through\nthe life of the project. Figure 1.2 Project vs.", "prev_page": {"page_num": 29, "segment_id": "00029"}, "next_page": {"page_num": 31, "segment_id": "00031"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["project", "product", "agile", "figure", "which", "design", "track", "mentality", "would", "change"], "content_type": "theory", "domain": "management|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": ["mobile application geared toward runners"], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": false, "has_antipatterns": true, "has_instructions": false, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["agile development", "project vs product mentality", "measurement in agile"], "key_concepts": ["agile methodology", "product mindset", "project management"], "problem_statement": "Agile teams struggle with measurement when they should be focusing on providing value.", "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00031", "page_num": 31, "segment": "8\n\nCHAPTER 1 Measuring agile performance\n\nTasks that\nyou’re tracking\nfor this project\n\nDuration as\na number, usually\nin days\n\nEach task shown from\nstart date to end date\nagainst the calendar\n\nFigure 1.3 An example Gantt chart\n\nwith agile product management used for consistent iterative delivery. This ends up\nputting project constraints on agile projects that don’t fit. An example would be using\na Gantt chart on an agile project. Gantt charts are great for tracking long-running\nprojects but usually cause heartache when used to track something with a definition\nthat is regularly in flux. An example Gantt chart is shown in figure 1.3.\n\n From a project-management perspective it would be great to have a clear prediction of the cost of a complex feature that may take several sprints to complete, which\nis why these tools end up tracking agile teams.\n\n1.2.3\n\nProblem: data is all over the place without a unified view\n\nYour team is generating a lot of data throughout your entire software development\nlifecycle (SDLC). Figure 1.4 shows the components that are typically used by an agile\nsoftware delivery team.\n\n The first problem is there is no standard for any of these boxes. There are several\nproject-tracking tools, plus different source control and continuous integration (CI)\nsystems, and your deployment and application-monitoring tools will vary depending\non your technology stack and what you’re delivering to your consumers. Different systems end up generating different reports that don’t always come together. In addition, there’s no product or group of products that encompasses all of the pieces of the\nsoftware development lifecycle.\n\nManage\ntasks and\nbugs\n\nProject\ntracking\n\nManage\ncode and\ncollaboration\n\nGenerate\nbuilds and\nrun tests\n\nMove code\nacross\nenvironments\n\nSource\ncontrol\n\nContinuous\nintegration\n\nDeployment\ntools\n\nEnsure\neverything\nis working\n\nApplication\nmonitoring\n\nFigure 1.4 Groups of systems used to deliver software and what they do\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "1", "chapter_title": "Measuring agile performance", "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "Tasks that\nyou’re tracking\nfor this project Figure 1.3 An example Gantt chart An example would be using\na Gantt chart on an agile project.", "l2_summary": "CHAPTER 1 Measuring agile performance Tasks that\nyou’re tracking\nfor this project Figure 1.3 An example Gantt chart An example would be using\na Gantt chart on an agile project. An example Gantt chart is shown in figure 1.3. Figure 1.4 shows the components that are typically used by an agile\nsoftware delivery team.", "prev_page": {"page_num": 30, "segment_id": "00030"}, "next_page": {"page_num": 32, "segment_id": "00032"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["agile", "project", "tracking", "figure", "gantt", "used", "tools", "software", "example", "chart"], "content_type": "theory", "domain": "management|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Gantt chart", "project-tracking tools", "source control", "continuous integration (CI)", "deployment and application-monitoring tools"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": false, "has_antipatterns": true, "has_instructions": false, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Gantt chart", "project-tracking tools", "source control", "continuous integration (CI)", "deployment and application-monitoring tools"], "topics": ["Agile performance measurement", "data management in agile projects", "tooling for software development lifecycle"], "key_concepts": ["agile project constraints", "Gantt chart limitations", "unified view of data across SDLC"], "problem_statement": "Data is scattered and lacks a unified view", "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00032", "page_num": 32, "segment": "What questions can metrics answer, and where do I get the data to answer them?\n\n9\n\nAre you\nmeeting\ncommitments?\n\nHow much\ncode is getting\nbuilt?\n\nHow long does\nit take you to get\nthings right?\n\nHow fast can you\nget changes to your\nconsumers?\n\nHow well\nis your system\nperforming?\n\nProject\ntracking\n\nSource\ncontrol\n\nContinuous\nintegration\n\nDeployment\ntools\n\nApplication\nmonitoring\n\nWhat is your\ncurrent pace?\n\nHow well is the\nteam working\ntogether?\n\nHow are your\ncustomers using\nyour system?\n\nFigure 1.5 Questions you can answer with data from systems in your SDLC.\n\nThe second issue is that people in different roles on the team will focus on using different tools throughout the SDLC. Scrum masters are likely looking for data around\nyour tasks in your project system, and developers pay the most attention to data in\nyour source control and CI systems. Depending on the size and makeup of your team,\nyou may have a completely different group looking after your application monitoring\nand even perhaps your deployment tools. Executives likely don’t care about raw\nsource control data and developers may not care about the general ledger, but when\npresented in the right way, having a unified view of all data is important in understanding how well teams are performing. No matter what angle you’re looking from,\nhaving only a piece of the whole picture obviously limits your ability to react to trends\nin data in the best possible way.\n\n1.3 What questions can metrics answer, and where do I\n\nget the data to answer them?\nIn the previous section we noted that all the data you’re collecting in your SDLC is a\nbarrier to understanding what’s going on. On the flip side there’s a huge opportunity\nto use all that data to see really interesting and insightful things about how your team\nworks. Let’s take the components of figure 1.4 and look at questions they answer in\nfigure 1.5.\n\n When you combine these different data points, you can start answering some even\n\nmore interesting big-picture questions like the ones shown in figure 1.6.\n\nHow good are\nthe team’s\nrequirements?\n\nAre you\nmaking the\nsystem better?\n\nAre you\ndelivering the\nright things?\n\nProject\ntracking\n\n+\n\nSource\ncontrol\n\nApplication\nmonitoring\n\n+\n\nSource\ncontrol\n\nProject\ntracking\n\n+\n\nApplication\nmonitoring\n\nFigure 1.6 Adding data together to answer high-level questions\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 2, "cols": 2, "data": [["How good are Are you Are you\nthe team’s making the delivering the\nrequirements? system better? right things?\nProject Source Application Source Project Application\n+ + +\ntracking control monitoring control tracking monitoring", ""], ["Project Source\n+\ntracking control", "Project Application\n+\ntracking monitoring"]], "markdown": "| How good are Are you Are you\nthe team’s making the delivering the\nrequirements? system better? right things?\nProject Source Application Source Project Application\n+ + +\ntracking control monitoring control tracking monitoring |  |\n|---|---|\n| Project Source\n+\ntracking control | Project Application\n+\ntracking monitoring |"}], "table_count": 1, "chapter_num": "1", "chapter_title": "Measuring agile performance", "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "What questions can metrics answer, and where do I get the data to answer them? Figure 1.5 Questions you can answer with data from systems in your SDLC. get the data to answer them?", "l2_summary": "What questions can metrics answer, and where do I get the data to answer them? How well\nis your system\nperforming? Figure 1.5 Questions you can answer with data from systems in your SDLC. 1.3 What questions can metrics answer, and where do I get the data to answer them? Figure 1.6 Adding data together to answer high-level questions", "prev_page": {"page_num": 31, "segment_id": "00031"}, "next_page": {"page_num": 33, "segment_id": "00033"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "answer", "questions", "what", "source", "control", "team", "figure", "system", "project"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["SDLC", "source control", "continuous integration", "deployment tools", "application monitoring"], "frameworks": [], "methodologies": ["Scrum"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["meeting commitments", "code built", "time to get things right", "speed of changes", "system performance", "current pace", "team collaboration", "customer usage"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["project tracking", "source control", "continuous integration", "deployment tools", "application monitoring"], "topics": ["metrics and data collection in SDLC", "team performance metrics", "tool usage across roles"], "key_concepts": ["SDLC components", "data-driven decision making", "unified view of data"], "problem_statement": "How to effectively use metrics and data from different tools in the software development lifecycle to improve team performance and understand project status.", "solution_approach": "Combining data from various SDLC tools (project tracking, source control, continuous integration, deployment tools, application monitoring) to answer high-level questions about team requirements, system improvement, and delivery accuracy.", "extraction_method": "lm"}}
{"segment_id": "00033", "page_num": 33, "segment": "10\n\nCHAPTER 1 Measuring agile performance\n\nSource\ncontrol\n\nContinuous\nintegration\n\nDeployment\ntools\n\nYour team is already generating\na ton of data through CI, your\ntask management system, and\nproduction-monitoring\nyour\ntools. These systems are likely\nto have simple APIs that you\ncan use to get real data that’s\nbetter for communication and\nfor determining KPIs and that\nyou can incorporate into your\nprocess to help your team\nachieve better performance. As\nyou know, the first step in the\nfeedback loop is to start collecting data. By putting it into a central database, you’re\nadding the systems from figure 1.4 to the feedback loop in figure 1.1 to get figure 1.7.\n\nFigure 1.7 Collecting data from numerous places\n\nCentral place\nto collect data\n\nApplication\nmonitoring\n\nProject\ntracking\n\n There are several ways to do this:\n\n■ Semantic logging and log aggregators\n■ Products like Datadog () or New Relic Insights (newrelic\n\n.com/insights)\n\n■ An open source database like Graphite (graphite.wikidot.com/) to collect and\n\ndisplay data\n\n■ A DIY system to collect and analyze metrics\n\nIf you’re interested in building something yourself, check out appendix A, where we\ntalk through that topic. If you have any of the other tools mentioned here, then by all\nmeans try to use them.\n\n For now let’s look at where you can get this data and what systems you should put\n\nin place if you don’t already have them.\n\n1.3.1\n\nProject tracking\n\nTools like JIRA Agile, Axosoft OnTime, LeanKit, TFS, Telerik TeamPulse, Planbox, and\nFogBugz can all track your agile team, and all have APIs you can tap into to combine\nthat data with other sources. All of these have the basics to help you track the common\nagile metrics, and although none goes much deeper, you can easily combine their data\nwith other sources through the APIs. From these systems you can get data such as how\nmany story points your team is completing, how many tasks you’re accomplishing, and\nhow many bugs are getting generated. A typical Scrum board is depicted in figure 1.8.\n\n Here are questions you can answer with project-tracking data alone:\n\n■ How well does your team understand the project?\n■ How fast is the team moving?\n■ How consistent is the team in completing work?\n\nProject-tracking data is explored in depth in chapter 3.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "1", "chapter_title": "Measuring agile performance", "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "Figure 1.7 Collecting data from numerous places Central place\nto collect data Here are questions you can answer with project-tracking data alone:", "l2_summary": "CHAPTER 1 Measuring agile performance By putting it into a central database, you’re\nadding the systems from figure 1.4 to the feedback loop in figure 1.1 to get figure 1.7. Figure 1.7 Collecting data from numerous places Central place\nto collect data Here are questions you can answer with project-tracking data alone: Project-tracking data is explored in depth in chapter 3.", "prev_page": {"page_num": 32, "segment_id": "00032"}, "next_page": {"page_num": 34, "segment_id": "00034"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "team", "have", "figure", "project", "agile", "tools", "systems", "tracking", "apis"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": ["Datadog", "New Relic", "Graphite", "JIRA Agile", "Axosoft OnTime", "LeanKit", "TFS", "Telerik TeamPulse", "Planbox", "FogBugz"], "people": [], "products": [], "technologies": ["APIs", "semantic logging", "log aggregators", "open source database", "DIY system"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["KPIs", "story points", "tasks", "bugs"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["CI", "task management system", "production-monitoring tools", "Datadog", "New Relic Insights", "Graphite", "JIRA Agile", "Axosoft OnTime", "LeanKit", "TFS", "Telerik TeamPulse", "Planbox", "FogBugz"], "topics": ["Measuring agile performance", "Collecting data", "Project tracking"], "key_concepts": ["feedback loop", "central database", "APIs", "data collection"], "problem_statement": "How to measure and improve agile team performance through data collection and analysis.", "solution_approach": "Implementing a central place for collecting data from various sources, using APIs and tools like CI systems, task management systems, and production-monitoring tools.", "extraction_method": "lm"}}
{"segment_id": "00034", "page_num": 34, "segment": "What questions can metrics answer, and where do I get the data to answer them?\n\n11\n\nFigure 1.8 A typical Scrum board to track tasks in a sprint\n\n1.3.2\n\nSource control\n\nSource control is where the actual work is done and collaboration across development\nteams happens. From here you can see which files are changing and by how much.\nSome source control systems allow you to get code reviews and comments, but in\nother cases you need additional systems to get that type of information. Tools like\nStash, Bitbucket, and GitHub have rich REST-based APIs that can get you a wealth of\ninformation about your codebase. If you’re still using SVN or something even older,\nthen you can still get data, just not as conveniently as Git- or Mercurial-based systems.\nIn that case you may need something like FishEye and Crucible to get more data\naround code reviews and comments about your code.\n\n Here are two questions you can answer from source control alone:\n\n■ How much change is happening in your codebase?\n■ How well is/are your development team(s) working together?\n\nWe dive deep into source control in chapter 4.\n\n1.3.3\n\nThe build system\n\nAfter someone checks something into source control, it typically goes into your build\nsystem, which is where the code from multiple check-ins is integrated, unit tests are\nrun, your code is packaged into something that can be deployed somewhere, and\nreports are generated. All of this is called continuous integration (CI). From here you\ncan get lots of great information on your code: you can see how well your team’s\nchanges are coordinated, run your codebase against rule sets to ensure you’re not\nmaking silly mistakes, check your test coverage, and see automated test results.\n\n CI is an essential part of team software development, and there are several systems\nthat help you get going quickly, including TeamCity, Jenkins, Hudson, and Bamboo.\nSome teams have taken CI past the integration phase and have their system deploy\ntheir code while they’re at it. This is called continuous delivery (CD). Many of the\nsame systems can be employed to do CD, but there are products that specialize in it,\nlike ThoughtWorks, Go CD, Electric Cloud, and Nolio.\n\n Whether you’re doing CI or CD, the main thing you need to care about is the information that is generated when your code is getting built, inspected, and tested. The\nmore mature a CI/CD process your team has, the more data you can get out of it.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "1", "chapter_title": "Measuring agile performance", "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "What questions can metrics answer, and where do I get the data to answer them? Some source control systems allow you to get code reviews and comments, but in\nother cases you need additional systems to get that type of information. Here are two questions you can answer from source control alone:", "l2_summary": "What questions can metrics answer, and where do I get the data to answer them? From here you can see which files are changing and by how much. Some source control systems allow you to get code reviews and comments, but in\nother cases you need additional systems to get that type of information. In that case you may need something like FishEye and Crucible to get more data\naround code reviews and comments about your code. Here are two questions you can answer from source control alone: We dive deep into source control in chapter 4.", "prev_page": {"page_num": 33, "segment_id": "00033"}, "next_page": {"page_num": 35, "segment_id": "00035"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["code", "source", "control", "systems", "data", "information", "something", "team", "answer", "where"], "content_type": "tutorial", "domain": "devops|architecture", "complexity": "intermediate", "companies": ["Stash", "Bitbucket", "GitHub", "FishEye", "Crucible", "TeamCity", "Jenkins", "Hudson", "Bamboo", "ThoughtWorks", "Go CD", "Electric Cloud", "Nolio"], "people": [], "products": [], "technologies": ["source control systems", "Git", "Mercurial", "SVN", "REST-based APIs", "code reviews", "continuous integration", "continuous delivery"], "frameworks": [], "methodologies": ["Scrum"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["How much change is happening in your codebase", "How well is/are your development team(s) working together"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Stash", "Bitbucket", "GitHub", "FishEye", "Crucible", "TeamCity", "Jenkins", "Hudson", "Bamboo", "ThoughtWorks", "Go CD", "Electric Cloud", "Nolio"], "topics": ["source control", "build system", "continuous integration/continuous delivery"], "key_concepts": ["code reviews", "REST-based APIs", "codebase changes", "development team collaboration", "continuous integration", "continuous delivery"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00035", "page_num": 35, "segment": "12\n\nCHAPTER 1 Measuring agile performance\n\n Here are questions you can answer from CI alone:\n\n■ How fast are you delivering changes to your consumer?\n■ How fast can you deliver changes to your consumer?\n■ How consistently does your team do STET work?\n\nMore details on the data you can get from this step are found in chapter 5.\n\n1.3.4\n\nSystem monitoring\n\nOnce your code goes into production, you should have some type of system that looks\nat it to make sure it’s working and that tells you if something goes wrong, such as\nwhether a website becomes unresponsive or if your mobile app starts to crash every time\na user opens it. If you’re doing a really great job with your testing, you likely are paying\nattention to your system monitoring during your testing phase as well making sure you\ndon’t see any issues from a support perspective before your code goes into production.\n The problem with system monitoring is that it’s largely reactive, not proactive. Ideally all your data should be as close to the development cycle as possible, in which case\nyou’ll be able to react to it as a team quickly. By the time you’re looking at systemmonitoring data in a production environment your sprint is done, the code is out, and\nif there’s a problem, then you’re usually scrambling to fix it rather than learning and\nreacting to it with planned work.\n\n Let’s look at ways to mitigate this problem. One way is to use system monitoring as\nyou test your code before it gets to production. There are a number of ways your code\ncan make it to production. Typically you see something like the flow shown in figure\n1.9, where a team works in STET local development environment and pushes changes\nto an integration environment, and where multiple change sets are tested together\nand a QA environment verifies what the end user or customer will get before you ship\nyour code to production.\n\n Because teams typically have multiple environments in the path to production, to\nmake system monitoring data as proactive as possible you should be able to access it as\n\nTest\nindividual\nchanges\n\nTest\nmultiple\nchanges\n\nVerify what\ncustomer\ngets\n\nShip\nto the\ncustomer\n\nLocal development\n\nIntegration\n\nQA\n\nProduction\n\nFigure 1.9 Typical environment flow in the path to production\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "1", "chapter_title": "Measuring agile performance", "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "One way is to use system monitoring as\nyou test your code before it gets to production. There are a number of ways your code\ncan make it to production. Figure 1.9 Typical environment flow in the path to production", "l2_summary": "■ How fast can you deliver changes to your consumer? The problem with system monitoring is that it’s largely reactive, not proactive. One way is to use system monitoring as\nyou test your code before it gets to production. There are a number of ways your code\ncan make it to production. Test\nmultiple\nchanges Figure 1.9 Typical environment flow in the path to production", "prev_page": {"page_num": 34, "segment_id": "00034"}, "next_page": {"page_num": 36, "segment_id": "00036"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["production", "system", "code", "changes", "monitoring", "environment", "data", "team", "goes", "should"], "content_type": "theory", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["Delivery speed", "Consistency of STET work"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Measuring agile performance", "System monitoring", "Environment flow to production"], "key_concepts": ["CI (Continuous Integration)", "STET work", "System monitoring", "Proactive vs Reactive monitoring"], "problem_statement": "Mitigating the problem of reactive system monitoring in a production environment", "solution_approach": "Using system monitoring during testing phases and having multiple environments in the path to production", "extraction_method": "lm"}}
{"segment_id": "00036", "page_num": 36, "segment": "Analyzing what you have and what to do with the data\n\n13\n\nclose to the development environment as possible, ideally in the integration and/or\nQA stages.\n\n A second way to mitigate this problem is to release new code to only a small number of your consumers and monitor how their activity affects the system. This is usually\nreferred to as a canary deploy and is becoming more common in agile teams practicing\nCD.\n\n Depending on your deployment platform, different tools are available for system\nmonitoring. New Relic, AppDynamics, and Dynatrace are all popular tools in this\nspace. We will go into much more detail about these in chapter 6.\n\n All of the data we’ve looked at so far can tell you a lot about your team and how\neffectively you’re working together. But before you can make this data useful, you\nneed to figure out what is good and what is bad, especially in an agile world where\nyour core metrics are relative.\n\n Here are questions you can answer from your production-monitoring systems:\n\n■ How well is your code functioning?\n■ How well are you serving the consumer?\n\n1.4\n\nAnalyzing what you have and what to do with the data\nThe second step in the feedback loop is analysis, or figuring out what to do with all the\ndata you’ve collected. This is where you ask questions, look for trends, and associate\ndata points with behaviors in order to understand what is behind your team’s performance trends.\n\n Essentially what you need to do is get all the data you’ve collected and run some\nkind of computation on it, as shown in figure 1.10, to determine what the combined\ndata points can tell you. The best place to start is with the question you’re trying to\nanswer. When you’re doing this, be careful and ask yourself why you want to know\nwhat you’re asking. Does it really help you answer your question, solve a problem, or\ntrack something that you want to ensure you’re improving? When you’re trying to figure out what metrics to track, it’s easy to fall into a rabbit hole of “it would be great to\n\nDeployment\ntools\n\nApplication\nmonitoring\n\nContinuous\nintegration\n\nProject\ntracking\n\nSource\ncontrol\n\nFigure 1.10 X is what you want to answer; some combination of your data can get you there.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "1", "chapter_title": "Measuring agile performance", "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "Analyzing what you have and what to do with the data Analyzing what you have and what to do with the data\nThe second step in the feedback loop is analysis, or figuring out what to do with all the\ndata you’ve collected. Figure 1.10 X is what you want to answer; some combination of your data can get...", "l2_summary": "Analyzing what you have and what to do with the data But before you can make this data useful, you\nneed to figure out what is good and what is bad, especially in an agile world where\nyour core metrics are relative. Here are questions you can answer from your production-monitoring systems: Analyzing what you have and what to do with the data\nThe second step in the feedback loop is analysis, or figuring out what to do with all the\ndata you’ve collected. Essentially what you need to do is get all the data you’ve collected and run some\nkind of computation on it, as shown in figure 1.10, to determine what the combined\ndata points can tell you. Figure 1.10 X is what you want to answer; some combination of your data can get you there.", "prev_page": {"page_num": 35, "segment_id": "00035"}, "next_page": {"page_num": 37, "segment_id": "00037"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["what", "data", "figure", "answer", "tools", "monitoring", "want", "analyzing", "have", "integration"], "content_type": "tutorial", "domain": "devops|architecture", "complexity": "intermediate", "companies": ["New Relic", "AppDynamics", "Dynatrace"], "people": [], "products": [], "technologies": ["deployment platform", "canary deploy"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["code functioning", "consumer service"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["New Relic", "AppDynamics", "Dynatrace"], "topics": ["canary deploy", "system monitoring", "data analysis"], "key_concepts": ["deployment environment", "production-monitoring systems", "metrics for team performance"], "problem_statement": "Mitigating risks in software deployment and analyzing data to improve team performance", "solution_approach": "Using canary deploys, system monitoring tools, and defining metrics", "extraction_method": "lm"}}
{"segment_id": "00037", "page_num": 37, "segment": "14\n\nCHAPTER 1 Measuring agile performance\n\nhave.” Be wary of that. Stick with “just in time” instead of “just in case” when it comes\nto thinking about metrics. Then plug X into figure 1.10, where X is the question you\nwant to answer.\n\n1.4.1\n\nFiguring out what matters\n\nIf you’re swimming in a sea of data, it’s hard to figure out what data answers what\nquestion and which metrics you should track. One strategy I find useful when I’m trying to figure out which metrics matter is mind mapping.\n\n Mind mapping is a brainstorming technique where you start with an idea and then\nkeep deconstructing it until it’s broken down into small elements. If you’re not familiar with mind mapping, a great tool to start with is XMind (/), a powerful and free (for the basic version) tool that makes mind mapping pretty simple.\n\n If you take a simple example, “What is our ideal pace?” you can break that down\n\ninto smaller questions:\n\n■ What is our current velocity?\n■ Are we generating tech debt?\n■ What are other teams in the company doing?\n\nFrom there you could break those questions down into smaller ones, or you can start\nidentifying places where you can get that data. An example mind map is shown in figure 1.11.\n\n Thinking a project through, mapping, and defining your questions give you a\n\nplace to start collecting the type of data you need to define your metrics.\n\n1.4.2\n\nVisualizing your data\n\nHaving data from all over the place in a single database that you’re applying formulas\nto is great. Asking everyone in your company to query that data from a command line\nprobably isn’t the best approach to communicate effectively. Data is such a part of the\nfabric of how business is done today that there is a plethora of frameworks for\n\nBreaking down\nyour questions\n\nWhere to get data\nto answer them\n\nWhat data points\nto look at\n\nFigure 1.11\nBreaking down\nquestions with\nXMind\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "1", "chapter_title": "Measuring agile performance", "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "Visualizing your data Where to get data\nto answer them What data points\nto look at", "l2_summary": "If you’re swimming in a sea of data, it’s hard to figure out what data answers what\nquestion and which metrics you should track. into smaller questions: Visualizing your data Where to get data\nto answer them What data points\nto look at Figure 1.11\nBreaking down\nquestions with\nXMind", "prev_page": {"page_num": 36, "segment_id": "00036"}, "next_page": {"page_num": 38, "segment_id": "00038"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "what", "figure", "mind", "mapping", "down", "questions", "metrics", "where", "start"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": ["XMind"], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["velocity", "tech debt"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["XMind"], "topics": ["measuring agile performance", "mind mapping", "data visualization"], "key_concepts": ["just in time metrics", "breaking down questions", "visualizing data"], "problem_statement": "how to effectively measure and visualize agile performance", "solution_approach": "using mind mapping for breaking down questions, defining metrics, and visualizing data", "extraction_method": "lm"}}
{"segment_id": "00038", "page_num": 38, "segment": "Analyzing what you have and what to do with the data\n\n15\n\nMany completed\ntasks that didn’t impact\nstory points.\n\nHuge dips in\nsprints 54 and 56.\n\nData from project tracking\n\n400\n\n300\n\n200\n\n100\n\n0\n53\n\nTotal tasks\ncompleted\n\nStory points\ncompleted\n\nFigure 1.12 Project\ntracking data for a\nteam for a few sprints\n\n54\n\n55\n\n56\n\n57\n\n58\n\nSprint\n\nvisualizing it. Ideally you should use a distributed system that allows everyone access\nto the charts, graphs, radiators, dashboards, and whatever else you want to show.\n\n Keep in mind that data can be interpreted numerous ways, and statistics can be\nconstrued to prove different points of view. It’s important to display the data you want\nto show in such a way that it, as clearly as possible, answers the questions you’re asking.\nLet’s look at one example.\n\n If you want to see how productive your team is, you can start by finding out how\nmany tasks are being completed. The problem with measuring the number of tasks is\nthat a handful of very difficult tasks could be much more work than many menial\nones. To balance that possibility you can add the amount of effort that went into completing those tasks, measured in figure 1.12 with story points.\n\n Whoa! Look at those dips around sprints 54 and 56! This obviously points to a\nproblem, but what is the problem? You know that the development team was working\nreally hard, but it didn’t seem like they got as much done in those sprints. Now let’s\ntake a look at what was going on in source control over the same time period, as\nshown in figure 1.13.\n\nData from source control\n\nPull requests and code\nreviews are trending up.\n\n800\n\n600\n\n400\n\n200\n\n0\n53\n\n54\n\n55\n\n56\n\n57\n\n58\n\nSprint\n\nLicensed to Mark Watson <nordickan@gmail.com>\n\nPull requests\n\nCode reviews\n\nFigure 1.13 Source\ncontrol data for a team\nover a few sprints", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "1", "chapter_title": "Measuring agile performance", "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "Story points\ncompleted Figure 1.12 Project\ntracking data for a\nteam for a few sprints Data from source control", "l2_summary": "Data from project tracking Total tasks\ncompleted Story points\ncompleted Figure 1.12 Project\ntracking data for a\nteam for a few sprints Data from source control Figure 1.13 Source\ncontrol data for a team\nover a few sprints", "prev_page": {"page_num": 37, "segment_id": "00037"}, "next_page": {"page_num": 39, "segment_id": "00039"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "tasks", "points", "sprints", "what", "completed", "figure", "team", "many", "story"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["distributed system", "charts", "graphs", "radiator", "dashboards", "source control"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["story points", "tasks completed", "sprints"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["charts", "graphs", "radiator", "dashboards", "source control"], "topics": ["project tracking", "data visualization", "team productivity"], "key_concepts": ["story points", "sprints", "tasks completed", "data interpretation"], "problem_statement": "Identifying and addressing dips in team productivity during specific sprints", "solution_approach": "Analyzing project tracking data, source control activity, and visualizing the data to identify issues", "extraction_method": "lm"}}
{"segment_id": "00039", "page_num": 39, "segment": "16\n\nCHAPTER 1 Measuring agile performance\n\n1.5\n\nIf anything, it looks like the development teams were doing more work over time---a\nlot more work! So if you’re writing more code, and the number of bugs seems to be\npretty constant relative to the tasks you’re getting done, but you’re not consistent in\nthe amount of stuff you’re delivering, what’s the real issue? You may not have the\nanswer yet, but you do have enough data to start asking more questions.\n\nApplying lessons learned\nApplying lessons learned can be the hardest part of the feedback loop because it\nimplies behavioral changes for your team. In other words, collecting and analyzing\ndata are technical problems; the final part is a human problem. When you’re developing a software product, you’re constantly tweaking and evolving code, but it’s not\nalways easy to tweak your team’s behavior.\n\n Whenever you’re trying to change something because it’s not good, you can easily\nperceive that someone is doing something bad or wrong, and who likes to be told\nthat? The first key is always keeping an open mind. Remember that we’re all human,\nno one is perfect, and we always have the capacity to improve. When things aren’t perfect (and are they ever?), then you have an opportunity to make them better.\n\n When you’re presenting trends that you want to improve on, ensure that you’re\nkeeping a positive spin on things. Focus on the positive that will come from the\nchange, not the negative that you’re trying to avoid. Always strive to be better instead\nof running away from the bad.\n\n Always steer away from blame or finger pointing. Data is a great tool when used for\ngood, but many people fear measurement because it can also be misunderstood. An\neasy example of this would be measuring lines of code (LOC) written by your development team. LOC doesn’t always point to how much work a developer is getting done.\nSomething that has a lot of boilerplate code that gets autoa tool may\nhave a very high LOC; an extremely complex algorithm that takes a while to tune may\nhave a very low LOC. Most developers will tell you the lower LOC is much better in this\ncase (unless you’re talking to the developer who wrote the code generator for the former). It’s always important to look at trends and understand that all the data you collect is part of a larger picture that most likely the team in the trenches will understand\nthe best. Providing context and staying flexible when interpreting results is an important part of ensuring successful adoption of change. Don’t freak out if your productivity drops on every Wednesday; maybe there’s some good reason for that. The\nimportant thing is that everyone is aware of what’s happening, what you’re focused on\nimproving, and how you’re measuring the change you want to see.\n\n1.6\n\nTaking ownership and measuring your team\nDevelopment teams should be responsible for tracking themselves through metrics\nthat are easy to obtain and communicate. Agile frameworks have natural pauses to\nallow your team to check and adjust. At this point you should have an idea of the good\nand bad of how you approach and do your work. Now, take the bull by the horns and\nstart measuring!\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "1", "chapter_title": "Measuring agile performance", "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "CHAPTER 1 Measuring agile performance You may not have the\nanswer yet, but you do have enough data to start asking more questions. An\neasy example of this would be measuring lines of code (LOC) written by your development team.", "l2_summary": "CHAPTER 1 Measuring agile performance You may not have the\nanswer yet, but you do have enough data to start asking more questions. Remember that we’re all human,\nno one is perfect, and we always have the capacity to improve. An\neasy example of this would be measuring lines of code (LOC) written by your development team. LOC doesn’t always point to how much work a developer is getting done. At this point you should have an idea of the good\nand bad of how you approach and do your work.", "prev_page": {"page_num": 38, "segment_id": "00038"}, "next_page": {"page_num": 40, "segment_id": "00040"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["have", "always", "team", "measuring", "code", "when", "more", "work", "data", "part"], "content_type": "theory", "domain": "programming|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": ["Agile"], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["productivity", "code quality"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["measuring agile performance", "lessons learned from data", "team behavior changes"], "key_concepts": ["data-driven decision making", "open-mindedness", "positive feedback loop"], "problem_statement": "Improving team productivity and code quality in an Agile environment", "solution_approach": "Collecting, analyzing data, applying lessons learned, and fostering a positive change mindset", "extraction_method": "lm"}}
{"segment_id": "00040", "page_num": 40, "segment": "Taking ownership and measuring your team\n\n17\n\nTeam 1\n\n200\n\ni\n\ns 150\nt\nn\no\np\ny\nr\no\nt\nS\n\n100\n\n50\n\n0\n53\n\n54\n\n55\n\n56\n\n57\n\n58\n\n59\n\n60\n\n61\n\n62\n\nSprints\n\n1.6.1 Getting buy-in\n\nFigure 1.14 Team 1’s\ncomplete story points\nover time\n\nYou can start collecting data and figuring out metrics on your own, but ideally you want\nto work with your team to make sure everyone is on the same page. These metrics are\nbeing used to measure the team as a whole so it’s important that everyone understands\nwhat you’re doing and why and also knows how to contribute to the process.\n\n You can introduce the concepts we’ve talked about so far to get your team ready to\ndo better performance tracking at any time, but perhaps the most natural time would\nbe when your team breaks to see how STET doing. This could be at the end of a sprint,\nafter a production release, at the end of the week, or even at the end of the day. Wherever that break is on your team is where you can introduce these concepts.\n\n Let’s look at a team which is using Scrum and can’t seem to maintain a consistent\nvelocity. Their Scrum master is keeping track of their accomplishments sprint after\nsprint and graphing it; as shown in figure 1.14, it’s not looking good.\n\n After showing that to leadership, everyone agrees that the team needs to be more\nconsistent. But what’s causing the problem? The development team decides to pull\ndata from all the developers across all projects to see if there’s someone in particular\nwho needs help with their work. Lo and behold, they find that Joe Developer, a member of Team 1, is off the charts during some of the dips, as shown in figure 1.15. After\ndigging a bit deeper, it is determined that Joe Developer has specific knowledge of the\nproduct a different team was working on and was actually contributing to multiple\n\nJoe Developer’s points\n\n50.0\n\ni\n\ns 37.5\nt\nn\no\np\ny\nr\no\nt\nS\n\n25.0\n\n12.5\n\n0.0\n\n53\n\n54\n\n55\n\n56\n\n57\n\n58\n\n59\n\n60\n\n61\n\n62\n\n63\n\n64\n\nSprints\n\nLicensed to Mark Watson <nordickan@gmail.com>\n\nFigure 1.15 Joe\nDeveloper’s\ncontribution is off the\ncharts, but his team is\nnot performing well.", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "1", "chapter_title": "Measuring agile performance", "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "Taking ownership and measuring your team Figure 1.14 Team 1’s\ncomplete story points\nover time Figure 1.15 Joe\nDeveloper’s\ncontribution is off the\ncharts, but his team is\nnot performing well.", "l2_summary": "Taking ownership and measuring your team Figure 1.14 Team 1’s\ncomplete story points\nover time Wherever that break is on your team is where you can introduce these concepts. After showing that to leadership, everyone agrees that the team needs to be more\nconsistent. Lo and behold, they find that Joe Developer, a member of Team 1, is off the charts during some of the dips, as shown in figure 1.15. Figure 1.15 Joe\nDeveloper’s\ncontribution is off the\ncharts, but his team is\nnot performing well.", "prev_page": {"page_num": 39, "segment_id": "00039"}, "next_page": {"page_num": 41, "segment_id": "00041"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": true, "has_citations": false, "key_terms": ["team", "figure", "developer", "time", "everyone", "sprint", "sprints", "points", "data", "metrics"], "content_type": "tutorial", "domain": "management|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Scrum"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": true, "has_metrics": true, "metrics": ["velocity", "story points"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["team performance tracking", "Scrum methodology", "velocity measurement"], "key_concepts": ["buy-in from team", "performance metrics", "velocity consistency", "data collection"], "problem_statement": "Team's inconsistent velocity and the need to identify contributors to the issue", "solution_approach": "Collecting data, identifying individual contributions, and addressing performance issues", "extraction_method": "lm"}}
{"segment_id": "00041", "page_num": 41, "segment": "18\n\nCHAPTER 1 Measuring agile performance\n\nteams during that time. He was overworked, yet it looked as if the team were underperforming.\n\n In this case it’s pretty easy to show that the commitment to multiple projects hurts\nthe individual teams, and apparently Joe Developer is way more productive when\nworking on the other team’s product. People are usually eager to show trends like\nthis, especially when they indicate that individual performance is great but for some\nreason forces outside the team are hurting overall productivity. If you’re being measured, then you might as well ensure that the entire story is being told. In this case the\ndata ended up pointing to a problem that the team could take action on: fully dedicate Joe Developer to one team to help avoid missing goals on another team and to\nnot burn your engineers as you’re delivering product.\n\n Collecting data is so easy that you shouldn’t need buy-in from product sponsors to\nget it started. Remember that collecting metrics needs to be cheap, so cheap that you\nshouldn’t have to ask for permission or resources to do it. If you can show the value of\nthe data you’re collecting by using it to point to problems and corresponding solutions and ultimately improve your code-delivery process, then you will likely make\nyour product sponsor extremely happy. In this case it’s usually better to start collecting\ndata that you can share with sponsors to show them you know your stuff rather than\ntrying to explain what you’re planning to do before you go about doing it.\n\n1.6.2 Metric naysayers\n\nThere will likely be people in your group who want nothing to do with measuring\ntheir work. Usually this stems from the fear of the unknown, fear of Big Brother, or a\nlack of control. The whole point here is that teams should measure themselves, not\nhave some external person or system tell them what’s good and bad. And who doesn’t\nwant to get better? No one is perfect---we all have a lot to learn and we can always\nimprove. Nevertheless here are some arguments I’ve heard as I’ve implemented these\ntechniques on various teams:\n\n■ People don’t like to be measured. When we are children, our parents/guardians tell\nus we are good or bad at things. In school, we are graded on everything we do.\nWhen we go out to get a job, we’re measured against the competition, and\nwhen we get a job, we’re constantly evaluated. We’re being measured all the\ntime. The question is, do you want to provide the yardstick or will someone else\nprovide it?\n\n■ Metrics are an invasion of my privacy. Even independent developers use source\ncontrol. The smallest teams use project tracking of some kind, and ideally\nyou’re using some kind of CI/CD to manage your code pipeline. All of the data\nto measure you is already out there; you’re adding to it every day and it’s a\nbyproduct of good software practice. Adding it all together for feedback on\nimprovement potential isn’t an invasion of privacy as much as it’s a way to make\nsure you’re managing and smoothing out the bumps in the agile development\nroad.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "1", "chapter_title": "Measuring agile performance", "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "teams during that time. Collecting data is so easy that you shouldn’t need buy-in from product sponsors to\nget it started. We’re being measured all the\ntime.", "l2_summary": "teams during that time. Collecting data is so easy that you shouldn’t need buy-in from product sponsors to\nget it started. And who doesn’t\nwant to get better? ■ People don’t like to be measured. When we go out to get a job, we’re measured against the competition, and\nwhen we get a job, we’re constantly evaluated. We’re being measured all the\ntime.", "prev_page": {"page_num": 40, "segment_id": "00040"}, "next_page": {"page_num": 42, "segment_id": "00042"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["team", "teams", "when", "some", "data", "show", "product", "measured", "collecting", "case"], "content_type": "theory|practice", "domain": "management|programming", "complexity": "intermediate", "companies": [], "people": ["Joe Developer"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["productivity", "commitment to multiple projects"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["measuring agile performance", "data collection for improvement", "metric naysayers"], "key_concepts": ["individual vs team productivity", "data-driven decision making", "self-measurement"], "problem_statement": "The commitment to multiple projects hurts individual teams and overall productivity.", "solution_approach": "Fully dedicate team members to one project at a time to improve productivity and avoid missing goals.", "extraction_method": "lm"}}
{"segment_id": "00042", "page_num": 42, "segment": "Summary\n\n19\n\n■ Metrics make our process too heavy. If anything, metrics help you identify how to\nimprove your process. If you choose the right data to look at, then you can find\nparts of your process that are too heavy, figure out how to streamline them, and\nuse metrics to track your progress. If it feels like metrics are making your process too heavy because someone is collecting data and creating the metrics\nmanually, then you have a golden opportunity to improve your process by automating metrics collection and reporting.\n\n■ Metrics are hard; it will take too much time. Read on, my friend! There are easy ways\nto use out-of-the-box technology to obtain metrics very quickly. In appendices A\nand B we outline open source tools you can use to get metrics from your existing systems with little effort. The key is using the data you have and simply making metrics the byproduct of your work.\n\n1.7\n\nSummary\nThis chapter showed you where to find data to measure your team and your process\nand an overview of what to do with it. At this point you’ve learned:\n\n■ Measuring agile development is not straightforward.\n■ By collecting data from the several systems used in your SDLC, you can answer\n\nsimple questions.\n\n■ By combining data from multiple systems in your SDLC, you can answer bigpicture questions.\n\n■ By using mind mapping you can break questions down into small enough\n\nchunks to collect data.\n\n■ By using simple techniques, measuring agile performance isn’t so hard.\n■ Showing metrics to your teammates easily demonstrates value and can easily\n\nfacilitate buy-in.\n\nChapter 2 puts this in action through an extended case study where you can see a\nteam applying these lessons firsthand.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "1", "chapter_title": "Measuring agile performance", "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "■ Metrics make our process too heavy. If anything, metrics help you identify how to\nimprove your process. The key is using the data you have and simply making metrics the byproduct of your work.", "l2_summary": "■ Metrics make our process too heavy. If anything, metrics help you identify how to\nimprove your process. If it feels like metrics are making your process too heavy because someone is collecting data and creating the metrics\nmanually, then you have a golden opportunity to improve your process by automating metrics collection and reporting. ■ Metrics are hard; it will take too much time. The key is using the data you have and simply making metrics the byproduct of your work. ■ By collecting data from the several systems used in your SDLC, you can answer", "prev_page": {"page_num": 41, "segment_id": "00041"}, "next_page": {"page_num": 43, "segment_id": "00043"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["metrics", "data", "process", "heavy", "systems", "using", "questions", "summary", "improve", "then"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["process improvement", "streamlining processes"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["open source tools"], "topics": ["metrics in agile development", "data collection for process improvement", "automation of metrics collection"], "key_concepts": ["metrics are not inherently heavy", "easy ways to collect metrics", "using data from existing systems"], "problem_statement": "Metrics making the process too heavy or taking too much time", "solution_approach": "Automating metrics collection and reporting, using out-of-the-box technology", "extraction_method": "lm"}}
{"segment_id": "00043", "page_num": 43, "segment": "Observing a live project\n\nThis chapter covers\n\n■ Tracking progress of multiple development\n\nefforts\n\n■ Using Elasticsearch (EC) and Kibana to track\n\nand visualize metrics over time\n\n■ Communicating progress back to leadership\n\n■ Using data to improve day-to-day operations\n\nIn chapter 1 we walked through some of the concepts around agile development and\nsaw how a team is typically measured, looked at problems that crop up, and got an\noverview of how to solve those problems. Now let’s look at a team that put these concepts into action, how they did it, and how it all turned out---all through a case study.\n\n2.1\n\nA typical agile project\nSoftware development is flexible and can move very quickly. Every project is different, so any one project isn’t going to encapsulate all of the characteristics of every\nproject.\n\n Let’s take a look at Blastamo Music, LLC, a company that makes guitar pedals\nand uses an e-commerce site written by their development team. As the company\n\n20\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "1", "chapter_title": "Measuring agile performance", "section_num": "8.5", "section_title": "Case study: finding anomalies in lead time 173", "l1_summary": "Observing a live project ■ Tracking progress of multiple development Every project is different, so any one project isn’t going to encapsulate all of the characteristics of every\nproject.", "l2_summary": "Observing a live project ■ Tracking progress of multiple development In chapter 1 we walked through some of the concepts around agile development and\nsaw how a team is typically measured, looked at problems that crop up, and got an\noverview of how to solve those problems. Now let’s look at a team that put these concepts into action, how they did it, and how it all turned out---all through a case study. A typical agile project\nSoftware development is flexible and can move very quickly. Every project is different, so any one project isn’t going to encapsulate all of the characteristics of every\nproject.", "prev_page": {"page_num": 42, "segment_id": "00042"}, "next_page": {"page_num": 44, "segment_id": "00044"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["project", "development", "team", "chapter", "progress", "using", "concepts", "agile", "problems", "look"], "content_type": "case_study", "domain": "management|devops", "complexity": "intermediate", "companies": ["Blastamo Music, LLC"], "people": [], "products": ["guitar pedals", "e-commerce site"], "technologies": [], "frameworks": [], "methodologies": ["agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["progress of multiple development efforts", "day-to-day operations"], "has_case_study": true, "case_study_company": "Blastamo Music, LLC", "tools_mentioned": ["Elasticsearch (EC)", "Kibana"], "topics": ["tracking progress", "using Elasticsearch and Kibana", "communicating progress to leadership", "improving day-to-day operations"], "key_concepts": ["agile development", "metrics tracking", "visualization"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00044", "page_num": 44, "segment": "A problem arises\n\n21\n\ngrew, it built better products by replacing the electronics in the pedals with software\nand acquired several smaller companies that had the technology and resources\nneeded to grow even faster. Among these technologies was an e-commerce system that\nhad several features the leadership team wanted for their site.\n\n2.1.1 How Blastamo Music used agile\n\nAs with many teams, the agile process used by our case study grew along with the organization, instead of being very textbook. The Blastamo team used Scrum as the cornerstone of their agile process, they used cards for their tasks that were estimated and\nworked on, they had a task board on which they moved the tasks along based on the\nstatus, and they had release processes that varied from team to team. The typical flow\nof the cards was:\n\n■\n\nIn definition---The card is being defined by a product owner.\n\n■ Dev ready---The card is defined and ready to be coded.\n\n■\n\nIn dev---A developer is working on it.\n\n■ QA ready---Development is finished and it’s waiting for a member of the QA\n\nteam to check it.\nIn QA---A member of the QA team is checking it.\n\n■\n\n■ Done---The feature is ready for production.\n\nThe system groups and technologies the team used are shown in figure 2.1.\n\n The agile development team used JIRA (https:///software/jira)\nto manage the cards for their tasks that they estimated and worked on, GitHub\n(https://github.com/) to manage their source control, Jenkins (http://jenkins-ci\n.org/) for CI and deployments, New Relic (http://newrelic.com/) for application\nmonitoring, and Elasticsearch/Logstash/Kibana (ELK) (/\noverview/) for production log analysis.\n\n2.2\n\nA problem arises\nIn one of the acquisitions, the team got a robust piece of e-commerce software that\ncomplemented a lot of their current site’s functionality. Management thought the\n\nManage\ntasks and\nbugs\n\nProject\ntracking\n\nJIRA\n\nManage\ncode and\ncollaboration\n\nGenerate\nbuilds and\nrun tests\n\nMove code\nacross\nenvironments\n\nSource\ncontrol\n\nGitHub\n\nContinuous\nintegration\n\nDeployment\ntools\n\nJenkins\n\nEnsure\neverything\nis working\n\nApplication\nmonitoring\n\nNew Relic\nand ELK\n\nFigure 2.1 Blastamo’s agile pipeline and the software it uses\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_3", "rows": 2, "cols": 2, "data": [["Continuous\nintegration", "Deployment\ntools"], ["Jenkins", ""]], "markdown": "| Continuous\nintegration | Deployment\ntools |\n|---|---|\n| Jenkins |  |"}], "table_count": 1, "chapter_num": "1", "chapter_title": "Measuring agile performance", "section_num": "2.1.1", "section_title": "How Blastamo Music used agile", "l1_summary": "2.1.1 How Blastamo Music used agile In QA---A member of the QA team is checking it. The system groups and technologies the team used are shown in figure 2.1.", "l2_summary": "Among these technologies was an e-commerce system that\nhad several features the leadership team wanted for their site. 2.1.1 How Blastamo Music used agile The Blastamo team used Scrum as the cornerstone of their agile process, they used cards for their tasks that were estimated and\nworked on, they had a task board on which they moved the tasks along based on the\nstatus, and they had release processes that varied from team to team. In QA---A member of the QA team is checking it. The system groups and technologies the team used are shown in figure 2.1. Manage\ntasks and\nbugs", "prev_page": {"page_num": 43, "segment_id": "00043"}, "next_page": {"page_num": 45, "segment_id": "00045"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["team", "used", "agile", "software", "tasks", "ready", "manage", "blastamo", "cards", "jira"], "content_type": "case_study", "domain": "devops|architecture", "complexity": "intermediate", "companies": ["Blastamo Music", "null"], "people": [], "products": ["e-commerce system"], "technologies": ["JIRA", "GitHub", "Jenkins", "New Relic", "Elasticsearch/Logstash/Kibana (ELK)"], "frameworks": ["Scrum"], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": false, "metrics": [], "has_case_study": true, "case_study_company": "Blastamo Music", "tools_mentioned": ["JIRA", "GitHub", "Jenkins", "New Relic", "Elasticsearch/Logstash/Kibana (ELK)"], "topics": ["Agile Development", "DevOps Practices", "Software Evolution"], "key_concepts": ["agile process", "Scrum methodology", "continuous integration and deployment"], "problem_statement": "How Blastamo Music managed the evolution of their software development processes and technologies after acquiring new capabilities.", "solution_approach": "Blastamo Music adopted Scrum as a cornerstone for their agile process, utilized various tools for task management, source control, CI/CD, application monitoring, and log analysis.", "extraction_method": "lm"}}
{"segment_id": "00045", "page_num": 45, "segment": "22\n\nCHAPTER 2 Observing a live project\n\nQueries for\nwarnings and errors\nin the application logs.\n\nA small number of\nissues is expected\nin production.\n\nHuge jump in the\nnumber of errors and\nwarnings is not good.\n\nFigure 2.2 Kibana visualizing log analysis for the case-study project. Error rates have headed off the\ncharts for the last two days.\n\nnew software would provide a huge boost in productivity. The development teams\nwere excited to use cool cutting-edge technology that they could develop against.\n\n At first the integration seemed simple. The developers integrated the new system\nwith their existing system, put the changes through testing, and got it out in production. After the launch the operations team noticed a significant uptick in errors in their\nproduction logs. The graph they saw looked something like the one in figure 2.2.\n\n The operations team alerted the development team, which jumped in and started\ntriaging the issue. The development team quickly noticed that the integration with\nthe new system wasn’t as straightforward as they thought. Several scenarios were causing significant issues with the application in the aftermath of the integration, which\nwere going to need a heavy refactor of the code base to fix. The development team\nproposed to the leadership team that they spend time refactoring the core components of the application by creating interfaces around them, rebuilding the faulty\nparts, and replacing them to ensure the system was stable and reliable.\n\n Leadership agreed that the refactor had to be done but didn’t want it to adversely\nimpact new feature development. Leadership kept a close eye on both efforts, asking\nthe development team for data, data, and more data so that if additional problems\narose, they could make the best decision possible.\n\n2.3\n\nDetermining the right solution\nFaced with the task of reflecting their accomplishments and progress with solid data,\nthe development team started with two questions:\n\n■ How can we show the amount of work the team is doing?\n■ How can we show the type of work the team is doing?\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "2", "chapter_title": "Observing a live project", "section_num": "2.1.1", "section_title": "How Blastamo Music used agile", "l1_summary": "After the launch the operations team noticed a significant uptick in errors in their\nproduction logs. The operations team alerted the development team, which jumped in and started\ntriaging the issue. The development team quickly noticed that the integration with\nthe new system wasn’t as...", "l2_summary": "Queries for\nwarnings and errors\nin the application logs. After the launch the operations team noticed a significant uptick in errors in their\nproduction logs. The operations team alerted the development team, which jumped in and started\ntriaging the issue. The development team quickly noticed that the integration with\nthe new system wasn’t as straightforward as they thought. ■ How can we show the amount of work the team is doing? ■ How can we show the type of work the team is doing?", "prev_page": {"page_num": 44, "segment_id": "00044"}, "next_page": {"page_num": 46, "segment_id": "00046"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["team", "development", "system", "data", "errors", "application", "production", "integration", "leadership", "project"], "content_type": "case_study", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["new software", "cool cutting-edge technology", "Kibana"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": false, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["error rates"], "has_case_study": true, "case_study_company": null, "tools_mentioned": ["Kibana"], "topics": ["integration issues", "refactoring", "production monitoring"], "key_concepts": ["log analysis", "error rate monitoring", "refactoring codebase"], "problem_statement": "Integration of new software caused a significant increase in errors in production logs.", "solution_approach": "The development team proposed to refactor the core components of the application by creating interfaces around them, rebuilding faulty parts, and replacing them to ensure system stability and reliability.", "extraction_method": "lm"}}
{"segment_id": "00046", "page_num": 46, "segment": "Determining the right solution\n\n23\n\nAre you\nmeeting\ncommitments?\n\nHow much\ncode is getting\nbuilt?\n\nHow long does\nit take you to get\nthings right?\n\nHow fast can you\nget changes to your\nconsumers?\n\nHow well\nis your system\nperforming?\n\nProject\ntracking\n\nSource\ncontrol\n\nContinuous\nintegration\n\nDeployment\ntools\n\nApplication\nmonitoring\n\nWhat is your\ncurrent pace?\n\nHow well is the\nteam working\ntogether?\n\nHow are your\ncustomers using\nyour system?\n\nFigure 2.3 The type, amount, and target of work done by a team can be retrieved by querying your\nproject-tracking and source-control management systems.\n\nThe leadership team gave the go-ahead to fix the issues with the new e-commerce\ncomponents, but they wanted to get new features out at the same time. To point out\nprogress on both tracks, the development team wanted to show what they were doing\nand how much work they were assigned.\n\n The development team first needed to figure out where to get the data. If they\ncould tap into the systems used for delivery, then they could probably get everything\nthey were looking for. The team decided that the first two places they should focus on\nwere project tracking and source control, as highlighted in figure 2.3.\n\n The team decided to start looking at data aggregated weekly. While operating in\ntwo-week sprints, weekly data would show them where they were midsprint so they\ncould adjust if necessary, and data at the end of the sprint would show them where\nthey ended up. From their project tracking system they decided to capture the following data points:\n\n■ Total done---The total number of cards completed. This should show the volume\n\nof work.\n\n■ Velocity---Calculating the effort the tasks took over time. Before a sprint started,\nthe team would estimate how long they thought their tasks would take and note\nthose estimates on the cards that represented their tasks. The velocity of the\nteam would be calculated every sprint by adding up the estimates that were ultimately completed by the end of the sprint.\n\n■ Bugs---If they were making progress on their refactor, then the number of bugs\n\nin the system should start to decrease.\n\n■ Tags---There were two types of high-level tasks: refactor work and new features.\nThey used labels to tag each task with the type of work they were doing. This\nallowed them to show progress on each effort separately.\n\n■ Recidivism rate---The rate at which cards move backward in the process. If a task\nmoves to done but it’s not complete or has bugs, it’s moved back, thus causing\nthe recidivism rate to go up. If you use B to represent the number of times a\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "2", "chapter_title": "Observing a live project", "section_num": "2.1.1", "section_title": "How Blastamo Music used agile", "l1_summary": "How well\nis your system\nperforming? How well is the\nteam working\ntogether? The development team first needed to figure out where to get the data.", "l2_summary": "How long does\nit take you to get\nthings right? How well\nis your system\nperforming? How well is the\nteam working\ntogether? How are your\ncustomers using\nyour system? The development team first needed to figure out where to get the data. The team decided to start looking at data aggregated weekly.", "prev_page": {"page_num": 45, "segment_id": "00045"}, "next_page": {"page_num": 47, "segment_id": "00047"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["team", "work", "show", "data", "would", "system", "project", "tracking", "sprint", "tasks"], "content_type": "practice", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["project-tracking systems", "source-control management systems", "deployment tools", "application monitoring"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["velocity", "bugs", "recidivism rate"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["project-tracking systems", "source-control management systems", "deployment tools", "application monitoring"], "topics": ["Project Tracking", "Source Control", "Continuous Integration", "Deployment Tools", "Application Monitoring"], "key_concepts": ["velocity", "bugs", "recidivism rate", "sprints", "cards", "labels"], "problem_statement": "Determining the right solution for tracking progress and managing work in a development team.", "solution_approach": "Using project-tracking systems, source-control management systems, continuous integration, deployment tools, and application monitoring to track velocity, bugs, recidivism rate, and other metrics.", "extraction_method": "lm"}}
{"segment_id": "00047", "page_num": 47, "segment": "24\n\nCHAPTER 2 Observing a live project\n\ncard moves backward and F to represent the number of times a card moves forward, then recidivism can be calculated with the formula (B / (F + B)) * 100.\n\nNOTE The maximum recidivism rate for complete tasks would be 50%, which\nwould mean that your cards moved forward as many times as they moved\nbackward.\n\nThose data points should give a pretty complete picture of the action in the project\ntracking system (PTS). The team also decided to get the following data point out of\ntheir SCM.\n\n■\n\n CLOC---The total amount of change in the codebase itself.\n\nThe development team was already using ELK for log analysis, and the leadership\nteam decided to use the same system for graphing their data. To do this they needed a\nway to get data from JIRA and GitHub into EC for indexing and searching.1\n\n JIRA and GitHub have rich APIs that expose data. The development team decided\nto take advantage of those APIs with an in-house application that obtained the data\nthey needed and sent it to EC. The high-level architecture is shown in figure 2.4.\n\nNOTE A setup for EC and Kibana can be found in appendix A, and the code\nfor the application that can get data from source systems and index it with EC\ncan be found in appendix B.\n\nOnce the development team started collecting the data, they could use the Kibana\ndashboards they were familiar with from their log analysis to start digging into it.\n\n1 Custom\n\napp queries\nsystems\nfor data.\n\nData\ncollector\n\n2 Data is sent to\nElasticsearch\nfor indexing.\n\n3 Data is\n\nvisualized\nwith Kibana.\n\nElasticsearch\n\nKibana\n\n4 Data analysis\n\nshows progress\nback to the\nleadership\nteam.\n\nFigure 2.4 The component architecture for the system\n\n1 For more information on EC I recommend Elasticsearch in Action (Manning Publications, 2015), www\n\n.manning.com/hinman/.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "2", "chapter_title": "Observing a live project", "section_num": "2.1.1", "section_title": "How Blastamo Music used agile", "l1_summary": "To do this they needed a\nway to get data from JIRA and GitHub into EC for indexing and searching.1 app queries\nsystems\nfor data. 2 Data is sent to\nElasticsearch\nfor indexing.", "l2_summary": "The team also decided to get the following data point out of\ntheir SCM. The development team was already using ELK for log analysis, and the leadership\nteam decided to use the same system for graphing their data. To do this they needed a\nway to get data from JIRA and GitHub into EC for indexing and searching.1 JIRA and GitHub have rich APIs that expose data. app queries\nsystems\nfor data. 2 Data is sent to\nElasticsearch\nfor indexing.", "prev_page": {"page_num": 46, "segment_id": "00046"}, "next_page": {"page_num": 48, "segment_id": "00048"}, "extended_fields": {"has_code": false, "has_formulas": true, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "team", "kibana", "system", "decided", "development", "analysis", "elasticsearch", "project", "card"], "content_type": "practice", "domain": "architecture|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["JIRA", "GitHub", "ELK", "Elasticsearch", "Kibana"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["recidivism rate", "CLOC"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["JIRA", "GitHub", "ELK", "Elasticsearch", "Kibana"], "topics": ["Project tracking system analysis", "Data collection and visualization", "Integration of JIRA and GitHub with ELK stack"], "key_concepts": ["Recidivism rate calculation", "Code change measurement (CLOC)", "Log analysis using ELK stack", "Custom data collector application"], "problem_statement": null, "solution_approach": "Collecting and visualizing project tracking system data through a custom-built application that integrates JIRA, GitHub, and Elasticsearch for log analysis.", "extraction_method": "lm"}}
{"segment_id": "00048", "page_num": 48, "segment": "Determining the right solution\n\n25\n\nThey collected data for a sprint in the new operating model and went back a few\nsprints to get a baseline for analysis. Using this data they generated the charts shown\nin figures 2.5, 2.6, and 2.7.\n\nThe points are aggregated\nweekly for the graphs.\n\nBugs and recidivism were trending\ndown towards the end of a sprint.\n\nFigure 2.5 Showing recidivism and bugs in a single graph because both should\ntrend in the same direction\n\nSplitting the data by tags allows the\nvelocity of each effort to be tracked separately.\n\nAfter the split, velocity\nstarts to drop just a bit.\n\nFigure 2.6 Breaking velocity out by effort after sorting data with tags\n\nY-axis is in\nthousands\n\nThe amount of code changes\nhas been increasing a lot\n\nFigure 2.7 The chart graph showing how much code the team was changing over\ntime. The Y-axis is in thousands to make it easier to read.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "2", "chapter_title": "Observing a live project", "section_num": "2.1.1", "section_title": "How Blastamo Music used agile", "l1_summary": "Using this data they generated the charts shown\nin figures 2.5, 2.6, and 2.7. Figure 2.6 Breaking velocity out by effort after sorting data with tags Y-axis is in\nthousands", "l2_summary": "Using this data they generated the charts shown\nin figures 2.5, 2.6, and 2.7. Figure 2.5 Showing recidivism and bugs in a single graph because both should\ntrend in the same direction Splitting the data by tags allows the\nvelocity of each effort to be tracked separately. Figure 2.6 Breaking velocity out by effort after sorting data with tags Y-axis is in\nthousands Figure 2.7 The chart graph showing how much code the team was changing over\ntime.", "prev_page": {"page_num": 47, "segment_id": "00047"}, "next_page": {"page_num": 49, "segment_id": "00049"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "figure", "velocity", "sprint", "bugs", "recidivism", "showing", "graph", "tags", "effort"], "content_type": "practice", "domain": "devops|architecture", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["charts", "sprints", "operating model"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["recidivism", "bugs", "velocity", "code changes"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["data analysis", "sprint tracking", "effort velocity", "code changes over time"], "key_concepts": ["recidivism and bugs trend", "splitting data by tags", "velocity of each effort", "code changes increase"], "problem_statement": null, "solution_approach": "collecting and analyzing data through sprints to track recidivism, bugs, velocity, and code changes", "extraction_method": "lm"}}
{"segment_id": "00049", "page_num": 49, "segment": "26\n\n2.4\n\nCHAPTER 2 Observing a live project\n\nNow the development team was ready to start digesting the data to give leadership the\nobjective breakdowns they wanted to see.\n\nAnalyzing and presenting the data\nThe first thing the development team noticed was that their velocity had dropped.\nThat was probably to be expected because they had just refocused the team into a new\nworking model. When they divided the efforts, they had a nice, even split in velocity\nbetween them, which was just what they wanted. If they could keep this up, they would\nbe in great shape. But after the end of the next sprint they started seeing trends that\nweren’t so good, as shown in figures 2.8, 2.9, and 2.10.\n\n These are the key things the development team noticed:\n\n■ Velocity was going down again. This indicated that they were still being impacted\n\nby the split in work and weren’t able to adjust.\n\nRecidivism is creeping up\nand bugs are jumping sharply.\n\nFigure 2.8 After another sprint recidivism has started creeping up and bugs are increasing sharply.\n\nVelocity for both teams is\nstarting to drop consistently.\n\nFigure 2.9 Velocity for both teams is starting to suffer.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "2", "chapter_title": "Observing a live project", "section_num": "2.1.1", "section_title": "How Blastamo Music used agile", "l1_summary": "Analyzing and presenting the data\nThe first thing the development team noticed was that their velocity had dropped. These are the key things the development team noticed: Figure 2.9 Velocity for both teams is starting to suffer.", "l2_summary": "Analyzing and presenting the data\nThe first thing the development team noticed was that their velocity had dropped. These are the key things the development team noticed: ■ Velocity was going down again. Figure 2.8 After another sprint recidivism has started creeping up and bugs are increasing sharply. Velocity for both teams is\nstarting to drop consistently. Figure 2.9 Velocity for both teams is starting to suffer.", "prev_page": {"page_num": 48, "segment_id": "00048"}, "next_page": {"page_num": 50, "segment_id": "00050"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["velocity", "team", "development", "data", "wanted", "noticed", "just", "split", "sprint", "started"], "content_type": "practice", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["velocity", "recidivism", "bugs"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["project management", "team velocity", "sprint analysis"], "key_concepts": ["velocity", "recidivism", "bugs"], "problem_statement": "The development team is facing issues with their project velocity and the quality of delivered work after a recent reorganization.", "solution_approach": "Monitoring and analyzing sprint data to identify trends and make necessary adjustments.", "extraction_method": "lm"}}
{"segment_id": "00050", "page_num": 50, "segment": "Analyzing and presenting the data\n\n27\n\nAmount of code changed\nis increasing rapidly\n\nFigure 2.10 The amount of code changed across both teams is skyrocketing.\n\n■ Velocity for new features dropped sharply. This indicated that the work being done by\nthe refactor was severely hurting the team’s ability to deliver anything new.\n\n■ CLOC was going up. The team was changing a lot more code.\n■ Bugs and recidivism were rising. The overall product was suffering for the combination refactor and feature work.\n\nAccording to this, the refactor was causing significant churn in the codebase and\napparently leaving a host of new bugs in its wake. New feature development velocity\nhad hit the floor because that work was being done on top of a shaky foundation.\n\n The team needed to adjust to these findings. Team members got together and\nstarted reviewing the list of bugs to determine root cause and validate or disprove\ntheir hypotheses. They discovered that as the team dug into the refactor, changes they\nmade in one part of the codebase were causing other parts to break. They had a general lack of unit tests across the integrated product that caused them to find bugs after\nthe refactored code was delivered. This led to a Whac-A-Mole situation, where fixing\none issue ended up creating at least one more to chase after.\n\n2.4.1\n\nSolving the problems\n\nThe team knew they had to make the following changes to get their project back on\ntrack:\n\n■ Ensure that they had enough automated testing across the application to prevent\nnew bugs from popping up after code was refactored. This should help them find\ntangential problems as they come up and avoid the Whac-A-Mole effect.\n\n■ Stop building features for at least a sprint in order to either\n\n■ Completely focus on the refactor to stabilize the foundation enough to build\n\nnew features on.\n\n■ Virtualize the back-end services so user-facing features could be developed\n\nagainst them while the refactor continued underneath it.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "2", "chapter_title": "Observing a live project", "section_num": "2.1.1", "section_title": "How Blastamo Music used agile", "l1_summary": "■ Velocity for new features dropped sharply. This indicated that the work being done by\nthe refactor was severely hurting the team’s ability to deliver anything new. The team was changing a lot more code.", "l2_summary": "Amount of code changed\nis increasing rapidly Figure 2.10 The amount of code changed across both teams is skyrocketing. ■ Velocity for new features dropped sharply. This indicated that the work being done by\nthe refactor was severely hurting the team’s ability to deliver anything new. The team was changing a lot more code. ■ Bugs and recidivism were rising.", "prev_page": {"page_num": 49, "segment_id": "00049"}, "next_page": {"page_num": 51, "segment_id": "00051"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["refactor", "team", "code", "bugs", "features", "work", "amount", "changed", "velocity", "done"], "content_type": "case_study", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["refactor", "unit tests", "CLOC"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["Velocity for new features", "CLOC", "Bugs and recidivism"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["Code refactoring impact on development velocity", "Testing and bug management", "Balancing refactor and feature work"], "key_concepts": ["refactor impact", "code churn", "bug introduction", "testing importance"], "problem_statement": "Refactor causing significant code changes, decreasing new feature velocity, and introducing bugs", "solution_approach": "Implementing automated testing to prevent future bugs, focusing on refactor stabilization, and virtualizing backend services", "extraction_method": "lm"}}
{"segment_id": "00051", "page_num": 51, "segment": "28\n\nCHAPTER 2 Observing a live project\n\nService virtualization\n\nService virtualization is the practice of recreating the behavior of a system synthetically\nfor isolated development and testing. Basically, you can emulate a dependency of a\nfeature you need to build with virtualization, build and test the feature on top of the\nvirtualized service, and deploy your new feature on top of the real service. This technique is particularly handy when consumer-facing features that need to iterate quickly\ndepend on large back-end systems that can’t change as fast.\n\nAccording to the data they could see the following:\n\n■ Before the refactor the development team was averaging 12 points/developer/\n\nsprint (pds).\n\n■ The refactor team was still averaging 12 pds while changing a lot more code.\n■ The new-feature team was down to 4 pds.\n\nThese results led them to the following conclusions:\n\n■ Given their productivity was cut to one-third, if they could stop working for two\nsprints and get back to their average productivity on the third sprint, they\nwould get the same amount of work done as if they worked at one-third productivity for three sprints.\n\n■ Based on their current velocity they could also infer that the refactor was slated\n\nto take another three sprints.\n\nThey decided that if they repurposed the entire new-feature team to tighten the\nteam’s automation for testing and virtualization for two sprints, they could easily get\nback to their average productivity and even ultimately become more productive by\nproactively finding bugs earlier in the development cycle. Now they had to convince\nthe leadership team to let them stop working on features for two sprints.\n\n2.4.2\n\nVisualizing the final product for leadership\n\nThe development team had a strong argument to change how they were working and\nthe data to back it up. But the charts that they found valuable would probably confuse\nthe leadership team without a lengthy explanation on the context. The final step was\nto break down this data into metrics that indicated how things were going in an intuitive and concise way. To do this they decided to add panels to their dashboard that\nfocused on the high-level data points the leadership team cared about.\n\n To highlight their argument they broke their data down into the following points:\n\n■ The velocity of the entire team before the split\n■ The average drop in velocity per developer of the feature team during the\n\nrefactor\n\n■ The increase in bug count every sprint after the split\n■ The difference in bug counts between well-tested code and poorly tested code\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "2", "chapter_title": "Observing a live project", "section_num": "2.1.1", "section_title": "How Blastamo Music used agile", "l1_summary": "Service virtualization ■ Before the refactor the development team was averaging 12 points/developer/ ■ The new-feature team was down to 4 pds.", "l2_summary": "Service virtualization According to the data they could see the following: ■ Before the refactor the development team was averaging 12 points/developer/ ■ The refactor team was still averaging 12 pds while changing a lot more code. ■ The new-feature team was down to 4 pds. ■ The velocity of the entire team before the split\n■ The average drop in velocity per developer of the feature team during the", "prev_page": {"page_num": 50, "segment_id": "00050"}, "next_page": {"page_num": 52, "segment_id": "00052"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["team", "feature", "data", "sprints", "service", "virtualization", "development", "back", "could", "refactor"], "content_type": "case_study", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["service virtualization"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["pds", "velocity"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["service virtualization", "team productivity", "refactoring"], "key_concepts": ["service virtualization", "feature development", "bug count"], "problem_statement": "Decreased team velocity due to refactoring and feature development", "solution_approach": "Reallocating resources for automation and testing", "extraction_method": "lm"}}
{"segment_id": "00052", "page_num": 52, "segment": "Analyzing and presenting the data\n\n29\n\nOnce the development team had presented all of their data, they hit leadership with\ntheir proposal to drop feature development for two sprints in order to get back to\ntheir previous level of productivity. Given the current state of things, they had nothing\nto lose.\n\n The leadership team gave them the go-ahead, and they all lived happily ever after.\n\nDashboards for the right audience\n\nWe’ll get a lot deeper into creating dashboards and communicating across the organization in chapter 9 when we explore ways to visualize the same data for different\naudiences. In this example the team used static data for their presentation.\n\nThe development team was having success with their data visualization, so they\ndecided to experiment to try to get an idea of the properties of their codebase without\nhaving intimate knowledge of it. As an experiment they also decided to show their\ncodebase as a CodeFlower (/CodeFlower/).\n\nCodeFlowers for a new perspective\n\nCodeFlowers are one example of visualizing your codebase in a very different way.\nWe’ll touch on them here and come back to different data-visualization and communication possibilities later in chapter 9.\n\nTest data\n\nLong lines represent\npackage structure\n\nApplication\n\nFirst,\nthe development team generated\na CodeFlower on one\nof their newly built,\nmodular, and welltested web\nservice\nprojects. That generated the flower shown\nin figure 2.11.\n\nFigure 2.11 A CodeFlower of\na clean, well-written codebase\n\nLicensed to Mark Watson <nordickan@gmail.com>\n\nTests\n\nMore tests", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "2", "chapter_title": "Observing a live project", "section_num": "2.1.1", "section_title": "How Blastamo Music used agile", "l1_summary": "Analyzing and presenting the data In this example the team used static data for their presentation. As an experiment they also decided to show their\ncodebase as a CodeFlower (/CodeFlower/).", "l2_summary": "Analyzing and presenting the data In this example the team used static data for their presentation. The development team was having success with their data visualization, so they\ndecided to experiment to try to get an idea of the properties of their codebase without\nhaving intimate knowledge of it. As an experiment they also decided to show their\ncodebase as a CodeFlower (/CodeFlower/). We’ll touch on them here and come back to different data-visualization and communication possibilities later in chapter 9. Figure 2.11 A CodeFlower of\na clean, well-written codebase", "prev_page": {"page_num": 51, "segment_id": "00051"}, "next_page": {"page_num": 53, "segment_id": "00053"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "team", "development", "codebase", "codeflower", "different", "leadership", "back", "dashboards", "chapter"], "content_type": "case_study", "domain": "programming|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["CodeFlower"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": false, "has_antipatterns": false, "has_instructions": false, "has_metrics": false, "metrics": [], "has_case_study": true, "case_study_company": null, "tools_mentioned": ["CodeFlower"], "topics": ["Data Visualization", "Codebase Analysis", "Development Team Proposal"], "key_concepts": ["CodeFlower visualization", "Data-driven decision making", "Team productivity"], "problem_statement": "The development team's productivity dropped, leading to a proposal to drop feature development for two sprints.", "solution_approach": "Using CodeFlower visualization to analyze the codebase and regain productivity.", "extraction_method": "lm"}}
{"segment_id": "00053", "page_num": 53, "segment": "30\n\nCHAPTER 2 Observing a live project\n\n Note a few interesting points from this codebase:\n\n■ The application itself has a clean package structure.\n■ There are as many tests as there is functional code.\n■ There is plenty of test data, which indicates that there are real tests for many\n\nscenarios.\n\nThen they generated a CodeFlower on their older e-commerce application codebase.\nAn excerpt is shown in figure 2.12.\n\n Here are a few notes from this codebase:\n\n■\n\nIt is huge! That usually indicates it’s difficult to maintain, deploy, and understand.\n\n■ There is a lot of XML and SQL, both difficult to maintain.\n■ There are tests, but not nearly as many tests as functional code.\n■ The package structure is complex and not well connected.\n\nOf the two CodeFlower representations, which application would you rather have to\nmodify and deploy?\n\n The development team was now using data visualization in different ways to shape\nhow they operated, and they were able to turn this into real-time data that the leadership team could peek at whenever they wanted.\n\nTests\n\nSQL files\n\nDAOs\n\nDomain\nobjects\n\nServices\n\nXML\n\nMessage\nconverters\n\nFigure 2.12\nAn excerpt from a\nCodeFlower generated\nfrom an extremely\ncomplex application\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "2", "chapter_title": "Observing a live project", "section_num": "2.1.1", "section_title": "How Blastamo Music used agile", "l1_summary": "■ There are as many tests as there is functional code. ■ There is plenty of test data, which indicates that there are real tests for many ■ There are tests, but not nearly as many tests as functional code.", "l2_summary": "■ There are as many tests as there is functional code. ■ There is plenty of test data, which indicates that there are real tests for many Then they generated a CodeFlower on their older e-commerce application codebase. ■ There is a lot of XML and SQL, both difficult to maintain. ■ There are tests, but not nearly as many tests as functional code. Figure 2.12\nAn excerpt from a\nCodeFlower generated\nfrom an extremely\ncomplex application", "prev_page": {"page_num": 52, "segment_id": "00052"}, "next_page": {"page_num": 54, "segment_id": "00054"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["there", "tests", "application", "codebase", "many", "data", "codeflower", "package", "structure", "functional"], "content_type": "case_study", "domain": "programming|architecture", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["CodeFlower", "XML", "SQL", "Java"], "frameworks": [], "methodologies": ["DevOps"], "programming_languages": ["Java"], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": false, "metrics": [], "has_case_study": true, "case_study_company": null, "tools_mentioned": ["CodeFlower"], "topics": ["codebase analysis", "testing practices", "package structure"], "key_concepts": ["clean package structure", "adequate testing", "complexity in codebases"], "problem_statement": "Comparing the maintainability and complexity of two different e-commerce application codebases", "solution_approach": "Using CodeFlower for visualization to understand and improve codebase structures", "extraction_method": "lm"}}
{"segment_id": "00054", "page_num": 54, "segment": "2.5\n\nBuilding on the system and improving their processes\n\n31\n\nAre you\nmeeting\ncommitments?\n\nHow much\ncode is getting\nbuilt?\n\nHow long does\nit take you to get\nthings right?\n\nHow fast can you\nget changes to your\nconsumers?\n\nHow well\nis your system\nperforming?\n\nProject\ntracking\n\nSource\ncontrol\n\nContinuous\nintegration\n\nDeployment\ntools\n\nApplication\nmonitoring\n\nWhat is your\ncurrent pace?\n\nHow well is the\nteam working\ntogether?\n\nHow are your\ncustomers using\nyour system?\n\nFigure 2.13 By adding data from the build and deployment system, the development team was able to\nadd release dates to their analysis to track how changes they made to their process affected delivery\ntime.\n\nBuilding on the system and improving their processes\nAfter showing how just a few pieces of data can give better insight into the application\nlifecycle, the development team decided to start pulling even more data into the system, expanding the set of reports it could generate. They were looking at how well\ntheir system was performing through their log analysis, and they were measuring team\nperformance through their PTS and SCM systems. One of the main outcomes they\nwanted to effect by improving their velocity was to improve the time it took to deliver\nchanges that improved the application to their consumers. To see how their deployment schedule correlated with their project tracking, they decided to add release data\ninto the mix. Getting CI and deployment data was the next logical step, as highlighted\nin figure 2.13.\n\n Because they used their CI system to release code, they could pull build information from their deploy job and add releases to their analysis. Upon doing that, their\ndashboard started to look like the one shown in figure 2.14.\n\nThe tag icons show releases,\nwith info on mouse-over.\n\nFigure 2.14 Adding releases to the dashboard. Now the team can see how their process affects the\nspeed at which they release code.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "2", "chapter_title": "Observing a live project", "section_num": "2.1.1", "section_title": "How Blastamo Music used agile", "l1_summary": "How well\nis your system\nperforming? How well is the\nteam working\ntogether? How are your\ncustomers using\nyour system?", "l2_summary": "Building on the system and improving their processes How much\ncode is getting\nbuilt? How fast can you\nget changes to your\nconsumers? How well\nis your system\nperforming? How well is the\nteam working\ntogether? How are your\ncustomers using\nyour system?", "prev_page": {"page_num": 53, "segment_id": "00053"}, "next_page": {"page_num": 55, "segment_id": "00055"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["system", "team", "data", "deployment", "figure", "release", "improving", "code", "changes", "well"], "content_type": "practice", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["build and deployment system", "CI system", "PTS", "SCM systems", "application monitoring"], "frameworks": [], "methodologies": ["DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["commitments", "code built", "time to get things right", "changes to consumers", "system performance"], "has_case_study": true, "case_study_company": null, "tools_mentioned": ["build and deployment system", "CI system", "PTS", "SCM systems", "application monitoring"], "topics": ["Improving DevOps processes", "Metrics for tracking application lifecycle", "Integration of CI/CD tools"], "key_concepts": ["release dates", "process improvement", "team performance", "customer usage"], "problem_statement": "Improving the speed and quality of software delivery", "solution_approach": "Implementing a more comprehensive data-driven approach to track and improve DevOps processes", "extraction_method": "lm"}}
{"segment_id": "00055", "page_num": 55, "segment": "32\n\nCHAPTER 2 Observing a live project\n\nThe release data over time could show the development team how the changes in\ntheir processes affected their releases and, conversely if releases impacted the work\nthey performed on every sprint. Now that they had a nice dashboard and were aggregating a lot of great data about how their team was performing, they started using it as\na health monitor for the team.\n\n2.5.1 Using data to improve what they do every day\n\nOnce they were collecting data and could display all of it, they wanted to ensure that\nevery member of the development team was paying attention to it. They made sure\neveryone had the URL to the dashboard in their favorites, and they projected it on\nmonitors when they weren’t being used for meetings. They used the dashboards in\ntheir sprint retrospectives and frequently talked about which metrics would be\naffected by the proposed changes to the process.\n\n They made it all the way around the feedback loop we talked about in chapter 1\n\nand as shown in figure 2.15.\n\n Now that the development team had made the loop once, the next question was\nhow frequently they should go all the way around it. The data collection was automated so they had continuous data that they could react to at any time. But they settled on letting their sprints run their course and did their analysis and application\nbefore each new sprint started.\n\n Before they were collecting and analyzing all of this data, they were only tracking\nvelocity every sprint to make sure their completion rate stayed consistent. The better\nthey got at mining their data, the more insight they obtained into how they were operating as a team and the better they became at making informed decisions on the best\nways to operate. When problems came up, they looked at the data to figure out where\n\n4 Repeat\n\n3 React (Apply):\n\nAdjust your team based\non your findings.\n\n1 Collect Data:\n\nGet as much data\nas you can from your\napplication lifecycle.\n\nUsing their findings they\nconvinced leadership to\nallow them to change\ndirection.\n\nThe team built a Grails\napp that got the data\nthey needed.\n\n2 Measure (Analyze):\nAsk questions, find\ntrends, make\nhypotheses.\n\nFigure 2.15 The feedback loop\nas applied by our case-study team\n\nThey used the ELK stack\nto index, search, and\nvisualize their data.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "2", "chapter_title": "Observing a live project", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "2.5.1 Using data to improve what they do every day The data collection was automated so they had continuous data that they could react to at any time. The team built a Grails\napp that got the data\nthey needed.", "l2_summary": "Now that they had a nice dashboard and were aggregating a lot of great data about how their team was performing, they started using it as\na health monitor for the team. 2.5.1 Using data to improve what they do every day Once they were collecting data and could display all of it, they wanted to ensure that\nevery member of the development team was paying attention to it. They made it all the way around the feedback loop we talked about in chapter 1 The data collection was automated so they had continuous data that they could react to at any time. The team built a Grails\napp that got the data\nthey needed.", "prev_page": {"page_num": 54, "segment_id": "00054"}, "next_page": {"page_num": 56, "segment_id": "00056"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "team", "every", "sprint", "could", "development", "using", "made", "used", "loop"], "content_type": "case_study", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Grails", "ELK stack"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["velocity", "process performance"], "has_case_study": true, "case_study_company": null, "tools_mentioned": ["ELK stack"], "topics": ["Data collection and analysis in DevOps", "Feedback loop implementation", "Improving team processes through data-driven decisions"], "key_concepts": ["feedback loop", "data-driven decision making", "sprint retrospectives", "velocity tracking"], "problem_statement": "How to improve development team performance by collecting, analyzing, and applying data from their processes.", "solution_approach": "Implementing a dashboard for continuous monitoring of team performance metrics and using the insights gained to adjust processes.", "extraction_method": "lm"}}
{"segment_id": "00056", "page_num": 56, "segment": "Summary\n\n33\n\nthe problem really was instead of making snap decisions on where they thought the\nproblem was. Finally, when they wanted to make recommendations to leadership, they\nknew how to communicate their thoughts up the chain in a meaningful way that\ncould effect change.\n\n A positive side effect to this was that they felt free to experiment when it came to\ntheir development process. Because they now had a system that allowed them to measure changes as they made them, they started a new practice in which the team would\nadopt any changes a member suggested, as long as they had could measure their\nhypotheses.\n\n2.6\n\nSummary\n\n In this chapter we followed a team through the implementation of an agile performance metrics system and demonstrated how they used the system. In this extended\ncase study the development team used the following techniques that you can also use\nto start measuring and improving your team:\n\n■ Start with the data you have.\n■ Build on what you have using the frameworks you’re familiar with.\n■ Frame data collection around questions you want to answer.\n■ When you find negative trends, determine solutions that have measureable outcomes.\n\n■ Visualize your data for communication at the right levels:\n■ The more detail an execution team has, the better equipped they will be to\n\nunderstand and react to trends.\n\n■ People outside the team (like leadership teams) typically need rollup data or\ncreative visualizations so they don’t require a history lesson to understand what\nthey are seeing.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "2", "chapter_title": "Observing a live project", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "In this chapter we followed a team through the implementation of an agile performance metrics system and demonstrated how they used the system. In this extended\ncase study the development team used the following techniques that you can also use\nto start measuring and improving your team: ■ Start...", "l2_summary": "the problem really was instead of making snap decisions on where they thought the\nproblem was. A positive side effect to this was that they felt free to experiment when it came to\ntheir development process. In this chapter we followed a team through the implementation of an agile performance metrics system and demonstrated how they used the system. In this extended\ncase study the development team used the following techniques that you can also use\nto start measuring and improving your team: ■ Start with the data you have. ■ Visualize your data for communication at the right levels:\n■ The more detail an execution team has, the better equipped they will be to", "prev_page": {"page_num": 55, "segment_id": "00055"}, "next_page": {"page_num": 57, "segment_id": "00057"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["team", "data", "when", "system", "have", "summary", "problem", "leadership", "could", "effect"], "content_type": "case_study", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["leadership"], "products": [], "technologies": [], "frameworks": ["agile performance metrics system"], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["negative trends", "measureable outcomes"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["agile performance metrics implementation", "team development process improvement", "data-driven decision making"], "key_concepts": ["snap decisions", "meaningful communication", "hypothesis testing", "visual data representation"], "problem_statement": "The team was not effectively communicating their thoughts to leadership and making snap decisions on the problem.", "solution_approach": "Implementing an agile performance metrics system allowed for better communication, hypothesis testing, and visualization of data.", "extraction_method": "lm"}}
{"segment_id": "00057", "page_num": 57, "segment": "34\n\nCHAPTER 2 Observing a live project\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "2", "chapter_title": "Observing a live project", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "CHAPTER 2 Observing a live project Licensed to Mark Watson <nordickan@gmail.com>", "l2_summary": "CHAPTER 2 Observing a live project Licensed to Mark Watson <nordickan@gmail.com>", "prev_page": {"page_num": 56, "segment_id": "00056"}, "next_page": {"page_num": 58, "segment_id": "00058"}, "flags": {"continuity_gap": true}}
{"segment_id": "00058", "page_num": 58, "segment": "Part 2\n\nCollecting and analyzing\nyour team’s data\n\nIn part 1 you learned the concepts of creating and using agile metrics, discovered where to get the data, and observed a team that put the metrics into practice. In part 2 you’ll learn about the details of each data source in the\ndevelopment lifecycle, what each of these data sources can tell you, and how to\nstart collecting and visualizing them. In each chapter you’ll learn how to maximize the use of these systems to provide as much data as possible to gain insight\ninto your team as well as what metrics you can get from each system alone.\n\n Chapter 3 starts with the most common place to collect data about your\nteam, your project tracking system (PTS). We’ll look at different task types, estimations, and workflow metrics.\n\n Chapter 4 shows you the data you can get from your source control management (SCM) system. We’ll look at centralized versus decentralized systems, workflows to use to maximize the data you can get from your systems, and key metrics.\n Chapter 5 shows you the data you can get from your continuous integration\n(CI) and deployment systems. We’ll look at CI, deployments, and automated test\nresults to analyze the different data you can get from various stages in your build\ncycle.\n\n Chapter 6 shows you the data you can get from your application performance monitoring (APM) system. We’ll look at what your production system can\ntell you through different types of data and instrumentation that can give you\nbetter insight into your systems.\n\n Each chapter ends in a case study so you can see how the techniques from\n\nthe chapter can be applied in a real-world scenario.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "2", "chapter_title": "Observing a live project", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "Collecting and analyzing\nyour team’s data Chapter 4 shows you the data you can get from your source control management (SCM) system. Chapter 5 shows you the data you can get from your continuous integration\n(CI) and deployment systems.", "l2_summary": "Collecting and analyzing\nyour team’s data In part 2 you’ll learn about the details of each data source in the\ndevelopment lifecycle, what each of these data sources can tell you, and how to\nstart collecting and visualizing them. Chapter 4 shows you the data you can get from your source control management (SCM) system. We’ll look at centralized versus decentralized systems, workflows to use to maximize the data you can get from your systems, and key metrics. Chapter 5 shows you the data you can get from your continuous integration\n(CI) and deployment systems. Chapter 6 shows you the data you can get from your application performance monitoring (APM) system.", "prev_page": {"page_num": 57, "segment_id": "00057"}, "next_page": {"page_num": 59, "segment_id": "00059"}, "flags": {"continuity_gap": true}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["data", "chapter", "metrics", "each", "systems", "system", "team", "look", "part", "what"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["project tracking system", "source control management", "continuous integration and deployment systems", "application performance monitoring"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["agile metrics", "task types", "estimations", "workflow metrics", "centralized versus decentralized systems", "key metrics", "CI, deployments, automated test results", "production system data"], "has_case_study": true, "case_study_company": null, "tools_mentioned": ["project tracking system", "source control management", "continuous integration and deployment systems", "application performance monitoring"], "topics": ["collecting and analyzing team's data", "data sources in development lifecycle", "metrics from project tracking system", "metrics from source control management", "metrics from continuous integration and deployment systems", "metrics from application performance monitoring"], "key_concepts": ["agile metrics", "project tracking system", "source control management", "continuous integration and deployment systems", "application performance monitoring"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00059", "page_num": 59, "segment": "36\n\nCHAPTER\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "2", "chapter_title": "Observing a live project", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "Licensed to Mark Watson <nordickan@gmail.com>", "l2_summary": "Licensed to Mark Watson <nordickan@gmail.com>", "prev_page": {"page_num": 58, "segment_id": "00058"}, "next_page": {"page_num": 60, "segment_id": "00060"}, "flags": {"continuity_gap": true}}
{"segment_id": "00060", "page_num": 60, "segment": "Trends and data from\nproject-tracking systems\n\nThis chapter covers\n\n■ What information raw data from a project\n\ntracking system conveys\n\n■ How to utilize your PTS to collect the right data\n\nto give insight into your process\n\n■ How to get data from your PTS into your\n\nmetrics-collection system\n\n■ Trends you can learn from data in your PTS\n\n■ What you’re missing by relying only on projecttracking data\n\nPerhaps the most obvious place to find data that points to your team’s performance\nis your PTS. Common PTSs include JIRA, Trello, Rally, Axosoft On Time Scrum, and\nTelerik TeamPulse. This is where tasks are defined and assigned, bugs are entered\nand commented on, and time is associated with estimates and real work. Essentially\nyour project’s PTS is the intersection between time and the human-readable definition of your work. Figure 3.1 shows where this system lives in the scope of your\napplication delivery lifecycle.\n\n37\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "2", "chapter_title": "Observing a live project", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "Trends and data from\nproject-tracking systems ■ How to get data from your PTS into your ■ Trends you can learn from data in your PTS", "l2_summary": "Trends and data from\nproject-tracking systems ■ What information raw data from a project tracking system conveys ■ How to utilize your PTS to collect the right data ■ How to get data from your PTS into your ■ Trends you can learn from data in your PTS", "prev_page": {"page_num": 59, "segment_id": "00059"}, "next_page": {"page_num": 61, "segment_id": "00061"}, "flags": {"continuity_gap": true}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "project", "system", "time", "trends", "tracking", "what", "where", "work"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": ["JIRA", "Trello", "Rally", "Axosoft On Time Scrum", "Telerik TeamPulse"], "people": [], "products": [], "technologies": [], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["process insight", "performance"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["JIRA", "Trello", "Rally", "Axosoft On Time Scrum", "Telerik TeamPulse"], "topics": ["project-tracking systems", "data collection", "metrics and trends"], "key_concepts": ["raw data from PTS", "utilizing PTS for insight", "getting data into metrics-collection system", "learning trends from PTS data"], "problem_statement": "Relying only on project tracking data", "solution_approach": "Collecting the right data to give insight into your process", "extraction_method": "lm"}}
{"segment_id": "00061", "page_num": 61, "segment": "38\n\nCHAPTER 3 Trends and data from project-tracking systems\n\nManage\ntasks and\nbugs\n\nProject\ntracking\n\nManage\ncode and\ncollaboration\n\nGenerate\nbuilds and\nrun tests\n\nMove code\nacross\nenvironments\n\nSource\ncontrol\n\nContinuous\nintegration\n\nDeployment\ntools\n\nEnsure\neverything\nis working\n\nApplication\nmonitoring\n\nYou are here\n\nFigure 3.1 The first data source in your application lifecycle is your PTS. This is where tasks are\ndefined and assigned, bugs are managed, and time is associated with tasks.\n\nTrends that can be found in your PTS can go a long way in showing you how your team\nis performing. It’s typically the only system that teams will use to track their progress,\nso it’s a good place for us to start.\n\n Because your PTS is tracking time and tasks, you can use it to answer the following\n\nquestions:\n\n■ How well does your team understand the project?\n■ How fast is the team moving?\n■ How consistent is the team in completing work?\n\nBut some of these still end up leaving a lot to interpretation and could be clarified\nwith additional data. Here are a few examples:\n\n■ How hard is the team working? Although you can partially get this information\nfrom your PTS, you should supplement it with source-control data to get a better picture.\n\n■\n\n■ Who are the top performers on the team? Data from your PTS is only a part of the picture here; you should combine this with source-control and potentially production-monitoring data.\nIs your software built well? The question of quality comes mostly from other\nsources of data like test results, monitoring metrics, and source control, but PTS\ndata does support these questions by showing how efficient a team is at creating\nquality products.\n\nTo enable you to generate the most meaningful possible analyses from your data, first\nI’ll lay out guidelines to help you collect quality data to work with.\n\nA word on estimates\n\nIn this chapter and throughout this book you’ll notice that I talk a lot about estimating\nwork. For the agile team, estimating work goes beyond trying to nail down better predictability; it also encourages the team to understand the requirements in greater\ndepth, think harder about what they’re building before they start, and have a greater\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "3", "chapter_title": "Trends and data from project-tracking systems", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "CHAPTER 3 Trends and data from project-tracking systems Figure 3.1 The first data source in your application lifecycle is your PTS. ■ How hard is the team working?", "l2_summary": "CHAPTER 3 Trends and data from project-tracking systems Figure 3.1 The first data source in your application lifecycle is your PTS. ■ How well does your team understand the project? ■ How fast is the team moving? ■ How consistent is the team in completing work? ■ How hard is the team working?", "prev_page": {"page_num": 60, "segment_id": "00060"}, "next_page": {"page_num": 62, "segment_id": "00062"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "team", "source", "tasks", "control", "work", "project", "tracking", "monitoring", "here"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["PTS", "source-control", "test results", "monitoring metrics"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["team performance", "efficiency", "quality"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["PTS", "source-control", "test results", "monitoring metrics"], "topics": ["project tracking systems", "team performance metrics", "software development lifecycle"], "key_concepts": ["PTS", "task management", "bug tracking", "code collaboration", "continuous integration", "deployment tools", "application monitoring"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00062", "page_num": 62, "segment": "Typical agile measurements using PTS data\n\n39\n\ninvestment in what they’re building by committing to a timeframe with their peers.\nYou’ll notice that estimations are a part of the case studies at the end of several chapters and are used in combination with other data to see a broader picture of your team’s\noperation. I encourage all of my development teams to estimate their work for all of\nthese reasons, and I encourage you to consider estimating work if you’re not doing\nit already.\n\n3.1\n\nTypical agile measurements using PTS data\nBurn down and velocity are the two most commonly used metrics to track agile teams\nand both are derived from estimations. Burn down is the amount of work done over\ntime, which can be derived by plotting the total number of incomplete estimates or\ntasks over time next to the average number of complete estimates or tasks over time.\nWith burn down you typically have a time box that everything is supposed to be done\nwithin, and you can see how close you are to completing all the tasks you’ve committed to. If you simply toe the line and use your PTS out of the box, you’ll get burn down\nand velocity as standard agile reports.\n\n3.1.1 Burn down\n\nAn example burn down chart is shown in figure 3.2. In the figure the guideline represents the ideal scenario where your estimates burn down over time; the remaining values represent the actual number of tasks that are not closed in the current timeline.\n\nThe mathematically perfect\nline is just a guideline; it will\nnever be parallel with actuals.\n\nThis team got most of their\nwork done in the last few\ndays of their sprint.\n\nRemaining\nvalues\n\nGuideline\n\nNon-working\ndays\n\ns\nt\n\ni\n\nn\no\np\ny\nr\no\nt\nS\n\n120\n\n100\n\n80\n\n60\n\n40\n\n20\n\n0\n\nJun 19\n\nJun 20\n\nJun 23\n\nJun 24\n\nJun 25\n\nJun 26\n\nJun 27\n\nJun 30\n\nJul 1\n\nFigure 3.2 An example burn down chart\n\nTime\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 4, "cols": 2, "data": [["", ""], ["", ""], ["", ""], ["", ""]], "markdown": "|  |  |\n|---|---|\n|  |  |\n|  |  |\n|  |  |"}, {"table_id": "table_2", "rows": 2, "cols": 2, "data": [["", ""], ["", ""]], "markdown": "|  |  |\n|---|---|\n|  |  |"}], "table_count": 2, "chapter_num": "3", "chapter_title": "Trends and data from project-tracking systems", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "Typical agile measurements using PTS data An example burn down chart is shown in figure 3.2. Figure 3.2 An example burn down chart", "l2_summary": "Typical agile measurements using PTS data Typical agile measurements using PTS data\nBurn down and velocity are the two most commonly used metrics to track agile teams\nand both are derived from estimations. Burn down is the amount of work done over\ntime, which can be derived by plotting the total number of incomplete estimates or\ntasks over time next to the average number of complete estimates or tasks over time. If you simply toe the line and use your PTS out of the box, you’ll get burn down\nand velocity as standard agile reports. An example burn down chart is shown in figure 3.2. Figure 3.2 An example burn down chart", "prev_page": {"page_num": 61, "segment_id": "00061"}, "next_page": {"page_num": 63, "segment_id": "00063"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["burn", "down", "time", "agile", "work", "tasks", "data", "done", "number", "estimates"], "content_type": "tutorial", "domain": "programming|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["PTS"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["burn down", "velocity"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["PTS"], "topics": ["Agile measurements", "Burn down chart", "Velocity metric"], "key_concepts": ["estimations", "timeboxing", "work tracking"], "problem_statement": "Improving team productivity and project management through agile metrics", "solution_approach": "Using PTS for burn down and velocity tracking", "extraction_method": "lm"}}
{"segment_id": "00063", "page_num": 63, "segment": "40\n\nCHAPTER 3 Trends and data from project-tracking systems\n\nBurn down is very popular, but its value is limited due to the empirical nature of agile\ndevelopment. You will never have a smooth curve, and even though it’s used as a\nguideline, its value is overestimated. So much so in fact that many Scrum coaches are\ndeprecating the use of the burn down chart, and its use on agile teams is starting to\ndiminish. Although it does provide a guideline to where you are against your commitment for a time period, burn down alone can’t give you the whole story.\n\n3.1.2\n\nVelocity\n\nVelocity is a relative measurement that tracks the consistency of a team’s completed\nestimates over time. The idea with velocity is that a team should be able to consistently\ncomplete their work as they define it. An example velocity chart is shown in figure 3.3.\n Velocity can be calculated by graphing the amount you estimated against the\namount you actually got done. Velocity ends up implying a few things about your team:\n\n■ How good your team is at estimating work\n■ How consistently your team gets work done\n■ How consistently your team can make and meet commitments\n\nEven though velocity is a staple agile metric, its relative nature makes it difficult to pinpoint where problems really are. If velocity is inconsistent, there could be a number of\nissues, such as:\n\n■ Your team is not doing a good job at estimating.\n■ Your team has commitment issues.\n\nThe commitment represents\nwhat the team thought they\ncould accomplish.\n\nThe team strives for\nconsistent estimation\nand completion.\n\nCommitment\n\nCompleted\n\ns\nt\n\ni\n\nn\no\np\ny\nr\no\nt\nS\n\n140\n\n120\n\n100\n\n80\n\n60\n\n20\n\n0\n\nSprint 1\nSample tray\n\nSprint 2\nGrowler\n\nSprint 3\nKeg\n\nSprint 4\nPub crawl\n\nSprint 5\n99 bottles\n\nSprint 6\nThe bar\n\nFigure 3.3 Velocity example\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 11, "cols": 17, "data": [["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""]], "markdown": "|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |"}], "table_count": 1, "chapter_num": "3", "chapter_title": "Trends and data from project-tracking systems", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "Velocity ends up implying a few things about your team: ■ Your team has commitment issues. Figure 3.3 Velocity example", "l2_summary": "Velocity is a relative measurement that tracks the consistency of a team’s completed\nestimates over time. The idea with velocity is that a team should be able to consistently\ncomplete their work as they define it. An example velocity chart is shown in figure 3.3. Velocity ends up implying a few things about your team: ■ Your team has commitment issues. Figure 3.3 Velocity example", "prev_page": {"page_num": 62, "segment_id": "00062"}, "next_page": {"page_num": 64, "segment_id": "00064"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["team", "velocity", "sprint", "commitment", "burn", "down", "agile", "consistently", "work", "value"], "content_type": "theory", "domain": "programming|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Scrum", "Agile"], "frameworks": [], "methodologies": ["Agile", "Scrum"], "programming_languages": [], "has_best_practices": false, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["Velocity", "Burn down"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Agile development", "Scrum methodology", "Burn down chart", "Velocity metric"], "key_concepts": ["Empirical nature of agile development", "Limitations of burn down charts", "Importance of velocity in team consistency", "Estimation and commitment issues"], "problem_statement": "The limitations and challenges associated with using burn down charts and the importance of considering other metrics like velocity.", "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00064", "page_num": 64, "segment": "Typical agile measurements using PTS data\n\n41\n\n■ Scope could be changing in the middle of your development cycle.\n■ You may be hitting troublesome tech debt.\n\nThe key data point behind velocity that we’ll be working with is the estimates your\nteam puts on their tasks. We’ll be diving into estimations later in the chapter and\nbreaking them down against other data points in chapter 7.\n\n3.1.3\n\nCumulative flow\n\nCumulative flow shows how much work aggregated by type is allocated to your team\nover time. The cumulative flow diagram is typically used for identifying bottlenecks in\nthe process by visually representing when a type of task is increasing faster than the\nothers. In figure 3.4 you see an example cumulative flow diagram where a team is\ncompleting hardly any work and suddenly their to-do list jumps off the charts.\n\n As with other charts, this tells only a part of the picture. It’s possible that tasks were\nsimply too big and the team started breaking them down into smaller chunks. It’s also\npossible that this team is struggling to complete work and they end up getting more\nwork piled on as an unrealistic task.\n\n Cumulative flow can be useful when you use the correct task types for your work. If\nyou have tasks for integration and you see that part of your graph starts to get fat, you\nknow there’s a bottleneck somewhere in your integration.\n\nThis team isn't\nfinishing anything.\n\nSuddenly they're getting\nasked to do a lot more.\n\nTo do\n\nIn progress\n\nDone\n\ns\ne\nu\ns\ns\n\ni\n\nf\n\no\nr\ne\nb\nm\nu\nN\n\n1000\n\n900\n\n800\n\n700\n\n600\n\n500\n\n400\n\n300\n\n200\n\n100\n\n0\n\nJan 16\n\nJan 24\n\nFeb 1\n\nFeb 8\n\nFeb15\n\nFeb 22\n\nFigure 3.4 An example cumulative flow diagram showing a team getting asked to do a lot more than they\nhave been able to accomplish historically\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "3", "chapter_title": "Trends and data from project-tracking systems", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "Cumulative flow shows how much work aggregated by type is allocated to your team\nover time. In figure 3.4 you see an example cumulative flow diagram where a team is\ncompleting hardly any work and suddenly their to-do list jumps off the charts. Figure 3.4 An example cumulative flow diagram showing a...", "l2_summary": "Cumulative flow shows how much work aggregated by type is allocated to your team\nover time. In figure 3.4 you see an example cumulative flow diagram where a team is\ncompleting hardly any work and suddenly their to-do list jumps off the charts. It’s also\npossible that this team is struggling to complete work and they end up getting more\nwork piled on as an unrealistic task. Cumulative flow can be useful when you use the correct task types for your work. This team isn't\nfinishing anything. Figure 3.4 An example cumulative flow diagram showing a team getting asked to do a lot more than they\nhave been able to accomplish historically", "prev_page": {"page_num": 63, "segment_id": "00063"}, "next_page": {"page_num": 65, "segment_id": "00065"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["team", "cumulative", "flow", "work", "data", "tasks", "diagram", "task", "getting", "more"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": false, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["velocity", "cumulative flow"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Agile measurements", "Velocity estimation", "Cumulative flow diagram"], "key_concepts": ["scope changes", "technical debt", "estimations", "bottlenecks", "task types"], "problem_statement": "Identifying issues in the development process such as changing scope and technical debt.", "solution_approach": "Using velocity estimation and cumulative flow diagrams to monitor and address these issues.", "extraction_method": "lm"}}
{"segment_id": "00065", "page_num": 65, "segment": "42\n\nCHAPTER 3 Trends and data from project-tracking systems\n\n3.1.4\n\nLead time\n\nLead time is the amount of time between when a task is started and when it is completed. Teams that practice Kanban focus on throughput, and as a result they usually\nput a heavy emphasis on lead time; the faster they can move tasks through the pipes,\nthe better throughput they can achieve. Some Scrum teams use lead time as well as\nvelocity because the two metrics tell you different things; velocity tells you how much\nyour team can get done in relation to what they think they can get done, and lead\ntime tells you how long it takes to get things done.\n\n The problem with lead time alone is that often teams don’t pay close attention to\nwhat composes it. Lead time includes creating a task, defining it, working on it, testing\nit, and releasing it. If you can’t decompose it, then it’s hard to figure out how to\nimprove it. We’ll explore decomposing lead time in later chapters.\n\n For teams that are simply receiving small tickets and pushing them through the system, lead time is a good indicator of how efficient the team can be. But if you have a\nteam that has ownership of the product they’re building and is vested in the design of\nthat product, you usually don’t have such a simple delivery mechanism. In fact, with\nthe state of automation you could even argue that if your tasks are so small that you\ndon’t have to estimate them, then maybe you don’t need a human to do them.\n\n Lead time becomes a very valuable metric if you’re practicing CD and can measure\nall of the parts of your delivery process to find efficiencies. It’s a great high-level efficiency indicator of the overall process your team follows.\n\n3.1.5 Bug counts\n\nBugs represent inconsistencies in your software. Different teams will have different\ndefinitions of what a bug is, ranging from a variation from your application’s spec to\nunexpected behaviors that get noticed after development is complete.\n\n Bugs will pop up at different times depending on how your team works together.\nFor example, a team with embedded quality engineers may not generate bugs before\na feature is complete because they’re working side by side with feature engineers and\nthey work out potential problems before they’re published. A team with an offshore\nQA team will end up generating tickets for bugs because testing is staggered with feature development.\n\n Figure 3.5 shows two different charts that track bugs over time. Because bugs represent things you don’t want your software to do, teams strive for low bug counts as an\nindicator of good software quality.\n\n Bugs can also be broken down by severity. A noncritical bug might be that you\ndon’t have the right font on the footer of a website. A critical or blocker bug may be\nthat your application crashes when your customers log into your application. You\nmight be willing to release software that has a high bug count if all the bugs are of a\nlow severity.\n\n The inconsistency with bugs is that severity is also a relative term. To a designer the\nwrong font in the footer may be a critical bug, but to a developer concerned with\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "3", "chapter_title": "Trends and data from project-tracking systems", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "Lead time is the amount of time between when a task is started and when it is completed. We’ll explore decomposing lead time in later chapters. Figure 3.5 shows two different charts that track bugs over time.", "l2_summary": "Lead time is the amount of time between when a task is started and when it is completed. The problem with lead time alone is that often teams don’t pay close attention to\nwhat composes it. We’ll explore decomposing lead time in later chapters. For teams that are simply receiving small tickets and pushing them through the system, lead time is a good indicator of how efficient the team can be. Bugs represent inconsistencies in your software. Figure 3.5 shows two different charts that track bugs over time.", "prev_page": {"page_num": 64, "segment_id": "00064"}, "next_page": {"page_num": 66, "segment_id": "00066"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["time", "lead", "bugs", "team", "teams", "different", "have", "because", "software", "when"], "content_type": "theory", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Kanban", "Scrum", "CD"], "frameworks": [], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["lead time", "bug counts"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Lead Time", "Bug Counts"], "key_concepts": ["lead time", "throughput", "velocity", "continuous delivery", "bug severity"], "problem_statement": "Improving efficiency and quality in software development processes", "solution_approach": "Using metrics like lead time and bug counts to identify areas for improvement", "extraction_method": "lm"}}
{"segment_id": "00066", "page_num": 66, "segment": "Typical agile measurements using PTS data\n\n43\n\nEscaped bugs\n\nNumber of bugs that weren’t\nfound in the development cycle\n\nCompleted bugs\n\nOpen bugs\n\ns\ne\nu\ns\ns\nI\n\n3\n\n2\n\n1\n\n0\n\nWeek 50, 2014\nWeek 41, 2014\nWeek 40, 2014\nWeek 43, 2014\nWeek 47, 2014\nWeek 46, 2014\nWeek 45, 2014\nWeek 39, 2014\nWeek 38, 2014\nWeek 37, 2014\nWeek 49, 2014\nWeek 48, 2014\nWeek 44, 2014\nWeek 42, 2014\n\nTotal issues: 11\nPeriod: last 90 days\n(grouped weekly)\n\nFound bugs\n\nNumber of bugs that were\nfound in the development cycle\n\nCompleted bugs\n\nOpen bugs\n\n200\n\n150\n\ns\ne\nu\ns\ns\nI\n\n100\n\n50\n\n0\n\nWeek 50, 2014\nWeek 49, 2014\nWeek 47, 2014\nWeek 46, 2014\nWeek 45, 2014\nWeek 44, 2014\nWeek 41, 2014\nWeek 38, 2014\nWeek 37, 2014\nWeek 43, 2014\nWeek 48, 2014\nWeek 42, 2014\nWeek 40, 2014\nWeek 39, 2014\n\nTotal issues: 1616\nPeriod: last 90 days\n(grouped weekly)\n\nFigure 3.5 Two charts showing bug counts and at what point they were found\n\nfunctionality, maybe it shouldn’t even be a bug in the first place. In this case, severity is\nin the eye of the beholder.\n\n The charts and graphs you’ve seen up to this point are all useful in their own right\neven though they measure relative data. To go beyond the standard PTS graphs there\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 6, "cols": 11, "data": [["", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", ""]], "markdown": "|  |  |  |  |  |  |  |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|---|---|\n|  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |"}, {"table_id": "table_2", "rows": 14, "cols": 25, "data": [["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""]], "markdown": "|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |"}], "table_count": 2, "chapter_num": "3", "chapter_title": "Trends and data from project-tracking systems", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "Week 50, 2014\nWeek 41, 2014\nWeek 40, 2014\nWeek 43, 2014\nWeek 47, 2014\nWeek 46, 2014\nWeek 45, 2014\nWeek 39, 2014\nWeek 38, 2014\nWeek 37, 2014\nWeek 49, 2014\nWeek 48, 2014\nWeek 44, 2014\nWeek 42, 2014 Total issues: 11\nPeriod: last 90 days\n(grouped weekly) Week 50, 2014\nWeek 49, 2014\nWeek 47, 2014\nWeek...", "l2_summary": "Number of bugs that weren’t\nfound in the development cycle Week 50, 2014\nWeek 41, 2014\nWeek 40, 2014\nWeek 43, 2014\nWeek 47, 2014\nWeek 46, 2014\nWeek 45, 2014\nWeek 39, 2014\nWeek 38, 2014\nWeek 37, 2014\nWeek 49, 2014\nWeek 48, 2014\nWeek 44, 2014\nWeek 42, 2014 Total issues: 11\nPeriod: last 90 days\n(grouped weekly) Number of bugs that were\nfound in the development cycle Week 50, 2014\nWeek 49, 2014\nWeek 47, 2014\nWeek 46, 2014\nWeek 45, 2014\nWeek 44, 2014\nWeek 41, 2014\nWeek 38, 2014\nWeek 37, 2014\nWeek 43, 2014\nWeek 48, 2014\nWeek 42, 2014\nWeek 40, 2014\nWeek 39, 2014 Total issues: 1616\nPeriod: last 90 days\n(grouped weekly)", "prev_page": {"page_num": 65, "segment_id": "00065"}, "next_page": {"page_num": 67, "segment_id": "00067"}, "flags": {"continuity_gap": true}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["week", "2014", "bugs", "found", "data", "number", "development", "cycle", "completed", "open"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["PTS"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": false, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["Escaped bugs", "Found bugs"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["PTS"], "topics": ["Agile measurements", "Bug tracking"], "key_concepts": ["Escaped bugs", "Completed bugs", "Open bugs"], "problem_statement": "Tracking and measuring bug management in the development cycle", "solution_approach": "Using PTS data to track escaped and found bugs over time", "extraction_method": "lm"}}
{"segment_id": "00067", "page_num": 67, "segment": "44\n\nCHAPTER 3 Trends and data from project-tracking systems\n\n3.2\n\nare some best practices you can follow to ensure you have data that’s enriched for\nanalysis and ultimately for combination with other data sources.\n\nPrepare for analysis; generate the richest set of data you can\nAs you collect data, keep in mind that your analysis will only be as good as the data you\ncollect; how your team uses your PTS will translate into what you can glean from the\ndata in the system. To set yourself and your team up for success by getting the most\nmeaningful analysis from this data, this section offers tips on working with your PTS to\nhelp ensure you’re generating as much useful data as possible from it.\n\n Most APIs will give you something like a list of work items based on the query you\nmake, and each work item will have many pieces of data attached to it. The data you\nget back will give you information such as:\n\n■ Who is working on what\n■ When things get done in a good way\n\nThose two statements are general and loaded with hidden implications. We break\nthem down in figure 3.6.\n\n You should look for the following pieces of data in your tasks so you can index\n\nthem, search them, and aggregate them for better analysis:\n\n■ User data.\n■ Who worked on the task?\n■ Who assigned the task?\n■ Who defined the task?\n■ Time data.\n■ When was the task created?\n■ When was it complete?\n■ What was the original estimate of the task?\n\nThe members on\nyour team who get\nassigned things\n\nThe same as\nTHINGS---anything\nthat is assignable\n\nWHO is working on WHAT\n\nWHEN THINGS get DONE in a GOOD way\n\nStart/end\ndates and\nestimates\n\nTasks, bugs,\nor anything that\ncan get assigned\nand tracked\n\nEnd dates\naffected by\nthe definition\nof DONE\n\nThe team was\nhappy with what it\ntook to get there\n\nFigure 3.6 Parsing the loaded words that tell you how PTS is used\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "3", "chapter_title": "Trends and data from project-tracking systems", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "CHAPTER 3 Trends and data from project-tracking systems The data you\nget back will give you information such as: ■ Who assigned the task?", "l2_summary": "CHAPTER 3 Trends and data from project-tracking systems The data you\nget back will give you information such as: ■ Who is working on what\n■ When things get done in a good way ■ Who worked on the task? ■ Who assigned the task? WHO is working on WHAT", "prev_page": {"page_num": 66, "segment_id": "00066"}, "next_page": {"page_num": 68, "segment_id": "00068"}, "flags": {"continuity_gap": true}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "analysis", "will", "what", "task", "team", "when", "things", "good", "working"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["KPIs for data analysis"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Project-tracking systems", "Data enrichment for analysis", "Best practices for data collection and analysis"], "key_concepts": ["Preparation for analysis", "Rich data generation", "User, time, and task data indexing"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00068", "page_num": 68, "segment": "Prepare for analysis; generate the richest set of data you can\n\n45\n\nWho, what, and when are the building blocks that we’re going to focus on in this\nchapter. Some raw response data is demonstrated in the following listing.\n\nListing 3.1 Excerpts from the raw data in a typical API response\n\nWho\n\nWhat\n\n{\n \"issues\": [\n {\n \"key\": \"AAA-3888\",\n \"fields\": {\n \"created\": \"2012-01-19T14:50:03.000+0000\",\n \"creator\": {\n \"name\": \"jsmit1\",\n \"emailAddress\": \"Joseph.Smith@blastamo.com\",\n \"displayName\": \"Smith, Joseph\",\n },\n \"estimate\": 8,\n \"assignee\": {\n \"name\": \"jsmit1\",\n \"emailAddress\": \"Joseph.Smith@nike.com\",\n \"displayName\": \"Smith, Joseph\",\n },\n \"issuetype\": {\n \"name\": “Task”,\n \"subtask\": false\n },\n...\n\nWhat\n\nWho\n\nWhen\n\n3.2.1\n\nTip 1: Make sure everyone uses your PTS\n\nThis first tip may seem obvious if you want to ensure you can get data from your system: make your PTS your one source of truth. Even though many teams use web-based\nproject tracking, you’ll still often see sticky notes on walls, email communication\naround issues, spreadsheets for tracking tasks and test cases, and other satellite systems that help you manage your work. All of these other systems generate noise and\nconfusion and detract from your ability to get data that you can work with. If you’ve\ninvested in a PTS, use it as the single source of truth for your development teams if you\nreally want to mine data around how your team is functioning. Otherwise, the effort\nof collecting all the data from disparate sources becomes a tedious, manual, and timeconsuming effort. It’s best to spend your time analyzing data rather than collecting it.\n Using your PTS is especially relevant when you have multiple teams that aren’t\ncolocated. Agile tends to work best with teams that are working together in the same\nplace; it makes communication and collaboration much easier and allows everyone to\ncontribute during planning, estimation, and retrospective meetings. But it’s not\nalways possible to have teams in the same place, and many companies have development teams spread around the world. If this is the case, try to ensure that everyone is\nusing the same system and using it in the same way.\n\n To be able to track everything your team is doing, you should have a task for everything. That doesn’t mean you should require cards for bathroom breaks, but you do\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "3", "chapter_title": "Trends and data from project-tracking systems", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "Prepare for analysis; generate the richest set of data you can Listing 3.1 Excerpts from the raw data in a typical API response Tip 1: Make sure everyone uses your PTS", "l2_summary": "Prepare for analysis; generate the richest set of data you can Some raw response data is demonstrated in the following listing. Listing 3.1 Excerpts from the raw data in a typical API response Tip 1: Make sure everyone uses your PTS This first tip may seem obvious if you want to ensure you can get data from your system: make your PTS your one source of truth. Using your PTS is especially relevant when you have multiple teams that aren’t\ncolocated.", "prev_page": {"page_num": 67, "segment_id": "00067"}, "next_page": {"page_num": 69, "segment_id": "00069"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["data", "teams", "joseph", "smith", "have", "same", "what", "when", "name", "everyone"], "content_type": "tutorial", "domain": "programming|management", "complexity": "intermediate", "companies": ["Blastamo", "Nike"], "people": ["Joseph Smith"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["PTS"], "topics": ["data collection", "team collaboration", "Agile methodology"], "key_concepts": ["single source of truth", "task management", " PTS usage"], "problem_statement": "Collecting and managing data from multiple sources in a development team", "solution_approach": "Using a Project Tracking System (PTS) as the single source of truth for task management and team collaboration", "extraction_method": "lm"}}
{"segment_id": "00069", "page_num": 69, "segment": "46\n\nCHAPTER 3 Trends and data from project-tracking systems\n\nneed to identify meaningful tasks and ensure you have the ability to create tasks in\nyour system for them.\n\n When creating a task, the most important thing to think about is that it should\nnever take more than a few days to accomplish. If you find your team estimating a task\nat more than a few days of work, that typically means they don’t understand it enough\nto put real time on it or it’s too big to be considered a single task. I’ve seen that when\nteams estimate their work to take more than a few days, those tasks tend to be underestimated more than 50% of the time.\n\n Whatever you’re working on needs to be broken down into small chunks so that\nyou can track the completion of the work in a meaningful way and so that the development team understands what it will take to move tasks over the line.\n\n3.2.2\n\nTip 2: Tag tasks with as much data as possible\n\nTask types should be pretty simple; you have a task either for new work or for fixing a\nbug in something that’s already done. Getting any fancier than that is a great way to\nconfuse your team, so it’s best to keep it simple. Tagging is a great way to add data that\nyou may want to use for analysis later on that isn’t part of the default data of a task. By\ndefault every task should have a description, start and end dates, the person to whom\nit’s assigned, an estimate and the project to which it belongs. Other potential, useful\ndata for which you may have no fields, include a campaign with a feature, team name,\nproduct names, and target release. Just as you can create hash tags for anything you\ncan think of #justfortheheckofit on multiple social media platforms, you can do the\nsame in your PTS.\n\n The alternative to adding tags in most systems is creating custom fields. This is the\nequivalent of adding columns to a flat database table; the more fields you add, the\nmore unwieldy it becomes to work with. Teams are forced into using the structure that\nexists, which may not translate into how they work. When new people join the team or\nwhen new efforts spin up, it becomes cumbersome to add all the data required in the\nsystem just to create tasks. This is visualized in figure 3.7.\n\n Adding data in the form of tags allows you to add the data you need when you\nneed it and allows you to return to tasks once they’re complete to add any kind of data\nyou want to report on. This is valuable because it helps you figure out what works well\n\nWould you rather fill out\nall of these boxes?\n\nOr just add tags indicating\nthe data that you care about?\n\nGood\n\nRefactor\n\nvs.\n\nYolo project\n\nSprint 42\n\nFigure 3.7 The difference between organizing flat project data and adding relevant metadata with tags\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 3, "cols": 38, "data": [["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""]], "markdown": "|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |"}], "table_count": 1, "chapter_num": "3", "chapter_title": "Trends and data from project-tracking systems", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "CHAPTER 3 Trends and data from project-tracking systems I’ve seen that when\nteams estimate their work to take more than a few days, those tasks tend to be underestimated more than 50% of the time. Tip 2: Tag tasks with as much data as possible", "l2_summary": "CHAPTER 3 Trends and data from project-tracking systems need to identify meaningful tasks and ensure you have the ability to create tasks in\nyour system for them. I’ve seen that when\nteams estimate their work to take more than a few days, those tasks tend to be underestimated more than 50% of the time. Tip 2: Tag tasks with as much data as possible Tagging is a great way to add data that\nyou may want to use for analysis later on that isn’t part of the default data of a task. Or just add tags indicating\nthe data that you care about?", "prev_page": {"page_num": 68, "segment_id": "00068"}, "next_page": {"page_num": 70, "segment_id": "00070"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "tasks", "task", "more", "work", "when", "than", "team", "tags", "project"], "content_type": "tutorial", "domain": "management|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": true, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["project-tracking systems"], "topics": ["task management", "agile methodology", "task estimation"], "key_concepts": ["small tasks", "task tagging", "estimation accuracy"], "problem_statement": "Managing and estimating tasks in project-tracking systems effectively", "solution_approach": "Breaking down work into small, manageable tasks; using tags for additional data; ensuring accurate task estimations", "extraction_method": "lm"}}
{"segment_id": "00070", "page_num": 70, "segment": "Prepare for analysis; generate the richest set of data you can\n\n47\n\nfor your team even if it’s contrary to normal trends. For example, the number of comments on a ticket could mean that your team is communicating well, but it could also\nmean that the original requirements were very poor and they were flushed out\nthrough comments on the card. By going back and marking cards as “worked well” or\n“train wreck,” for example, you can find trends that may not have been visible before.\n In addition to the other good reasons for using tags, this method of organizing your\ntasks allows you to sort, aggregate, or map-reduce your data much easier. Instead of\nmodifying queries and domain objects to get good analysis, you can simply add more\ntags, and you’ll be able to see clear trends in your data, as you’ll see later in this chapter.\n Some PTSs like JIRA support tags or labels and have explicit fields for them. In JIRA\nthe data comes back in a format that’s nice and easy to work, as shown in the next listing.\n\nListing 3.2 Example labels JSON block in a JIRA API response\n\nlabels:\n[\n \"Cassandra\",\n \"Couchbase\",\n \"TDM\"\n],\n\nEach element represents\na separate tag\n\nOther PTSs may not have this feature, but that doesn’t have to stop you. You can get\ninto the habit of hashtagging your cards to ensure they can be grouped and analyzed\nlater on. You may end up getting data like that shown in the following listing.\n\nListing 3.3 Example comments JSON block from an API call with hashtags in the text\n\ncomments: [\n {\n body: \"To figure out the best way to test this mobile app we should\nfirst figure out if we're using calabash or robotium. #automation\n#calabash\n#poc #notdefined\"\n\n }\n]\n\nHashtags used to note\ndetails for later analysis\n\nIf this is the data you have, that’s fine. This may not be as simple to parse as the previous listing, but there are hash tags that you can parse and reduce into meaningful\ndata that can be used later.\n\n3.2.3\n\nTip 3: Estimate how long you think your tasks will take\n\nEstimating how long tasks will take can be pretty tricky in an agile world. It can be\ntough to figure out how long something can take when definitions are often in flux\nand products are constantly changing. Estimates are important from a project management perspective because they will eventually allow you to predict how much work\nyou can get done in a certain amount of time. They also show you if your team really\nunderstands the work that they’re doing.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "3", "chapter_title": "Trends and data from project-tracking systems", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "Prepare for analysis; generate the richest set of data you can You can get\ninto the habit of hashtagging your cards to ensure they can be grouped and analyzed\nlater on. If this is the data you have, that’s fine.", "l2_summary": "Prepare for analysis; generate the richest set of data you can Listing 3.2 Example labels JSON block in a JIRA API response You can get\ninto the habit of hashtagging your cards to ensure they can be grouped and analyzed\nlater on. Listing 3.3 Example comments JSON block from an API call with hashtags in the text If this is the data you have, that’s fine. This may not be as simple to parse as the previous listing, but there are hash tags that you can parse and reduce into meaningful\ndata that can be used later.", "prev_page": {"page_num": 69, "segment_id": "00069"}, "next_page": {"page_num": 71, "segment_id": "00071"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["data", "have", "listing", "example", "comments", "tags", "later", "analysis", "team", "trends"], "content_type": "tutorial", "domain": "management|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": ["JIRA"], "technologies": ["Cassandra", "Couchbase", "TDM", "calabash", "robotium"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["clear trends in data"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["JIRA"], "topics": ["task organization", "tagging tasks", "estimating task duration"], "key_concepts": ["tags/labels for organizing tasks", "analyzing data through tags", "estimation in agile environments"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00071", "page_num": 71, "segment": "48\n\nCHAPTER 3 Trends and data from project-tracking systems\n\n One thing that seems to always challenge teams is what estimations mean. Story\npoints are supposed to relate to effort, yet every team I’ve ever been on ends up translating those points to time, so in most cases I find it easier for teams to use a timebased estimation. There are a plethora of blogs, articles, and chapters on the association of time and story points, but if there’s one thing I’ve learned it’s this: embrace\nwhatever comes most naturally to your team. If your team estimates in time, then use\ndays or half-days as story points. If they like the concept of effort, then go with it. If\nyou try to fight whatever people understand as estimates, you’ll end up with numbers\nwhose relativity constantly changes as you try to change their association of time to\nestimations. By embracing what comes naturally to your team, you may not end up\nwith a textbook way of estimating your tasks, but you will end up with more reliable\ndata over time.\n\n For teams I’ve managed it normally takes three to four iterations before they can\nstart to trust their estimations. The best way to see if your estimations are on track is to\nlook at them over time next to the total number of tasks that you complete over time.\n Even if you get to a point where your estimations and completed work are consistent, using estimations alone can’t tell you if you are under- or overestimating. Overestimating work means your team is underachieving for some reason; underestimating\ntypically leads to team burnout and an unsustainable pace of development. The tricky\nthing here is that estimations alone don’t give you that insight into your team; you\nhave to combine estimations with other data to see how well your team is estimating.\nFigure 3.8 may look great to you if you’re trying to hit a consistent goal over time.\n\n But how do you know if your estimates are any good? All you know is that you’re\ngetting done what you thought you could get done, not if you did as much as you\ncould or if you were doing too much.\n\n Flat-line trends occur when you see the same value over and over so your chart is\njust a flat line. Your team might be overestimating if you see a flat-line trend in your\nestimates over time, or they might be overestimating if you get done a lot more than\n\nThe goal and estimates\nare always only a few\npoints apart.\n\nPoints\ncompleted\n\nSprint goal\n\n115\n\n110\n\n62\n\n63\n\n64\n\nSprint\n\n65\n\n66\n\nFigure 3.8 Estimates are spot on every time!\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "3", "chapter_title": "Trends and data from project-tracking systems", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "If your team estimates in time, then use\ndays or half-days as story points. The best way to see if your estimations are on track is to\nlook at them over time next to the total number of tasks that you complete over time. Figure 3.8 Estimates are spot on every time!", "l2_summary": "One thing that seems to always challenge teams is what estimations mean. There are a plethora of blogs, articles, and chapters on the association of time and story points, but if there’s one thing I’ve learned it’s this: embrace\nwhatever comes most naturally to your team. If your team estimates in time, then use\ndays or half-days as story points. The best way to see if your estimations are on track is to\nlook at them over time next to the total number of tasks that you complete over time. Figure 3.8 may look great to you if you’re trying to hit a consistent goal over time. Figure 3.8 Estimates are spot on every time!", "prev_page": {"page_num": 70, "segment_id": "00070"}, "next_page": {"page_num": 72, "segment_id": "00072"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["time", "team", "estimations", "points", "estimates", "overestimating", "data", "thing", "teams", "what"], "content_type": "theory", "domain": "management|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["estimations", "completed tasks"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["estimation methods", "iteration management", "team trust building"], "key_concepts": ["story points", "time-based estimations", "flat-line trends"], "problem_statement": "challenges in estimation and tracking project progress", "solution_approach": "embrace natural team estimation methods, combine with other data for insights", "extraction_method": "lm"}}
{"segment_id": "00072", "page_num": 72, "segment": "Prepare for analysis; generate the richest set of data you can\n\n49\n\nyou commit to over time. At a glance this may not seem so bad; after all, you’re either\nbeing consistent or you’re beating your commitments---project managers of the world\nrejoice! But you don’t want your team to get bored, and you want to be as efficient and\nproductive as possible; overestimating equals underachieving. As I continue, we’ll add\nmore data to help you spot potential overestimating.\n\n It’s also possible to see the same flat-line trend if your team is overestimating. If you\nhave a team that is overestimating and you get the chart shown in figure 3.8, it could\nmean that your team is pushing too hard to get their work done, which is typically\nunsustainable. Once the trend breaks, it will result in much less work completed or people leaving the team in frustration. In this case relying on estimation to tell the whole\npicture could mean devastation is looming even though the trends look good.\n\n As you can see, estimates alone can give you an idea of how your team is performing, but they can also hide things that you need your attention. Next, we’ll add volume\nand bug count to our estimates to show how you can better identify how your team is\ndoing.\n\n3.2.4\n\nTip 4: Clearly define when tasks are done\n\nDone means a task is complete. Despite the seemingly simple definition, done can be a\ntough thing for the business and the agile team to figure out and agree on. Because\nthe data in your PTS associates time with tasks, the criteria for which tasks are complete are key because they determine when to put an end time on a task. Often, done\nmeans that the task in question is ready to deploy to your consumers. If you group lots\nof changes into deployments and have a separate process for delivering code, then\nmeasuring the time it takes to get ready for a deployment cycle makes sense. If you\nmove to a model where you’re continuously deploying changes to your consumers, it’s\nbetter to categorize tasks as done after they’ve been deployed. Whatever your deployment methodologies are, here are some things to consider when defining done for\nyour team’s tasks:\n\n■ Whatever was originally asked for is working.\n■ Automated tests exist for whatever you built.\n■ All tests across the system you’re developing against pass.\n■ A certain number of consumers are using the change.\n■ You are measuring the business value of the change.\n\nMake sure you collaborate with your team and stakeholders on the definition of done\nfor a task, and keep it simple to ensure that everyone understands it. Without a clear\nunderstanding and agreement, you’re setting yourself up for unhappiness down the\nline when people don’t agree when something is complete and you don’t have data\nthat you can trust.\n\n Once a task is marked as done it becomes history, and any additional work should\nbecome a new task. It’s often tempting to reopen tasks after they’ve been completed if\nthe requirements change or a bug is found in a different development cycle. It’s much\nbetter to leave the task as complete and open another task or bug fix. If you have a\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "3", "chapter_title": "Trends and data from project-tracking systems", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "If you\nhave a team that is overestimating and you get the chart shown in figure 3.8, it could\nmean that your team is pushing too hard to get their work done, which is typically\nunsustainable. Tip 4: Clearly define when tasks are done Done means a task is complete.", "l2_summary": "It’s also possible to see the same flat-line trend if your team is overestimating. If you\nhave a team that is overestimating and you get the chart shown in figure 3.8, it could\nmean that your team is pushing too hard to get their work done, which is typically\nunsustainable. Tip 4: Clearly define when tasks are done Done means a task is complete. Often, done\nmeans that the task in question is ready to deploy to your consumers. Whatever your deployment methodologies are, here are some things to consider when defining done for\nyour team’s tasks:", "prev_page": {"page_num": 71, "segment_id": "00071"}, "next_page": {"page_num": 73, "segment_id": "00073"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["team", "done", "task", "tasks", "when", "data", "time", "overestimating", "have", "complete"], "content_type": "tutorial", "domain": "management|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["team performance", "estimation accuracy"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["task estimation", "definition of done", "team performance metrics"], "key_concepts": ["overestimating", "flat-line trend", "definition of done"], "problem_statement": "Improving team performance and accuracy in task estimations", "solution_approach": "Using metrics, defining clear criteria for 'done', and continuously monitoring trends", "extraction_method": "lm"}}
{"segment_id": "00073", "page_num": 73, "segment": "50\n\nCHAPTER 3 Trends and data from project-tracking systems\n\nHuge spikes\nin total done and\nbackward movement\n\nInconsistent\ncompletions\n\n400\n\n300\n\n200\n\n100\n\nTotal done\n\nBack\nfrom QA\n\nPoints\ncomplete\n\nSprint goal\n\n0\n54\n\n55\n\n56\n\n57\n\nSprint\n\n58\n\n59\n\n60\n\nFigure 3.9 Bad trends that you can see when completion criteria are not well defined\n\ngood agreement on what it means to be done and a task meets those criteria, there\nreally is no good reason to make it undone.\n\n If you don’t have a solid agreement on what it means to complete your tasks and\nyour team doesn’t mark tasks as done in a consistent way, then you won’t be able to\ntrust most of your data. This is because a lot of what you want to know from your PTS\ncenters on how long tasks take to complete versus how long you thought they were\ngoing to take. If you don’t have an accurate way to determine when things are complete, it’s hard to know with any certainty how long tasks actually take.\n\n You can tell when your completion criterion is not well defined when you start to\n\nsee unusual spikes in volume and backward movement, as shown in figure 3.9.\n\n This happens for either of the following reasons:\n\n■ Tasks that were complete end up back in the work stream.\n■ Tasks aren’t validated and are sent back to the development team for further\n\nwork.\n\nAnother trend you can see from figure 3.9 is that the completions of the team jump\nup and down. With unplanned work making its way back to the team, it becomes\ntough to complete the original commitment. In this case you can see the sprint goal\nstart to drop more and more over time as the team tries to get to a consistent velocity.\nIn reality they need to figure out why things are moving back from QA and why the\nnumber of tasks entering the work stream is spiking every few sprints.\n\n3.2.5\n\nTip 5: Clearly define when tasks are completed in a good way\n\nGood and bad are the epitome of relative terms. Each team has a different idea of what\nthey think is good based on their history, the product they’re working on, how they’re\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "3", "chapter_title": "Trends and data from project-tracking systems", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "Figure 3.9 Bad trends that you can see when completion criteria are not well defined ■ Tasks that were complete end up back in the work stream. Tip 5: Clearly define when tasks are completed in a good way", "l2_summary": "Figure 3.9 Bad trends that you can see when completion criteria are not well defined You can tell when your completion criterion is not well defined when you start to ■ Tasks that were complete end up back in the work stream. ■ Tasks aren’t validated and are sent back to the development team for further Another trend you can see from figure 3.9 is that the completions of the team jump\nup and down. Tip 5: Clearly define when tasks are completed in a good way", "prev_page": {"page_num": 72, "segment_id": "00072"}, "next_page": {"page_num": 74, "segment_id": "00074"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["tasks", "complete", "team", "back", "when", "good", "done", "figure", "what", "work"], "content_type": "theory", "domain": "management|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "Scrum"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["Total done", "Back from QA", "Points complete"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Project-tracking systems", "Completion criteria", "Trends in project management"], "key_concepts": ["Inconsistent completions", "Huge spikes", "Backward movement"], "problem_statement": "Unreliable completion criteria leading to inaccurate data and inconsistent task statuses", "solution_approach": "Clearly defining when tasks are completed", "extraction_method": "lm"}}
{"segment_id": "00074", "page_num": 74, "segment": "Prepare for analysis; generate the richest set of data you can\n\n51\n\nimplementing agile, and the opinions of its members. Go through your list of completed tasks and start tagging your cards as good, bad, or in between. As demonstrated\ntime and again, different patterns mean different things based on the team; absolute\npatterns simply don’t exist in a relative world. By tagging your cards based on how well\nyou think the process worked in addition to adding other tags that describe the task,\nyou can identify patterns that are contributing to the performance of the team. As\nyou’ll see as you start working with your data, tags will become the metadata around\nyour work; the power of working with data in this way is that you can find trends that\ntell you how you’re working now and help you identify what needs to be tweaked in\norder to improve.\n\n This relates directly to section 3.2.2 when we talked about tagging tasks and working with that data later. If you map tasks as good or bad and track that against volume,\nyou’ll end up with a pretty good indicator of how happy your team is. In this case good\nmeans that the task worked well, there were no problems around it, and everyone is\nhappy with how it played out. Bad would mean that the team was not happy with the\ntask for some reason; examples could be that they weren’t happy with how the requirements were written, there was another problem that made this task more difficult to\ncomplete, or the estimation was way off. In other environments a Niko-niko Calendar\nmay be used for this, where every day team members put an emoticon representing\nhow happy they are on the wall. Mapping out how satisfied you are with the completion\nof individual tasks is a much easier and more subtle way to figure out how the team is\nfeeling about what they’re working on. It can also help you gain insight into your estimate trends. As we mentioned earlier, flat-line estimates may make project managers\nhappy, but that doesn’t mean everything is going well on the team. Figure 3.10 is an\nexample of a team that is flat-lining estimates and completion and is also tagging cards\nas “happy” or “sad” based on how well the developer thought it was done.\n\nThe team is\ncompleting almost exactly\nwhat they’re estimating.\n\nHuge drop in\nhappiness with the\nconverse in sadness.\n\n120\n\n90\n\n60\n\n30\n\n0\n53\n\nCompleted\n\nEstimates\n\nSad\n\nHappy\n\n54\n\n55\n\n56\n\n57\n\n58\n\nSprint\n\nFigure 3.10 Estimates and completion mapped with how the developer thought the card was\ncompleted\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "3", "chapter_title": "Trends and data from project-tracking systems", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "Go through your list of completed tasks and start tagging your cards as good, bad, or in between. If you map tasks as good or bad and track that against volume,\nyou’ll end up with a pretty good indicator of how happy your team is. Figure 3.10 is an\nexample of a team that is flat-lining estimates...", "l2_summary": "Go through your list of completed tasks and start tagging your cards as good, bad, or in between. This relates directly to section 3.2.2 when we talked about tagging tasks and working with that data later. If you map tasks as good or bad and track that against volume,\nyou’ll end up with a pretty good indicator of how happy your team is. In this case good\nmeans that the task worked well, there were no problems around it, and everyone is\nhappy with how it played out. Figure 3.10 is an\nexample of a team that is flat-lining estimates and completion and is also tagging cards\nas “happy” or “sad” based on how well the developer thought it was done. Figure 3.10 Estimates and completion mapped with how the developer thought the card was\ncompleted", "prev_page": {"page_num": 73, "segment_id": "00073"}, "next_page": {"page_num": 75, "segment_id": "00075"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["team", "happy", "working", "data", "tasks", "tagging", "good", "well", "task", "estimates"], "content_type": "tutorial", "domain": "management|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": true, "has_metrics": true, "metrics": ["team satisfaction", "estimate trends"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Agile methodology", "task tagging", "team performance metrics"], "key_concepts": ["tagging tasks as good, bad or in between", "mapping estimates and completion with team satisfaction", "identifying trends for process improvement"], "problem_statement": "Improving team performance and identifying areas for process improvement", "solution_approach": "Tagging tasks and mapping them against estimates to identify patterns and trends", "extraction_method": "lm"}}
{"segment_id": "00075", "page_num": 75, "segment": "52\n\nCHAPTER 3 Trends and data from project-tracking systems\n\nI’ve seen the chart shown in figure 3.10 on tight deadline projects where a team\nneeded to get an almost unrealistic amount of work done. They started off very happy\nbecause they were finishing more than they thought they could and the thrill of it\nmade them feel good. But this lasted for only a few sprints before the team started to\nget tired. Maybe they were still completing their work, but they weren’t as excited\nabout it, and as a result they were tagging their cards as “sad.” This is a great example\nof how additional data can give you real insight into your team outside the normal\nmeasurement metrics.\n\n3.3\n\nKey project management metrics; spotting trends in data\nInstead of focusing on the typical agile metrics, we’ll look at what some key PTS data\nmeans when combined. The four metrics we’re going to combine to help you get a\nbroader picture of the performance of the team are:\n\n■ Estimates---The perceived amount of effort that a team gives to a task before\n\nthey work on it\n\n■ Volume---The number of tasks that are completed\n■ Bugs---The number of defects that are created and worked by a team\n■ Recidivism---Tasks that someone said were good enough to move forward in the\n\nprocess but ended up getting moved back\n\nAs we take a more in-depth look at interpreting this data, you’ll see the inconsistencies\nand ambiguity that result from looking at small bits of all the data you generate in\nyour software development lifecycle.\n\n3.3.1\n\nTask volume\n\nThe data we’ve charted in this chapter so far is the amount of estimated effort completed over time. Measuring estimates is a good place to start because it’s already typical practice on agile teams and provides a familiar starting place.\n\n Volume is the number of work items your team is getting done. This is a bit different than measuring estimates, where you’re evaluating the amount of estimated effort\nregardless of the number of tasks you complete. Tracking volume adds a bit more\ndepth to the estimates and bugs because it helps you determine a few key items:\n\n■ How big are your tasks?\n■ What is the ratio of new work to fixing things and revisiting old work?\n■ Are tasks coming in from outside the intake process?\n\nThe delta between estimation and actual time is a valuable metric because it shows\nyou a few potential things:\n\n■ How well your team understands the product they’re working on\n■ How well the team understands the requirements\n■ How well your requirements are written\n■ How technically mature your team is\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "3", "chapter_title": "Trends and data from project-tracking systems", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "■ Estimates---The perceived amount of effort that a team gives to a task before Volume is the number of work items your team is getting done. ■ How big are your tasks?", "l2_summary": "CHAPTER 3 Trends and data from project-tracking systems But this lasted for only a few sprints before the team started to\nget tired. ■ Estimates---The perceived amount of effort that a team gives to a task before Volume is the number of work items your team is getting done. ■ How big are your tasks? ■ How well your team understands the product they’re working on\n■ How well the team understands the requirements\n■ How well your requirements are written\n■ How technically mature your team is", "prev_page": {"page_num": 74, "segment_id": "00074"}, "next_page": {"page_num": 76, "segment_id": "00076"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["team", "data", "work", "tasks", "amount", "because", "metrics", "estimates", "volume", "number"], "content_type": "theory|practice", "domain": "management|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["Estimates", "Volume", "Bugs", "Recidivism"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["Project-tracking systems", "Agile metrics", "Team performance analysis"], "key_concepts": ["Trends and data from project-tracking systems", "Key project management metrics", "Estimates, Volume, Bugs, Recidivism"], "problem_statement": "Analyzing team performance and identifying trends in project-tracking systems", "solution_approach": "Combining key metrics to gain a broader picture of team performance", "extraction_method": "lm"}}
{"segment_id": "00076", "page_num": 76, "segment": "Key project management metrics; spotting trends in data\n\n53\n\nAdding velocity to your estimates will show you how many tasks are getting completed\nversus how much estimated effort you put into tasks. There should be a significant\ndelta between the two because each task would have an estimate greater than 1 point.\nIf your team takes on 10 tasks each with an estimate of 3 points, then your volume will\nbe 10 and your velocity target will be 30 (10 tasks * 3 estimate points).\n\n When you notice the delta between volume and estimation shrinking, that’s usually an indication that something is wrong. Usually that means you’re doing a lot of\nwork you didn’t plan for. If your task volume is 30 and your target estimate is 30, then\nyou either have 30 tasks all estimated at 1 point or you have tasks creeping into your\nwork stream that you never estimated. To figure out where that is coming from you’ll\nhave to dig into your data a bit more.\n\n3.3.2 Bugs\n\nThe previous data will help you dig into your estimates to figure out how accurate they\nare, but there’s still more data you can add to further hone your understanding. Adding bugs to the picture will give you an idea of what kind of tasks your team is working\non. This is an especially important metric for teams that don’t estimate bugs, because\nthat work would have been only partially visible to you so far.\n\n A bug represents a defect in your software, which can mean different things to different people. If a feature makes its way to done, all the necessary parties sign off on\ncompletion, and it turns out that there’s a defect that causes negative effects to your\nconsumers, you have a bug. Bugs are important to track because they point to the\nquality of the software your team is producing. You should pay attention to two bug\ntrends bug creation rate and bug completion rate.\n\n Bug creation rate can be calculated by getting the count of all of the tasks of type\n“bug” or “defect” by create date. Bug completion rate is calculated by the count of all\ntasks of type “bug” or “defect” by completion date.\n\n Usually you want to see the exact opposite from bug creation and bug completion;\nit’s good when bugs are getting squashed---that means bug completion is high. It’s\nbad when you’re generating a lot of bugs---that means bug creation is high. Let’s take\na look at bug completion in the scope of the big picture. Figure 3.11 is an aggregate of\nbugs, volume, and story points, the data that we’ve collected and put together so far.\n\n Figure 3.11 shows that this team doesn’t output many bugs, and estimates and volume seem to be fairly closely aligned, which at a glance is pretty good. But there are\nsome instances where volume shows that work is being completed, but no points are\nassociated with those tasks and no bugs are completed at the same time. This shows\nthat the team is picking up unplanned work, which is bad because:\n\n■\n\n■\n\nIt’s not budgeted for.\nIt impacts in a negative way the deliverables the team committed to.\n\nNotice how around 09-11 even though volume shows work is getting completed, no\nstory points are completed. This is a great example of how unplanned work can hurt\nthe deliverables that a team commits to. After seeing this trend, this team needs to\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "3", "chapter_title": "Trends and data from project-tracking systems", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "If your team takes on 10 tasks each with an estimate of 3 points, then your volume will\nbe 10 and your velocity target will be 30 (10 tasks * 3 estimate points). You should pay attention to two bug\ntrends bug creation rate and bug completion rate. Bug completion rate is calculated by the count of...", "l2_summary": "If your team takes on 10 tasks each with an estimate of 3 points, then your volume will\nbe 10 and your velocity target will be 30 (10 tasks * 3 estimate points). If your task volume is 30 and your target estimate is 30, then\nyou either have 30 tasks all estimated at 1 point or you have tasks creeping into your\nwork stream that you never estimated. You should pay attention to two bug\ntrends bug creation rate and bug completion rate. Bug creation rate can be calculated by getting the count of all of the tasks of type\n“bug” or “defect” by create date. Bug completion rate is calculated by the count of all\ntasks of type “bug” or “defect” by completion date. Usually you want to see the exact opposite from bug creation and bug completion;\nit’s good when bugs are getting squashed---that means bug completion is high.", "prev_page": {"page_num": 75, "segment_id": "00075"}, "next_page": {"page_num": 77, "segment_id": "00077"}, "extended_fields": {"has_code": false, "has_formulas": true, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["tasks", "bugs", "team", "volume", "work", "completion", "have", "data", "will", "completed"], "content_type": "tutorial", "domain": "management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["velocity", "bug creation rate", "bug completion rate"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["project management metrics", "velocity estimation", "bug tracking"], "key_concepts": ["velocity", "estimation accuracy", "bug creation and completion rates"], "problem_statement": "Improving project management through accurate task estimation and bug tracking", "solution_approach": "Using velocity to track task completion against estimates, monitoring bug trends for software quality", "extraction_method": "lm"}}
{"segment_id": "00077", "page_num": 77, "segment": "54\n\nCHAPTER 3 Trends and data from project-tracking systems\n\nVolume indicates tasks were completed,\nyet no bugs were completed.\n\nNo story\npoints done\n\nDefault widget\n\n28\n\n21\n\n14\n\n7\n\n0\n08-29\n\n08-31\n\n09-02\n\n09-04\n\n09-06\n\n09-08\n\n09-10\n\n09-12\n\n09-14\n\n09-16\n\n09-18\n\nStory points\n\nVolume\n\nBugs\n\n08-30\n\n09-01\n\n09-03\n\n09-05\n\n09-07\n\n09-09\n\n09-11\n\n09-13\n\n09-15\n\n09-17\n\nSprint\n\nFigure 3.11 Bugs, volume, and estimates tracked together\n\nfind the root of the unplanned work and get rid of it. If they must complete it, then\nthey should estimate it and plan for it like everything else in the work stream.\n\n3.3.3 Measuring task movement; recidivism and workflow\n\nThe final piece of data to add to this chart is the movement of tasks through your\nteam’s workflow. This data gives you great insight into the health of your team by\nchecking how tasks move through the workflow.\n\n For this we’ll look at recidivism, which is the measurement of tasks as they move\nbackward in the predefined workflow. If a task moves from development to QA, fails\nvalidation, and moves back to development, this would increase the recidivism rate.\n\n Spikes in this data point can indicate potential problems:\n\n■ There’s a communication gap somewhere on the team.\n■ Completion criteria (a.k.a. done) are not defined clearly to everyone on the\n\nteam.\n\n■ Tasks are being rushed, usually due to pressure to hit a release date.\n\nIf the number of tasks that move backward isn’t spiking but is consistently high, then\nyou have an unhealthy amount of churn happening on your team that should be\naddressed.\n\n Figure 3.12 adds this data point to the charts you’ve been looking at so far in this\n\nchapter.\n\n At a glance this may be too much data to digest all at once. If you look at only the\ntasks that came back from QA, which is measuring cards that moved backward, you\ncan see that spikes end up around bad things, such as huge spikes in tasks complete or\nhuge dips in productivity. The large spikes in backward tasks along with the large\nspikes in total tasks complete and a flat-line sprint goal representing estimations tell\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "3", "chapter_title": "Trends and data from project-tracking systems", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "Volume indicates tasks were completed,\nyet no bugs were completed. 3.3.3 Measuring task movement; recidivism and workflow The final piece of data to add to this chart is the movement of tasks through your\nteam’s workflow.", "l2_summary": "CHAPTER 3 Trends and data from project-tracking systems Volume indicates tasks were completed,\nyet no bugs were completed. 3.3.3 Measuring task movement; recidivism and workflow The final piece of data to add to this chart is the movement of tasks through your\nteam’s workflow. This data gives you great insight into the health of your team by\nchecking how tasks move through the workflow. For this we’ll look at recidivism, which is the measurement of tasks as they move\nbackward in the predefined workflow.", "prev_page": {"page_num": 76, "segment_id": "00076"}, "next_page": {"page_num": 78, "segment_id": "00078"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["tasks", "data", "team", "spikes", "workflow", "backward", "volume", "bugs", "complete", "recidivism"], "content_type": "tutorial", "domain": "management|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["story points", "volume", "bugs", "recidivism rate"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["project-tracking systems", "task movement", "recidivism and workflow"], "key_concepts": ["unplanned work", "communication gap", "clear completion criteria", "tasks being rushed"], "problem_statement": "Identifying and addressing issues in project-tracking systems to improve team performance", "solution_approach": "Tracking bugs, volume, story points, and task movement through the workflow; measuring recidivism rate", "extraction_method": "lm"}}
{"segment_id": "00078", "page_num": 78, "segment": "400\n\n300\n\n200\n\n100\n\n0\n53\n\nKey project management metrics; spotting trends in data\n\n55\n\nTotal done\n\nBugs\n\nPoints\n\nBack from QA\n\nSprint goal\n\n54\n\n55\n\n56\n\n57\n\n58\n\n59\n\n60\n\nSprint\n\nFigure 3.12 An example of all the data we have so far\n\nyou that your team must be opening up tasks that were previously marked as complete\nand putting them back into the work stream for some reason. That behavior results in\na lot of unplanned work, and your team should discuss this trend to figure out why\nthey’re doing this and how to fix the problem.\n\n3.3.4\n\nSorting with tags and labels\n\nTags and labels help you associate different tasks by putting an objective label on\nthem. This helps you see associations between properties of tasks that you may not\nhave thought of slicing up before. Figure 3.13 shows a simple aggregation of tags for a\nspecific project. In this example all you can see is what the most common tags are and\nthe count of the different tags.\n\nBy labeling or tagging your data you can find\ntrends in other data that relate to it.\n\nFigure 3.13 A breakdown of tags for a specific project\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "3", "chapter_title": "Trends and data from project-tracking systems", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "Figure 3.12 An example of all the data we have so far Sorting with tags and labels Figure 3.13 A breakdown of tags for a specific project", "l2_summary": "Figure 3.12 An example of all the data we have so far Sorting with tags and labels Tags and labels help you associate different tasks by putting an objective label on\nthem. Figure 3.13 shows a simple aggregation of tags for a\nspecific project. In this example all you can see is what the most common tags are and\nthe count of the different tags. Figure 3.13 A breakdown of tags for a specific project", "prev_page": {"page_num": 77, "segment_id": "00077"}, "next_page": {"page_num": 79, "segment_id": "00079"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["tags", "data", "figure", "project", "tasks", "trends", "back", "sprint", "example", "have"], "content_type": "tutorial", "domain": "management|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["Total done", "Bugs"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Project Management Metrics", "Sprint Goals", "Tagging and Labeling Tasks"], "key_concepts": ["Key project management metrics", "Spotting trends in data", "Sorting with tags and labels"], "problem_statement": "Unplanned work due to reopening tasks marked as complete", "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00079", "page_num": 79, "segment": "56\n\nCHAPTER 3 Trends and data from project-tracking systems\n\nAdding another metric against labels\nwill allow you to see correlations between\ntags and that metric---here, development time.\n\nFigure 3.14 Viewing development time side by side with tags\n\nIn figure 3.14 we add another metric into the mix, development time.\n\n Now you can sort the two charts by filtering the data behind these charts by your\nlabels. The Kibana dashboard allows you to click any chart to filter the rest of the\ncharts on a dashboard by that data. You can begin by clicking tasks labeled “integration” in the Labels panel from figure 3.15.\n\n In this case you can see that tasks labeled “integration” take a particularly long\ntime: 17, 13, or 5 full days. Conversely, you can also sort on development time to see\nwhich labels take a long time, as shown in figure 3.16.\n\nIf you sort by tasks labeled\n“integration” you see the\nother associated tags.\n\nDevelopment time seems to\ntake a long time when you sort\nby tasks labeled “integration.”\n\nFigure 3.15 Sorting data by labels and seeing the effect on development time\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "3", "chapter_title": "Trends and data from project-tracking systems", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "Figure 3.14 Viewing development time side by side with tags In figure 3.14 we add another metric into the mix, development time. Conversely, you can also sort on development time to see\nwhich labels take a long time, as shown in figure 3.16.", "l2_summary": "Figure 3.14 Viewing development time side by side with tags In figure 3.14 we add another metric into the mix, development time. In this case you can see that tasks labeled “integration” take a particularly long\ntime: 17, 13, or 5 full days. Conversely, you can also sort on development time to see\nwhich labels take a long time, as shown in figure 3.16. Development time seems to\ntake a long time when you sort\nby tasks labeled “integration.” Figure 3.15 Sorting data by labels and seeing the effect on development time", "prev_page": {"page_num": 78, "segment_id": "00078"}, "next_page": {"page_num": 80, "segment_id": "00080"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["time", "development", "labels", "figure", "data", "sort", "tasks", "labeled", "integration", "metric"], "content_type": "tutorial", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Kibana"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["development time"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Kibana"], "topics": ["project-tracking systems", "data visualization", "metric correlation"], "key_concepts": ["correlation between tags and metrics", "filtering data by labels", "development time analysis"], "problem_statement": "Analyzing development time in relation to project tags", "solution_approach": "Using Kibana dashboard for filtering and sorting tasks", "extraction_method": "lm"}}
{"segment_id": "00080", "page_num": 80, "segment": "Case study: identifying tech debt trending with project tracking data\n\n57\n\nSorting by development time you\ncan see all labels that take\na long time to complete.\n\nFigure 3.16 Sorting labels by development time\n\n3.4\n\nIn chapter 7 we take a closer look at exploring data and the relationship between data\npoints.\n\nCase study: identifying tech debt trending with project\ntracking data\nNow that we’ve talked about this data and have it populating our graphs, I’ll share a\nreal-world scenario where this data alone helped a team come to terms with a problem, make adjustments, and get to a better place.\n\n Our team was using Scrum and had a two-week sprint cadence. We were working\non a release that had a rather large and interrelated set of features with many stakeholders. As with most teams, we started with tracking velocity to ensure that we were\nconsistently estimating and completing our work every sprint. We did this so we could\ntrust our estimates enough to be able to deliver more predictably over time and so\nstakeholders could get an idea of approximately when the whole set of features they\nwanted would be delivered.\n\n In this case we couldn’t hit a consistent number of story points at the end of every\nsprint. To complicate things we didn’t have a typical colocated small agile team; we\nhad two small teams in different time zones that were working toward the same large\nproduct launch.\n\n In summary the current state of the team was:\n\n■ Geographically distributed team working toward the same goal\n■ Agile development of a large interrelated feature set\n\nWe were asking these questions:\n\n■ Why are we not normalizing on a consistent velocity?\n■ What can we do to become more predictable for our stakeholders?\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "3", "chapter_title": "Trends and data from project-tracking systems", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "Case study: identifying tech debt trending with project tracking data Figure 3.16 Sorting labels by development time In chapter 7 we take a closer look at exploring data and the relationship between data\npoints.", "l2_summary": "Case study: identifying tech debt trending with project tracking data Sorting by development time you\ncan see all labels that take\na long time to complete. Figure 3.16 Sorting labels by development time In chapter 7 we take a closer look at exploring data and the relationship between data\npoints. Our team was using Scrum and had a two-week sprint cadence. We were working\non a release that had a rather large and interrelated set of features with many stakeholders.", "prev_page": {"page_num": 79, "segment_id": "00079"}, "next_page": {"page_num": 81, "segment_id": "00081"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "time", "team", "case", "tracking", "development", "sprint", "working", "large", "stakeholders"], "content_type": "case_study", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "Scrum"], "programming_languages": [], "has_best_practices": false, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["velocity"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["tech debt trending", "project tracking data", "Scrum methodology"], "key_concepts": ["geographically distributed team", "consistent velocity", "predictability for stakeholders"], "problem_statement": "Why are we not normalizing on a consistent velocity?", "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00081", "page_num": 81, "segment": "58\n\nCHAPTER 3 Trends and data from project-tracking systems\n\nLarge variances in estimated points completed;\nideally this line should be close to flat.\n\n200\n\n150\n\n100\n\n50\n\nPoints\n\n0\n53\n\n54\n\n55\n\nSprint\n\n56\n\n57\n\nFigure 3.17 Story points are jumping all over the place.\n\nThe goal was to maintain a consistent velocity so we could communicate more effectively to our stakeholders what they could expect to test by the end of every sprint.\nThe velocity of the team is shown in figure 3.17.\n\n In this case we were tracking data over sprints and using it in our retrospectives to\ndiscuss where improvements could be made. From sprints 53--57 we were seeing significant inconsistency in the estimates we were completing. To get a handle on why\nour estimates were not consistently lining up, we decided to also start tracking volume\nto compare what we thought we could get done against the number of tasks we were\ncompleting. In theory, estimates should be higher than volume but roughly completed in parallel; that got us the graph shown in figure 3.18.\n\n Whoa! That doesn’t look right at all! Volume and estimates are mostly parallel, but\naccording to this we’re getting the same amount of tasks completed as points. Because\n\nTotal complete tasks are almost parallel to points\ncomplete; ideally total tasks should be much lower.\n\n200\n\n150\n\n100\n\n50\n\nPoints\n\nTotal done\n\n0\n53\n\n54\n\n55\n\nSprint\n\n56\n\n57\n\nFigure 3.18 Adding volume to the picture\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "3", "chapter_title": "Trends and data from project-tracking systems", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "Figure 3.17 Story points are jumping all over the place. The velocity of the team is shown in figure 3.17. Figure 3.18 Adding volume to the picture", "l2_summary": "Figure 3.17 Story points are jumping all over the place. The velocity of the team is shown in figure 3.17. In theory, estimates should be higher than volume but roughly completed in parallel; that got us the graph shown in figure 3.18. Volume and estimates are mostly parallel, but\naccording to this we’re getting the same amount of tasks completed as points. Total complete tasks are almost parallel to points\ncomplete; ideally total tasks should be much lower. Figure 3.18 Adding volume to the picture", "prev_page": {"page_num": 80, "segment_id": "00080"}, "next_page": {"page_num": 82, "segment_id": "00082"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["points", "figure", "could", "estimates", "volume", "tasks", "tracking", "completed", "should", "sprint"], "content_type": "case_study", "domain": "programming|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["project-tracking systems", "sprint", "story points", "velocity"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": false, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["velocity", "points", "tasks"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["project-tracking systems", "sprint velocity", "story points estimation"], "key_concepts": ["velocity consistency", "estimation accuracy", "volume tracking"], "problem_statement": "Inconsistent story point completion and the need for better estimation", "solution_approach": "Tracking volume to improve estimation accuracy", "extraction_method": "lm"}}
{"segment_id": "00082", "page_num": 82, "segment": "Case study: identifying tech debt trending with project tracking data\n\n59\n\nMost of the total tasks that are\ncomplete are bugs; ideally bugs are a small\npercentage of the total completed tasks.\n\nPoints\n\nTotal done\n\nBugs\n\n200\n\n150\n\n100\n\n50\n\n0\n53\n\n54\n\nFigure 3.19 Adding bugs\n\n55\n\nSprint\n\n56\n\n57\n\neach task should have an estimate attached to it, points should be much higher than\ntasks. After a closer look at the data we noticed that a lot of bugs were also getting\ncompleted, so we added that to our chart (figure 3.19).\n\n At this point we were much closer to the full picture. The huge volume of bugs that\nwe had to power through every sprint was causing volume to go way up, and because\nthe team wasn’t estimating bugs we weren’t getting the number of estimated features\ndone that we thought we could. We all discussed the problem, and in this case the\ndevelopment team pointed to the huge amount of tech debt that had been piling up\nover time. Most of the things we completed ended up having edge cases and race conditions that caused problems with the functionality of the product. To show the\namount of work that was moving backward in sprint, we also started mapping that out,\nas shown in figure 3.20.\n\nA high number of tasks that moves backward is\nnormally an indicator of significant problems.\n\n200\n\n150\n\n100\n\n50\n\nPoints\n\nTotal done\n\nBugs\n\nBackward\ntasks\n\n0\n53\n\n54\n\n55\n\nSprint\n\n56\n\n57\n\nFigure 3.20 Adding tasks that move backward to the chart\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "3", "chapter_title": "Trends and data from project-tracking systems", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "Most of the total tasks that are\ncomplete are bugs; ideally bugs are a small\npercentage of the total completed tasks. Figure 3.19 Adding bugs Figure 3.20 Adding tasks that move backward to the chart", "l2_summary": "Case study: identifying tech debt trending with project tracking data Most of the total tasks that are\ncomplete are bugs; ideally bugs are a small\npercentage of the total completed tasks. Figure 3.19 Adding bugs each task should have an estimate attached to it, points should be much higher than\ntasks. After a closer look at the data we noticed that a lot of bugs were also getting\ncompleted, so we added that to our chart (figure 3.19). Figure 3.20 Adding tasks that move backward to the chart", "prev_page": {"page_num": 81, "segment_id": "00081"}, "next_page": {"page_num": 83, "segment_id": "00083"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["bugs", "tasks", "total", "figure", "sprint", "backward", "completed", "points", "done", "case"], "content_type": "case_study", "domain": "programming|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["bugs", "tech debt", "edge cases", "race conditions"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": false, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["total done", "bugs", "backward tasks"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["tech debt", "bug management", "sprint tracking"], "key_concepts": ["tech debt", "bug estimation", "sprint volume"], "problem_statement": "Identifying tech debt and its impact on bug completion in sprints", "solution_approach": "Mapping out backward tasks to better understand the issue", "extraction_method": "lm"}}
{"segment_id": "00083", "page_num": 83, "segment": "60\n\nCHAPTER 3 Trends and data from project-tracking systems\n\nWhere the check and\nadjust happened\n\nTeam starts\nto normalize\n\nIndication of\nrepeating pattern;\ntime to adjust again\n\nTeam Jira stats over time\n\n280\n\n210\n\n140\n\n70\n\n0\n53\n\n54\n\n55\n\n56\n\n57\n\n58\n\n59\n\n60\n\n61\n\n62\n\n63\n\n64\n\n65\n\n66\n\nPoints\n\nTotal done\n\nBugs\n\nBackward\ntasks\n\nFigure 3.21 The complete data over time\n\nSprint\n\nMost of the tech debt stemmed from decisions that we made in earlier sprints to hit an\nimportant release date. To combat the issue we decided to have a cleanup sprint\nwhere we would refactor the problem code and clean up the backlog of bugs. Once\nwe did that, the number of bugs in sprint was drastically reduced and the number of\nbug tasks that moved backward also went way down. After using this data to point to a\nproblem, come up with a solution, try it, and watch the data change, we started watching these charts sprint over sprint. Eventually tech debt started to build again, which\nwas clearly visible in our metrics collection system and shown in figure 3.21, starting in\nsprint 62.\n\n We saw the same trend starting to happen again, so we took the same approach; we\nhad a cleanup sprint in 65 and managed to get the team stable and back in control\nwith a much smaller hit than back in sprint 58.\n\n Even though this data paints a pretty good picture of a problem and the effects of\na change on a team, it’s still only the tip of the iceberg. In the next chapter we’ll start\nlooking at even more data that you can add to your system and what it can tell you.\n\n3.5\n\nSummary\nThe project tracking system is the first place most people look for data to measure\ntheir teams. These systems are designed to track tasks, estimates, and bugs in your\ndevelopment lifecycle and alone they can give you a lot of insight into how your team\nis performing. In this chapter you learned:\n\n■ Your PTS contains a few main pieces of raw data: what, when, and who.\n■ The richer the raw data in the system, the more comprehensive the analysis. To\n\nenable this your team should follow these guidelines:\n■ Always use the PTS.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 4, "cols": 2, "data": [["", ""], ["", ""], ["", ""], ["", ""]], "markdown": "|  |  |\n|---|---|\n|  |  |\n|  |  |\n|  |  |"}], "table_count": 1, "chapter_num": "3", "chapter_title": "Trends and data from project-tracking systems", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "CHAPTER 3 Trends and data from project-tracking systems Team starts\nto normalize Figure 3.21 The complete data over time", "l2_summary": "CHAPTER 3 Trends and data from project-tracking systems Team starts\nto normalize Team Jira stats over time Figure 3.21 The complete data over time After using this data to point to a\nproblem, come up with a solution, try it, and watch the data change, we started watching these charts sprint over sprint. ■ The richer the raw data in the system, the more comprehensive the analysis.", "prev_page": {"page_num": 82, "segment_id": "00082"}, "next_page": {"page_num": 84, "segment_id": "00084"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "sprint", "team", "bugs", "system", "chapter", "time", "again", "tasks", "problem"], "content_type": "practice", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Jira"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["Tech debt", "Bugs", "Backward tasks"], "has_case_study": true, "case_study_company": null, "tools_mentioned": ["Jira"], "topics": ["Project tracking systems", "Tech debt management", "Sprint analysis"], "key_concepts": ["Tech debt", "Sprint cleanup", "Metrics for team performance"], "problem_statement": "Managing tech debt and its impact on the development process", "solution_approach": "Identifying patterns, implementing cleanup sprints, and monitoring metrics", "extraction_method": "lm"}}
{"segment_id": "00084", "page_num": 84, "segment": "Summary\n\n61\n\n■ Tag tasks with as much data as possible.\n■ Estimate your work.\n■ Clearly define the criteria for completion (done).\n■ Retroactively tag tasks with how happy you were about them.\n\n■ Tagging tasks in your PTS allows you to analyze your data in novel ways.\n■ Adding a narrative gives context to changes over time.\n■ Velocity and burn down are a good starting point for analysis, but adding more\n\ndata gives you a clearer picture of the trends you notice in those metrics.\n\n■ Pulling key data out of your PTS into a time-data series makes it easier to see\n\nhow your team is performing.\nJust about anything can be distilled into a chart in your analytics system.\n\n■\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "3", "chapter_title": "Trends and data from project-tracking systems", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "■ Tag tasks with as much data as possible. ■ Retroactively tag tasks with how happy you were about them. ■ Tagging tasks in your PTS allows you to analyze your data in novel ways.", "l2_summary": "■ Tag tasks with as much data as possible. ■ Retroactively tag tasks with how happy you were about them. ■ Tagging tasks in your PTS allows you to analyze your data in novel ways. ■ Adding a narrative gives context to changes over time. data gives you a clearer picture of the trends you notice in those metrics. ■ Pulling key data out of your PTS into a time-data series makes it easier to see", "prev_page": {"page_num": 83, "segment_id": "00083"}, "next_page": {"page_num": 85, "segment_id": "00085"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["data", "tasks", "adding", "gives", "time"], "content_type": "tutorial", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["Velocity", "Burn down"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["PTS"], "topics": ["Task Tagging", "Work Estimation", "Completion Criteria", "Data Analysis"], "key_concepts": ["Tag tasks with data", "Estimate work", "Define completion criteria", "Retroactively tag tasks", "Analyze data in PTS", "Velocity and burn down metrics", "Time-data series analysis"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00085", "page_num": 85, "segment": "Trends and data\nfrom source control\n\nThis chapter covers\n\n■ Learning from your SCM system’s data alone\n\n■ Utilizing your SCM to get the richest data\n\npossible\n\n■ Getting data from your SCM systems into your\n\nmetrics collection system\n\n■ Learning trends from your SCM systems\n\nProject tracking is a great place to start when you’re looking for data that can give\nyou insight into the performance of your team. The next data mine that you want\nto tap into is your source control management (SCM) system. In the scope of our\napplication lifecycle, this is highlighted in figure 4.1.\n\n SCM is where the action is; that’s where developers are checking in code, adding\nreviews, and collaborating on solutions. I’ve often been on teams where it was like\npulling teeth to get developers to move cards or add comments in the project tracking system, which ends up leading to a huge blind spot for anyone outside the\ndevelopment team interested in the progress of a project. You can eliminate that\nblind spot by looking at the datathe use of your SCM.\n\n62\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "3", "chapter_title": "Trends and data from project-tracking systems", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "■ Learning from your SCM system’s data alone ■ Utilizing your SCM to get the richest data ■ Getting data from your SCM systems into your", "l2_summary": "Trends and data\nfrom source control ■ Learning from your SCM system’s data alone ■ Utilizing your SCM to get the richest data ■ Getting data from your SCM systems into your ■ Learning trends from your SCM systems The next data mine that you want\nto tap into is your source control management (SCM) system.", "prev_page": {"page_num": 84, "segment_id": "00084"}, "next_page": {"page_num": 86, "segment_id": "00086"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["data", "system", "project", "where", "trends", "source", "control", "learning", "systems", "tracking"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["source control management", "project tracking system"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["performance of your team", "progress of a project"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["source control management system", "project tracking system"], "topics": ["learning from SCM data", "utilizing SCM for rich data", "getting SCM data into metrics collection", "trends from SCM systems"], "key_concepts": ["source control management (SCM)", "project tracking", "data analysis", "team performance metrics", "application lifecycle"], "problem_statement": "Getting insight into team performance and project progress without relying solely on project tracking systems.", "solution_approach": "Utilizing source control management systems to gather rich data for metrics collection and trend analysis.", "extraction_method": "lm"}}
{"segment_id": "00086", "page_num": 86, "segment": "What is source control management?\n\n63\n\nManage\ntasks and\nbugs\n\nProject\ntracking\n\nManage\ncode and\ncollaboration\n\nGenerate\nbuilds and\nrun tests\n\nMove code\nacross\nenvironments\n\nSource\ncontrol\n\nContinuous\nintegration\n\nDeployment\ntools\n\nEnsure\neverything\nis working\n\nApplication\nmonitoring\n\nPrevious\nchapter\n\nYou are here\n\nFigure 4.1 You are here: SCM in the scope of the application lifecycle.\n\nIf we revise the questions we asked\nin chapter 3, we get the types of\nwho, what, and how through data\nfrom SCM, as shown in figure 4.2.\n\n Here are two questions you can\n\nanswer from your SCM systems:\n\n■ How much change is happening in your codebase?\n■ How well is/are your develteam(s) working\n\nopment\ntogether?\n\nThe members on\nyour team who get\nassigned things\n\nSpecific code changes\nto your software\nproducts\n\nWHO is working on WHAT\n\nWHO is helping WHOM\n\nHow your team is\ncollaborating on\ntheir changes\n\nHOW MUCH effort the work is taking\n\nWhat code changes\nneed to be done to\ncomplete tasks\n\nFigure 4.2 The who, what, and how from SCM\n\nIf you combine SCM data with your\nPTS data from chapter 3, you can get really interesting insights into these issues:\n\n■ Are your tasks appropriately sized?\n■ Are your estimates accurate?\n■ How much is your team really getting done?\n\nWe take a much closer look at these questions in chapter 7 when we dive into combining different data sources. Our system is ready to go, but before we start taking a look\nat our data, let’s make sure you’re clear on the basic concepts of SCM.\n\n4.1 What is source control management?\n\nIf you’re working on a software production, you should already know what SCM is. But\nbecause one of my golden rules is never to assume anything, I’ll take this opportunity\nto tell you just in case.\n\n The software product that you’re working on ends up getting compiled and\ndeployed at some point. Before that happens, your team is working on the source, or\nthe code that comprises the functionality you want to ship to your consumers. Because\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "3", "chapter_title": "Trends and data from project-tracking systems", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "What is source control management? WHO is working on WHAT Figure 4.2 The who, what, and how from SCM", "l2_summary": "What is source control management? If we revise the questions we asked\nin chapter 3, we get the types of\nwho, what, and how through data\nfrom SCM, as shown in figure 4.2. ■ How well is/are your develteam(s) working WHO is working on WHAT Figure 4.2 The who, what, and how from SCM 4.1 What is source control management?", "prev_page": {"page_num": 85, "segment_id": "00085"}, "next_page": {"page_num": 87, "segment_id": "00087"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["what", "working", "code", "data", "source", "chapter", "much", "team", "control", "tasks"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["source control management", "continuous integration", "deployment tools"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["change in codebase", "team collaboration"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["source control management systems", "continuous integration tools", "deployment tools"], "topics": ["source control management", "continuous integration", "deployment tools", "application monitoring"], "key_concepts": ["source control management", "codebase changes", "team collaboration", "task management", "build and test automation", "environmental deployment"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00087", "page_num": 87, "segment": "64\n\nCHAPTER 4 Trends and data from source control\n\nyou likely have more than one person working on this source, you need to manage it\nsomewhere to ensure that changes aren’t getting overwritten, good collaboration is\nhappening, and developers aren’t stepping on each other’s toes. The place where this\nis managed and the history and evolution of your code are stored is your SCM system.\n Popular SCM systems today include Subversion (SVN), Git, Mercurial, and CVS. SCM\nsystems have evolved significantly over time, but switching from one type of SCM to\nanother isn’t always easy. This is because if all of your code and history are in one SCM\nsystem, moving all of it and asking your development team to start using a new system\nusually takes quite a bit of effort. There are tools to help you migrate from one system\nto another, and following those tools leads you down the path of the evolution of source\ncontrol. For example, a tool called cvs2svn will help you migrate from CVS to the newer\nand more popular SVN. Google “migrating SVN to CVS”; most of the links will point you\nin the other direction. There are also tools that help you migrate from SVN to the newer\nGit or Mercurial, but going in the opposite direction isn’t so common.\n\n If you’re not using SCM for your project, I strongly recommend you start using it.\nEven teams of one developer benefit from saving the history of their codebase over\ntime; if you make a mistake or need to restore some functionality that you took out, SCM\nmakes it easy to go back in time and bring old code that you got rid of back to life.\n\n4.2\n\nPreparing for analysis: generate the richest set of data you can\nThe most obvious data you can get from these systems is how much code is changing\nin CLOC and how much individuals on the team are changing it. As long as you’re\nusing source control, you have that data. But if you want to answer more interesting\nquestions like the ones I posed at the beginning of the chapter:\n\n■ How well is your team working together?\n■ Are your estimations accurate?\n■ Are your tasks appropriately sized?\n■ How much is your team really getting done?\n\nthen you should use the following tips to ensure you’re generating the richest set of\ndata you can.\n\nChanging SCM in the enterprise\n\nAs I talk about generating rich data in the next few sections, I’ll be talking through\npotential changes to how your team operates that might not seem realistic. If you’re\nworking at a nimble startup or are about to start a new project, then choosing or changing the type of SCM you use may not be a big deal. If you’re from a large enterprise\ncompany that has used CVS forever and refuses to hear anything about these newfangled SCM systems, a good data point to use to justify changing systems is the\namount of data you can get out of them and how they can improve collaboration on\nyour team. If you find yourself in that situation, keep that in mind as you read through\nthe following sections.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "4", "chapter_title": "Trends and data from source control", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "CHAPTER 4 Trends and data from source control As long as you’re\nusing source control, you have that data. Changing SCM in the enterprise", "l2_summary": "CHAPTER 4 Trends and data from source control Popular SCM systems today include Subversion (SVN), Git, Mercurial, and CVS. SCM\nsystems have evolved significantly over time, but switching from one type of SCM to\nanother isn’t always easy. If you’re not using SCM for your project, I strongly recommend you start using it. As long as you’re\nusing source control, you have that data. Changing SCM in the enterprise", "prev_page": {"page_num": 86, "segment_id": "00086"}, "next_page": {"page_num": 88, "segment_id": "00088"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["data", "team", "systems", "changing", "source", "code", "system", "using", "control", "have"], "content_type": "tutorial", "domain": "programming|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Subversion (SVN)", "Git", "Mercurial", "CVS"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["cvs2svn"], "topics": ["source control management", "SCM systems", "code migration"], "key_concepts": ["source control management (SCM)", "Subversion (SVN)", "Git", "Mercurial", "CVS", "code migration tools"], "problem_statement": "Managing code changes and ensuring collaboration among developers", "solution_approach": "Using SCM systems like SVN, Git, Mercurial, or CVS and migrating between them if necessary", "extraction_method": "lm"}}
{"segment_id": "00088", "page_num": 88, "segment": "Preparing for analysis: generate the richest set of data you can\n\n65\n\n4.2.1\n\nTip 1: Use distributed version control and pull requests\n\nThere are plenty of options\nwhen it comes to choosing a\nversion control system to use.\nWhen you do choose a system,\nthink about the type of data\nthat you can get out of it as part\nof its overall value. The first\nchoice to make when choosing\nan SCM system is whether to go\nwith a distributed or a centralized system. A centralized system is based on a central server\nthat’s the source of truth for all\ncode. To make changes and save the history of them, you must be connected to the\ncentral repository. A centralized SCM system is shown in figure 4.3.\n\nFigure 4.3 Centralized SCM system: there is one place\neveryone must get to.\n\nVersion\ncontrol\n\n In a centralized SCM model the type of metadata that’s stored along with the commit is usually just a comment. That’s better than nothing, but it isn’t much.\n\n With a distributed system everyone who checks out the project then has a complete\nrepository on their local development machine. Distributed version control systems\n(DVCSs) typically have individuals or groups who make significant changes to the\ncodebase and try to merge it later. Allowing everyone to have their own history and to\ncollaborate outside of the master repo gives a team a lot more flexibility and opportunity for distributed collaboration. A DVCS is illustrated in figure 4.4.\n\nUltimately changes\ncome back to the central\nrepo to get deployed.\n\nLocal\nrepo\n\nLocal\nrepo\n\nDevelopers can\ncollaborate outside\nof the central repo.\n\nCentral\nrepo\n\nLocal\nrepo\n\nLocal\nrepo\n\nFigure 4.4 Distributed version control system (DVCS): everyone has a repo.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 2, "cols": 2, "data": [["Version\ncontrol", ""], ["", ""]], "markdown": "| Version\ncontrol |  |\n|---|---|\n|  |  |"}, {"table_id": "table_2", "rows": 2, "cols": 2, "data": [["Local\nrepo", ""], ["", ""]], "markdown": "| Local\nrepo |  |\n|---|---|\n|  |  |"}, {"table_id": "table_3", "rows": 2, "cols": 2, "data": [["Local\nrepo", ""], ["", ""]], "markdown": "| Local\nrepo |  |\n|---|---|\n|  |  |"}, {"table_id": "table_4", "rows": 2, "cols": 2, "data": [["Central\nrepo", ""], ["", ""]], "markdown": "| Central\nrepo |  |\n|---|---|\n|  |  |"}, {"table_id": "table_5", "rows": 2, "cols": 2, "data": [["Local\nrepo", ""], ["", ""]], "markdown": "| Local\nrepo |  |\n|---|---|\n|  |  |"}, {"table_id": "table_6", "rows": 2, "cols": 2, "data": [["Local\nrepo", ""], ["", ""]], "markdown": "| Local\nrepo |  |\n|---|---|\n|  |  |"}], "table_count": 6, "chapter_num": "4", "chapter_title": "Trends and data from source control", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "A centralized SCM system is shown in figure 4.3. Figure 4.3 Centralized SCM system: there is one place\neveryone must get to. Figure 4.4 Distributed version control system (DVCS): everyone has a repo.", "l2_summary": "Tip 1: Use distributed version control and pull requests There are plenty of options\nwhen it comes to choosing a\nversion control system to use. The first\nchoice to make when choosing\nan SCM system is whether to go\nwith a distributed or a centralized system. A centralized SCM system is shown in figure 4.3. Figure 4.3 Centralized SCM system: there is one place\neveryone must get to. Figure 4.4 Distributed version control system (DVCS): everyone has a repo.", "prev_page": {"page_num": 87, "segment_id": "00087"}, "next_page": {"page_num": 89, "segment_id": "00089"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["system", "repo", "distributed", "version", "control", "centralized", "central", "local", "figure", "everyone"], "content_type": "tutorial", "domain": "programming|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["version control system", "distributed version control system", "centralized version control system"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["version control system", "centralized version control system", "distributed version control system"], "topics": ["version control systems", "centralized vs distributed systems", "metadata storage in version control"], "key_concepts": ["distributed version control system (DVCS)", "centralized version control system", "metadata storage"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00089", "page_num": 89, "segment": "66\n\nCHAPTER 4 Trends and data from source control\n\nNote in figure 4.4 that there’s still a central repo where the master history is stored\nand the final product is deployed from.\n\n SCM systems have two primary focal points:\n\n■ Saving the history of everything that happens to the code\n■ Fostering great collaboration\n\nCentralized SCM systems have all the history in one place: the central repo. DVCSs save\nthe history of changes no matter where they happen and as a result have a much\nricher set of data available around code changes. In addition, DVCSs typically have\nRESTful APIs, which make getting that data and reporting on it with familiar constructs plug and play at this point.\n\n From the perspective of someone who wants to get as much data about your team’s\nprocess as possible, it’s much better to use a DVCS because you can tell a lot more\nabout how the team is working, how much collaboration is happening, and where in\nyour source code changes are happening.\n\n A common practice in teams that use DVCSs is the idea of a pull request. When a\ndeveloper has a change or a set of changes that they want to merge with the master\ncodebase, they submit their changes in a pull request. The pull request includes a\nlist of other developers who should review the request. The developers have the\nopportunity to comment before giving their approve/deny verdict. This is illustrated\nin figure 4.5.\n\nChanged code\nis submitted for\npeer review via\na pull request.\n\nPeers review\nand comment.\n\nDeveloper\ncreates a branch and\nchanges the code.\n\nApproved pull\nrequests are merged\nto master.\n\nFigure 4.5 The pull request workflow for source control usage\n\nDVCS workflows\n\nThe most popular DVCS workflows are the feature-branch workflow and gitflow.\n\nThe feature-branch workflow is the simpler one. The overall concept is that all new\ndevelopment gets its own branch, and when development is done, developers submit\na pull request to get their feature incorporated into the master.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "4", "chapter_title": "Trends and data from source control", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "The pull request includes a\nlist of other developers who should review the request. Developer\ncreates a branch and\nchanges the code. Figure 4.5 The pull request workflow for source control usage", "l2_summary": "A common practice in teams that use DVCSs is the idea of a pull request. When a\ndeveloper has a change or a set of changes that they want to merge with the master\ncodebase, they submit their changes in a pull request. The pull request includes a\nlist of other developers who should review the request. Changed code\nis submitted for\npeer review via\na pull request. Developer\ncreates a branch and\nchanges the code. Figure 4.5 The pull request workflow for source control usage", "prev_page": {"page_num": 88, "segment_id": "00088"}, "next_page": {"page_num": 90, "segment_id": "00090"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["pull", "request", "changes", "have", "code", "data", "master", "history", "much", "branch"], "content_type": "theory", "domain": "devops|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["source control", "DVCS", "SCM systems", "RESTful APIs"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Git"], "topics": ["source control systems", "DVCS vs SCM", "pull request workflow"], "key_concepts": ["centralized repository", "distributed version control systems", "feature-branch workflow", "gitflow"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00090", "page_num": 90, "segment": "Preparing for analysis: generate the richest set of data you can\n\n67\n\nGitflow also focuses on using pull requests and feature branches but puts some formality around the branching structure that your team uses. A separate branch, used\nto aggregate features, is usually called develop or next. But before merging a feature\ninto the develop branch, there’s typically another pull request. Then, after all features\nare developed and merged, they are merged into the master branch to be deployed\nto the consumer.\n\nFor information about Git and using workflows, check out Git in Practice by Mike\nMcQuaid (Manning, 2014), which goes into great detail about these workflows and\nhow to apply them.\n\nThis collaboration among developers is then saved as metadata along with the change.\nGitHub is an example system that has a rich API with all of this data that will help you\nget better insight into your team’s performance. The following listing shows an\nabridged example of data you can get from the GitHub API about a pull request.\n\nListing 4.1 Abridged example response from the GitHub API\n\nGeneral information\nabout the pull requests\n\n{\n \"state\": \"open\",\n \"title\": \"new-feature\",\n \"body\": \"Please pull these awesome changes\",\n \"created_at\": \"2011-01-26T19:01:12Z\",\n \"updated_at\": \"2011-01-26T19:01:12Z\",\n \"closed_at\": \"2011-01-26T19:01:12Z\",\n \"merged_at\": \"2011-01-26T19:01:12Z\",\n \"_links\": {\n \"self\": {\n \"href\": \"https://api.github.com/repos/octocat/Hello-World/pulls/1\"\n },\n \"html\": {\n \"href\": https://github.com/octocat/Hello-World/pull/1\n },\n \"issue\": {\n \"href\": \"https://api.github.com/repos/octocat/Hello-World/issues/1\"\n\nDates are great\nfor charting\n\nFollow the link\nfor details\non this node\n\n },\n \"comments\": {\n \"href\": https://api.github.com/repos/octocat/Hello-World/issues/1\n ➥ /comments\n },\n \"review_comments\": {\n \"href\": \"https://api.github.com/repos/octocat/Hello-World/pulls/1\n ➥ /comments\"\n \"merge_commit_sha\": \"e5bd3914e2e596debea16f433f57875b5b90bcd6\",\n \"merged\": true,\n \"mergeable\": true,\n \"merged_by\": {\n \"login\": \"octocat\",\n \"id\": 1,\n\nIf this has already\nbeen merged\n\nWho merged the\npull request\n\nLinks to all of\n the comments\n\nLinks to the\ncomments\n for reviews\n\nIf this can\nbe merged\n\nThe ID of the\nparent commit\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "4", "chapter_title": "Trends and data from source control", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "},\n \"comments\": {\n \"href\": https://api.github.com/repos/octocat/Hello-World/issues/1\n ➥ /comments\n },\n \"review_comments\": {\n \"href\": \"https://api.github.com/repos/octocat/Hello-World/pulls/1\n ➥ /comments\"\n \"merge_commit_sha\": \"e5bd3914e2e596debea16f433f57875b5b90bcd6\",\n \"merged\": true,\n...", "l2_summary": "Listing 4.1 Abridged example response from the GitHub API {\n \"state\": \"open\",\n \"title\": \"new-feature\",\n \"body\": \"Please pull these awesome changes\",\n \"created_at\": \"2011-01-26T19:01:12Z\",\n \"updated_at\": \"2011-01-26T19:01:12Z\",\n \"closed_at\": \"2011-01-26T19:01:12Z\",\n \"merged_at\": \"2011-01-26T19:01:12Z\",\n \"_links\": {\n \"self\": {\n \"href\": \"https://api.github.com/repos/octocat/Hello-World/pulls/1\"\n },\n \"html\": {\n \"href\": https://github.com/octocat/Hello-World/pull/1\n },\n \"issue\": {\n \"href\": \"https://api.github.com/repos/octocat/Hello-World/issues/1\" },\n \"comments\": {\n \"href\": https://api.github.com/repos/octocat/Hello-World/issues/1\n ➥ /comments\n },\n \"review_comments\": {\n \"href\": \"https://api.github.com/repos/octocat/Hello-World/pulls/1\n ➥ /comments\"\n \"merge_commit_sha\": \"e5bd3914e2e596debea16f433f57875b5b90bcd6\",\n \"merged\": true,\n \"mergeable\": true,\n \"merged_by\": {\n \"login\": \"octocat\",\n...", "prev_page": {"page_num": 89, "segment_id": "00089"}, "next_page": {"page_num": 91, "segment_id": "00091"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["merged", "github", "pull", "octocat", "comments", "href", "https", "hello", "world", "2011"], "content_type": "tutorial", "domain": "devops|programming", "complexity": "intermediate", "companies": ["GitHub"], "people": ["Mike McQuaid"], "products": [], "technologies": ["Git", "pull requests", "feature branches", "develop branch", "master branch"], "frameworks": [], "methodologies": ["Gitflow"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["GitHub"], "topics": ["Gitflow workflow", "pull requests", "branching structure"], "key_concepts": ["feature branches", "develop branch", "master branch", "pull request process"], "problem_statement": null, "solution_approach": "using Gitflow with pull requests and formal branching", "extraction_method": "lm"}}
{"segment_id": "00091", "page_num": 91, "segment": "68\n\nCHAPTER 4 Trends and data from source control\n\n },\n \"comments\": 10,\n \"commits\": 3,\n \"additions\": 100,\n \"deletions\": 3,\n \"changed_files\": 5\n }\n\nKey stats; very\nuseful for charting\n\nAs you can see, there’s a lot of great data that you can start working with from this\nrequest. The key difference between the data you can get from DVCS over centralized\nVCS is data around collaboration; you can not only see how much code everyone on\nyour team is changing, but you can also see how much they’re participating in each\nother’s work by looking at pull request activity.\n\nLinking in RESTFUL APIs\n\nAs shown in listing 4.1, GitHub uses a technique called linking in its API. In the case\nof listing 4.1, you’ll notice that several sections of the response are HTTP URLs. The\nidea with linking is that the API itself should give you all the information you need to\nget more. In this case, if you want to get all of the data about the different comments,\nfollow the link in the comments block. Listing 4.1 is an abridged example to show you\ngenerally what type of data you can get back from the API, but in a full response there\nwill be many links that point to the details inside each node. RESTful APIs often use\nlinking this way to make pagination, discovery of data, and decoupling different types\nof data generally easier.\n\nLet’s look at the metrics you can pull out of this data and how to combine it with the\nmetrics you already have from our PTS in chapter 4.\n\n4.3\n\nThe data you’ll be working with; what you can get from SCM\nIf you use the system that we built in appendix A, you can create a service that plugs in\nand gets data from your DVCS. Because the data that you can get from a DVCS is so\nmuch richer than that from centralized SCM systems and because they have great APIs\nto work with, this section will focus on getting data from DVCSs. We’ll also explore getting data from a centralized SCM at a high level.\n\n4.3.1\n\nThe data you can get from a DVCS\n\nThe simplest structure you can get from a DVCS is a commit, as shown in figure 4.6.\n\n Figure 4.6 contains a message, an ID that’s usually in the form of an SHA (secure\nhash algorithm), parent URLs that link to the previous history of the commit, and the\nname of the person who created the code that was committed. Keep in mind that who\nwrote the code can be someone different than who commits it to the repo. If you’re\nfollowing good gitflow or feature-branch workflows, you’ll see this often; a developer\nwill submit a pull request and typically the last reviewer or the owner repo will accept\nand usually commit the change.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "4", "chapter_title": "Trends and data from source control", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "CHAPTER 4 Trends and data from source control As you can see, there’s a lot of great data that you can start working with from this\nrequest. The data you can get from a DVCS", "l2_summary": "CHAPTER 4 Trends and data from source control As you can see, there’s a lot of great data that you can start working with from this\nrequest. Let’s look at the metrics you can pull out of this data and how to combine it with the\nmetrics you already have from our PTS in chapter 4. The data you’ll be working with; what you can get from SCM\nIf you use the system that we built in appendix A, you can create a service that plugs in\nand gets data from your DVCS. The data you can get from a DVCS The simplest structure you can get from a DVCS is a commit, as shown in figure 4.6.", "prev_page": {"page_num": 90, "segment_id": "00090"}, "next_page": {"page_num": 92, "segment_id": "00092"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "dvcs", "linking", "will", "comments", "request", "centralized", "much", "code", "pull"], "content_type": "tutorial", "domain": "programming|devops", "complexity": "intermediate", "companies": ["GitHub"], "people": ["Mark Watson"], "products": [], "technologies": ["DVCS", "centralized VCS", "Git", "RESTful APIs"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": false, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["comments", "commits", "additions", "deletions", "changed_files"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["GitHub"], "topics": ["source control metrics", "DVCS vs centralized VCS", "RESTful APIs"], "key_concepts": ["pull request activity", "collaboration data", "linking in RESTful APIs"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00092", "page_num": 92, "segment": "The data you’ll be working with; what you can get from SCM\n\n69\n\nid\nmessage\nurl\n\n1\n\nCommit\n\n1\n\n1\n\n1\n\n1\n\n(User)Author\n\n(User)Committer\n\nname\ndate\n\nname\ndate\n\nurl\nid\n\nBasic info about\nthe commit.\n\n*\n\nParents\n\nWhoever authored the\nchange; it's possible that this\nis someone who submitted\na pull request.\n\nWhoever committed the\nchange; If someone else\nsubmitted a pull request, this\nwill be different than\nthe author.\n\nObject representing the history\nof this commit---where\nit came from.\n\nFigure 4.6 The object structure of a commit in a typical DVCS\n\nSpeaking of pull requests, the typical structure for them is shown in figure 4.7.\n\n In figure 4.7 you have the author of the pull request and a link to all the reviewers\nwho took part in the pull request workflow. You can also get all the comments for that\npull request and the history of where it came from and where it was submitted.\n\n Commits tell you all the code that’s making it into the repo. Pull requests tell you\nabout all the code that tried to make it into the repo. Pull requests are essentially a\n\nPullRequest\n\nid\ntitle\ncreatedDate\nupdatedDate\n\n1\n\n1\n\n1\n\n1\n\nBasic info about\nthe pull request.\n\n1\n\n*\n\n1\n\n*\n\n(Repository)fromRef\n\nComments\n\n(User)Author\n\n(User)Reviewer\n\nname\nproject\nid\n\ncomment\nid\n\nname\nemail\ndisplayName\n\nname\nemail\ndisplayName\n\nThe repo this request\ncame from; when using\ndistributed VCS this helps\noutline the history.\n\nA list of comments\nassociated with this\npull request.\n\nObject representing\nthe user who made\nthe pull request.\n\nObject representing\nthe user(s) who reviewed\nthe pull request.\n\nFigure 4.7 The data structures you can get from a pull request in DVCS\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 5, "cols": 2, "data": [["Commit", ""], ["id\nmessage\nurl", ""], ["", "1 1\n1"], ["(User)Committer", ""], ["name\ndate", ""]], "markdown": "| Commit |  |\n|---|---|\n| id\nmessage\nurl |  |\n|  | 1 1\n1 |\n| (User)Committer |  |\n| name\ndate |  |"}, {"table_id": "table_4", "rows": 3, "cols": 2, "data": [["PullRequest", ""], ["id\ntitle\ncreatedDate\nupdatedDate", ""], ["", "1\n1"]], "markdown": "| PullRequest |  |\n|---|---|\n| id\ntitle\ncreatedDate\nupdatedDate |  |\n|  | 1\n1 |"}], "table_count": 2, "chapter_num": "4", "chapter_title": "Trends and data from source control", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "In figure 4.7 you have the author of the pull request and a link to all the reviewers\nwho took part in the pull request workflow. Pull requests are essentially a Basic info about\nthe pull request.", "l2_summary": "In figure 4.7 you have the author of the pull request and a link to all the reviewers\nwho took part in the pull request workflow. Pull requests are essentially a Basic info about\nthe pull request. A list of comments\nassociated with this\npull request. Object representing\nthe user who made\nthe pull request. Object representing\nthe user(s) who reviewed\nthe pull request.", "prev_page": {"page_num": 91, "segment_id": "00091"}, "next_page": {"page_num": 93, "segment_id": "00093"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": true, "has_citations": false, "key_terms": ["pull", "request", "user", "name", "commit", "author", "object", "figure", "submitted", "representing"], "content_type": "tutorial", "domain": "devops|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["DVCS", "SCM"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": false, "has_antipatterns": false, "has_instructions": true, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["DVCS"], "topics": ["Git commits", "Pull requests in DVCS"], "key_concepts": ["commit structure", "pull request workflow", "repository references", "user roles in pull requests"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00093", "page_num": 93, "segment": "70\n\nCHAPTER 4 Trends and data from source control\n\nPullRequest\n\nid\ntitle\ncreatedDate\nupdatedDate\n\n(User)Reviewer\n\nname\nemail\ndisplayName\n\nComments\n\ncomment\nid\n\n(User)Author\n\nname\nemail\ndisplayName\n\nCommit\n\nsha\ndiff\n\nWHO is helping WHOM\n\nHOW MUCH effort\n\nWHO is working on WHAT\n\nWHAT was done\n\nFigure 4.8 Mapping your questions back to the objects you can get from the DVCS API\n\nrequest to put code into the repo, and by their nature they’re much richer in data\nthan commits. When a user submits a pull request, they ask their peers for comments,\nreviews, and approval. The data that is generated from the pull request process can\ntell you not only who is working on what and how much code they’re writing but also\nwho is helping them and how much cooperation is going on across the team.\n\n Now that you see what kind of data you get back, you can start mapping that data\n\nback to the questions you want answered, as shown in figure 4.8.\n\n Here’s what you know so far:\n\n■ Who is doing what from the User objects that you get back for reviewers and\n\nauthors\n\n■ Who is collaborating through the User objects representing reviewers\n■ How much collaboration is going on through the comments\n\nAll of this is great if you’re looking for data about your team. In addition, you can parse\nthe information from the code changes to find out which files are changing and by how\nmuch. With this information you can answer additional questions like the following:\n\n■ Where are the most changes happening (change hot spots in your code)?\n■ Who spends the most time in which modules (hot spots by user)?\n■ Who is changing the most code?\n\nHot spots in your code point to where the most change is happening. This could\nmean that your team is developing a new feature and they’re iterating on something\nnew or changing something---this is what you’d expect to see. Things you might not\nexpect are spots that you have to iterate every time you change something else---these\nare usually indications of code that’s too tightly coupled or poorly designed. Spots like\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "4", "chapter_title": "Trends and data from source control", "section_num": "2.5.1", "section_title": "Using data to improve what they do every day", "l1_summary": "WHO is working on WHAT ■ Who is doing what from the User objects that you get back for reviewers and ■ Who is changing the most code?", "l2_summary": "WHO is working on WHAT Now that you see what kind of data you get back, you can start mapping that data ■ Who is doing what from the User objects that you get back for reviewers and ■ Where are the most changes happening (change hot spots in your code)? ■ Who spends the most time in which modules (hot spots by user)? ■ Who is changing the most code?", "prev_page": {"page_num": 92, "segment_id": "00092"}, "next_page": {"page_num": 94, "segment_id": "00094"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["what", "code", "data", "user", "much", "spots", "back", "most", "comments", "questions"], "content_type": "tutorial", "domain": "devops|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["source control", "DVCS API", "pull request", "code changes"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["change hot spots", "code changes"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["pull requests", "source control", "code changes"], "key_concepts": ["pull request process", "hot spots in code", "cooperation across the team"], "problem_statement": "Identifying areas of high code change and understanding team collaboration", "solution_approach": "Analyzing pull request data to map questions about team activity", "extraction_method": "lm"}}
{"segment_id": "00094", "page_num": 94, "segment": "The data you’ll be working with; what you can get from SCM\n\n71\n\nthis are time-consuming, often simply because of tech debt that needs to be cleaned\nup. Showing the amount of time spent on problem spots is good evidence that you\nshould fix them for the sake of saving time once the change is made.\n\n4.3.2 Data you can get from centralized SCM\n\nEven though I keep trash-talking SVN and CVS, both are common SCM systems in use by\nmany developers today. This goes back to the fact that changing your SCM system is\ngoing to be a disruption to your team at some level. To move from SVN to Git, for example, your team will have to learn Git, stop working while the code gets migrated, migrate\nthe code, and then get used to working in a different workflow. You can definitely\nexpect this to slow your team down temporarily, but the benefits you’ll get out of the\ndata alone are worth it. This data will give you such greater insight that it gives you the\npotential to find problems and shift direction much faster than the data you can get\nfrom centralized SCM. The move to DVCS also encourages and fosters better collaboration through pull requests and, in my opinion as a developer, is nicer and easier to use.\n\n If I haven’t convinced you\nyet, you can still get some\ndata out of your centralized\nSCM system. Basically, you can\nget who made the change\nand what the change was, as\ndemonstrated in figure 4.9.\n\nWHO is working on WHAT\n\n(User)Author\n\n1\n\n*\n\nCommit\n\nFigure 4.9 The data you can get out of centralized SCM\n\n This data alone is often\nmisleading. You can tell how\nmany\nlines of code were\nchanged and by whom, but that doesn’t give you an idea of effort or productivity. A\njunior programmer can make many commits and write a lot of bad code, whereas a\nsenior developer can find a much more concise way to do the same work, so looking at\nvolume doesn’t indicate productivity. Additionally, some people like to start hacking\naway at a problem and hone it as they go; others like to think about it to make sure\nthey get it right the first time. Both methods are perfectly viable ways to work, but the\ndata you’ll get from them in this context is totally different.\n\n The data from centralized SCM can be useful when combined with other information, such as how many tasks are getting done from your PTS or whether tasks are\nmoving in the wrong direction. Using those as your indicators of good and bad will\nhelp you determine what your SCM data means.\n\n4.3.3 What you can tell from SCM alone\n\nThe minimum set of data you can get from SCM is the amount of code everyone on\nyour team is changing. With no other data you can get answers to questions like these:\n\n■ Who is changing the code?---Who is working on what parts of the product? Who is\n\ncontributing the most change?\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 5, "cols": 3, "data": [["WHO is working on WHAT\n(User)Author\nCommit\n1 *", "", ""], ["(User)Author", "", ""], ["", "", "Commit"], ["", "", ""], ["", "", ""]], "markdown": "| WHO is working on WHAT\n(User)Author\nCommit\n1 * |  |  |\n|---|---|---|\n| (User)Author |  |  |\n|  |  | Commit |\n|  |  |  |\n|  |  |  |"}], "table_count": 1, "chapter_num": "4", "chapter_title": "Trends and data from source control", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "The data you’ll be working with; what you can get from SCM 4.3.2 Data you can get from centralized SCM Figure 4.9 The data you can get out of centralized SCM", "l2_summary": "The data you’ll be working with; what you can get from SCM 4.3.2 Data you can get from centralized SCM Basically, you can\nget who made the change\nand what the change was, as\ndemonstrated in figure 4.9. Figure 4.9 The data you can get out of centralized SCM 4.3.3 What you can tell from SCM alone With no other data you can get answers to questions like these:", "prev_page": {"page_num": 93, "segment_id": "00093"}, "next_page": {"page_num": 95, "segment_id": "00095"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "what", "code", "working", "centralized", "time", "change", "many", "team", "changing"], "content_type": "theory", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["SVN", "Git", "SCM"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["time spent on problem spots", "effort or productivity"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["SVN", "Git", "SCM"], "topics": ["centralized SCM vs DVCS", "data insights from SCM", "team collaboration and workflow"], "key_concepts": ["tech debt", "code changes", "commit history"], "problem_statement": "Managing code changes and improving team productivity", "solution_approach": "Using decentralized version control systems like Git to gain better insights and improve collaboration", "extraction_method": "lm"}}
{"segment_id": "00095", "page_num": 95, "segment": "72\n\nCHAPTER 4 Trends and data from source control\n\n■ How much change is happening in your codebase?---What parts of your product are\n\nundergoing the most change?\n\nAt its core SCM tracks changes to the codebase of your product, which is measured in\nCLOC. Just as velocity is a relative metric whose number means something different to\nevery team that tracks it, CLOC and LOC are the epitome of relative metrics. LOC varies based on the language your code is written in and the coding style of the developers on the team. A Java program will typically have a much higher LOC than a Rails,\nGrails, or Python program because it’s a much wordier language, but it’s even difficult\nto compare two Java programs without getting into discussions on whose style is better.\nAn example is the following listing.\n\nListing 4.2 Two identical conditional statements\n\nif(this == that) {\ndoSomething(this);\nelse {\ndoSomethingElse(that);\n}\n\nStatement 1 = 5 LOC\n\n(this == that) ? doSomething(this) : doSomethingElse(that);\n\nStatement\n2 = 1 LOC\n\nIn this simple case two statements do the same thing, yet their LOC is very different.\nBoth perform the same and produce the same result. The reason to choose one over\nthe other is personal preference, so which is better? Examples like this show that LOC\nis a metric that can’t tell you if changing code is good or bad without taking into\naccount the trends over time.\n\n There are a number of standard charts for SCM data for trending change. If you\nlook at any GitHub project, you’ll find standard charts that you can get to through the\nPulse and Graph sections. FishEye produces the same charts for SVN-based projects.\nAs an example in anticipation of our next chapter, we’ll look at the GoCD repo in\nGitHub, which is the source for a CI and pipeline management server.\n\n Figure 4.10 shows the GitHub Pulse tab, which gives you an overview of the current\nstate of the project. The most valuable section here is the pull request overview on the\nleft side of the page.\n\n The Pulse tab is pretty cool, but just like checking your own pulse, it only tells you\nif the project you’re looking at is alive or not. In the context of GitHub, where open\nsource projects tend to live, it’s important to know if a project is alive or dead so\npotential consumers can decide if they want to use it. To get data that’s more relevant\nfor your purposes, check out the Graphs section. It has the typical breakdowns of\nCLOC and LOC that you’d expect to see from your source control project. Let’s start\nwith contribution over time, shown in figure 4.11.\n\n The first chart shows you total contribution in CLOC. Below that you can see the\ndevelopers who have contributed to the codebase and how much they’ve contributed.\nThis is interesting because you can see who is adding, deleting, and committing the\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "4", "chapter_title": "Trends and data from source control", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "CHAPTER 4 Trends and data from source control Figure 4.10 shows the GitHub Pulse tab, which gives you an overview of the current\nstate of the project. It has the typical breakdowns of\nCLOC and LOC that you’d expect to see from your source control project.", "l2_summary": "CHAPTER 4 Trends and data from source control At its core SCM tracks changes to the codebase of your product, which is measured in\nCLOC. LOC varies based on the language your code is written in and the coding style of the developers on the team. In this simple case two statements do the same thing, yet their LOC is very different. Figure 4.10 shows the GitHub Pulse tab, which gives you an overview of the current\nstate of the project. It has the typical breakdowns of\nCLOC and LOC that you’d expect to see from your source control project.", "prev_page": {"page_num": 94, "segment_id": "00094"}, "next_page": {"page_num": 96, "segment_id": "00096"}, "extended_fields": {"has_code": true, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["project", "source", "much", "which", "cloc", "same", "github", "pulse", "data", "change"], "content_type": "theory", "domain": "devops|programming", "complexity": "intermediate", "companies": ["Alibaba Cloud", "GitHub"], "people": ["Mark Watson"], "products": ["GoCD repo"], "technologies": ["SCM", "CLOC", "LOC", "Java", "Rails", "Grails", "Python"], "frameworks": [], "methodologies": [], "programming_languages": ["Java"], "has_best_practices": false, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["CLOC", "LOC"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["GitHub Pulse", "FishEye"], "topics": ["source control metrics", "code change trends", "programming languages comparison"], "key_concepts": ["SCM tracking changes", "CLOC vs LOC", "trending analysis"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00096", "page_num": 96, "segment": "The data you’ll be working with; what you can get from SCM\n\n73\n\nmost to your codebase. At a glance this may seem like a good way to measure developer productivity, but it’s only a piece of the puzzle. I’ve seen bad developers committing and changing more code than good developers, and I’ve seen great developers at\nthe top of the commit graph in others.\n\nCurrent state of pull\nrequests for this project.\n\nOverview of change set\nthat has happened recently.\n\nClick here for\nthe Pulse tab.\n\nFigure 4.10 The Pulse page on GitHub gives you info on pull requests, issues, and amount of change. This is from\nthe GoCD repo (github.com/gocd/gocd/pulse/weekly).\n\nTotal commit data; the quantity\nof change, nothing about quality.\n\nFigure 4.11 The graph of contributors over time. From GoCD\n(github.com/gocd/gocd/graphs/contributors).\n\nClick here for\nthe Graphs tab.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "4", "chapter_title": "Trends and data from source control", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "Click here for\nthe Pulse tab. This is from\nthe GoCD repo (github.com/gocd/gocd/pulse/weekly). From GoCD\n(github.com/gocd/gocd/graphs/contributors).", "l2_summary": "Click here for\nthe Pulse tab. Figure 4.10 The Pulse page on GitHub gives you info on pull requests, issues, and amount of change. This is from\nthe GoCD repo (github.com/gocd/gocd/pulse/weekly). Figure 4.11 The graph of contributors over time. From GoCD\n(github.com/gocd/gocd/graphs/contributors). Click here for\nthe Graphs tab.", "prev_page": {"page_num": 95, "segment_id": "00095"}, "next_page": {"page_num": 97, "segment_id": "00097"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["gocd", "developers", "change", "pulse", "github", "data", "good", "seen", "commit", "graph"], "content_type": "reference", "domain": "devops", "complexity": "intermediate", "companies": ["GoCD"], "people": ["Mark Watson"], "products": [], "technologies": ["GitHub", "GoCD"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": false, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["commit graph", "pull requests", "issues"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["GitHub", "GoCD Pulse", "GoCD Graphs"], "topics": ["developer productivity metrics", "code commit analysis", "GitHub features"], "key_concepts": ["commit graph", "pull requests", "issues", "change set"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00097", "page_num": 97, "segment": "74\n\nCHAPTER 4 Trends and data from source control\n\nCommits\naggregated by week.\n\nA trend of highs and lows is appearing;\nit would be cool to add PTS data.\n\nCommits by day based\non selected week.\n\nThe team seems to be most\nproductive on Wednesdays.\n\nFigure 4.12 Total commits to this repository over time show the amount of code changed from week to week\nand day to day. From GoCD (github.com/gocd/gocd/graphs/commit-activity).\n\nNext up are the commits over time, as shown in figure 4.12. This shows you how many\ncode changes there are and when they happen.\n\n Figure 4.12 shows the number of commits. To get to CLOC, click the next tab:\n\nCode Frequency.\n\n In figure 4.13 you can see the huge addition, which was the birth of this project in\n\nGitHub. After bit of intense CLOC, it normalizes somewhat.\n\n The last standard chart in GitHub is the punch card, shown in figure 4.14.\n This is valuable in showing you when the most code change occurs in your project.\nIn figure 4.14 you can see that between 10 a.m. and 5 p.m. the most change occurs,\nwith a bit of extra work bleeding early or later than that---pretty much what you’d\nexpect if your team is putting in a normal workweek. If you start to see the bubbles\ngetting bigger on Fridays or at the end of sprints, that’s typically a sign that your team\nis waiting until the last minute to get their code in, which is usually not good. Another\nbad sign is seeing days your team should have off (typically Saturday and Sunday)\nstarting to grow. These trends may not point to the problem, but they at least indicate\nthat there is a problem.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "4", "chapter_title": "Trends and data from source control", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "Next up are the commits over time, as shown in figure 4.12. Figure 4.12 shows the number of commits. In figure 4.14 you can see that between 10 a.m.", "l2_summary": "Commits\naggregated by week. Commits by day based\non selected week. Figure 4.12 Total commits to this repository over time show the amount of code changed from week to week\nand day to day. Next up are the commits over time, as shown in figure 4.12. Figure 4.12 shows the number of commits. In figure 4.14 you can see that between 10 a.m.", "prev_page": {"page_num": 96, "segment_id": "00096"}, "next_page": {"page_num": 98, "segment_id": "00098"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["figure", "commits", "code", "week", "team", "most", "gocd", "github", "trends", "data"], "content_type": "tutorial", "domain": "devops", "complexity": "intermediate", "companies": ["GoCD"], "people": [], "products": [], "technologies": ["source control", "GitHub", "GoCD"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["code changes", "productivity"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["GoCD", "GitHub"], "topics": ["source control trends", "commit activity analysis", "code frequency", "punch card for code change times"], "key_concepts": ["commit activity over time", "productivity patterns", "code changes visualization"], "problem_statement": "Identifying productivity patterns and potential issues in a development team's workflow", "solution_approach": "Analyzing commit activity, code frequency, and punch card data to understand and improve team productivity", "extraction_method": "lm"}}
{"segment_id": "00098", "page_num": 98, "segment": "The data you’ll be working with; what you can get from SCM\n\n75\n\nHuge addition and deletion\nwhen project starts its life.\n\nUpward spikes above zero line\nare LOC added. Downward\nspikes are LOC deleted.\n\nBalanced deletes and\nadditions point to refactors\nrather than new features.\n\nFigure 4.13 The Code Frequency tab shows how much code is changing over time in CLOC. From GoCD\n(github.com/gocd/gocd/graphs/code-frequency).\n\nMost work done from 9 am to 5 pm, Monday\nthrough Friday. That’s pretty normal.\n\nFigure 4.14 The SCM punch card chart shows the days when the most change in your code occurs.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "4", "chapter_title": "Trends and data from source control", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "Figure 4.13 The Code Frequency tab shows how much code is changing over time in CLOC. From GoCD\n(github.com/gocd/gocd/graphs/code-frequency). Figure 4.14 The SCM punch card chart shows the days when the most change in your code occurs.", "l2_summary": "The data you’ll be working with; what you can get from SCM Upward spikes above zero line\nare LOC added. Downward\nspikes are LOC deleted. Figure 4.13 The Code Frequency tab shows how much code is changing over time in CLOC. From GoCD\n(github.com/gocd/gocd/graphs/code-frequency). Figure 4.14 The SCM punch card chart shows the days when the most change in your code occurs.", "prev_page": {"page_num": 97, "segment_id": "00097"}, "next_page": {"page_num": 99, "segment_id": "00099"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["code", "gocd", "when", "spikes", "figure", "frequency", "shows", "most"], "content_type": "practice", "domain": "devops|programming", "complexity": "intermediate", "companies": ["GoCD"], "people": ["Mark Watson"], "products": [], "technologies": ["SCM", "CLOC"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": false, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["Code changes over time"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["GoCD", "GitHub"], "topics": ["SCM analysis", "Code frequency", "Development hours distribution"], "key_concepts": ["Code addition and deletion", "LOC changes", "Development time patterns"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00099", "page_num": 99, "segment": "76\n\nCHAPTER 4 Trends and data from source control\n\nCommits by different\nfrequencies; still only\ncounting LOC\n\nFigure 4.15 SCM\ndata rendered\nthrough FishEye\n\nIf you’re using SVN, then you’ll have to use a third-party tool that gets the data from\nyour VCS and displays it for you. Figure 4.15 shows the previous data as visualized by\nFishEye (/software/fisheye).\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "4", "chapter_title": "Trends and data from source control", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "CHAPTER 4 Trends and data from source control Figure 4.15 SCM\ndata rendered\nthrough FishEye Figure 4.15 shows the previous data as visualized by\nFishEye (/software/fisheye).", "l2_summary": "CHAPTER 4 Trends and data from source control Commits by different\nfrequencies; still only\ncounting LOC Figure 4.15 SCM\ndata rendered\nthrough FishEye If you’re using SVN, then you’ll have to use a third-party tool that gets the data from\nyour VCS and displays it for you. Figure 4.15 shows the previous data as visualized by\nFishEye (/software/fisheye). Licensed to Mark Watson <nordickan@gmail.com>", "prev_page": {"page_num": 98, "segment_id": "00098"}, "next_page": {"page_num": 100, "segment_id": "00100"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "fisheye", "figure"], "content_type": "reference", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["SVN", "FishEye"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": false, "has_antipatterns": false, "has_instructions": false, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["FishEye"], "topics": ["source control management", "SVN visualization", "third-party tools"], "key_concepts": ["commits by different frequencies", "LOC counting", "SCM data visualization"], "problem_statement": "Visualizing SVN commit data", "solution_approach": "Using FishEye for SCM data visualization", "extraction_method": "lm"}}
{"segment_id": "00100", "page_num": 100, "segment": "Key SCM metrics: spotting trends in your data\n\n77\n\nA note about FishEye\n\nIf you’re using centralized SCM, specifically SVN, then FishEye is the standard commercial tool used to depict SCM data. There are also open source tools that you can\nget to run these stats locally, but FishEye is a standard commercial off-the-shelf (COTS)\nproduct that teams use to get this data into pretty charts on web pages that everyone\ncan see.\n\nIf you want to get this data from the API to show your own graphs, check out the\nRepository Statistics API: GET /repos/:owner/:repo/stats/contributors.1 This will\nreturn data in the structure shown in figure 4.16.\n\n If you’re starting with the basics, this is a pretty convenient API to work with.\n\n4.4\n\nKey SCM metrics: spotting trends in your data\nInstead of focusing on LOC, we’re going to look at some of the richer data you can get\nif your team is submitting pull requests and using CI to build and deploy code. We’re\ngoing to combine that with the data we’re already looking at from chapter 3.\n\n We’ll look at the following data from our SCM:\n\n■ Pull requests\n■ Denied pull requests\n■ Merged pull requests\n■ Commits\n■ Reviews\n\nStatistics\n\ntotalCommits\n\n1\n\n1\n\n*\n\n(User)Author\n\nname\nproject\nid\n\nThe authors who are\nparticipating in a repo\n\n*\n\nWeeks\n\nstartOfWeek\nadditions\ndeletions\ncommits\n\nThe amount of change\nbroken out by week\n\nFigure 4.16 GitHub Repository Statistics data structure\n\n1 “Get contributors list with additions, deletions, and commit counts,” developer.github.com/v3/repos/\n\nstatistics/#contributors.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "4", "chapter_title": "Trends and data from source control", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "Key SCM metrics: spotting trends in your data We’ll look at the following data from our SCM: Figure 4.16 GitHub Repository Statistics data structure", "l2_summary": "Key SCM metrics: spotting trends in your data If you want to get this data from the API to show your own graphs, check out the\nRepository Statistics API: GET /repos/:owner/:repo/stats/contributors.1 This will\nreturn data in the structure shown in figure 4.16. We’re\ngoing to combine that with the data we’re already looking at from chapter 3. We’ll look at the following data from our SCM: Figure 4.16 GitHub Repository Statistics data structure statistics/#contributors.", "prev_page": {"page_num": 99, "segment_id": "00099"}, "next_page": {"page_num": 101, "segment_id": "00101"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "statistics", "pull", "requests", "fisheye", "contributors", "metrics", "spotting", "trends", "using"], "content_type": "tutorial", "domain": "devops|programming", "complexity": "intermediate", "companies": ["GitHub"], "people": ["Mark Watson"], "products": [], "technologies": ["SVN", "FishEye", "API", "SCM", "Git", "GitHub"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": false, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["Pull requests", "Denied pull requests", "Merged pull requests", "Commits", "Reviews"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["FishEye", "Repository Statistics API"], "topics": ["SCM metrics", "Data visualization", "API usage"], "key_concepts": ["Pull requests", "Commits", "Reviews", "Trends spotting"], "problem_statement": "How to effectively use SCM tools and APIs for data analysis and visualization", "solution_approach": "Utilizing FishEye, API endpoints, and specific metrics like pull requests and commits", "extraction_method": "lm"}}
{"segment_id": "00101", "page_num": 101, "segment": "78\n\nCHAPTER 4 Trends and data from source control\n\n■ Comments\n■ CLOC (helps calculate risk)\n\n4.4.1\n\nCharting SCM activity\n\nIf you’re using standard SCM systems, then you probably already have the basic LOC\nstats that we discussed earlier. To get some information that can really help you, we’ll\nstart off by looking at pull request data to see what that tells you.\n\n If you start off with something simple like the number of pull requests, you simply\n\nhave to count the requests.\n\n As shown in figure 4.17, if all is well you should have a lot more comments than\npull requests. This delta is dependent on how you do pull requests on your team. If\nyou have a team with four to six developers, you may open the pull request to the\nentire team. In this case you’ll probably see something like two to three members of\nthe team commenting on pull requests. If you have a bigger team, then opening the\npull request to the entire team may be a bit heavy-handed, so a developer may open it\nto just four to six people with the intention of getting feedback from two or three\npeers. However you do it, you should see comments parallel pull requests, and you\nshould see comments and reviews be at least double the value of the number of pull\nrequests, as you saw in figure 4.17.\n\n One thing to note is that figure 4.17 is aggregating data every sprint. That means\nyou’re seeing the total pull requests and the total comments after the sprint period\n(usually two to three weeks) is complete. If you’re practicing Kanban and/or tracking\nthese stats with greater granularity, then another trend you might want to look for is\nthe difference between when pull requests are open, when comments are added, and\nwhen commits are finalized. Ideally you want pull requests to be committed soon after\nthey’re submitted. If for some reason you see the trend shown in figure 4.18, you may\nhave an issue.\n\nTeam SCM stats over time\n\nPull requests and comments\nare mostly parallel.\n\n100\n\n80\n\n60\n\n40\n\n20\n\n1\n\nComments\n\nPull\nrequests\n\n2\n\n3\n\nTeam stats\n\n4\n\n5\n\nFigure 4.17 Pull requests charted with pull request comments\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "4", "chapter_title": "Trends and data from source control", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "have to count the requests. Pull requests and comments\nare mostly parallel. Figure 4.17 Pull requests charted with pull request comments", "l2_summary": "have to count the requests. As shown in figure 4.17, if all is well you should have a lot more comments than\npull requests. This delta is dependent on how you do pull requests on your team. However you do it, you should see comments parallel pull requests, and you\nshould see comments and reviews be at least double the value of the number of pull\nrequests, as you saw in figure 4.17. Pull requests and comments\nare mostly parallel. Figure 4.17 Pull requests charted with pull request comments", "prev_page": {"page_num": 100, "segment_id": "00100"}, "next_page": {"page_num": 102, "segment_id": "00102"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["pull", "requests", "comments", "team", "have", "figure", "stats", "request", "data", "then"], "content_type": "tutorial", "domain": "devops|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["source control management systems", "pull requests"], "frameworks": [], "methodologies": ["Kanban"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["comments", "pull requests"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["source control management", "pull request analysis", "team collaboration metrics"], "key_concepts": ["CLOC", "SCM activity", "comments on pull requests", "pull request comments ratio"], "problem_statement": "Improving team collaboration and identifying potential issues in source control management processes", "solution_approach": "Analyzing pull request data, specifically the number of comments relative to pull requests, to ensure effective communication and timely feedback", "extraction_method": "lm"}}
{"segment_id": "00102", "page_num": 102, "segment": "Case study: moving to the pull request workflow and incorporating quality engineering\n\n79\n\nComments are lagging\nbehind pull requests.\n\nCode reviews and\ncommits happen all at\nonce and pull requests stop.\n\nTeam SCM stats over time\n\nComments\n\nPull\nrequests\n\nCommits\n\n20\n\n15\n\n10\n\n5\n\n0\n10-1\n\n10-2\n\n10-3\n\nDate\n\n10-4\n\n10-5\n\nFigure 4.18 Commits and comments lagging behind pull requests is usually a bad pattern.\n\nFor some reason your team isn’t doing code reviews when they’re submitted. This increases the time it takes to complete issues. The biggest danger in leaving code reviews\nopen is that after a while they lose context or developers will be pressured to close the\nissues and move them along. Ideally you want that gap to be as short as possible.\n\n Depending on the SCM system, you can also see some pull requests denied. This is\nan interesting data point as a check to your pull request quality. Typically even the best\nteam should have pull requests denied due to mistakes of some kind---everyone\nmakes mistakes. You can add denied pull requests to your chart and expect to see a\nsmall percentage of them as a sign of a healthy process. If your denied pull requests\nare high, that’s likely a sign that either the team isn’t working well together or you\nhave a bunch of developers trying to check in bad code. If you don’t have any denied\npull requests, that’s usually a sign that code reviewers aren’t doing their jobs and are\napproving pull requests to push them through the process.\n\nCase study: moving to the pull request workflow and\nincorporating quality engineering\nWe’ve been through how to collect and analyze SCM data, so it’s time to show it in\naction. In the case study that follows, we demonstrate how to move to the pull request\nworkflow and integrate quality engineers on the team for better software quality. This\nreal-world scenario shows you to apply this to your process.\n\n The team in question was generating a lot of bugs. Many of them were small issues\nthat should have been caught much earlier in the development process. Regardless,\nevery time the team cut a release they would deploy their code, turn it over to the\nquality management (QM) team, and wait for the bugs to come in. They decided to\nmake a change to improve quality.\n\n4.5\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "4", "chapter_title": "Trends and data from source control", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "Case study: moving to the pull request workflow and incorporating quality engineering Comments are lagging\nbehind pull requests. Figure 4.18 Commits and comments lagging behind pull requests is usually a bad pattern.", "l2_summary": "Case study: moving to the pull request workflow and incorporating quality engineering Comments are lagging\nbehind pull requests. Code reviews and\ncommits happen all at\nonce and pull requests stop. Team SCM stats over time Figure 4.18 Commits and comments lagging behind pull requests is usually a bad pattern. This is\nan interesting data point as a check to your pull request quality.", "prev_page": {"page_num": 101, "segment_id": "00101"}, "next_page": {"page_num": 103, "segment_id": "00103"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["pull", "requests", "team", "quality", "code", "denied", "request", "time", "have", "process"], "content_type": "case_study", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["SCM system"], "frameworks": [], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["Comments", "Pull requests", "Commits"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["pull request workflow", "code reviews", "quality engineering"], "key_concepts": ["lagging comments behind pull requests", "denied pull requests as a quality check", "improving software quality through better processes"], "problem_statement": "Team is generating many bugs due to lack of timely code reviews and poor SCM practices.", "solution_approach": "Moving to the pull request workflow and incorporating quality engineering for better software quality.", "extraction_method": "lm"}}
{"segment_id": "00103", "page_num": 103, "segment": "80\n\nCHAPTER 4 Trends and data from source control\n\nCommits and pull requests\nare pretty much parallel.\n\nBugs aren’t\ntrending down.\n\nSCM stats against bugs\n\n60\n\n50\n\n40\n\n30\n\n20\n\n15\n\nPull\nrequests\n\nCommits\n\nBugs\n\n16\n\n17\n\nSprint\n\n18\n\n19\n\nFigure 4.19 Bugs aren’t trending down as the team starts doing pull requests.\n\nAfter discussing the issue, the team decided to try out the pull request workflow. They\nwere already using Git, but the developers were committing their code to a branch\nand merging it all into the master before cutting a release. They decided to start tracking commits, pull requests, and bugs to see if using pull requests decreased their bug\ncount. After a few sprints they produced the graph shown in figure 4.19.\n\n To make trends a bit easier to see, we divided the pull requests and commits by\ntwo so there wasn’t such a discrepancy between the metrics. The result is shown in\nfigure 4.20.\n\nDividing pull requests\nand commits in half to\nenhance bug trending\n\nSCM stats against bugs\n\n28\n\n26\n\n24\n\n22\n\n20\n\n15\n\nPull\nrequests\n\nCommits\n\nBugs\n\n16\n\n17\n\nSprint\n\n18\n\n19\n\nFigure 4.20 The same data with variance decreases between bugs and other data\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "4", "chapter_title": "Trends and data from source control", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "Commits and pull requests\nare pretty much parallel. Figure 4.19 Bugs aren’t trending down as the team starts doing pull requests. They decided to start tracking commits, pull requests, and bugs to see if using pull requests decreased their bug\ncount.", "l2_summary": "Commits and pull requests\nare pretty much parallel. Bugs aren’t\ntrending down. SCM stats against bugs Figure 4.19 Bugs aren’t trending down as the team starts doing pull requests. They decided to start tracking commits, pull requests, and bugs to see if using pull requests decreased their bug\ncount. Dividing pull requests\nand commits in half to\nenhance bug trending", "prev_page": {"page_num": 102, "segment_id": "00102"}, "next_page": {"page_num": 104, "segment_id": "00104"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["pull", "requests", "bugs", "commits", "figure", "data", "trending", "trends", "aren", "down"], "content_type": "case_study", "domain": "devops|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Git", "pull request workflow"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["pull requests", "commits", "bugs"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["source control metrics", "pull request workflow implementation", "bug tracking"], "key_concepts": ["commit and pull request parallelism", "trend analysis of bugs", "Git usage in development process"], "problem_statement": "Decrease in bug count through the use of pull requests", "solution_approach": "Implementing a pull request workflow to track commits, pull requests, and bugs", "extraction_method": "lm"}}
{"segment_id": "00104", "page_num": 104, "segment": "Case study: moving to the pull request workflow and incorporating quality engineering\n\n81\n\nSCM stats against bugs\n\nNot much commenting is\nhappening on this team.\n\n30\n\n23\n\n16\n\n9\n\n2\n15\n\nPull\nrequests\n\nCommits\n\nBugs\n\nComments\n\n16\n\n17\n\nSprint\n\n18\n\n19\n\nFigure 4.21 Adding comments to the graph and finding an ugly trend\n\nThat makes it a lot easier to see the variance. As you can tell from figure 4.20, there\nwasn’t much change. Even though there’s a big dip in bugs from sprint 18 to 19, bugs\nweren’t decreasing over time; there was just a big jump in bugs in sprint 18. After discussing the situation, the team decided to add more data points to the mix. To see\nhow much collaboration was happening in the pull requests, they began adding comments to their graphs. That resulted in the chart shown in figure 4.21. To keep things\nconsistent, we divided comments by two.\n\n Figure 4.21 shows that there weren’t many comments along with the pull requests,\nwhich implies there wasn’t much collaboration going on. Because the bug trend\nwasn’t changing, it looked like the changes to their process weren’t having any effect\nyet. The workflow itself wasn’t producing the change the development team wanted;\nthey needed to make a bigger impact on their process. To do this they decided that\ndevelopers should act like the QM team when they were put on a pull request. The\nperspective they needed wasn’t just “is this code going to solve the problem?” but “is\nthis code well built and what can go wrong with it?” There was some concern about\ndevelopers accomplishing less if they had to spend a lot of time commenting on other\ndevelopers’ code and acting like the QM team. They moved one of their QM members\nover to the development team as a coach, and the team agreed that if this would result\nin fewer bugs then the effort spent up front was time well spent. They started taking\nthe time to comment on each other’s code and ended up iterating quite a bit more on\ntasks before checking them in. A few sprints of this process resulted in the graph\nshown in figure 4.22.\n\n Figure 4.22 shows that as collaboration between development and quality\nincreased---in this case shown through comments in pull requests---the number of\nbugs went down. This was great news for the team, so they decided to take the process\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "4", "chapter_title": "Trends and data from source control", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "Not much commenting is\nhappening on this team. As you can tell from figure 4.20, there\nwasn’t much change. Figure 4.21 shows that there weren’t many comments along with the pull requests,\nwhich implies there wasn’t much collaboration going on.", "l2_summary": "Not much commenting is\nhappening on this team. Figure 4.21 Adding comments to the graph and finding an ugly trend As you can tell from figure 4.20, there\nwasn’t much change. Even though there’s a big dip in bugs from sprint 18 to 19, bugs\nweren’t decreasing over time; there was just a big jump in bugs in sprint 18. That resulted in the chart shown in figure 4.21. Figure 4.21 shows that there weren’t many comments along with the pull requests,\nwhich implies there wasn’t much collaboration going on.", "prev_page": {"page_num": 103, "segment_id": "00103"}, "next_page": {"page_num": 105, "segment_id": "00105"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["team", "bugs", "pull", "comments", "figure", "there", "wasn", "much", "requests", "time"], "content_type": "case_study", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": true, "has_metrics": true, "metrics": ["SCM stats against bugs", "Pull requests", "Commits", "Bugs", "Comments"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["pull request workflow", "quality engineering", "bug tracking", "team collaboration"], "key_concepts": ["pull request workflow", "quality engineering", "code comments", "bug reduction"], "problem_statement": "Decrease in bug count and increase team collaboration", "solution_approach": "Implementing pull request workflow, adding code comments, and involving quality engineers in the development process", "extraction_method": "lm"}}
{"segment_id": "00105", "page_num": 105, "segment": "82\n\nCHAPTER 4 Trends and data from source control\n\nSCM stats against bugs\n\nEverything is trending\nin the right direction. Yay!\n\n30\n\n23\n\n16\n\n9\n\n2\n15\n\nPull\nrequests\n\nCommits\n\nBugs\n\nComments\n\n16\n\n17\n\n18\n\nSprint\n\n19\n\n20\n\n21\n\nFigure 4.22 Everything is trending in the right direction!\n\none step further. The development managers brought in another member of the QM\nteam to work with the developers on code reviews and quality checks to avoid throwing code over the wall to the whole QM team.\n\nTest engineers\n\nFor a long time the role of the quality department in software engineering has involved\nchecking to make sure features were implemented to spec. That’s not an engineering\ndiscipline, and as a result many people in the QA/QM space were not engineers. To\ntruly have an autonomous team, quality engineering has to be a significant part of\nthe team. The role of the quality engineer, a.k.a. QE, a.k.a. SDET, a.k.a. test engineer,\nhas started to become more and more popular. But as quality moves from one state\nto another in the world of software engineering, this role isn’t clearly defined, and often\nyou have either someone with an old quality background who recently learned to write\ncode or you have an expert in testing technology. Neither of these works; you need a\nsenior engineer with a quality mindset. This topic could fill another book, so we’ll leave\nit at that.\n\nOver time, commits and pull requests started increasing as well. As the development\nteam started thinking with a quality mindset, they started writing better code and producing fewer bugs. The combined QM and development teams found and fixed many\nissues before deploying their code to the test environment.\n\n4.6\n\nSummary\nSource control is where your code is being written and reviewed and is a great source\nto complement PTS data for better insight into how your team is operating. Using the\npull request workflow and distributed version control can give you a lot more data\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "4", "chapter_title": "Trends and data from source control", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "CHAPTER 4 Trends and data from source control To\ntruly have an autonomous team, quality engineering has to be a significant part of\nthe team. The role of the quality engineer, a.k.a.", "l2_summary": "CHAPTER 4 Trends and data from source control The development managers brought in another member of the QM\nteam to work with the developers on code reviews and quality checks to avoid throwing code over the wall to the whole QM team. To\ntruly have an autonomous team, quality engineering has to be a significant part of\nthe team. The role of the quality engineer, a.k.a. test engineer,\nhas started to become more and more popular. As the development\nteam started thinking with a quality mindset, they started writing better code and producing fewer bugs.", "prev_page": {"page_num": 104, "segment_id": "00104"}, "next_page": {"page_num": 106, "segment_id": "00106"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["quality", "team", "code", "engineering", "started", "data", "source", "control", "bugs", "pull"], "content_type": "practice", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["source control", "pull request workflow", "distributed version control"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["SCM stats against bugs", "pull requests", "commits", "bugs", "comments"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["source control trends", "quality management in software engineering", "code review and quality checks"], "key_concepts": ["SCM stats against bugs", "pull requests", "commits", "bugs", "comments", "code reviews", "quality mindset"], "problem_statement": "Improving code quality and reducing bugs through better source control practices and quality management.", "solution_approach": "Implementing pull request workflows, distributed version control, and integrating development and quality management teams.", "extraction_method": "lm"}}
{"segment_id": "00106", "page_num": 106, "segment": "Summary\n\n83\n\nthan non-distributed SCMs. Often web-based DVCS systems, like GitHub, will have\nbuilt-in charts and graphs you can use to get a picture of how your team is using them.\n\n■ Teams use source control management to manage their codebase.\n■ Here are some things you can learn from SCM data alone:\n\n■ Who is changing the code?\n■ How much change is happening in the codebase?\n\n■ Here are some questions you can answer with just SCM data:\n\n■ Who is working on what?\n■ Who is helping whom?\n■ How much effort is going into the work?\n\n■ Use pull requests with DVCS to obtain the richest data out of your SCM.\n■ Look for these key trends from SCM:\n\n■ Relationship between pull requests, commits, and comments.\n■ Denied pull requests versus merged pull requests.\n■ CLOC over time.\n■ SCM data against PTS data to see how they affect each other.\n■ DVCSs are superior to centralized ones for a variety of reasons:\n\n■ They provide much richer data than centralized SCM systems.\n■ They can improve the development process by using recommended flows.\n■ They tend to have RESTful APIs for easier data collecting.\n\n■ Pull requests combined with comments and code reviews add another dimension to the team’s collaborative process.\n\n■ GitHub with its Pulse and Graph tabs contains a lot of useful information about\n\nthe health of the project.\n\n■ Visualization techniques are available for centralized VCS through commercial\n\nproducts.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "4", "chapter_title": "Trends and data from source control", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "■ Here are some things you can learn from SCM data alone: ■ Here are some questions you can answer with just SCM data: ■ Use pull requests with DVCS to obtain the richest data out of your SCM.", "l2_summary": "■ Here are some things you can learn from SCM data alone: ■ Here are some questions you can answer with just SCM data: ■ Use pull requests with DVCS to obtain the richest data out of your SCM. ■ Denied pull requests versus merged pull requests. ■ SCM data against PTS data to see how they affect each other. ■ They provide much richer data than centralized SCM systems.", "prev_page": {"page_num": 105, "segment_id": "00105"}, "next_page": {"page_num": 107, "segment_id": "00107"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["data", "pull", "requests", "much", "centralized", "than", "dvcs", "systems", "github", "have"], "content_type": "theory", "domain": "devops|management", "complexity": "intermediate", "companies": ["GitHub"], "people": [], "products": [], "technologies": ["source control management", "DVCS", "SCM", "pull requests", "RESTful APIs"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["who is changing the code", "how much change is happening in the codebase", "relationship between pull requests, commits, and comments", "denied pull requests versus merged pull requests", "CLOC over time", "SCM data against PTS data"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["GitHub"], "topics": ["source control management", "DVCS systems", "pull requests", "SCM data analysis"], "key_concepts": ["codebase management", "team collaboration", "data visualization", "development process improvement"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00107", "page_num": 107, "segment": "Trends and data from CI\nand deployment servers\n\nThis chapter covers\n\n■ What can you learn from your CI systems, CT,\n\nand CD data alone\n\n■ How to utilize your CI to get the richest data\n\npossible\n\n■ How the addition of data from CI and\n\ndeployment servers enhances and fills gaps in\nPTS and SCM data\n\nNow that we’ve been through project tracking and source control, we’re moving to\nthe next step, which is your CI system. In the scope of our application lifecycle, CI is\nhighlighted in figure 5.1.\n\n Your CI server is where you build your code, run tests, and stage your final artifacts, and, in some cases, even deploy them. Where there are common workflows\nwhen it comes to task management and source control, CI tends to vary greatly\nfrom team to team. Depending on what you’re building, how you’re building it is\nthe biggest variable at play here.\n\n Before we dive into collecting data, we’ll talk about the elements of continuous\ndevelopment so you know what to look for and what you can utilize from your\n\n84\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "4", "chapter_title": "Trends and data from source control", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "Trends and data from CI\nand deployment servers ■ How to utilize your CI to get the richest data ■ How the addition of data from CI and", "l2_summary": "Trends and data from CI\nand deployment servers ■ What can you learn from your CI systems, CT, ■ How to utilize your CI to get the richest data ■ How the addition of data from CI and deployment servers enhances and fills gaps in\nPTS and SCM data Depending on what you’re building, how you’re building it is\nthe biggest variable at play here.", "prev_page": {"page_num": 106, "segment_id": "00106"}, "next_page": {"page_num": 108, "segment_id": "00108"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["data", "what", "deployment", "servers", "utilize", "source", "control", "where", "team", "building"], "content_type": "tutorial", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["CI systems", "CT", "CD data", "PTS", "SCM"], "frameworks": [], "methodologies": ["Continuous development"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["KPIs"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["CI systems", "CT and CD data", "PTS and SCM data"], "key_concepts": ["Continuous integration", "Data utilization", "Project tracking", "Source control management"], "problem_statement": "How to effectively utilize CI, CT, and CD data for project tracking and source control management.", "solution_approach": "Collecting and analyzing data from CI systems to enhance project tracking and source control.", "extraction_method": "lm"}}
{"segment_id": "00108", "page_num": 108, "segment": "85\n\nManage\ntasks and\nbugs\n\nProject\ntracking\n\nManage\ncode and\ncollaboration\n\nGenerate\nbuilds and\nrun tests\n\nMove code\nacross\nenvironments\n\nSource\ncontrol\n\nContinuous\ndevelopment\n\nDeployment\ntools\n\nEnsure\neverything\nis working\n\nApplication\nmonitoring\n\nFigure 5.1 You are here: continuous integration in the application lifecycle.\n\nPrevious chapters\n\nYou are here\n\ndevelopment process. If we revise the questions we asked in chapters 3 and 4 a bit, we\nget the types of who, what, and when shown in figure 5.2 through data from CI.\n\n Here are questions you can answer from your CI systems:\n\n■ How fast are you delivering changes to your consumer?\n■ How fast can you deliver changes to your consumer?\n■ How consistently does your team do their work?\n■ Are you producing good code?\n\nIf you combine CI data with your PTS data from chapter 3 and SCM data from chapter\n4, you can get some really interesting insights with these questions:\n\n■ Are your tasks appropriately sized?\n■ Are your estimates accurate?\n■ How much is your team actually getting done?\n\nWe’re going to take a much closer look at combining data in chapter 7. For now, we’ll\nmake sure you’re clear on what continuous (fill in the blank) means.\n\nBuild stats from CI show\nyou how well your team\ncan follow the process.\n\nYou can also break\nthis down by team\nmember.\n\nHow DISCIPLINED the team is\n\nHow CONSISTENTLY you are delivering\n\nHow GOOD your code is\n\nThorough test results\ncan point to overall\ncode quality.\n\nMaybe the most\nimportant metric you\ncan track.\n\nFigure 5.2 The hows from CI: the basic\nquestions you can answer\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "4", "chapter_title": "Trends and data from source control", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "Here are questions you can answer from your CI systems: How DISCIPLINED the team is How GOOD your code is", "l2_summary": "Here are questions you can answer from your CI systems: ■ How fast can you deliver changes to your consumer? ■ How consistently does your team do their work? How DISCIPLINED the team is How CONSISTENTLY you are delivering How GOOD your code is", "prev_page": {"page_num": 107, "segment_id": "00107"}, "next_page": {"page_num": 109, "segment_id": "00109"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["code", "data", "team", "questions", "continuous", "figure", "here", "chapter", "manage", "tasks"], "content_type": "tutorial", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["CI", "PTS", "SCM"], "frameworks": [], "methodologies": ["Continuous Integration"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["How fast are you delivering changes to your consumer?", "How fast can you deliver changes to your consumer?", "How consistently does your team do their work?", "Are you producing good code?", "Are your tasks appropriately sized?", "Are your estimates accurate?", "How much is your team actually getting done?"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["CI systems", "PTS", "SCM"], "topics": ["Continuous Integration", "Application Monitoring", "Code Management"], "key_concepts": ["Continuous Integration", "Build Stats", "Test Results", "Team Discipline", "Consistency", "Code Quality"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00109", "page_num": 109, "segment": "86\n\nCHAPTER 5 Trends and data from CI and deployment servers\n\nDeveloper checks\na change into SCM.\n\nCI system runs tests and\nvarious other build tasks.\n\nOutput is a\ndeployable artifact.\n\nVersion control\nsystem (SCM)\n\nCI system\n\nThe CI system detects\nchanges to source control\nand kicks off builds.\n\nBuild artifact\n\nFigure 5.3 The simplest possible CI pipeline. When developers check in changes, the CI system\nbuilds an artifact.\n\n5.1 What is continuous development?\n\nIn today’s digital world consumers expect the software they interact with every day to\ncontinuously improve. Mobile devices and web interfaces are ubiquitous and are evolving so rapidly that the average consumer of data expects interfaces to continually be\nupdated and improved. To be able to provide your consumers the most competitive\nproducts, the development world has adapted by designing deployment systems that\ncontinuously integrate, test, and deploy changes. When used to their full potential, continuous practices allow development teams to hone their consumer’s experience multiple times a day.\n\n5.1.1\n\nContinuous integration\n\nContinuous development starts with CI, the practice of continuously building and testing your code as multiple team members update it. The simplest possible CI pipeline\nis shown in figure 5.3.\n\n In theory, CI allows teams to collaborate on the same codebase without stepping\non each other’s toes. SCM provides the source of truth for your codebase; multiple\ndevelopers can work on the same software product at the same time and be aware of\neach other’s changes. The CI system takes one or more of the changes that are made\nin SCM and runs a build script that at its simplest will ensure that the code you’re\nworking on compiles. But CI systems are basically servers that are designed to run multiple jobs as defined in the build script for an application and therefore are capable of\nextremely flexible and potentially complex flows. Typical CI jobs include running\ntests, packaging multiple modules, and copying artifacts.\n\n Common CI systems include Jenkins (jenkins-ci.org/), Hudson (hudson-ci.org/),\nBamboo (atlassian.com/software/bamboo), TeamCity (/teamcity/),\nand Travis CI (travis-ci.org/). Although each has slightly different features, they all do\nthe same thing, which is take a change set from your SCM system, run any build scripts\nyou have in your codebase, and output the result somewhere.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "5", "chapter_title": "Trends and data from CI and deployment servers", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "CI system runs tests and\nvarious other build tasks. Version control\nsystem (SCM) 5.1 What is continuous development?", "l2_summary": "CI system runs tests and\nvarious other build tasks. Version control\nsystem (SCM) The CI system detects\nchanges to source control\nand kicks off builds. Figure 5.3 The simplest possible CI pipeline. When developers check in changes, the CI system\nbuilds an artifact. 5.1 What is continuous development?", "prev_page": {"page_num": 108, "segment_id": "00108"}, "next_page": {"page_num": 110, "segment_id": "00110"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["system", "build", "changes", "multiple", "continuous", "development", "same", "other", "artifact", "simplest"], "content_type": "theory", "domain": "devops|architecture", "complexity": "beginner", "companies": ["Jenkins", "Hudson", "Bamboo", "TeamCity", "Travis CI"], "people": [], "products": [], "technologies": ["CI", "SCM", "build scripts", "tests", "artifacts"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Jenkins", "Hudson", "Bamboo", "TeamCity", "Travis CI"], "topics": ["Continuous Integration", "CI Pipeline", "Version Control System (SCM)"], "key_concepts": ["continuous development", "CI system", "deployable artifact", "build script"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00110", "page_num": 110, "segment": "What is continuous development?\n\n87\n\nA bit about Jenkins\n\nJenkins, formerly called Hudson, is an open source CI system that is very popular,\nhas a plethora of plugins for additional functionality, is extremely easy to set up and\nadministrate, and has a very active community. When Hudson became commercial\nsoftware, the open source version continued to evolve as Jenkins. Because Jenkins\nis by far the most popular build system, we’ll be using it for our examples in this book.\nIf you’re not familiar with it and want more information, check out the Jenkins home\npage: jenkins-ci.org/. Note that everything we do with Jenkins in this book could easily\nbe applied to other CI systems.\n\nBuild scripts are the brain of your CI process. A build script is code that tells your CI\nsystem how to compile your software, put it together, and package it. Your build script\nis essentially the instructions that your CI system should follow when it integrates code.\nCommon build frameworks are Ant, Nant, Ivy, Maven, and Gradle. Build scripts are\npowerful tools that manage dependencies, determine when to run tests and what tests\nto run, and check for certain conditions in a build to determine whether it should\ncontinue or stop and return an error. Figure 5.4 shows graphically what a typical build\nscript will control.\n\n Think of your build script as a set of instructions that describes how to build and\n\npackage your code into something you can deploy.\n\nThe CI system detects\nchanges to SCM and\ncalls the build script.\n\nBuild steps can be\nrun in sequence\nor in parallel.\n\nCI server\n\nSCM system\n\nBuild script controls\nthe build process and\nruns on the CI server.\n\nThe output from\nany step can influence\nsubsequent ones.\n\nGet dependencies\n\nStatic analysis\n\nPackage app\n\nRun tests\n\nCreate reports\n\nDeploy\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\nMake sure you have\neverything needed to\nbuild your app.\n\nCheck code coverage,\nrun unit tests, run\nsecurity scans, etc.\n\nPackage your app so\nit's ready for deploy.\n\nOnce your app is\nready, run any integration\ntests you have.\n\nGenerate any reports\nthat haven’t already\nbeen generated.\n\nFigure 5.4 A build script running on a CI server controlling a multistep build process\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 3, "cols": 2, "data": [["", ""], ["", ""], ["", ""]], "markdown": "|  |  |\n|---|---|\n|  |  |\n|  |  |"}], "table_count": 1, "chapter_num": "5", "chapter_title": "Trends and data from CI and deployment servers", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "Build scripts are the brain of your CI process. Think of your build script as a set of instructions that describes how to build and Build script controls\nthe build process and\nruns on the CI server.", "l2_summary": "Build scripts are the brain of your CI process. Think of your build script as a set of instructions that describes how to build and The CI system detects\nchanges to SCM and\ncalls the build script. Build steps can be\nrun in sequence\nor in parallel. Build script controls\nthe build process and\nruns on the CI server. Figure 5.4 A build script running on a CI server controlling a multistep build process", "prev_page": {"page_num": 109, "segment_id": "00109"}, "next_page": {"page_num": 111, "segment_id": "00111"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["build", "jenkins", "script", "system", "tests", "code", "package", "what", "when", "check"], "content_type": "tutorial", "domain": "devops|architecture", "complexity": "intermediate", "companies": ["Jenkins"], "people": [], "products": [], "technologies": ["CI system", "Ant", "Nant", "Ivy", "Maven", "Gradle"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": false, "has_antipatterns": false, "has_instructions": true, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Jenkins", "CI system"], "topics": ["Continuous Development", "Build Scripts", "CI Process"], "key_concepts": ["continuous development", "Jenkins", "build scripts", "CI process", "multistep build process"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00111", "page_num": 111, "segment": "88\n\nCHAPTER 5 Trends and data from CI and deployment servers\n\n5.1.2\n\nContinuous delivery\n\nInterestingly enough, the very first principle in the Agile Manifesto calls out “continuous delivery.”\n\n“Our highest priority is to satisfy the customer through early and continuous delivery of\nvaluable software.”1\n\n ---Principles behind the Agile Manifesto\n\nThe term continuous delivery was thoroughly explored by Jez Humble and David Farley\nin Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation (Addison-Wesley, 2010). CD builds on CI by taking the step of orchestrating\nmultiple builds, coordinating different levels of automated testing, and in advanced\ncases moving the code your CI system built into a production environment for your\nconsumers. Although you can do CD with CI systems, there are specialized CI systems\nthat make CD a bit easier. The idea is built on a few facts:\n\n■ Consumers demand and expect constant improvement to their products.\n\n■\n\n■\n\nIt’s easier to support a bunch of small changes over time than large change sets\nall at once.\nIt’s easier to track the value of change when it’s small and targeted.\n\nExamples of CD systems are Go (www.go.cd), Electric Cloud (electric-cloud.com),\nAnsible (/continuous-delivery), and Octopus (octopusdeploy.com).\nSome of these systems can be used for CI as well but have additional capabilities to\ncoordinate multiple parts of a build chain. This becomes particularly useful when you\nhave complex build chains such as the one in figure 5.5.\n\nBuild AMI\n\nRun\nintegration\ntests\n\nDeploy\nsomewhere\n\nRun more\nintegration\ntests\n\nYes\n\nTests\npass?\n\nNo\n\nRoute\nconsumers\nto new build\n\nYes\n\nTests\npass?\n\nMonitor traffic\nand health\n\nNo\n\nAll\ngood?\n\nNo\n\nRoll\nback\ndeploy\n\nBuild\ndeployable\npackage\n\nRun unit\ntests\n\nYes\n\nTests\npass?\n\nNo\n\nStop\nbuild\n\nNotify\nsomeone\n\nFigure 5.5 An example of a complex build chain\n\n1\n\n “We follow these principles,” agilemanifesto.org/principles.html.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "5", "chapter_title": "Trends and data from CI and deployment servers", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "Run\nintegration\ntests Build\ndeployable\npackage Figure 5.5 An example of a complex build chain", "l2_summary": "“Our highest priority is to satisfy the customer through early and continuous delivery of\nvaluable software.”1 The term continuous delivery was thoroughly explored by Jez Humble and David Farley\nin Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation (Addison-Wesley, 2010). Run\nintegration\ntests Route\nconsumers\nto new build Build\ndeployable\npackage Figure 5.5 An example of a complex build chain", "prev_page": {"page_num": 110, "segment_id": "00110"}, "next_page": {"page_num": 112, "segment_id": "00112"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["build", "continuous", "delivery", "tests", "systems", "principles", "consumers", "easier", "pass", "deployment"], "content_type": "theory", "domain": "devops", "complexity": "intermediate", "companies": ["Go", "Electric Cloud", "Ansible", "Octopus"], "people": ["Jez Humble", "David Farley"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Go", "Electric Cloud", "Ansible", "Octopus"], "topics": ["Continuous Delivery", "Agile Manifesto", "Build Chains"], "key_concepts": ["continuous delivery", "CI systems", "automated testing", "production environment"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00112", "page_num": 112, "segment": "What is continuous development?\n\n89\n\nIn figure 5.5 getting code all the way to production is completely automated and relies\nheavily on testing at every stage of the deployment. It starts off with unit tests, builds\nthe machine image that will get deployed, handles the deployment, and makes sure\nnothing breaks when it finally makes it to the consumer. Complexity and orchestration like this are what make the difference in using a tool for CI or CD.\n\nMoving to continuous delivery\n\nAlthough most software projects have CI, getting your team to CD is first a cultural\nshift. Having your entire team understand and accept that small changes will be\ndeployed to your consumer automatically and frequently can be a bit scary for teams\nthat release even as frequently as every few weeks. Once you come to grips with the\nfact that you can change your consumer’s experience multiple times a day, you’re likely\ngoing to have to make significant changes to your build systems to accommodate the\ntypes of automation you’ll need. You’ll also likely need to make big changes in how\nyou think about testing, because all testing in a CD world should rely on trustworthy\nautomation rather than manual button pushing. What should be obvious (but rarely\nis) is to get the most out of CD you should have a good sense of why you’re doing it\nand how you’re monitoring the success of your changes. Fortunately, if you’re implementing what you’re learning in this book, monitoring should be the easy part.\n\n5.1.3\n\nContinuous testing\n\nContinuous testing (CT) is also a part of CI; it’s the practice of having tests continuously\nrunning on your codebase as you make changes to it. Teams that take the time to automate their testing process, write tests while they’re writing their code, and run those tests\nconstantly throughout the build process and on their local development environments\nend up with another rich set of data to use in the continuous improvement process.\n\n To clarify where this testing happens and what this testing is, we take another look\n\nat figure 5.5 but highlight the testing in figure 5.6.\n\nBuild\ndeployable\npackage\npackage\n\nRun unit\ntests\n\nTests run at\nevery stage\nand determine\nif code should\nmove or not\n\nYes\n\nTests\npass?\n\nNo\n\nStop\nbuild\n\nNotify\nsomeone\n\nBuild AMI\n\nRun\nintegration\ntests\n\nDeploy\nsomewhere\n\nRun more\nintegration\ntests\n\nYes\n\nTests\npass?\n\nNo\n\nRoute\nconsumers\nto new build\n\nYes\n\nTests\npass?\n\nMonitor traffic\nand health\n\nNo\n\nAll\ngood?\n\nNo\n\nRoll\nback\ndeploy\n\nFigure 5.6 Continuous testing in the scope of CI/CD\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "5", "chapter_title": "Trends and data from CI and deployment servers", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "What is continuous development? Run\nintegration\ntests Figure 5.6 Continuous testing in the scope of CI/CD", "l2_summary": "What is continuous development? at figure 5.5 but highlight the testing in figure 5.6. Tests run at\nevery stage\nand determine\nif code should\nmove or not Run\nintegration\ntests Run more\nintegration\ntests Figure 5.6 Continuous testing in the scope of CI/CD", "prev_page": {"page_num": 111, "segment_id": "00111"}, "next_page": {"page_num": 113, "segment_id": "00113"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["tests", "testing", "continuous", "build", "what", "changes", "should", "figure", "make", "code"], "content_type": "theory", "domain": "devops|architecture", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["unit tests", "machine image", "integration tests"], "frameworks": [], "methodologies": ["CI/CD"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["figure 5.5", "figure 5.6"], "topics": ["continuous development", "continuous delivery", "continuous testing"], "key_concepts": ["CI/CD automation", "cultural shift for CD", "automated testing in CI/CD"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00113", "page_num": 113, "segment": "90\n\nCHAPTER 5 Trends and data from CI and deployment servers\n\nFigure 5.6 highlights the fact that every stage in the CD pipeline relies on automated\ntests (CT) to inform the decision on moving to the next stage in the deployment. If\nthis data is rich enough to inform a decision on moving through the automated\ndeployment process, it can also provide good insight into how your team is operating.\nA couple of things that this can help point to, especially when paired with VCS, are:\n\n■ Specific trouble spots in your code that are slowing you down---By identifying sections\nof your codebase that have significant technical debt, you can anticipate updating those sections to take longer than expected. You can use this information to\nadjust estimates or set expectations with stakeholders.\n\n■ Your team’s commitment to automation---Having great automation is key to allowing\nyour team to move faster and get changes out to consumers. Gauging this helps\ndetermine the potential delivery speed of your team.\n\nGetting data from CT is as easy as tapping into the test results that are published in\nyour CI system because the CI/CD system publishes and parses these results.\n\n5.2\n\nPreparing for analysis: generate the richest set of data you can\nThe current health of your build and the build history are default data points your CI\nsystem provides. You can also get results from test runs or whatever build steps you\ndefine. But if you want to answer more interesting questions like the ones I posed at\nthe beginning of the chapter like\n\n■ How fast can you deliver code to the consumer?\n■ Are your estimations accurate?\n■ Are you getting tasks done right the first time?\n■ How much is your team actually getting done?\n\nthen you should use the following tips to ensure you’re generating the richest set of\ndata you can.\n\n5.2.1\n\nSet up a delivery pipeline\n\nA delivery pipeline goes beyond your build scripts. Once your product is tested, built,\nand staged, other coordination typically has to happen. You may need to kick off tests\nfor dependent systems, deploy code with environment-specific parameters based on\ntest results, or combine parallel builds of different parts of your larger system for a complete deployment. Automating more complex build and deploy scenarios can be tricky\nand is exactly what your pipeline specializes in. By automating your complex build process, you can get data out of your pipeline that tells you how fast you’re delivering products to your consumers. When you break down that data point, you can also find where\nyou can make the most improvement to deliver products faster and more efficiently.\n If you have a CI system in place, that’s great; everything you did in the last section\nshould just plug in and start generating reports for you. If you don’t, we recommend\ntaking a look at Jenkins or GoCD (www.go.cd/) for your pipeline. GoCD is a CI system\nlike Jenkins or TeamCity, and all of them are capable of building your code, running\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "5", "chapter_title": "Trends and data from CI and deployment servers", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "CHAPTER 5 Trends and data from CI and deployment servers ■ How fast can you deliver code to the consumer? Set up a delivery pipeline", "l2_summary": "CHAPTER 5 Trends and data from CI and deployment servers A couple of things that this can help point to, especially when paired with VCS, are: Preparing for analysis: generate the richest set of data you can\nThe current health of your build and the build history are default data points your CI\nsystem provides. You can also get results from test runs or whatever build steps you\ndefine. ■ How fast can you deliver code to the consumer? Set up a delivery pipeline", "prev_page": {"page_num": 112, "segment_id": "00112"}, "next_page": {"page_num": 114, "segment_id": "00114"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "pipeline", "system", "build", "team", "deployment", "code", "results", "also", "delivery"], "content_type": "theory", "domain": "devops", "complexity": "intermediate", "companies": ["Jenkins", "GoCD"], "people": [], "products": [], "technologies": ["CI/CD systems", "automated tests", "VCS"], "frameworks": [], "methodologies": ["DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["delivery speed", "estimations accuracy", "tasks completion"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Jenkins", "GoCD"], "topics": ["CI/CD pipeline", "automated testing", "data analysis for improvement"], "key_concepts": ["automated tests in CI/CD", "technical debt identification", "commitment to automation"], "problem_statement": "Improving the efficiency and accuracy of software development processes through better data collection and analysis.", "solution_approach": "Implementing a robust CI/CD pipeline with automated testing, analyzing test results for insights, and setting up delivery pipelines for complex build and deployment scenarios.", "extraction_method": "lm"}}
{"segment_id": "00114", "page_num": 114, "segment": "The data you’ll be working with: what you can get from your CI APIs\n\n91\n\nyour tests, and generating reports that you can bring into your metrics collection system. The cool thing about GoCD is the idea of a pipeline as a first-class citizen, so breaking your build/deploy into steps that are easy to visualize is completely standard and\nnormal. Jenkins is extremely popular and can do pipeline management using plugins.\n\n Once your pipeline is set up you should consider the following:\n\n■ Using SonarQube () for static analysis gives you a very rich\n\n■\n\ndata point on the quality of your code.\nIf you can’t use SonarQube, tools you can build into your build process include\nCobertura (cobertura.github.io/cobertura/),\nJaCoCo (/\njacoco), or NCover (/).\n\n■ A standard test framework that gives you reports that are easily digestible is\nTestNG (testng.org/doc/index.html), which uses ReportNG (reportng\n.uncommons.org) for generating useful reports in your build system that are\navailable through an API.\n\nUsing these technologies in your pipeline will give you better insight into the quality\nof your code and help you find potential problems before they affect your consumers.\nThis insight into quality will also give you insight into your development process.\n\n5.3\n\nThe data you’ll be working with: what you can get from\nyour CI APIs\nBecause CI server data is so flexible, the data you can get out of it depends greatly on\nhow you set up your system. For simple builds it’s possible to get very little data, and\nfor more complex builds it’s possible to get a wealth of data on quality, build time, and\nrelease information. If you set up your CI system with data collection in mind, you can\nget great insight into your development cycle.\n\n5.3.1\n\nThe data you can get from your CI server\n\nYour build scripts define how your project is being built and what happens throughout your build process. In your build scripts you can create reports that are published\nthrough your CI system that give you details on every step you report on. The most\ncommon build servers have great APIs, which should be no surprise because they’re at\nthe heart of automation. If you use Jenkins, you can communicate completely\nthrough REST simply by putting /api/json?pretty=true at the end of any URL that you\ncan access. The following listing shows some of the data you’d get back from the main\nJenkins dashboard by examining Apache’s build server. For the entire response you\ncan look at Apache’s site: builds.apache.org/api/json?pretty=true.\n\nListing 5.1 Partial response from the Jenkins dashboard for Apache’s build server\n\n{\n\"assignedLabels\" : [\n{\n}\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "5", "chapter_title": "Trends and data from CI and deployment servers", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "The data you’ll be working with: what you can get from your CI APIs The data you’ll be working with: what you can get from\nyour CI APIs\nBecause CI server data is so flexible, the data you can get out of it depends greatly on\nhow you set up your system. The data you can get from your CI server", "l2_summary": "The data you’ll be working with: what you can get from your CI APIs data point on the quality of your code. If you can’t use SonarQube, tools you can build into your build process include\nCobertura (cobertura.github.io/cobertura/),\nJaCoCo (/\njacoco), or NCover (/). The data you’ll be working with: what you can get from\nyour CI APIs\nBecause CI server data is so flexible, the data you can get out of it depends greatly on\nhow you set up your system. If you set up your CI system with data collection in mind, you can\nget great insight into your development cycle. The data you can get from your CI server", "prev_page": {"page_num": 113, "segment_id": "00113"}, "next_page": {"page_num": 115, "segment_id": "00115"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["build", "data", "system", "reports", "pipeline", "jenkins", "quality", "insight", "server", "apache"], "content_type": "tutorial", "domain": "devops|architecture", "complexity": "intermediate", "companies": ["GoCD", "Jenkins", "SonarQube", "Apache"], "people": [], "products": [], "technologies": ["CI APIs", "pipeline management", "static analysis", "build scripts", "REST API"], "frameworks": ["TestNG", "ReportNG"], "methodologies": ["DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["quality of code", "build time", "release information"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["GoCD", "Jenkins", "SonarQube", "Cobertura", "JaCoCo", "NCover", "TestNG", "ReportNG"], "topics": ["CI/CD pipeline management", "static code analysis", "test framework integration", "data collection from CI APIs"], "key_concepts": ["pipeline as a first-class citizen", "quality insight", "build process reporting"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00115", "page_num": 115, "segment": "92\n\nCHAPTER 5 Trends and data from CI and deployment servers\n\n],\n\"mode\" : \"EXCLUSIVE\",\n\"nodeDescription\" : \"the master Jenkins node\",\n\"nodeName\" : \"\",\n\"numExecutors\" : 0,\n\"description\" : \"<a href=\\\"http:///\\\"><img\n\nsrc=\\\"https:///images/asf_logo_wide.gif\\\"></img></a>\\r\\n<p>\\r\\\nnThis is a public build and test server for <a\nhref=\\\"http://projects.apache.org/\\\">projects</a> of the\\r\\n<a\nhref=\\\"http:///\\\">Apache Software Foundation</a>.\nAll times on this server are UTC.\\r\\n</p>\\r\\n<p>\\r\\nSee the <a\nhref=\\\"http://wiki.apache.org/general/Jenkins\\\">Jenkins wiki page</a> for\nmore information\\r\\nabout this service.\\r\\n</p>\",\n\n\"jobs\" : [\n{\n\"name\" : \"Abdera-trunk\",\n\"url\" : \"https://builds.apache.org/job/Abdera-trunk/\",\n\"color\" : \"blue\"\n},\n{\n\"name\" : \"Abdera2-trunk\",\n\"url\" : \"https://builds.apache.org/job/Abdera2-trunk/\",\n\"color\" : \"blue\"\n}\n...\n \"mode\": \"EXCLUSIVE\",\n \"nodeDescription\": \"the master Jenkins node\",\n \"nodeName\": \"\",\n \"numExecutors\": 0,\n \"description\": \"This is a public build and test server for projects of\n\nthe Apache Software Foundation”,\n\n \"jobs\": [\n {\n \"name\": \"Abdera-trunk\",\n \"url\": \"https://builds.apache.org/job/Abdera-trunk/\",\n \"color\": \"blue\"\n },\n {\n \"name\": \"Abdera2-trunk\",\n \"url\": \"https://builds.apache.org/job/Abdera2-trunk/\",\n \"color\": \"blue\"\n },\n...\n\nOne key piece of data is missing from here that we’ve been depending on from every\nother data source we’ve looked at so far; did you notice it? Dates are missing! That\nmay seem to put a kink in things if you’re talking about collecting data and mapping it\nover time, but don’t worry; if you dig deeply enough you’ll find them. Jenkins assumes\nthat when you hit a URL you know the current date and time. If you’re going to take\nthis data and save it somewhere, you can simply schedule the data collection at the frequency at which you want to collect data and add the dates to the database yourself.\nAdditionally, if you look for something that references a specific time, you can get the\n\nLicensed to Mark Watson <nordickan@gmail.com>\n\nGeneral\n information\nabout your\nbuild server\n\nLists jobs available\n through this URL", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "5", "chapter_title": "Trends and data from CI and deployment servers", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "\"jobs\" : [\n{\n\"name\" : \"Abdera-trunk\",\n\"url\" : \"https://builds.apache.org/job/Abdera-trunk/\",\n\"color\" : \"blue\"\n},\n{\n\"name\" : \"Abdera2-trunk\",\n\"url\" : \"https://builds.apache.org/job/Abdera2-trunk/\",\n\"color\" : \"blue\"\n}\n... the Apache Software Foundation”, \"jobs\": [\n {\n \"name\": \"Abdera-trunk\",\n \"url\"...", "l2_summary": "CHAPTER 5 Trends and data from CI and deployment servers ],\n\"mode\" : \"EXCLUSIVE\",\n\"nodeDescription\" : \"the master Jenkins node\",\n\"nodeName\" : \"\",\n\"numExecutors\" : 0,\n\"description\" : \"<a href=\\\"http:///\\\"><img src=\\\"https:///images/asf_logo_wide.gif\\\"></img></a>\\r\\n<p>\\r\\\nnThis is a public build and test server for <a\nhref=\\\"http://projects.apache.org/\\\">projects</a> of the\\r\\n<a\nhref=\\\"http:///\\\">Apache Software Foundation</a>. \"jobs\" : [\n{\n\"name\" : \"Abdera-trunk\",\n\"url\" : \"https://builds.apache.org/job/Abdera-trunk/\",\n\"color\" : \"blue\"\n},\n{\n\"name\" : \"Abdera2-trunk\",\n\"url\" : \"https://builds.apache.org/job/Abdera2-trunk/\",\n\"color\" : \"blue\"\n}\n... the Apache Software Foundation”, \"jobs\": [\n {\n \"name\": \"Abdera-trunk\",\n \"url\": \"https://builds.apache.org/job/Abdera-trunk/\",\n \"color\": \"blue\"\n },\n {\n \"name\": \"Abdera2-trunk\",\n \"url\": \"https://builds.apache.org/job/Abdera2-trunk/\",\n \"color\"...", "prev_page": {"page_num": 114, "segment_id": "00114"}, "next_page": {"page_num": 116, "segment_id": "00116"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["apache", "trunk", "data", "jenkins", "https", "href", "http", "server", "name", "abdera"], "content_type": "tutorial", "domain": "devops", "complexity": "intermediate", "companies": ["Apache Software Foundation"], "people": [], "products": ["Jenkins", "Abdera-trunk", "Abdera2-trunk"], "technologies": ["Jenkins", "Apache Software Foundation"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": false, "has_antipatterns": false, "has_instructions": true, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Jenkins"], "topics": ["CI/CD", "Jenkins configuration", "Apache projects"], "key_concepts": ["master Jenkins node", "public build and test server", "jobs in Jenkins"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00116", "page_num": 116, "segment": "The data you’ll be working with: what you can get from your CI APIs\n\n93\n\ndata from that if you want. The next listing shows data from a specific build that has a\ndate/time in the response.\n\nListing 5.2 Jenkins response from a specific build\n\n{\n \"actions\": [\n {},\n {\n \"causes\": [\n {\n \"shortDescription\": \"[URLTrigger] A change within the\n\nAn array of actions\nfrom this build\n\n response URL invocation (<a href=\\\"triggerCauseAction\\\n \">log</a>)\"\n\n }\n ]\n },\n {},\n {\n \"buildsByBranchName\": {\n \"origin/master\": {\n \"buildNumber\": 1974,\n \"buildResult\": null,\n \"marked\": {\n \"SHA1\": \"54ad73e1adb22fd84fdd1dfb5c28175f743d1960\",\n \"branch\": [\n {\n \"SHA1\":\n\nDetails about what\ncode is getting built\n\n\"54ad73e1adb22fd84fdd1dfb5c28175f743d1960\",\n\n \"name\": \"origin/master\"\n }\n ]\n },\n \"revision\": {\n \"SHA1\": \"54ad73e1adb22fd84fdd1dfb5c28175f743d1960\",\n \"branch\": [\n {\n \"SHA1\":\n\n\"54ad73e1adb22fd84fdd1dfb5c28175f743d1960\",\n\n \"name\": \"origin/master\"\n }\n ]\n }\n }\n },\n \"lastBuiltRevision\": {\n \"SHA1\": \"54ad73e1adb22fd84fdd1dfb5c28175f743d1960\",\n \"branch\": [\n {\n \"SHA1\": \"54ad73e1adb22fd84fdd1dfb5c28175f743d1960\",\n \"name\": \"origin/master\"\n }\n ]\n },\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "5", "chapter_title": "Trends and data from CI and deployment servers", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "\"54ad73e1adb22fd84fdd1dfb5c28175f743d1960\", \"name\": \"origin/master\"\n }\n ]\n },\n \"revision\": {\n \"SHA1\": \"54ad73e1adb22fd84fdd1dfb5c28175f743d1960\",\n \"branch\": [\n {\n \"SHA1\": \"54ad73e1adb22fd84fdd1dfb5c28175f743d1960\",", "l2_summary": "Listing 5.2 Jenkins response from a specific build }\n ]\n },\n {},\n {\n \"buildsByBranchName\": {\n \"origin/master\": {\n \"buildNumber\": 1974,\n \"buildResult\": null,\n \"marked\": {\n \"SHA1\": \"54ad73e1adb22fd84fdd1dfb5c28175f743d1960\",\n \"branch\": [\n {\n \"SHA1\": \"54ad73e1adb22fd84fdd1dfb5c28175f743d1960\", \"name\": \"origin/master\"\n }\n ]\n },\n \"revision\": {\n \"SHA1\": \"54ad73e1adb22fd84fdd1dfb5c28175f743d1960\",\n \"branch\": [\n {\n \"SHA1\": \"54ad73e1adb22fd84fdd1dfb5c28175f743d1960\", \"name\": \"origin/master\"\n }\n ]\n }\n }\n },\n \"lastBuiltRevision\": {\n \"SHA1\": \"54ad73e1adb22fd84fdd1dfb5c28175f743d1960\",\n \"branch\": [\n {\n \"SHA1\": \"54ad73e1adb22fd84fdd1dfb5c28175f743d1960\",\n \"name\": \"origin/master\"\n }\n ]\n },", "prev_page": {"page_num": 115, "segment_id": "00115"}, "next_page": {"page_num": 117, "segment_id": "00117"}, "extended_fields": {"has_code": true, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["sha1", "54ad73e1adb22fd84fdd1dfb5c28175f743d1960", "origin", "master", "data", "build", "response", "branch", "name", "what"], "content_type": "tutorial", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["CI APIs", "Jenkins"], "frameworks": [], "methodologies": [], "programming_languages": ["JavaScript"], "has_best_practices": false, "has_antipatterns": false, "has_instructions": true, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Jenkins"], "topics": ["CI/CD", "APIs", "Build Information"], "key_concepts": ["CI APIs", "Jenkins Build Response", "Code Revision"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00117", "page_num": 117, "segment": "94\n\nCHAPTER 5 Trends and data from CI and deployment servers\n\nLinks back to\nthe Git code\nrepository\n\nHow long did\nthis build take?\n\nDid the build\npass or fail?\n\nLists generated artifacts\n\n \"remoteUrls\": [\n \"https://git-wip-us.apache.org/repos/asf/mesos.git\"\n ],\n \"scmName\": \"\"\n },\n {},\n {},\n {}\n ],\n \"artifacts\": [],\n \"building\": false,\n \"description\": null,\n \"duration\": 3056125,\n \"estimatedDuration\": 2568862,\n \"executor\": null,\n \"fullDisplayName\": \"mesos-reviewbot #1974\",\n \"id\": \"2014-10-12_03-11-40\",\n \"keepLog\": false,\n \"number\": 1974,\n \"result\": \"SUCCESS\",\n \"timestamp\": 1413083500974,\n \"url\": \"https://builds.apache.org/job/mesos-reviewbot/1974/\",\n \"builtOn\": \"ubuntu-5\",\n \"changeSet\": {\n \"items\": [],\n \"kind\": \"git\"\n },\n \"culprits\": []\n}\n\nGenerated build number\n\nIs this building now?\n\nHow long should a build take?\n\nWhen did this build complete?\n\nAh, there’s that date! If you’re collecting data moving forward, then it’s not a big deal,\nbut if you want to find trends from the past, you’ll have to do a bit more querying to\nget what you want.\n\n What you want from CI is the frequency at which your builds are good or bad. This\ndata will give you a pretty good indication of the overall health of your project. If\nbuilds are failing most of the time, then a whole host of things can be going wrong,\nbut suffice it to say that something is going wrong. If your builds are always good,\nthat’s usually a good sign, but it doesn’t mean that everything is working perfectly.\nThis metric is interesting by itself but really adds value to data you collect from the rest\nof your application lifecycle.\n\n The other interesting data you can get from CI is the data you generate yourself.\nBecause you control the build system, you can publish pretty much anything you want\nduring the build process and bring that data down to your analytics. These reports can\nanswer the question “Are you writing good code?” Here are some examples of tools\nand frameworks you can use to generate reports:\n\n■ TestNG---Use it to run many test types; ReportNG is its sister reporting format.\n■ SonarQube---Run it to get reports including code coverage, dependency analysis, and code rule analysis.\n\n■ Gatling---Has rich reporting capabilities for performance benchmarking.\n■ Cucumber---Use it for BDD-style tests.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "5", "chapter_title": "Trends and data from CI and deployment servers", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "How long did\nthis build take? Generated build number The other interesting data you can get from CI is the data you generate yourself.", "l2_summary": "CHAPTER 5 Trends and data from CI and deployment servers How long did\nthis build take? Did the build\npass or fail? Generated build number When did this build complete? The other interesting data you can get from CI is the data you generate yourself.", "prev_page": {"page_num": 116, "segment_id": "00116"}, "next_page": {"page_num": 118, "segment_id": "00118"}, "extended_fields": {"has_code": true, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["data", "build", "good", "code", "builds", "want", "mesos", "1974", "reports", "trends"], "content_type": "reference", "domain": "devops", "complexity": "intermediate", "companies": ["Apache"], "people": ["Mark Watson"], "products": [], "technologies": ["Git", "Mesos", "SonarQube", "TestNG", "ReportNG", "Gatling", "Cucumber"], "frameworks": [], "methodologies": ["DevOps"], "programming_languages": ["JSON"], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["build success rate", "build duration"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Git", "Mesos", "SonarQube", "TestNG", "ReportNG", "Gatling", "Cucumber"], "topics": ["CI/CD trends", "build metrics", "artifact generation", "reporting tools"], "key_concepts": ["continuous integration", "deployment servers", "build success rate", "build duration"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00118", "page_num": 118, "segment": "The data you’ll be working with: what you can get from your CI APIs\n\n95\n\nLet’s take a closer look at some of these test frameworks and the data that they\nprovide.\n\nTESTNG/REPORTNG\nTestNG is a popular test framework that can be used to run unit or integration tests.\nReportNG formats TestNG results as reports that are easy to read and easy to interface\nwith through XML or JSON. These reports give you the number of tests run, passed,\nand failed and the time it takes for all of your tests. You can also dig into each test run\nto find out what’s causing it to fail.\n\nSONARQUBE AND STATIC ANALYSIS\nSonarQube is a powerful tool that can give you a lot of data on your codebase, including how well it’s written and how well it’s covered by tests. There are books written on\nSonarQube, so we’ll just say you should use it. A good source is SonarQube in Action by\nG. Ann Campbell and Patroklos P. Papapetrou (Manning, 2013;/\npapapetrou/). We’ll talk a lot more about SonarQube in chapter 8 when we’re measuring what makes good software.\n\nGATLING\nGatling is a framework used for doing stress testing and benchmarking. You can use it\nto define user scenarios with a domain-specific language (DSL), change the number of\nusers over the test period, and see how your application performs. This type of testing\nadds another dimension to the question “Is your software built well?” Static analysis\nand unit tests can tell you if your code is written correctly, but stress testing tells you\nhow your consumers will experience your product. Using Gatling you can see the\nresponse times of your pages under stress, what your error rate looks like, and latency.\n\nBDD-STYLE TESTS\nBehavior-driven development (BDD) is a practice in which tests in the form of behaviors are written with a DSL. This makes understanding the impact of failing tests much\neasier because you can see what scenario your consumer expects that won’t work.\n\n At the end of the day the output of your BDD tests will be some form of test results\nor deployment results, so the reports you add to your build process can answer the following questions:\n\n■ How well does your code work against how you think it should work?\n■ How good are your tests?\n■ How consistent is your deploy process?\n\nAll of these questions point to how mature your development process is. Teams with\ngreat development processes will have rock-solid, consistent tests with complete test\ncoverage and frequent deploys that consistently add value to the consumers of their\nsoftware.\n\n If you want to read all about BDD, I recommend BDD In Action: Behavior-Driven\nDevelopment for the whole software lifecycle, by John Ferguson Smart (Manning, 2014;/smart/).\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "5", "chapter_title": "Trends and data from CI and deployment servers", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "The data you’ll be working with: what you can get from your CI APIs TESTNG/REPORTNG\nTestNG is a popular test framework that can be used to run unit or integration tests. ■ How good are your tests?", "l2_summary": "The data you’ll be working with: what you can get from your CI APIs TESTNG/REPORTNG\nTestNG is a popular test framework that can be used to run unit or integration tests. These reports give you the number of tests run, passed,\nand failed and the time it takes for all of your tests. SONARQUBE AND STATIC ANALYSIS\nSonarQube is a powerful tool that can give you a lot of data on your codebase, including how well it’s written and how well it’s covered by tests. ■ How good are your tests? ■ How consistent is your deploy process?", "prev_page": {"page_num": 117, "segment_id": "00117"}, "next_page": {"page_num": 119, "segment_id": "00119"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["tests", "test", "what", "sonarqube", "well", "written", "software", "development", "data", "testng"], "content_type": "tutorial", "domain": "programming|devops", "complexity": "intermediate", "companies": [], "people": ["G. Ann Campbell", "Patroklos P. Papapetrou", "John Ferguson Smart"], "products": ["SonarQube in Action", "BDD In Action: Behavior-Driven Development for the Whole Software Lifecycle"], "technologies": ["CI APIs", "XML", "JSON"], "frameworks": ["TestNG/ReportNG", "SonarQube", "Gatling"], "methodologies": ["Behavior-driven development (BDD)"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["number of tests run", "passed", "failed", "time taken", "response times", "error rate", "latency"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["SonarQube", "Gatling"], "topics": ["TestNG/ReportNG", "SonarQube and Static Analysis", "Gatling", "BDD-Style Tests"], "key_concepts": ["test frameworks", "data from CI APIs", "software metrics", "development process maturity"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00119", "page_num": 119, "segment": "96\n\nCHAPTER 5 Trends and data from CI and deployment servers\n\nThe weather report for a\nbuild shows how healthy it\nis: sunshine is good, clouds\nand rain are bad.\n\nEach row represents\na defined build. Within each\nbuild there are multiple\njobs over time.\n\nFigure 5.7 The weather report from Jenkins\n\n5.3.2 What you can tell from CI alone\n\nYour CI system is where code is integrated, tests are run (CT happens), and potentially\nthings are deployed across environments (CD). For our purposes we’ll show some\nthings from Jenkins, by far the most popular CI environment.\n\n The first report we’re going to talk about is the Jenkins dashboard weather report.\nBecause CI runs jobs for you, the first thing you see when you log in is the list of jobs\nit’s responsible for and how healthy they are. On the weather report you’ll see that the\nhealthier the build, the sunnier the weather, as shown in figure 5.7.\n\n The weather report tells you how frequently your build is broken or passing. By\nitself this data is interesting because you ultimately want passing builds so you can get\nyour code into production and in front of your consumers. But it’s not so bad to break\nbuilds from time to time because that could indicate that your team is taking risks---\nand calculated risks are good a lot of the time. You definitely don’t want unhealthy or\nstormy builds because that could indicate a host of problems. As with CLOC from SCM,\nyou should combine this with the other data you’ve collected so far to help get to the\nroot of problems and understand why you’re seeing the trends that you are.\n\n You can get a lot more data from your CI system if you’re generating reports from\nthe different parts of your build process and publishing results. Earlier we talked\nabout generating reports from your build script; this is where those reports become\naccessible and useful in the context of CI.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "5", "chapter_title": "Trends and data from CI and deployment servers", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "The weather report for a\nbuild shows how healthy it\nis: sunshine is good, clouds\nand rain are bad. Figure 5.7 The weather report from Jenkins On the weather report you’ll see that the\nhealthier the build, the sunnier the weather, as shown in figure 5.7.", "l2_summary": "The weather report for a\nbuild shows how healthy it\nis: sunshine is good, clouds\nand rain are bad. Within each\nbuild there are multiple\njobs over time. Figure 5.7 The weather report from Jenkins The first report we’re going to talk about is the Jenkins dashboard weather report. On the weather report you’ll see that the\nhealthier the build, the sunnier the weather, as shown in figure 5.7. The weather report tells you how frequently your build is broken or passing.", "prev_page": {"page_num": 118, "segment_id": "00118"}, "next_page": {"page_num": 120, "segment_id": "00120"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["build", "weather", "report", "data", "time", "because", "jobs", "jenkins", "builds", "reports"], "content_type": "theory", "domain": "devops", "complexity": "intermediate", "companies": ["Jenkins"], "people": ["Mark Watson"], "products": [], "technologies": ["CI", "CD", "SCM"], "frameworks": [], "methodologies": ["DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["build health", "frequency of broken builds"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Jenkins"], "topics": ["CI/CD trends", "weather report in Jenkins", "build health metrics"], "key_concepts": ["healthy build", "unhealthy build", "passing builds", "broken builds", "risk-taking"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00120", "page_num": 120, "segment": "Key CI metrics: spotting trends in your data\n\n97\n\n5.4\n\nKey CI metrics: spotting trends in your data\nThe most basic information you can get from CI is successful and failed builds. Even\nthough these seem like straightforward metrics, there’s some ambiguity in interpreting them.\n\n The most obvious problem is if your build fails all or most of the time; if that happens there’s obviously a big problem. Conversely, if your build passes all the time, that\nmight mean you have an awesome team, but it could also mean any of the following:\n\n■ Your team isn’t doing any meaningful work.\n■ You don’t have any tests that are running.\n■ Quality checks are disabled for your build.\n\nWhen you run up against this issue, it’s not a bad idea to pull a bit more data to sanitycheck your build history. In this case you can take a look at the details from your CI\nbuilds to explore some of the following:\n\n■ Test reports\n■ Total number of tests\n■ Percentage of passing and failing tests\n■ Static analysis\n■ Test coverage percentage\n■ Code violations\n\nIf you’re using your CI system to also handle your deployments, you can get build frequency out of there too, which is really the key metric that everything else should\naffect: how fast are you getting changes to your consumers?\n\n We’ll look at some of the data you can get from your CI system so you can track\n\nthese metrics along with the rest of the data you’re already collecting.\n\n5.4.1 Getting CI data and adding it to your charts\n\nThe easiest thing to get out of your CI system is build success frequency. The data\nyou’ll look at for this will be\n\n■ Successful versus failed builds\n■ How well is your code review process working?\n■ How good is your local development environment?\n\n■\n\nIs your team thinking about quality software?\n\n■ Deploy frequency\n■ How frequently do you get updates in front of your consumers?\n\nWhat you can get greatly depends on how you’re using your CI system. In our examples we’ll look at what you can get if you have a job for integrating code and a job for\ndeploying code.\n\n Starting off with the job that integrates code, we’ll look at the difference between\n\nsuccessful and failed builds, as shown in figure 5.8.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "5", "chapter_title": "Trends and data from CI and deployment servers", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "Key CI metrics: spotting trends in your data We’ll look at some of the data you can get from your CI system so you can track The data\nyou’ll look at for this will be", "l2_summary": "Key CI metrics: spotting trends in your data Key CI metrics: spotting trends in your data\nThe most basic information you can get from CI is successful and failed builds. We’ll look at some of the data you can get from your CI system so you can track The data\nyou’ll look at for this will be What you can get greatly depends on how you’re using your CI system. In our examples we’ll look at what you can get if you have a job for integrating code and a job for\ndeploying code.", "prev_page": {"page_num": 119, "segment_id": "00119"}, "next_page": {"page_num": 121, "segment_id": "00121"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "build", "look", "code", "metrics", "builds", "system", "most", "successful", "failed"], "content_type": "tutorial", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["CI", "build system", "code review process", "local development environment", "deployments"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["build success frequency", "test reports", "total number of tests", "percentage of passing and failing tests", "static analysis", "test coverage percentage", "code violations", "deploy frequency"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["CI metrics", "build success frequency", "test reports", "deploy frequency"], "key_concepts": ["successful and failed builds", "test coverage", "code violations", "quality checks", "build history analysis"], "problem_statement": "Interpreting CI build data, especially when all builds pass or fail frequently.", "solution_approach": "Analyzing additional metrics like test reports, static analysis, and deploy frequency to understand the true state of software development.", "extraction_method": "lm"}}
{"segment_id": "00121", "page_num": 121, "segment": "98\n\nCHAPTER 5 Trends and data from CI and deployment servers\n\nMost builds\nare passing.\n\nIt’s OK to have\nsome failed builds.\n\nGood and bad builds\n\n28\n\n21\n\n14\n\n7\n\n0\n15\n\nGood\nbuilds\n\nFailed\nbuilds\n\n16\n\n17\n\n18\n\n19\n\n20\n\nSprint\n\nFigure 5.8 A healthy-looking project based on good and bad builds\n\nIn figure 5.8 you can see some failed builds; that’s okay, especially on complex projects. You can start getting worried when your failed builds start to become a high percentage of your total builds, as shown in figure 5.9; that’s usually a signal that\nsomething is pretty wrong and you need to dig in.\n\n If figure 5.9 is bad, then you might think something like figure 5.10 is good.\n But as we mentioned earlier in the chapter, that may not always be the case. If you\npull more data into the picture like test coverage or test runs, as shown in figure 5.11,\n\nThis differential between good\nbuilds and failed builds is not good.\n\nGood and bad builds\n\n28\n\n21\n\n14\n\n7\n\n0\n15\n\nGood\nbuilds\n\nFailed\nbuilds\n\n16\n\n17\n\n18\n\n19\n\n20\n\nSprint\n\nFigure 5.9 A worrisome trend: lots of bad builds\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 2, "cols": 2, "data": [["", ""], ["", ""]], "markdown": "|  |  |\n|---|---|\n|  |  |"}], "table_count": 1, "chapter_num": "5", "chapter_title": "Trends and data from CI and deployment servers", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "Most builds\nare passing. It’s OK to have\nsome failed builds. This differential between good\nbuilds and failed builds is not good.", "l2_summary": "Most builds\nare passing. It’s OK to have\nsome failed builds. Figure 5.8 A healthy-looking project based on good and bad builds In figure 5.8 you can see some failed builds; that’s okay, especially on complex projects. This differential between good\nbuilds and failed builds is not good. Figure 5.9 A worrisome trend: lots of bad builds", "prev_page": {"page_num": 120, "segment_id": "00120"}, "next_page": {"page_num": 122, "segment_id": "00122"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["builds", "good", "figure", "failed", "chapter", "data", "some", "sprint", "start", "shown"], "content_type": "tutorial", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["CI and deployment servers", "test coverage", "test runs"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["percentage of failed builds", "test coverage", "test runs"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["CI and deployment trends", "build success rate", "test coverage importance"], "key_concepts": ["good and bad builds", "failed build percentage", "healthy project indicators", "worrisome trend signals"], "problem_statement": "Managing CI/CD pipeline health and identifying issues", "solution_approach": "Monitoring build success rates, test coverage, and test runs", "extraction_method": "lm"}}
{"segment_id": "00122", "page_num": 122, "segment": "Key CI metrics: spotting trends in your data\n\n99\n\nWhen no builds are failing,\nthat is usually a red flag.\n\nGood and bad builds\n\n28\n\n21\n\n14\n\n7\n\n0\n15\n\nGood\nbuilds\n\nFailed\nbuilds\n\n18\nFailed builds: 0\n\n16\n\n17\n\n18\n\n19\n\n20\n\nSprint\n\nFigure 5.10 It appears that everything is great: no builds are failing.\n\nyou can get a sanity check on whether your builds are worthwhile and doing something of value, or if they’re just adding one code delta to the next without checking\nanything.\n\n Figure 5.11 shows a team that’s generating good builds but not testing anything. In\nthis case what appeared to be a good trend is actually meaningless. If you’re not testing anything, then your builds need only compile to pass. You want to see something\nmore like figure 5.12.\n\nTest coverage\nis low and not\nimproving.\n\nTests aren’t\neven running.\n\nOf course, builds\nwill always pass when\nyou don't test.\n\nGood and bad builds\n\n28\n\n21\n\n14\n\n7\n\n0\n15\n\nGood\nbuilds\n\nFailed\nbuilds\n\nTest\ncoverage\n\nTests run\n\n16\n\n17\n\n18\n\n19\n\n20\n\nSprint\n\nFigure 5.11 This is really bad. Builds always pass but there are no tests running and barely any\ncoverage.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_2", "rows": 4, "cols": 3, "data": [["", "", ""], ["", "", ""], ["", "", ""], ["", "", ""]], "markdown": "|  |  |  |\n|---|---|---|\n|  |  |  |\n|  |  |  |\n|  |  |  |"}], "table_count": 1, "chapter_num": "5", "chapter_title": "Trends and data from CI and deployment servers", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "When no builds are failing,\nthat is usually a red flag. Figure 5.10 It appears that everything is great: no builds are failing. Figure 5.11 shows a team that’s generating good builds but not testing anything.", "l2_summary": "When no builds are failing,\nthat is usually a red flag. Figure 5.10 It appears that everything is great: no builds are failing. Figure 5.11 shows a team that’s generating good builds but not testing anything. If you’re not testing anything, then your builds need only compile to pass. Of course, builds\nwill always pass when\nyou don't test. Figure 5.11 This is really bad.", "prev_page": {"page_num": 121, "segment_id": "00121"}, "next_page": {"page_num": 123, "segment_id": "00123"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["builds", "good", "figure", "failed", "anything", "pass", "test", "coverage", "tests", "when"], "content_type": "tutorial", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["CI/CD", "builds", "tests", "test coverage"], "frameworks": [], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["build failures", "test coverage"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["CI metrics", "build trends", "test coverage importance"], "key_concepts": ["good builds vs failed builds", "test coverage", "build failures"], "problem_statement": "Identifying issues in CI/CD processes through metrics and trends", "solution_approach": "Analyzing build and test data to ensure meaningful testing", "extraction_method": "lm"}}
{"segment_id": "00123", "page_num": 123, "segment": "100\n\nCHAPTER 5 Trends and data from CI and deployment servers\n\nTest coverage is improving\nand more tests are being\nwritten and run.\n\nBuilds are consistent\nwith more tests.\n\nGood and bad builds\n\n80\n\n60\n\n40\n\n20\n\n0\n15\n\nGood\nbuilds\n\nFailed\nbuilds\n\nTest\ncoverage\n\nTests run\n\n16\n\n17\n\n18\n\n19\n\n20\n\nSprint\n\nFigure 5.12 There are no failing builds, code coverage is going up, and more tests are running\nevery time. It looks like this team has it together.\n\nAnother way to visualize your build data is to look at releases over time. If you’re\nreleasing at the end of every sprint or every few sprints, it might be more useful to\nshow release numbers as points on your other graphs, like in figure 5.13.\n\n But if you have an awesome team practicing CD and putting out multiple releases a\nday, putting each release on the graph would be tough to interpret. Instead, show the\nnumber of good releases and bad releases (releases that caused problems or had to be\n\nRelease numbers\ntracked along\nwith sprints\n\nGood and bad builds per sprint with releases\n\n80\n\n60\n\n40\n\n20\n\n3.4\n\n0\n15\n\n3.6\n\n3.7\n\n3.8\n\n16\n\n17\n\n18\n\n19\n\n20\n\nGood\nbuilds\n\nFailed\nbuilds\n\nTest\ncoverage\n\nTests run\n\nFigure 5.13 Releases charted with other data\n\nSprint\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 4, "cols": 3, "data": [["", "", ""], ["", "", ""], ["", "", ""], ["", "", ""]], "markdown": "|  |  |  |\n|---|---|---|\n|  |  |  |\n|  |  |  |\n|  |  |  |"}], "table_count": 1, "chapter_num": "5", "chapter_title": "Trends and data from CI and deployment servers", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "Builds are consistent\nwith more tests. Figure 5.12 There are no failing builds, code coverage is going up, and more tests are running\nevery time. Good and bad builds per sprint with releases", "l2_summary": "Test coverage is improving\nand more tests are being\nwritten and run. Builds are consistent\nwith more tests. Figure 5.12 There are no failing builds, code coverage is going up, and more tests are running\nevery time. Instead, show the\nnumber of good releases and bad releases (releases that caused problems or had to be Good and bad builds per sprint with releases Figure 5.13 Releases charted with other data", "prev_page": {"page_num": 122, "segment_id": "00122"}, "next_page": {"page_num": 124, "segment_id": "00124"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": true, "has_citations": false, "key_terms": ["builds", "releases", "tests", "good", "coverage", "more", "sprint", "data", "test", "figure"], "content_type": "tutorial", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["CI", "deployment servers"], "frameworks": [], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["test coverage", "tests run", "good builds", "failed builds"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["CI and deployment trends", "build consistency", "release management"], "key_concepts": ["test coverage improvement", "consistent builds", "good vs failed builds", "sprint-based metrics"], "problem_statement": "Improving test coverage and build consistency in CI/CD pipelines", "solution_approach": "Monitoring and visualizing build data over time, including test coverage and release numbers", "extraction_method": "lm"}}
{"segment_id": "00124", "page_num": 124, "segment": "Case study: measuring benefits of process change through CI data\n\n101\n\nThis represents all\nbuilds in a week.\n\nHere we filter by builds\nthat triggered a deployment.\n\nA fairly high percentage of failed builds\nindicates automation is catching problems.\n\nThe percentage of failed builds that\nrelease software is better but still really high.\n\nFigure 5.14 Showing all build pass/fail percentages for a team deploying code multiple times a day and\nthe percentage filtered by builds that trigger a release. By hovering over the charts in Kibana, you can\nsee the percentage associated with the pie slices.\n\nrolled back) along with your other build data. In this case, if your automation is really\ngood you’ll see a percentage of builds failing before releases and a much smaller percentage failing that actually release code. An example of this is shown in figure 5.14.\n\n The team depicted in figure 5.14 is building 31 times a week and releasing software\n6 times a week. Of their releases 33% fail, and of total builds 38.7% fail. Despite having a high failure rate for releases, the team still has four successful deployments a\nweek, which is pretty good. Now imagine a team that releases software twice a month\nwith a 33% failure rate; that would mean the team would sometimes go for an entire\nmonth without releasing anything to their customers. For teams that release daily, the\ntolerance for failure ends up being much higher due to the higher release frequency.\nTeams that release every two weeks usually are expected to have a successful deployment rate of close to 100%, a much harder target to hit.\n\n5.5\n\nCase study: measuring benefits of process change\nthrough CI data\nWe ended chapter 4 with a happy team that made some measurements, reacted to\nthem, and had great results. They moved to the pull request workflow on their development team and moved quality into the development process. They were able to\nmeasure some key metrics that were being generated from their development process\nto track progress and use those metrics to prove that the changes they were making\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "5", "chapter_title": "Trends and data from CI and deployment servers", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "This represents all\nbuilds in a week. The percentage of failed builds that\nrelease software is better but still really high. Of their releases 33% fail, and of total builds 38.7% fail.", "l2_summary": "Case study: measuring benefits of process change through CI data This represents all\nbuilds in a week. The percentage of failed builds that\nrelease software is better but still really high. Figure 5.14 Showing all build pass/fail percentages for a team deploying code multiple times a day and\nthe percentage filtered by builds that trigger a release. The team depicted in figure 5.14 is building 31 times a week and releasing software\n6 times a week. Of their releases 33% fail, and of total builds 38.7% fail.", "prev_page": {"page_num": 123, "segment_id": "00123"}, "next_page": {"page_num": 125, "segment_id": "00125"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["builds", "team", "percentage", "release", "process", "week", "releases", "case", "data", "high"], "content_type": "case_study", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["CI/CD", "Kibana"], "frameworks": [], "methodologies": ["DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["build pass/fail percentages", "percentage of failed builds that release software"], "has_case_study": true, "case_study_company": null, "tools_mentioned": ["Kibana"], "topics": ["CI/CD metrics", "deployment success rate", "release frequency"], "key_concepts": ["build pass/fail percentages", "failed builds triggering deployment", "successful deployments"], "problem_statement": "Measuring the benefits of process change through CI data", "solution_approach": "Analyzing build and release data to improve deployment success rates", "extraction_method": "lm"}}
{"segment_id": "00125", "page_num": 125, "segment": "102\n\nCHAPTER 5 Trends and data from CI and deployment servers\n\nSCM stats against bugs\n\nEverything is trending\nin the right direction. Yay!\n\n30\n\n23\n\n16\n\n9\n\n2\n15\n\nPull\nrequests\n\nCommits\n\nBugs\n\nComments\n\n16\n\n17\n\n18\n\nSprint\n\n19\n\n20\n\n21\n\nFigure 5.15 The graph the team ended with in the last chapter. Everything is looking good.\n\nwere paying dividends. As a reminder, they were looking at the chart in figure 5.15\nand feeling pretty good about themselves.\n\n The team was so excited that they brought their findings to their leadership to\nshow them the great work they were doing. There they were confronted with these\nquestions:\n\n■ Are you releasing faster now?\n■ Are you getting more done?\n\nIf we compare those to the questions we asked at the beginning of the chapter, they\nline up pretty well:\n\n■ How fast are you delivering or can you deliver changes to your consumer? = Are\n\nyou releasing faster?\n\n■ How consistently does your team do their work? = Are you getting more done?\n\nThe team decided to add more data to their graphs. The first question was “Are you\nreleasing faster?” Their deployments were controlled from their CI system; if they\ncould pull data from there, they could map their releases along with the rest of their\ndata.\n\n The second question was “Are you delivering more consistently?” In chapter 3 we\ntalked about the fairly standard measure of agile consistency: velocity. Another data\npoint that can be used to track consistency is good builds versus failed builds from the\nCI system. They added that to their charts as well, which output the graph shown in\nfigure 5.16.\n\n Based on the data shown here, there’s some bad news:\n\n■ Velocity isn’t consistent.\n■ Releases are far apart.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "5", "chapter_title": "Trends and data from CI and deployment servers", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "CHAPTER 5 Trends and data from CI and deployment servers Figure 5.15 The graph the team ended with in the last chapter. you releasing faster?", "l2_summary": "CHAPTER 5 Trends and data from CI and deployment servers Figure 5.15 The graph the team ended with in the last chapter. ■ Are you getting more done? you releasing faster? = Are you getting more done? The team decided to add more data to their graphs.", "prev_page": {"page_num": 124, "segment_id": "00124"}, "next_page": {"page_num": 126, "segment_id": "00126"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "chapter", "team", "more", "figure", "good", "there", "releasing", "faster", "bugs"], "content_type": "case_study", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["CI system", "SCM"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["velocity", "releases"], "has_case_study": true, "case_study_company": null, "tools_mentioned": ["CI system"], "topics": ["Trends in CI and deployment servers", "SCM statistics against bugs", "Velocity and release consistency"], "key_concepts": ["Agile methodology", "Continuous Integration (CI)", "Software Configuration Management (SCM)"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00126", "page_num": 126, "segment": "Case study: measuring benefits of process change through CI data\n\n103\n\nPoints completed\nare not consistent.\n\nReleases\nare still very\nfar apart.\n\nDelta between\ngood builds and failed\nbuilds is improving.\n\nCI, SCM, PTS, and\nrelease data\n\n28\n\n21\n\n14\n\n7\n\n0\n15\n\n16\n\n17\n\n2.3.1\n\n18\n\nSprint\n\n2.3.2\n\n19\n\n20\n\n22\n\nGood\nbuilds\n\nFailed\nbuilds\n\nSCM\ncommits...\n\nSCM\ncomments\n\nPTS sprint\npoints Co...\n\nFigure 5.16 Adding velocity and releases to our case study\n\nBut there is good news too; the delta between good and failed builds is improving.\nThe total number of builds is decreasing, which would be expected because the team\nis committing less code and doing more code reviews. Of the total builds, the percentage of failed builds has gone down significantly. This is an indicator of better consistency in delivering working code and also an indicator of higher code quality.\n\n4\n\nThere are more tasks estimated at\n16 than at any other value. Most tasks\nestimated at 16 aren't completed.\n\n So if their CI system was indicating that they were delivering quality code more\nconsistently, why were they not hitting a consistent velocity? At a closer look they realized that even though their\nSCM data was fairly consistent,\ntheir velocity was not. Based\non that, the team hypothesized that perhaps something\nwas wrong with their estimations. To try to dig deeper\ninto that, they decided to look\nat the distribution of estimations for a sprint. They used\nbase 2 estimating with the\nmaximum estimation being a\n16, so the possible estimation\nvalues for a task were 1, 2, 4,\n8, and 16. The typical distribution across sprints looked\nsomething like figure 5.17.\n\nFigure 5.17 The distribution of estimates for a sprint: how\nmany are completed and how many get pushed to the next\nsprint?\n\nPushed\n\nDone\n\n16\n\n1\n\n2\n\n3\n\n0\n\n8\n\n4\n\n2\n\n1\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_2", "rows": 3, "cols": 7, "data": [["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""]], "markdown": "|  |  |  |  |  |  |  |\n|---|---|---|---|---|---|---|\n|  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |"}], "table_count": 1, "chapter_num": "5", "chapter_title": "Trends and data from CI and deployment servers", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "Delta between\ngood builds and failed\nbuilds is improving. PTS sprint\npoints Co... Of the total builds, the percentage of failed builds has gone down significantly.", "l2_summary": "Points completed\nare not consistent. Delta between\ngood builds and failed\nbuilds is improving. CI, SCM, PTS, and\nrelease data PTS sprint\npoints Co... But there is good news too; the delta between good and failed builds is improving. Of the total builds, the percentage of failed builds has gone down significantly.", "prev_page": {"page_num": 125, "segment_id": "00125"}, "next_page": {"page_num": 127, "segment_id": "00127"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["builds", "sprint", "code", "good", "failed", "data", "completed", "consistent", "figure", "velocity"], "content_type": "case_study", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["CI", "SCM", "PTS", "release data"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": false, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["velocity", "releases", "delta between good builds and failed builds"], "has_case_study": true, "case_study_company": null, "tools_mentioned": ["CI system", "SCM", "PTS"], "topics": ["process change measurement", "code quality improvement", "velocity inconsistency"], "key_concepts": ["continuous integration", "source code management", "points estimation", "build consistency"], "problem_statement": "Inconsistent velocity despite improving build consistency", "solution_approach": "Reviewing estimations and distribution analysis", "extraction_method": "lm"}}
{"segment_id": "00127", "page_num": 127, "segment": "104\n\nCHAPTER 5 Trends and data from CI and deployment servers\n\nDelta between good\nbuilds and failed builds\ncontinues to improve.\n\nVelocity is\nstarting to trend\nthe right way.\n\nCI, SCM, PTS, and release data\n\n28\n\n21\n\n14\n\n7\n\n0\n15\n\n2.3.1\n\n2.3.2\n\n16\n\n17\n\n18\n\n19\n\nSprint\n\n20\n\n22\n\n23\n\n24\n\nGood\nbuilds\n\nFailed\nbuilds\n\nSCM\ncommits...\n\nSCM\ncomments\n\nPTS sprint\npoints Co...\n\nFigure 5.18 Velocity isn’t dipping anymore and the good/bad build trend is continuing to improve.\n\nAs shown in their analysis, most of their tasks were pretty big and the bigger tasks\ntended to not always get done. These big features were also pushing releases farther\napart because the team was missing their sprint goals and had to postpone a release\nuntil the tasks they wanted to release were done. If they could get their tasks back to\nan appropriate size, then perhaps they could get their velocity on track and have more\nfrequent, smaller releases.\n\n The team started to break their stories down further; anything that was estimated\nas a 16 was broken down into smaller, more manageable chunks of work. After a few\nsprints they noticed that their velocity started to increase, as shown in figure 5.18.\n\n In addition, their taskpoint\nhas\ndistribution\nchanged dramatically for the\nbetter, as shown in figure\n5.19.\n\n They were getting more\ndone and bad builds were\ndecreasing, but their stakeholders still wanted a lot of\nfeatures out of each release.\nThey made a team decision\nto start getting their smaller\npieces of work out to production more frequently. In the\nprevious chapter, the introduction of the pull request\n\nTasks are broken down into\nsmaller bits. They’re not getting\npushed out anymore.\n\nDone\n\nPushed\n\n16\n\n8\n\n4\n\n2\n\n1\n\n8\n\n6\n\n4\n\n2\n\n0\n\nFigure 5.19 A much better distribution of estimates for a\nsprint\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_2", "rows": 4, "cols": 7, "data": [["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""]], "markdown": "|  |  |  |  |  |  |  |\n|---|---|---|---|---|---|---|\n|  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |"}], "table_count": 1, "chapter_num": "5", "chapter_title": "Trends and data from CI and deployment servers", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "Delta between good\nbuilds and failed builds\ncontinues to improve. CI, SCM, PTS, and release data Tasks are broken down into\nsmaller bits.", "l2_summary": "Delta between good\nbuilds and failed builds\ncontinues to improve. CI, SCM, PTS, and release data PTS sprint\npoints Co... Figure 5.18 Velocity isn’t dipping anymore and the good/bad build trend is continuing to improve. Tasks are broken down into\nsmaller bits. Figure 5.19 A much better distribution of estimates for a\nsprint", "prev_page": {"page_num": 126, "segment_id": "00126"}, "next_page": {"page_num": 128, "segment_id": "00128"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": true, "has_citations": false, "key_terms": ["builds", "tasks", "velocity", "release", "sprint", "figure", "done", "more", "smaller", "good"], "content_type": "case_study", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["CI", "SCM", "PTS", "release data"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["velocity", "good builds", "failed builds"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["CI and deployment trends", "team velocity improvement", "task breakdown for better release frequency"], "key_concepts": ["velocity trend", "good vs failed builds", "task size impact on sprint goals"], "problem_statement": "The team was facing issues with large tasks leading to missed sprint goals, delayed releases, and high bad build rates.", "solution_approach": "Breaking down larger tasks into smaller, more manageable chunks improved velocity and reduced bad builds.", "extraction_method": "lm"}}
{"segment_id": "00128", "page_num": 128, "segment": "Summary\n\n105\n\nProductivity is\nconsistent.\n\nReleases are\nmore frequent.\n\nCI, SCM, PTS, and release data\n\n40\n\n30\n\n20\n\n10\n\n0\n15\n\n2.3.5\n\n2.3.6\n\n2.3.4\n\n2.3.1\n\n2.3.2\n\n16\n\n17\n\n18\n\n19\n\n20\n\n22\n\n23\n\n24\n\n25\n\n26\n\n27\n\nSprint\n\nGood\nbuilds\n\nFailed\nbuilds\n\nSCM\ncommits...\n\nSCM\ncomments\n\nPTS sprint\npoints Co...\n\nFigure 5.20 Adding releases to our graph to see frequency showed trends continuing to improve over time.\n\nworkflow and quality engineering into their process gave them a much higher degree\nof confidence in the quality of their work. As a result of those process improvements,\nthey could see their build success rate improving. Their confidence level of getting\nmore frequent releases was at an all-time high, so they started getting their\nchanges out to their consumers at the end of every sprint; the result is shown in\nfigure 5.20.\n\n The team finally got to a point where their releases were happening a lot more frequently because of their commitment to quality. Their confidence was at an all-time\nhigh because of their ability to make informed decisions and measure success. Now\nthey had a complete picture to show their leadership team that demonstrated the\nimprovements they were making as a team.\n\n5.6\n\nSummary\nThe large amount of data you can get from your CI system through CD/CI practices\ncan tell you a lot about your team and your processes. In this chapter we covered the\nfollowing topics:\n\n■ Continuous integration (CI) is the practice of integrating multiple change sets\n\nin code frequently.\n\n■ Continuous delivery (CD) is an agile practice of delivering change sets to your\n\nconsumer shortly after small code changes are complete.\n\n■ Continuous testing (CT) makes CD possible and is usually run by your CI system.\n■ Continuous development generates lots of data that can help you keep track of\n\nyour team.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "5", "chapter_title": "Trends and data from CI and deployment servers", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "Releases are\nmore frequent. CI, SCM, PTS, and release data PTS sprint\npoints Co...", "l2_summary": "Releases are\nmore frequent. CI, SCM, PTS, and release data PTS sprint\npoints Co... Figure 5.20 Adding releases to our graph to see frequency showed trends continuing to improve over time. Their confidence level of getting\nmore frequent releases was at an all-time high, so they started getting their\nchanges out to their consumers at the end of every sprint; the result is shown in\nfigure 5.20. The team finally got to a point where their releases were happening a lot more frequently because of their commitment to quality.", "prev_page": {"page_num": 127, "segment_id": "00127"}, "next_page": {"page_num": 129, "segment_id": "00129"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": true, "has_citations": false, "key_terms": ["team", "releases", "continuous", "more", "data", "sprint", "time", "quality", "confidence", "summary"], "content_type": "practice", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Continuous Integration", "Continuous Delivery", "Continuous Testing", "CI/CD practices", "SCM", "PTS"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["build success rate", "release frequency"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["Continuous Integration and Continuous Delivery", "Release Frequency", "Build Success Rate"], "key_concepts": ["CI/CD practices", "Sprint", "Build success rate", "Release frequency"], "problem_statement": "Improving build success rate and release frequency through CI/CD practices", "solution_approach": "Implementing CI/CD practices, focusing on quality assurance and frequent releases", "extraction_method": "lm"}}
{"segment_id": "00129", "page_num": 129, "segment": "106\n\nCHAPTER 5 Trends and data from CI and deployment servers\n\n■ CI/CD/CT data can tell you the following:\n\n■ How disciplined your team is\n■ How consistently you’re delivering\n■ How good your code is\n\n■ Setting up a delivery pipeline will enable you to get better data in your application lifecycle.\n\n■ You can learn a lot from the following CI data points:\n\n■ Successful and failed builds\n■ Tests reports\n■ Code coverage\n\n■ You can use multiple data points from your CI system to check the true meaning\n\nof your build trends.\n\n■ Combining CI, PTS, and SCM data gives you powerful analysis capabilities.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "5", "chapter_title": "Trends and data from CI and deployment servers", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "CHAPTER 5 Trends and data from CI and deployment servers ■ CI/CD/CT data can tell you the following: ■ You can learn a lot from the following CI data points:", "l2_summary": "CHAPTER 5 Trends and data from CI and deployment servers ■ CI/CD/CT data can tell you the following: ■ How disciplined your team is\n■ How consistently you’re delivering\n■ How good your code is ■ Setting up a delivery pipeline will enable you to get better data in your application lifecycle. ■ You can learn a lot from the following CI data points: ■ You can use multiple data points from your CI system to check the true meaning", "prev_page": {"page_num": 128, "segment_id": "00128"}, "next_page": {"page_num": 130, "segment_id": "00130"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["data", "trends", "following", "code", "points"], "content_type": "theory", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["CI/CD/CT", "PTS", "SCM"], "frameworks": [], "methodologies": ["DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["Successful and failed builds", "Tests reports", "Code coverage"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["CI/CD/CT data analysis", "Delivery pipeline setup", "Build trends monitoring"], "key_concepts": ["Team discipline", "Consistent delivery", "Code quality", "Data-driven decision making"], "problem_statement": "Improving software development and deployment processes", "solution_approach": "Implementing CI/CD/CT practices, setting up a delivery pipeline, analyzing build trends", "extraction_method": "lm"}}
{"segment_id": "00130", "page_num": 130, "segment": "Data from your\nproduction systems\n\nThis chapter covers\n\n■ How the tasks you’re working on provide value\n\nback to the consumer\n\n■ Adding production monitoring data to your\n\nfeedback loop\n\n■ Best practices to get the most out of your\n\napplication monitoring systems\n\nFigure 6.1 shows where the data covered in this chapter can be found in the software delivery lifecycle.\n\n Once your application is in production, there are a couple more types of data\nthat you can look at to help determine how well your team is performing: application performance monitoring (APM) data and business intelligence (BI) data.\n\n■ APM data shows you how well your application is performing from a technical point of view.\n\n■ BI data shows you how well your application is serving your consumer.\n\nData from your production systems mainly produces reactive metrics. Your release\ncycle is behind you; you thought your code was good enough for the consumer and\n\n107\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "5", "chapter_title": "Trends and data from CI and deployment servers", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "Data from your\nproduction systems ■ Adding production monitoring data to your ■ BI data shows you how well your application is serving your consumer.", "l2_summary": "Data from your\nproduction systems ■ Adding production monitoring data to your application monitoring systems ■ APM data shows you how well your application is performing from a technical point of view. ■ BI data shows you how well your application is serving your consumer. Data from your production systems mainly produces reactive metrics.", "prev_page": {"page_num": 129, "segment_id": "00129"}, "next_page": {"page_num": 131, "segment_id": "00131"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "application", "production", "systems", "consumer", "monitoring", "shows", "well", "chapter", "performing"], "content_type": "tutorial", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["application performance monitoring", "business intelligence"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["reactive metrics", "application performance monitoring data", "business intelligence data"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["production monitoring", "feedback loop", "best practices"], "key_concepts": ["production systems", "application performance monitoring (APM)", "business intelligence (BI)"], "problem_statement": "How to effectively monitor and improve application performance in production systems", "solution_approach": "Implementing APM and BI data for better insights and feedback", "extraction_method": "lm"}}
{"segment_id": "00131", "page_num": 131, "segment": "108\n\nCHAPTER 6 Data from your production systems\n\nManage\ntasks and\nbugs\n\nProject\ntracking\n\nManage\ncode and\ncollaboration\n\nGenerate\nbuilds and\nrun tests\n\nMove code\nacross\nenvironments\n\nSource\ncontrol\n\nContinuous\nintegration\n\nDeployment\ntools\n\nEnsure\neverything\nis working\n\nApplication\nmonitoring\n\nPrevious chapters\n\nYou are here\n\nFigure 6.1 You are here: application monitoring’s place in the application lifecycle\n\nyou put it out there. Now you have to watch to make sure you understand how it’s\nworking and if the changes you made are good---or not. Your sponsor will also likely\ncare how well the features you’re delivering are serving the consumer.\n\n Up to this point we’ve looked only at data that you can gather in your development\ncycle, most of which is typically used on agile teams. APM and BI data is also common,\nbut not something that the development team typically works with. In many cases APM\ndata is owned and watched by operations teams to make sure the systems are all working well and that nothing is going to blow up for your consumers. Once the application is up and running, a separate BI team mines the consumer data to ensure your\nproduct is meeting the business need for which it was built. This responsibility is\nshown in figure 6.2.\n\nThe development team\ncares a lot about building\nand releasing code.\n\nSysOps teams typically pay\nattention to the production\nsystem while developers\nwork on new features.\n\nBI analysts look at the\ndata being generated from the\napplication to determine how\nwell it’s serving the consumer.\n\nManage\ntasks and\nbugs\n\nProject\ntracking\n\nManage\ncode and\ncollaboration\n\nGenerate\nbuilds and\nrun tests\n\nMove code\nacross\nenvironments\n\nEnsure\neverything\nis working\n\nEnsure application\nmeets the\nbusiness need\n\nSource\ncontrol\n\nContinuous\nintegration\n\nDeployment\ntools\n\nApplication\nmonitoring\n\nData\nanalysis\n\nFigure 6.2 The data from the application lifecycle and the division of development and operations teams\n\nDevelopment\n\nOperations\n\nBI analyst\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "5", "chapter_title": "Trends and data from CI and deployment servers", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "Application\nmonitoring Figure 6.1 You are here: application monitoring’s place in the application lifecycle Application\nmonitoring", "l2_summary": "CHAPTER 6 Data from your production systems Manage\ncode and\ncollaboration Application\nmonitoring Figure 6.1 You are here: application monitoring’s place in the application lifecycle Application\nmonitoring Figure 6.2 The data from the application lifecycle and the division of development and operations teams", "prev_page": {"page_num": 130, "segment_id": "00130"}, "next_page": {"page_num": 132, "segment_id": "00132"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "application", "code", "development", "manage", "ensure", "working", "teams", "monitoring", "figure"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["application performance", "consumer satisfaction"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Application Monitoring", "Development vs Operations Roles", "Data Lifecycle Management"], "key_concepts": ["Application Performance Management (APM)", "Business Intelligence (BI)", "Continuous Integration/Deployment"], "problem_statement": "Ensuring the application is functioning as expected and meeting business needs after deployment", "solution_approach": "Implementing monitoring tools, separating development and operations roles, and analyzing consumer data", "extraction_method": "lm"}}
{"segment_id": "00132", "page_num": 132, "segment": "Preparing for analysis: generating the richest set of data you can\n\n109\n\nData from\nperformance monitoring\nand log analysis.\n\nHOW well is your system working?\n\nHOW well are you serving your consumers?\n\nUser tracking\nand conversion\nrates can indicate\ncustomer satisfaction.\n\nPerformance of your\napplication translates\ninto level of service.\n\nFigure 6.3 Questions that you can answer from production monitoring alone\n\nRealistically, the data that you can get from your consumer-facing production system is\nthe most valuable that you can collect and track because it tells you how well your system is working and if your consumers are happy. This is broken down in figure 6.3.\n\n If you tie this back to the rest of the data you’ve been collecting, you now have a\ncomplete picture of the product you’re building and enhancing from conception to\nconsumption. We’ll look more deeply into combining data from the entire lifecycle in\nchapter 7. For now we’ll focus on the data you can get out of your APM systems.\n\n There are plenty of tools that can do production monitoring for you, but if you\ndon’t think about how your system should report data when you’re building it, you’re\nnot going to get much help from them. Let’s start off talking about best practices and\nthings you can do to ensure you’re collecting the richest set of data you can to get the\nwhole picture on monitoring your system and using that data to improve your development process.\n\nA bit about DevOps\n\nOn a team that’s practicing DevOps, there isn’t much separation between who writes\nthe code (Dev) and who supports the systems the code runs on (Ops). That’s the whole\npoint; if the developers write the code, they should also know better than anyone else\nhow to support it and how to deploy it. DevOps is heavily associated with CD, which\nwe talked about in chapter 5. DevOps teams tend to have more control over their production monitoring systems because the same team owns the code and supports it\nin production.\n\n6.1\n\nPreparing for analysis: generating the richest set of\ndata you can\nThe nature of production monitoring systems is to do real-time analysis on data for\nquick feedback on software systems so you don’t have to do much to get data from\nthem. Even with the most low-touch system, there are techniques you can apply to\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "5", "chapter_title": "Trends and data from CI and deployment servers", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "Preparing for analysis: generating the richest set of data you can Data from\nperformance monitoring\nand log analysis. Figure 6.3 Questions that you can answer from production monitoring alone", "l2_summary": "Preparing for analysis: generating the richest set of data you can Data from\nperformance monitoring\nand log analysis. HOW well is your system working? Figure 6.3 Questions that you can answer from production monitoring alone For now we’ll focus on the data you can get out of your APM systems. Preparing for analysis: generating the richest set of\ndata you can\nThe nature of production monitoring systems is to do real-time analysis on data for\nquick feedback on software systems so you don’t have to do much to get data from\nthem.", "prev_page": {"page_num": 131, "segment_id": "00131"}, "next_page": {"page_num": 133, "segment_id": "00133"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "monitoring", "system", "production", "systems", "analysis", "devops", "code", "richest", "well"], "content_type": "tutorial", "domain": "devops|architecture", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["APM systems", "production monitoring systems"], "frameworks": [], "methodologies": ["DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["performance monitoring", "log analysis", "user tracking", "conversion rates"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["production monitoring tools"], "topics": ["production monitoring", "data collection", "DevOps practices"], "key_concepts": ["performance of the system", "user satisfaction", "real-time analysis"], "problem_statement": "Generating the richest set of data for system performance and user satisfaction", "solution_approach": "Implementing production monitoring systems, collecting various types of data, and ensuring best DevOps practices", "extraction_method": "lm"}}
{"segment_id": "00133", "page_num": 133, "segment": "110\n\nCHAPTER 6 Data from your production systems\n\ncollect better data that provides insight into its performance and how it’s being used\nby your consumers.\n\n A lot of the advice that you’re about to read crosses the line into BI. Understanding\nhow your consumer is using your site is key to ensuring you’re building the right product and adding the right features at the right time. I’ve been on several teams where\nBI was a separate organizational function that didn’t always communicate directly with\nthe development team, and that feedback certainly didn’t come back to the team\ndirectly after they released updates to the consumer. If possible, BI should be integrated tightly with the development team; the closer the two functions are to one\nanother, the better the tactics described in the next subsections will work.\n\nThe difference between BI and business success metrics\n\nEven though I’m advocating keeping the BI effort closely tied to your development\neffort, it makes sense to have BI as a separate function from application development.\nBI teams exist to crunch the data your application generates, looking for trends and\nfiguring out how different things affect each other. BI teams generate reports that show\ntrends and relationships that reflect how the data being generated affects the success\nof the business. Success metrics reflect how your application is being used and are\nthe indicators of consumer behavior that are unique to your application.\n\n6.1.1\n\nAdding arbitrary metrics to your development cycle\n\nIt is not uncommon that a team will get caught up in building, and even testing, a feature and then leave any possible instrumentation or tagging for production monitoring up to another team---if it’s done at all. As you’re building your features you should\nbe thinking about your consumer, how they’re using your product, and what you need\nto know to improve their experience. This mindset will help you do the right type of\nwork to get good data that can help you determine if you’re hitting your true goal: giving your consumer what they want.\n\n Two example frameworks that help with this concept are StatsD (github.com/etsy/\nstatsd/) and Atlas (github.com/Netflix/atlas/wiki). The concept behind them fits\nprecisely with what we’ve been discussing so far and a mentality that shouldn’t be\nmuch of a stretch: “measure anything, measure everything.” StatsD and Atlas allow\ndevelopers to easily add arbitrary metrics and telemetry in their code. These libraries\nare embedded into your code with the intention of pushing the ownership of arbitrary\nmonitoring onto the development team.\n\n These systems work by installing an agent into your application container or a daemon on your server that listens for metrics as they’re being sent from your application. That component then sends the metrics back to a central time-data series server\nfor indexing. This process is illustrated in figure 6.4.\n\n The most popular example for this is StatsD. The StatsD daemon runs on a node.js\nserver, but there are clients you can use to send data to that daemon in many different\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "6", "chapter_title": "Data from your production systems", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "CHAPTER 6 Data from your production systems Success metrics reflect how your application is being used and are\nthe indicators of consumer behavior that are unique to your application. Adding arbitrary metrics to your development cycle", "l2_summary": "CHAPTER 6 Data from your production systems collect better data that provides insight into its performance and how it’s being used\nby your consumers. Understanding\nhow your consumer is using your site is key to ensuring you’re building the right product and adding the right features at the right time. The difference between BI and business success metrics Success metrics reflect how your application is being used and are\nthe indicators of consumer behavior that are unique to your application. Adding arbitrary metrics to your development cycle", "prev_page": {"page_num": 132, "segment_id": "00132"}, "next_page": {"page_num": 134, "segment_id": "00134"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["data", "development", "team", "metrics", "application", "consumer", "statsd", "right", "building", "teams"], "content_type": "theory|practice", "domain": "data_science|programming|devops", "complexity": "intermediate", "companies": ["Etsy", "Netflix"], "people": [], "products": [], "technologies": ["node.js", "StatsD", "Atlas"], "frameworks": ["StatsD", "Atlas"], "methodologies": [], "programming_languages": ["node.js"], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["consumer behavior", "application performance"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["StatsD", "Atlas"], "topics": ["Data collection and monitoring", "BI integration with development", "Metrics for business success"], "key_concepts": ["consumer insights", "BI vs. application development", "arbitrary metrics", "telemetry"], "problem_statement": "Collecting better data to understand consumer behavior and improve product performance.", "solution_approach": "Integrating BI with development teams, using frameworks like StatsD and Atlas for monitoring.", "extraction_method": "lm"}}
{"segment_id": "00134", "page_num": 134, "segment": "Preparing for analysis: generating the richest set of data you can\n\n111\n\nMultiple servers send\ndata to the collector.\n\nMultiple applications\ncan live on a server.\n\nServer\n\nServer\n\nApplication\n\nApplication\n\nApplication\n\nApplication\n\nCollection\nagent/daemon\n\nCollection\nagent/daemon\n\nApplications\nsend data to\nthe daemon.\n\nSingle cluster of\ntime-series data\n\nDaemons send data\nto the time-series store.\n\nIncrements\na counter\n\nSets a\ngauge value\n\nFigure 6.4 An example architecture to collect arbitrary metrics\n\nlanguages. Once you have a time-series data store set up and get the daemon running,\nusing StatsD is as simple as the code shown in the following listing.\n\nListing 6.1 Using StatsD\n\nprivate static final StatsDClient statsd = new\n\nNonBlockingStatsDClient(\"ord.cwhd\", \"statsd-host\", 8125);\n\nstatsd.incrementCounter(\"continuosity\");\nstatsd.recordGaugeValue(\"improve-o-meter\", 1000);\nstatsd.recordExecutionTime(\"bag\", 25);\nstatsd.recordSetEvent(\"qux\", \"one\");\n\nRecords\na timer\n\nCreates the\nstatic client\n\nSaves an event\nwith a timestamp\n\nAs shown in the listing, using this library is a piece of cake. Create the client object\nand set the metrics as you see fit in your code. The four different types of data give you\nlots of flexibility to record metrics as you see fit. On my teams I like to make this type\nof telemetry a part of quality checks that are enforced by static analysis or peer review.\nIf you’re building code, ensure that you have the proper measurement in place to\nshow the consumer value of the feature you’re working on.\n\n Let’s say you have a site that sells some kind of merchandise and you want to track\nhow many users utilize a new feature in your code during the checkout process. You\ncould use the database of orders to sort through your data, but there is usually personal data as well as financial data in that database, which requires special permissions\nand security procedures to access. By using an arbitrary metric library, you can collect\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "6", "chapter_title": "Data from your production systems", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "Applications\nsend data to\nthe daemon. Daemons send data\nto the time-series store. Listing 6.1 Using StatsD", "l2_summary": "Preparing for analysis: generating the richest set of data you can Multiple servers send\ndata to the collector. Applications\nsend data to\nthe daemon. Single cluster of\ntime-series data Daemons send data\nto the time-series store. Listing 6.1 Using StatsD", "prev_page": {"page_num": 133, "segment_id": "00133"}, "next_page": {"page_num": 135, "segment_id": "00135"}, "extended_fields": {"has_code": true, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "statsd", "application", "daemon", "using", "code", "send", "server", "time", "series"], "content_type": "tutorial", "domain": "architecture|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["StatsD", "time-series data store"], "frameworks": [], "methodologies": [], "programming_languages": ["Java"], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["counter", "gauge value", "execution time", "set event"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["StatsD", "time-series data store"], "topics": ["metric collection architecture", "using StatsD for metrics"], "key_concepts": ["metric collection", "time-series data storage", "metrics types (counter, gauge value, execution time, set event)"], "problem_statement": "Collecting and managing metrics in a distributed system", "solution_approach": "Using a metric collection architecture with tools like StatsD", "extraction_method": "lm"}}
{"segment_id": "00135", "page_num": 135, "segment": "112\n\nCHAPTER 6 Data from your production systems\n\nany statistics you want on your consumer’s usage separately from the rest of the application. Because the data pertains to how your collective consumers use the site rather\nthan individual transactions with customer details, the security around it doesn’t have\nto be as stringent, which opens up the possibility of collecting and analyzing this data\nin real time.\n\n Another use case is as an indicator of quality---is your application doing what it’s\nsupposed to do? If it is, then you should see your arbitrary metrics behaving as you’d\nexpect. If you start to see those metrics displaying unusual patterns, that may indicate\nthat something isn’t working correctly.\n\n StatsD is designed to write to a server called Graphite (graphite.wikidot.com/faq),\na time-data series database with a front end that shows arbitrary charts rather simply.\nThat looks something like figure 6.5.\n\n In figure 6.5 you can see metrics that the team made up and are tracking to see\n\nhow their consumers are using the site.\n\nProduct specific\ndashboards\n\nEvents timeline\n\nFigure 6.5 A screenshot of Grafana, the web front end for the Graphite time-series database. Note\nthat the metrics displayed are arbitrary metrics defined in the code of the application.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "6", "chapter_title": "Data from your production systems", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "If it is, then you should see your arbitrary metrics behaving as you’d\nexpect. In figure 6.5 you can see metrics that the team made up and are tracking to see Note\nthat the metrics displayed are arbitrary metrics defined in the code of the application.", "l2_summary": "CHAPTER 6 Data from your production systems If it is, then you should see your arbitrary metrics behaving as you’d\nexpect. That looks something like figure 6.5. In figure 6.5 you can see metrics that the team made up and are tracking to see Figure 6.5 A screenshot of Grafana, the web front end for the Graphite time-series database. Note\nthat the metrics displayed are arbitrary metrics defined in the code of the application.", "prev_page": {"page_num": 134, "segment_id": "00134"}, "next_page": {"page_num": 136, "segment_id": "00136"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["metrics", "data", "application", "time", "arbitrary", "graphite", "figure", "consumers", "site", "rather"], "content_type": "theory", "domain": "devops|architecture", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Graphite", "StatsD", "Grafana"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": false, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["arbitrary metrics"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Graphite", "StatsD", "Grafana"], "topics": ["Data collection and analysis", "Real-time monitoring", "Quality assurance"], "key_concepts": ["real-time data collection", "arbitrary metrics", "quality indicators"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00136", "page_num": 136, "segment": "Preparing for analysis: generating the richest set of data you can\n\n113\n\nUsing Graphite\n\nGraphite is a time-data series databasea that different frameworks can write into. The\nweb-based front end in our example is Grafana, which charts anything users can define.\nThroughout this book we frequently use Kibana as our front end for charting and metrics\naggregation; Grafana is actually a fork of Kibana, so the two look very similar.\n\n a A time series database is a software system optimized for handling time series data, arrays of\nnumbers indexed by time (a datetime or a datetime range), en.wikipedia.org/wiki/Time_\nseries_database.\n\n6.1.2 Utilizing the features of your application performance\n\nmonitoring system\n\nThe data you can get from your application performance monitoring system helps\nyou troubleshoot issues that you can trace back to your code changes. This connection\nallows you to tie your application performance to your development cycle. By utilizing\nthe features of your APM system, you’ll be able to make the connection between your\nproduct’s performance and your development cycle much easier.\n\n The tools we’ve looked at for analyzing arbitrary data can also be used for APM, but\nthey require a lot of setup and maintenance. There are some production-ready tools\nthat specialize in monitoring the performance of your application with minimal setup\nand a host of features. Two popular systems that fall into this category are New Relic\n(newrelic.com/) and Datadog ().\n\n Typical monitoring systems don’t care about what your consumers are doing or\nexamples like the previous one, but they do look at an awful lot of things relating to\nthe health of the system. These allow you to monitor things like the following:\n\n■ Network connections\n■ CPU\n■ Memory usage\n■ Transactions\n■ Database connections\n■ Disk space\n■ Garbage collection\n■ Thread counts\n\nNew Relic is a hosted service that you can use for free if you don’t mind losing your\ndata every 24 hours. It has a great API so it’s not so tough to pull out data you need to\nstore and use the rest of it for alerting and real-time monitoring. New Relic also gives\nyou the ability to instrument your code with custom tracing so you can track specific\nparts of your code that you want to ensure are performing appropriately. An example\nof a trace annotation is:\n\n@Trace(dispatcher=true)\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "6", "chapter_title": "Data from your production systems", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "Graphite is a time-data series databasea that different frameworks can write into. 6.1.2 Utilizing the features of your application performance The data you can get from your application performance monitoring system helps\nyou troubleshoot issues that you can trace back to your code changes.", "l2_summary": "Preparing for analysis: generating the richest set of data you can Graphite is a time-data series databasea that different frameworks can write into. a A time series database is a software system optimized for handling time series data, arrays of\nnumbers indexed by time (a datetime or a datetime range), en.wikipedia.org/wiki/Time_\nseries_database. 6.1.2 Utilizing the features of your application performance The data you can get from your application performance monitoring system helps\nyou troubleshoot issues that you can trace back to your code changes. New Relic is a hosted service that you can use for free if you don’t mind losing your\ndata every 24 hours.", "prev_page": {"page_num": 135, "segment_id": "00135"}, "next_page": {"page_num": 137, "segment_id": "00137"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["data", "time", "system", "performance", "monitoring", "series", "application", "database", "features", "trace"], "content_type": "tutorial", "domain": "devops|architecture", "complexity": "intermediate", "companies": ["New Relic", "Datadog"], "people": [], "products": [], "technologies": ["time-data series database", "application performance monitoring system"], "frameworks": ["Grafana", "Kibana"], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["application performance", "code changes"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Graphite", "Grafana", "Kibana", "New Relic", "Datadog"], "topics": ["Application Performance Monitoring", "Monitoring System Features", "Time-Series Database"], "key_concepts": ["time-data series database", "application performance monitoring system", "metrics aggregation"], "problem_statement": "Troubleshooting issues traced back to code changes and connecting application performance to the development cycle", "solution_approach": "Utilizing APM systems like New Relic or Datadog for monitoring various aspects of the system", "extraction_method": "lm"}}
{"segment_id": "00137", "page_num": 137, "segment": "114\n\nCHAPTER 6 Data from your production systems\n\nIt’s a good idea to put a trace annotation on anything that you want to get monitoring\ndata about. Just about anything is a good candidate, but here are particular parts of\nthe code where you should definitely monitor:\n\n■ Connections to other systems\n■ Anything that you communicate with through an API\n■ Working with databases\n■ Document parsing\n■ Anything that runs concurrently\n\nUsing annotations in New Relic can give you a lot more data to be able to dig into and\nunderstand how your code is functioning. You should at a minimum set up traces on\ncritical business logic and any external interfaces. From the list just shown, if you’re\ngoing to annotate anything, ensure you measure the following:\n\n■ Database connections\n■ Connections to other APIs or web services\n\nAnother example of tying your data back to the development cycle is including\nreleases in your APM data. In New Relic you have to set up deployment recording,1\nwhich varies based on what type of application you’re running. Figure 6.6 demonstrates how New Relic shows this data.\n\nDeployment of a new version\nof a web-based application\n\nAfter a deployment, response\ntime drops significantly.\n\nBefore the deploy, throughput visibly\naffected performance; afterward, more\nthroughput doesn't affect it much.\n\nRequests per\nminute (rpm)\n\nFigure 6.6 New Relic charting data over time with a clear delineation of a code release\n\n1 “Setting up deployment notifications,” docs.newrelic.com/docs/apm/new-relic-apm/maintenance/settingdeployment-notifications.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "6", "chapter_title": "Data from your production systems", "section_num": "4.3.2", "section_title": "Data you can get from centralized SCM", "l1_summary": "CHAPTER 6 Data from your production systems Figure 6.6 demonstrates how New Relic shows this data. Figure 6.6 New Relic charting data over time with a clear delineation of a code release", "l2_summary": "CHAPTER 6 Data from your production systems It’s a good idea to put a trace annotation on anything that you want to get monitoring\ndata about. Figure 6.6 demonstrates how New Relic shows this data. Deployment of a new version\nof a web-based application Figure 6.6 New Relic charting data over time with a clear delineation of a code release 1 “Setting up deployment notifications,” docs.newrelic.com/docs/apm/new-relic-apm/maintenance/settingdeployment-notifications.", "prev_page": {"page_num": 136, "segment_id": "00136"}, "next_page": {"page_num": 138, "segment_id": "00138"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "anything", "relic", "deployment", "code", "connections", "systems", "good", "just", "should"], "content_type": "tutorial", "domain": "devops|architecture", "complexity": "intermediate", "companies": ["New Relic"], "people": [], "products": [], "technologies": ["API", "databases", "document parsing", "concurrent execution"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["response time", "throughput"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["New Relic"], "topics": ["monitoring data", "tracing annotations", "deployment tracking"], "key_concepts": ["trace annotation", "critical business logic", "external interfaces", "deployment recording"], "problem_statement": "Improving monitoring and understanding of code functionality in production systems", "solution_approach": "Implementing trace annotations, setting up traces on critical areas, and using APM tools for deployment tracking", "extraction_method": "lm"}}
{"segment_id": "00138", "page_num": 138, "segment": "Preparing for analysis: generating the richest set of data you can\n\n115\n\nAs shown in figure 6.6 it’s very clear how the deployment affected the application’s\nperformance. In this case average response time went way down, which at a glance\nlooks pretty good. It would be interesting to map that back to the iteration that produced that code to see if anything related to how the team operated affected this performance improvement.\n\n6.1.3 Using logging best practices\n\nYour app should be writing something out to logs; at a minimum it’s likely writing out\nerrors when they happen. Another way to collect arbitrary metrics would be to write\nthem in your logs and use something like Splunk () or the ELK (EC\n[], Logstash [logstash.net], and Kibana [/\nguide/en/kibana/current/]) stack to aggregate logs, index them for quick searching,\nand turn your searches into nice graphs that you can use for monitoring. If you’re\nlooking at your logs from that perspective, log as much as possible, particularly anything that would be of value when aggregated, charted, or further analyzed. Regardless of your favorite way to collect data, you should use some best practices when you\ndo logging to ensure that the data you’re saving is something you can work with. After\nall, if you’re not doing that, what’s the point of writing logs in the first place?\n\nUSE TIMESTAMPS, SPECIFICALLY IN ISO 8601\nAnything you log should have a timestamp. Most logging frameworks take care of this\nfor you, but to help make your logs even more searchable and readable use the standard date format ISO 8601 (en.wikipedia.org/wiki/ISO_8601). ISO 8601 is a date/time\nformat that’s human readable, includes time zone information, and best of all is a\nstandard that’s supported in most development languages. The following code shows\nthe difference between an ISO 8601 date and a UNIX epoch timestamp, which unfortunately many people still use.\n\n2014-10-24T19:17:30+00:00\n1414277900\n\nThe same date as a\nUNIX epoch timestamp\n\nA date in ISO 8601 format\n\nWhich would you rather read? The most annoying thing about the UNIX format is that\nif you are at all concerned with time zones you need an additional offset to figure out\nwhen the date happened relative to UTC time.\n\nUSE UNIQUE IDS IN YOUR LOGS, BUT BE CAREFUL OF CONSUMER DATA\nIf you want to be able to track down specific events in your logs, you should create IDs\nthat you can index and search on to track transactions through complex systems. But\nif you do this, make sure you’re not using any kind of sensitive data that could enable\nhackers to find out more about consumers, like their Social Security number or other\npersonal information.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "6", "chapter_title": "Data from your production systems", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "USE TIMESTAMPS, SPECIFICALLY IN ISO 8601\nAnything you log should have a timestamp. Most logging frameworks take care of this\nfor you, but to help make your logs even more searchable and readable use the standard date format ISO 8601 (en.wikipedia.org/wiki/ISO_8601). A date in ISO 8601 format", "l2_summary": "6.1.3 Using logging best practices Your app should be writing something out to logs; at a minimum it’s likely writing out\nerrors when they happen. USE TIMESTAMPS, SPECIFICALLY IN ISO 8601\nAnything you log should have a timestamp. Most logging frameworks take care of this\nfor you, but to help make your logs even more searchable and readable use the standard date format ISO 8601 (en.wikipedia.org/wiki/ISO_8601). ISO 8601 is a date/time\nformat that’s human readable, includes time zone information, and best of all is a\nstandard that’s supported in most development languages. A date in ISO 8601 format", "prev_page": {"page_num": 137, "segment_id": "00137"}, "next_page": {"page_num": 139, "segment_id": "00139"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["logs", "8601", "date", "data", "time", "would", "should", "when", "format", "which"], "content_type": "tutorial", "domain": "devops|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Splunk", "ELK (Elasticsearch, Logstash, Kibana)"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["average response time"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Splunk", "ELK (Elasticsearch, Logstash, Kibana)"], "topics": ["log analysis", "performance monitoring", "logging best practices"], "key_concepts": ["ISO 8601 timestamp", "unique IDs in logs", "data aggregation and visualization"], "problem_statement": "Improving application performance through better logging and data collection", "solution_approach": "Implementing logging best practices, using tools like Splunk or ELK stack for log analysis and monitoring", "extraction_method": "lm"}}
{"segment_id": "00139", "page_num": 139, "segment": "116\n\nCHAPTER 6 Data from your production systems\n\nUSE STANDARD LOG CATEGORIES AND FRAMEWORKS\nLog4J and other Log4 frameworks make adding logging to your application a breeze.\nInstead of trying to reinvent the wheel, use these common frameworks. In addition,\nuse the standard log levels of INFO, WARN, ERROR, and DEBUG so you can set your\nlogs to output the data you need when you need it.\n\n■\n\nINFO---Use this when you want to send information back to your logs.\n■ WARN---Use this when something isn’t going well but isn’t blowing up yet.\n■ ERROR---Something broke; always pay attention to these.\n■ DEBUG---Typically you don’t look at DEBUG logs in a production environment\nunless you’re trying to find a problem you can’t figure out from the rest of your\nlogs. Of course, this is great to have in nonproduction environments.\n\nPAY ATTENTION TO WHAT YOUR LOGS ARE TELLING YOU PROACTIVELY\nAlthough this may sound obvious, I’ve seen teams completely ignore their logs until\nthe consumer was reporting something they couldn’t reproduce. By that time it was\ntoo late; there were so many errors in the logs that trying to find the root of the problem became like searching for a needle in a haystack. Be a good developer and write\ndata to your logs; then be a good developer and pay attention to what your logs tell\nyou.\n\nUSE FORMATS THAT ARE EASY TO WORK WITH\nTo be able to analyze the data in your logs, you’ll want to make sure you’re writing\nthem in a format that’s easy to parse, search, and aggregate. The following guidelines\ngive details on how to do this:\n\n■ Log events in text instead of binary format. If you log things in binary, they’re going\n\nto require decoding before you can do anything with them.\n\n■ Make the logs easy for humans to read. Your logs are essentially auditing everything\nyour system does. If you want to make sense of that later, you had better be able\nto read what your logs are telling you.\n\n■ Use JSON, not XML. JSON is easy for developers to work with, easier to index, and\na more human-readable format. You can use standard libraries to write JSON\nout into your logs through your logger.\n\n■ Clearly mark key-value pairs in your logs. Tools like Splunk and ELK will have an\neasier time parsing data that looks like key=value, which will make it easier for\nyou to search for later.\n\n6.1.4 Using social network interaction to connect with your consumers\n\nOne easy way to stay in touch with your consumers is to have a solid social network presence. Depending on what your team is building, perhaps it doesn’t make sense to use\nTwitter, but even an internal social network like Yammer, Convo, or Jive can take advantage of that in the same way. If you can create hashtags that your consumers use to talk\nabout you, it’s very easy to get that data out of Twitter, Yammer, or other social networks\nto find out how well changes you make to your software affect your consumers.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "6", "chapter_title": "Data from your production systems", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "INFO---Use this when you want to send information back to your logs. ■ Make the logs easy for humans to read. You can use standard libraries to write JSON\nout into your logs through your logger.", "l2_summary": "CHAPTER 6 Data from your production systems USE STANDARD LOG CATEGORIES AND FRAMEWORKS\nLog4J and other Log4 frameworks make adding logging to your application a breeze. INFO---Use this when you want to send information back to your logs. Be a good developer and write\ndata to your logs; then be a good developer and pay attention to what your logs tell\nyou. ■ Make the logs easy for humans to read. You can use standard libraries to write JSON\nout into your logs through your logger.", "prev_page": {"page_num": 138, "segment_id": "00138"}, "next_page": {"page_num": 140, "segment_id": "00140"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["logs", "make", "data", "easy", "what", "like", "social", "consumers", "standard", "frameworks"], "content_type": "tutorial", "domain": "devops|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Log4J", "JSON", "Splunk", "ELK"], "frameworks": ["Log4 frameworks"], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Splunk", "ELK"], "topics": ["Logging best practices", "Log formats", "Social network interaction"], "key_concepts": ["Standard log categories and frameworks", "Proactive log monitoring", "Easy-to-work-with log formats"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00140", "page_num": 140, "segment": "Preparing for analysis: generating the richest set of data you can\n\n117\n\nOur app posted this\ncustom hashtag when\nusers tweeted their runs.\n\nFigure 6.7 Results for #nikeplus from the Nike+ Running app\n\nA simple example would be to promote a new feature by creating a hashtag that allows\nyour consumers to talk about it on their social network of choice. If you’re using Twitter, this becomes really easy to do. When I was working on the Nike+ Running app, we\neven built hashtags into the social posting feature when someone finished a run; then\nwe knew exactly how people were using the app and what they thought. Figure 6.7\nshows searching Twitter for the tags we implemented in our app.\n\n The Nike+ Running app was fun to work on because it has a strong fan base whose\nmembers are very vocal about how they use it. Building in social interaction and then\nfollowing what your users are doing gives you tremendous insight into how well your\napplications are serving your consumers. Because we appended the hashtag #nikeplus\non every Twitter post, it was really easy for us to see exactly how our consumers were\nusing the app and what they were thinking when they used it. This kind of data helped\nus shape new features and modify how the app worked to enhance the consumer\nexperience even further.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "6", "chapter_title": "Data from your production systems", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "Our app posted this\ncustom hashtag when\nusers tweeted their runs. Figure 6.7 Results for #nikeplus from the Nike+ Running app Figure 6.7\nshows searching Twitter for the tags we implemented in our app.", "l2_summary": "Our app posted this\ncustom hashtag when\nusers tweeted their runs. Figure 6.7 Results for #nikeplus from the Nike+ Running app If you’re using Twitter, this becomes really easy to do. When I was working on the Nike+ Running app, we\neven built hashtags into the social posting feature when someone finished a run; then\nwe knew exactly how people were using the app and what they thought. Figure 6.7\nshows searching Twitter for the tags we implemented in our app. Because we appended the hashtag #nikeplus\non every Twitter post, it was really easy for us to see exactly how our consumers were\nusing the app and what they were thinking when they used it.", "prev_page": {"page_num": 139, "segment_id": "00139"}, "next_page": {"page_num": 141, "segment_id": "00141"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["when", "hashtag", "nike", "running", "consumers", "social", "using", "twitter", "what", "data"], "content_type": "case_study", "domain": "programming", "complexity": "intermediate", "companies": ["Nike"], "people": [], "products": ["Nike+ Running app"], "technologies": [], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["consumer usage", "user feedback"], "has_case_study": true, "case_study_company": "Nike", "tools_mentioned": [], "topics": ["social media integration", "data collection", "user engagement"], "key_concepts": ["hashtag implementation", "social interaction analysis"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00141", "page_num": 141, "segment": "118\n\nCHAPTER 6 Data from your production systems\n\nNOTE Twitter has a great API that allows you to get this data into your data\nanalysis system, but the API isn’t public; you have to register an app to use it.\nIn addition, there are rate limits to it, so if you use it be careful to stay within\nallowable request limitations.\n\n6.2\n\nThe data you’ll be working with: what you can get from\nyour APM systems\nNow that I’ve given you myriad tips on how to maximize your data, let’s look at getting\nthe data out and how you can use it to hone your team’s performance. There are two\nbig categories of data that you can get from production:\n\n■ Application monitoring---The data your application is generating regardless of\n\nwhat your application does:\n■ Server and application health\n■ General logging\n\n■ BI data---Application-specific data that tells you how your consumers are interacting with your application:\n■ Captured as arbitrary metrics that you can make up as you go along\n■ Semantic logging, using strongly typed events for better log analysis\n\n6.2.1\n\nServer health statistics\n\nYour server health statistics are the indicators of how well your system is built. Looking at\ncrash rates, stack traces, and server health shows you if your code is running well or if it’s\neating resources and generating poor experiences. Typical things you can look at are:\n\n■ CPU usage\n■ Heap size\n■ Error rates\n■ Response times\n\nThe first important bit of data you can learn from New Relic dashboards is shown in figure 6.8, which shows how long web responses take and where the response time is spent.\n Because New Relic is looking at how much time consumers are spending on your\n\nsite, it can also tell you what the most popular pages are, as shown in figure 6.9.\n\n These metrics are interesting because they help inform you about the experience\nyour consumers are having with your applications. It’s important to set service-level\nagreements (SLAs) for your applications to help you define what you think is a good\nexperience. When you do this, don’t look at how your application performs now;\nthink of what your consumers expect. Are they okay with a web page that loads in four\nseconds or should the SLA be under one second? Do they expect to get results from\nentering data of some kind immediately, or do they just expect to dump some data in\nand see results later?\n\n Watching how your performance trends toward your SLAs becomes interesting as\nyou compare it to the data we’ve already looked at from project tracking, source\ncontrol, and CI. Are you improving team performance, at the expense of application\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "6", "chapter_title": "Data from your production systems", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "CHAPTER 6 Data from your production systems ■ Application monitoring---The data your application is generating regardless of what your application does:\n■ Server and application health\n■ General logging", "l2_summary": "CHAPTER 6 Data from your production systems The data you’ll be working with: what you can get from\nyour APM systems\nNow that I’ve given you myriad tips on how to maximize your data, let’s look at getting\nthe data out and how you can use it to hone your team’s performance. There are two\nbig categories of data that you can get from production: ■ Application monitoring---The data your application is generating regardless of what your application does:\n■ Server and application health\n■ General logging Server health statistics", "prev_page": {"page_num": 140, "segment_id": "00140"}, "next_page": {"page_num": 142, "segment_id": "00142"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "application", "what", "server", "health", "consumers", "look", "performance", "expect", "production"], "content_type": "tutorial", "domain": "devops|architecture", "complexity": "intermediate", "companies": ["Twitter", "New Relic"], "people": [], "products": [], "technologies": ["API", "application monitoring", "server and application health", "general logging", "BI data", "semantic logging", "strongly typed events"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["CPU usage", "Heap size", "Error rates", "Response times", "Web response time", "Most popular pages"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Twitter API", "New Relic dashboards"], "topics": ["Data from production systems", "Application monitoring", "BI data", "Server health statistics"], "key_concepts": ["API usage for data retrieval", "SLAs for application performance", "Performance metrics and trends"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00142", "page_num": 142, "segment": "The data you’ll be working with: what you can get from your APM systems\n\n119\n\nThe breakdown of\nthe response time\n\nFigure 6.8 New Relic dashboard overview showing the performance of the different layers of a web\napplication\n\nThe top pages broken\ndown by percentage\n\nThe amount of time consumers\nspend on your website\n\nFigure 6.9 The New Relic transactions view showing the pages on your website where consumers spend\nthe most time\n\nperformance, or is application performance a first-class citizen in your development\ncycle? When development teams are completely focused on new features and don’t\npay attention to how their product is performing in the wild, it’s easy to lose sight of\napplication performance.\n\nREACTING TO APPLICATION HEALTH\nKeeping your application healthy is an important factor in consumer satisfaction. Ideally you want to have great response times and an app that can scale up to whatever\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "6", "chapter_title": "Data from your production systems", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "The breakdown of\nthe response time Figure 6.8 New Relic dashboard overview showing the performance of the different layers of a web\napplication performance, or is application performance a first-class citizen in your development\ncycle?", "l2_summary": "The breakdown of\nthe response time Figure 6.8 New Relic dashboard overview showing the performance of the different layers of a web\napplication The amount of time consumers\nspend on your website Figure 6.9 The New Relic transactions view showing the pages on your website where consumers spend\nthe most time performance, or is application performance a first-class citizen in your development\ncycle? REACTING TO APPLICATION HEALTH\nKeeping your application healthy is an important factor in consumer satisfaction.", "prev_page": {"page_num": 141, "segment_id": "00141"}, "next_page": {"page_num": 143, "segment_id": "00143"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["application", "performance", "time", "response", "figure", "relic", "showing", "pages", "consumers", "spend"], "content_type": "practice", "domain": "devops|architecture", "complexity": "intermediate", "companies": ["New Relic"], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["response time", "consumers spend on website"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["New Relic"], "topics": ["APM systems", "application performance", "consumer satisfaction"], "key_concepts": ["performance monitoring", "response time analysis", "user engagement metrics"], "problem_statement": "Lack of focus on application performance in development cycles", "solution_approach": "Implementing APM tools and focusing on application health", "extraction_method": "lm"}}
{"segment_id": "00143", "page_num": 143, "segment": "120\n\nCHAPTER 6 Data from your production systems\n\nyour potential consumer base is. A good APM system will tell you when performance is\ngetting bad and will also usually tell you where your problems are. When you see performance problems, you should take the time in your current development cycle to\nfix or mitigate them.\n\n6.2.2\n\nConsumer usage\n\nAnother production monitoring strategy is to watch how your consumers are using\nyour site. There are a number of ways to track page hits and site usage; two examples\nof off-the-shelf products are Google Analytics (/analytics/) and Crittercism (/). These solutions not only track the number of people\nwho are using your products, but also how long they spent on certain pages and how\nthey flowed through the app, as well as something called conversion rate.\n\n Conversion rate is determined by dividing whatever you decide is success criteria\nfor using your application by total users. Success may be selling something, clicking\non an advertisement, or making a connection with another user. Conversion tells you\nhow successful you are at turning users into consumers.\n\nREACTING TO CONSUMER USAGE\nTrends in consumer usage help you determine how your application needs to evolve to\nbetter serve your consumer. If you have features that your consumers don’t use, then\nwhy are they even in your product? Hone the most popular features in your application\nto ensure your consumers are getting the best possible experience you can offer.\n\n Try to move your team to a model where you can do small, frequent releases. In\neach release be sure to measure the impact of the change you just deployed to help\ndetermine what feature to work on next.\n\n6.2.3\n\nSemantic logging analysis\n\nHow your consumers are using your system is key to continuously improving it. If\nyou’re collecting arbitrary metrics and page tracking, then there’s no limit to what you\ncan learn about your consumers and how they use your system.\n\n If you’re practicing semantic logging and logging custom metrics, then you can get\njust about any kind of data you want. Perhaps you have search functionality on your\nsite and you want to know what people are searching for so you can better tune your\ncontent of products based on what consumers are looking for. You could save every\nsearch to the database and then do some kind of data-mining exercise on it every so\noften, or you could log a strongly typed event with every search.\n\n Using the frameworks or logging that we talked about earlier in the chapter to view\nthis data in real time makes tracking metrics that you define as success criteria for\nyour features much easier. Let’s say that you have a gardening app where users enter\nthe plants in their garden and search for advice on how to make them as healthy as\npossible. If your business model is based on creating the most valuable content for\nyour users, then the searches people plug into your app are invaluable information\nbecause it tells you what your user base wants to read.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "6", "chapter_title": "Data from your production systems", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "Another production monitoring strategy is to watch how your consumers are using\nyour site. Conversion tells you\nhow successful you are at turning users into consumers. How your consumers are using your system is key to continuously improving it.", "l2_summary": "CHAPTER 6 Data from your production systems your potential consumer base is. Another production monitoring strategy is to watch how your consumers are using\nyour site. Conversion tells you\nhow successful you are at turning users into consumers. REACTING TO CONSUMER USAGE\nTrends in consumer usage help you determine how your application needs to evolve to\nbetter serve your consumer. How your consumers are using your system is key to continuously improving it.", "prev_page": {"page_num": 142, "segment_id": "00142"}, "next_page": {"page_num": 144, "segment_id": "00144"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["consumers", "consumer", "using", "then", "what", "data", "usage", "users", "logging", "search"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": ["Google", "Crittercism"], "people": [], "products": [], "technologies": [], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["conversion rate"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Google Analytics", "Crittercism"], "topics": ["APM systems", "consumer usage tracking", "semantic logging analysis"], "key_concepts": ["performance monitoring", "conversion rate", "user behavior analysis"], "problem_statement": "Improving application performance and understanding consumer behavior", "solution_approach": "Implementing APM systems, using analytics tools, and practicing semantic logging", "extraction_method": "lm"}}
{"segment_id": "00144", "page_num": 144, "segment": "The data you’ll be working with: what you can get from your APM systems\n\n121\n\n The key to semantic logging analysis is that it tells you what you set your system up\nto tell you. If you’re logging metrics that are valuable to your business and watching\nthem as you update your software products, they become some of the most valuable\nmetrics you can collect.\n\nREACTING TO DATA IN YOUR LOGS\nYou should strive for zero errors in your logs. For those of you laughing right now, at\nleast try to clean up your errors with every release. If you’re seeing large numbers of\nerrors or warnings in your log analysis, then you have a strong data point you can use\nto lobby for time to clean up tech debt.\n\n6.2.4\n\nTools used to collect production system data\n\nTable 6.1 is summary of the tools we’ve talked about and used in this chapter. Keep in\nmind that open source systems typically have a lot of setup you have to do yourself (DIY).\n\nTable 6.1 APM and BI tools used in this chapter and the data they provide\n\nProduct\n\nType of system\n\nData It provides\n\nCost model\n\nSplunk\n\nCloud-based with\nagents installed on\nyour servers for\ndata collection\n\nAPM and log aggregation and analysis. Splunk allows you to search\nthrough anything you send to it.\n\nPay for the amount of\ndata you store in it.\n\nDIY\n\nEC, Logstash,\nand Kibana\n(ELK)\n\nNew Relic\n\nCloud-based with\nagents stored on\nyour servers for\ndata collection\n\nGraphite and\nGrafana\n\nDIY\n\nOpen Web\nAnalytics\n\nDIY\n\nGoogle\nAnalytics\n\nCloud-based\n\nAPM and log aggregation and analysis. The ELK stack allows you to\nsearch through anything you send to\nit. This is commonly referred to as\n“open source Splunk.”\n\nUsing instrumentation, New Relic\ngives you lots of performance data\nranging from CPU and memory analysis to site usage statistics and a\ndetailed breakdown of where your\ncode spends the most of its time.\n\nLabor costs for setup and\nmaintenance along with\nthe cost of infrastructure\nand storage.\n\nFree if you don’t store\ndata for longer than a\nday; then you pay based\non the amount of data\nyou store.\n\nThis is the epitome of a DIY system.\nGraphite and Grafana, like the ELK\nstack, show nice charts on any type\nof time-data series you send to it.\n\nLabor costs for setup and\nmaintenance along with\nthe cost of infrastructure\nand storage.\n\nCollects data on how consumers\nnavigate through your site along with\npage hit counts.\n\nThe standard for collecting usage\nstatistics for web applications. It\ntracks how consumers navigate your\nsite, counts page hits, and helps\ntrack conversion rate.\n\nLabor costs for setup and\nmaintenance along with\nthe cost of infrastructure\nand storage.\n\nFree for most cases; the\npremium version has a\nflat fee.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 2, "cols": 4, "data": [["Product", "Type of system", "Data It provides", "Cost model"], ["", "Cloud-based with\nagents installed on\nyour servers for\ndata collection\nDIY\nCloud-based with\nagents stored on\nyour servers for\ndata collection\nDIY\nDIY\nCloud-based", "APM and log aggregation and analy-\nsis. Splunk allows you to search\nthrough anything you send to it.\nAPM and log aggregation and analy-\nsis. The ELK stack allows you to\nsearch through anything you send to\nit. This is commonly referred to as\n“open source Splunk.”\nUsing instrumentation, New Relic\ngives you lots of performance data\nranging from CPU and memory anal-\nysis to site usage statistics and a\ndetailed breakdown of where your\ncode spends the most of its time.\nThis is the epitome of a DIY system.\nGraphite and Grafana, like the ELK\nstack, show nice charts on any type\nof time-data series you send to it.\nCollects data on how consumers\nnavigate through your site along with\npage hit counts.\nThe standard for collecting usage\nstatistics for web applications. It\ntracks how consumers navigate your\nsite, counts page hits, and helps\ntrack conversion rate.", ""]], "markdown": "| Product | Type of system | Data It provides | Cost model |\n|---|---|---|---|\n|  | Cloud-based with\nagents installed on\nyour servers for\ndata collection\nDIY\nCloud-based with\nagents stored on\nyour servers for\ndata collection\nDIY\nDIY\nCloud-based | APM and log aggregation and analy-\nsis. Splunk allows you to search\nthrough anything you send to it.\nAPM and log aggregation and analy-\nsis. The ELK stack allows you to\nsearch through anything you send to\nit. This is commonly referred to as\n“open source Splunk.”\nUsing instrumentation, New Relic\ngives you lots of performance data\nranging from CPU and memory anal-\nysis to site usage statistics and a\ndetailed breakdown of where your\ncode spends the most of its time.\nThis is the epitome of a DIY system.\nGraphite and Grafana, like the ELK\nstack, show nice charts on any type\nof time-data series you send to it.\nCollects data on how consumers\nnavigate through your site along with\npage hit counts.\nThe standard for collecting usage\nstatistics for web applications. It\ntracks how consumers navigate your\nsite, counts page hits, and helps\ntrack conversion rate. |  |"}], "table_count": 1, "chapter_num": "6", "chapter_title": "Data from your production systems", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "The data you’ll be working with: what you can get from your APM systems Table 6.1 APM and BI tools used in this chapter and the data they provide Pay for the amount of\ndata you store in it.", "l2_summary": "The data you’ll be working with: what you can get from your APM systems Tools used to collect production system data Table 6.1 APM and BI tools used in this chapter and the data they provide Cloud-based with\nagents installed on\nyour servers for\ndata collection Pay for the amount of\ndata you store in it. Cloud-based with\nagents stored on\nyour servers for\ndata collection", "prev_page": {"page_num": 143, "segment_id": "00143"}, "next_page": {"page_num": 145, "segment_id": "00145"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["data", "analysis", "system", "setup", "cost", "based", "along", "most", "errors", "have"], "content_type": "tutorial", "domain": "devops|architecture", "complexity": "intermediate", "companies": ["Splunk", "New Relic"], "people": [], "products": ["Splunk", "EC, Logstash, and Kibana (ELK)", "New Relic", "Graphite and Grafana", "Open Web Analytics", "Google Analytics"], "technologies": [], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["errors", "warnings"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Splunk", "EC, Logstash, and Kibana (ELK)", "New Relic", "Graphite and Grafana", "Open Web Analytics", "Google Analytics"], "topics": ["APM systems", "log analysis", "data collection tools"], "key_concepts": ["semantic logging analysis", "error management", "tool selection for data collection"], "problem_statement": "Improving error handling and log analysis in APM systems", "solution_approach": "Using appropriate tools and best practices for log analysis", "extraction_method": "lm"}}
{"segment_id": "00145", "page_num": 145, "segment": "122\n\nCHAPTER 6 Data from your production systems\n\nTable 6.1 APM and BI tools used in this chapter and the data they provide (continued)\n\nProduct\n\nType of system\n\nData It provides\n\nCost model\n\nData Dog\n\nCloud-based\n\nAggregates data from all over the\nplace.\n\nCrittercism\n\nMobile library with\ncloud-based data\ncollectors\n\nYou can get crash rates and usage\nstatistics that help you figure out\nwhat your consumers are seeing\nand how to fix their issues.\n\nFree for small installations; you pay for how\nlong you want to retain\nyour data and the size of\nyour installation.\n\nLicense fees\n\n6.3\n\nCase study: a team moves to DevOps and continuous delivery\nOur case study team has a lot of the pieces in place to start utilizing the data from\ntheir production environment. They’ve been working to transition to a DevOps\nmodel, and the development team has started paying close attention to their production systems. They have log analysis, are using New Relic to monitor the health of their\nsystems, and are looking at key metrics in their development cycle to ensure their process is working well. They’ve even improved their CI system so much that they were\nable to start deploying small changes to production every day. They thought they had\nachieved continuous delivery and were pretty excited about it; now their consumers\nwere getting added value every day! Or so they thought.\n\n After a few weeks of operating like this, they got their biweekly report from the BI\nteam showing how consumers were using the system, and it hadn’t changed much at\nall. The BI team was tracking the conversion rate of consumers on the site, total visits\nfor the different pages across the site, and how long unique visitors stay on the site.\nThe data they were getting back looked like the dashboard in figure 6.10.\n\nConversion rate; how many consumers\ndo what you want them to\n\nApril 7, 2015\n\nApril 2, 2015\n\nApril 9, 2015\n\nApril 16, 2015\n\nApril 23, 2015\n\nSearch sent 7,262 total visits via 5,121 keywords\n\n7,262\nof Site Total:\n38.2%\n\n1.73\nSite Avg: 1.85\n(-5.03)\n\n00:02:11\nSite Avg: 00:02.89\n(-15.32%)\n\n79.44%\nSite Avg: 60.76%\n(17.55%)\n\n74.17%\nSite Avg: 67.90%\n(11.46)\n\nFigure 6.10 The dashboard\nthe BI team paid the closest\nattention to\n\nNew visitors\nto your site\n\nHow many people\ncome to your site and\nleave right away\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 2, "cols": 4, "data": [["Product", "Type of system", "Data It provides", "Cost model"], ["", "Cloud-based\nMobile library with\ncloud-based data\ncollectors", "Aggregates data from all over the\nplace.\nYou can get crash rates and usage\nstatistics that help you figure out\nwhat your consumers are seeing\nand how to fix their issues.", ""]], "markdown": "| Product | Type of system | Data It provides | Cost model |\n|---|---|---|---|\n|  | Cloud-based\nMobile library with\ncloud-based data\ncollectors | Aggregates data from all over the\nplace.\nYou can get crash rates and usage\nstatistics that help you figure out\nwhat your consumers are seeing\nand how to fix their issues. |  |"}], "table_count": 1, "chapter_num": "6", "chapter_title": "Data from your production systems", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "1.73\nSite Avg: 1.85\n(-5.03) 00:02:11\nSite Avg: 00:02.89\n(-15.32%) 79.44%\nSite Avg: 60.76%\n(17.55%)", "l2_summary": "CHAPTER 6 Data from your production systems 7,262\nof Site Total:\n38.2% 1.73\nSite Avg: 1.85\n(-5.03) 00:02:11\nSite Avg: 00:02.89\n(-15.32%) 79.44%\nSite Avg: 60.76%\n(17.55%) 74.17%\nSite Avg: 67.90%\n(11.46)", "prev_page": {"page_num": 144, "segment_id": "00144"}, "next_page": {"page_num": 146, "segment_id": "00146"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": true, "has_citations": false, "key_terms": ["site", "data", "team", "consumers", "april", "2015", "production", "systems", "system", "figure"], "content_type": "case_study", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": ["Data Dog", "Crittercism", "New Relic"], "technologies": [], "frameworks": [], "methodologies": ["DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["conversion rate", "total visits", "unique visitors stay time"], "has_case_study": true, "case_study_company": null, "tools_mentioned": ["Data Dog", "Crittercism", "New Relic"], "topics": ["DevOps transition", "continuous delivery", "BI dashboard analysis"], "key_concepts": ["production system monitoring", "log analysis", "metric tracking"], "problem_statement": "Achieving continuous delivery while ensuring consumer value addition", "solution_approach": "Implementing DevOps practices, using APM and BI tools for monitoring", "extraction_method": "lm"}}
{"segment_id": "00146", "page_num": 146, "segment": "Case study: a team moves to DevOps and continuous delivery\n\n123\n\nThe delivery team took a look at the BI report and realized that there was no direct\nlink between the work they were doing and the metrics being used to determine if features were successful. Because they were delivering features, the team needed to figure out how those features improved conversion and stickiness on their site. The\ndelivery team decided to start by bringing the BI team into their sprint planning to\nhelp them close the gap.\n\n For every new feature they were going to implement they asked, “What is the value\nto the consumer of this change, how do we measure it, and how does it affect conversion?” The next tweak the development team made was to add a More Info button\nthat had a Buy Now button on it. The feature was designed with the following theories\nin mind:\n\n■ They wanted to show many products on the page to a consumer at once, so they\n\nwould save space by showing info in a pop-up instead of inline.\n\n■ They thought that with that info consumers would be more likely to buy a product and therefore should click the Buy Now button from that pop-up.\n\nTo figure out how this affected conversion, they added a few custom metrics:\n\n■ Buy Now button clicks\n■ More Info clicks\n\nIf their theories were correct, they should see a significant number of clicks on the\nMore Info link and a high percentage of clicks on Buy Now after clicks on More Info,\nand conversion should go up.\n\n The team was already using ELK for their logging, so they simply added strongly\ntyped messages to their logs for the events they wanted to track. These started showing\nup in their dashboard, as shown in figure 6.11.\n\nEvents logged plotted over time\n\nFigure 6.11 The Kibana dashboard the team generated to track their statistics\n\nEvents stores as JSON documents.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "6", "chapter_title": "Data from your production systems", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "Case study: a team moves to DevOps and continuous delivery ■ Buy Now button clicks\n■ More Info clicks Figure 6.11 The Kibana dashboard the team generated to track their statistics", "l2_summary": "Case study: a team moves to DevOps and continuous delivery Because they were delivering features, the team needed to figure out how those features improved conversion and stickiness on their site. The\ndelivery team decided to start by bringing the BI team into their sprint planning to\nhelp them close the gap. ■ Buy Now button clicks\n■ More Info clicks If their theories were correct, they should see a significant number of clicks on the\nMore Info link and a high percentage of clicks on Buy Now after clicks on More Info,\nand conversion should go up. Figure 6.11 The Kibana dashboard the team generated to track their statistics", "prev_page": {"page_num": 145, "segment_id": "00145"}, "next_page": {"page_num": 147, "segment_id": "00147"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["team", "info", "more", "clicks", "figure", "conversion", "button", "delivery", "features", "should"], "content_type": "case_study", "domain": "devops|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["ELK", "BI report", "Kibana dashboard"], "frameworks": [], "methodologies": ["DevOps", "continuous delivery"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["Buy Now button clicks", "More Info clicks"], "has_case_study": true, "case_study_company": null, "tools_mentioned": ["ELK", "BI report", "Kibana dashboard"], "topics": ["DevOps implementation", "Continuous delivery", "Feature value measurement"], "key_concepts": ["closing the gap between development and BI teams", "custom metrics for feature evaluation", "using logs for tracking statistics"], "problem_statement": "No direct link between work done by the delivery team and metrics used to determine feature success", "solution_approach": "Incorporating BI team in sprint planning, adding custom metrics, using ELK for logging", "extraction_method": "lm"}}
{"segment_id": "00147", "page_num": 147, "segment": "124\n\nCHAPTER 6 Data from your production systems\n\nNow the team was completing the picture. They could see how they were developing,\nwhat they were developing, and how it affected their consumers when they deployed\nchanges to the consumer. Now they could use that data to shape what features they\nshould work on next and how they could tweak their development process to ensure\nthey were delivering things the right way.\n\n6.4\n\nSummary\nThe final piece of the data picture is the data your application generates as your consumers interact with it. In this chapter you learned:\n\n■ Application performance monitoring and business intelligence typically aren’t\n\nmeasured by development teams.\n\n■ APM gives you insight into how well your application is built, and BI tells you\n\nhow your consumers use it:\n■ Server health statistics show you how well your application is performing.\n■ Arbitrary stats and semantic logging give you measureable stats on how your\n\napplication is being used.\n\n■ Teams using a DevOps model are more likely to have access to APM data.\n■ Add arbitrary metrics collection to your code to collect application specific\ndata. Netflix Servo and StatsD are popular open source libraries to help with\nthis.\n\n■ Use logging best practices to get as much data as you can from your log analysis:\n\n■ Use ISO8601-based timestamps.\n■ Use unique IDs in your logs, but be careful of consumer data.\n■ Use standard log categories and frameworks.\n■ Pay attention to what your logs are telling you proactively.\n■ Use formats that are easy to work with.\n\n■ Using social networks allows you to better connect with your consumers.\n■ Use arbitrary monitoring or semantic logging to provide feedback to the development team based on BI data to let them gauge effectiveness of new features.\n■ A variety of open source and commercial tools are available for application\n\nmonitoring and collecting BI.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "6", "chapter_title": "Data from your production systems", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "CHAPTER 6 Data from your production systems how your consumers use it:\n■ Server health statistics show you how well your application is performing. application is being used.", "l2_summary": "CHAPTER 6 Data from your production systems Summary\nThe final piece of the data picture is the data your application generates as your consumers interact with it. how your consumers use it:\n■ Server health statistics show you how well your application is performing. application is being used. ■ Use logging best practices to get as much data as you can from your log analysis: ■ Use ISO8601-based timestamps.", "prev_page": {"page_num": 146, "segment_id": "00146"}, "next_page": {"page_num": 148, "segment_id": "00148"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["data", "application", "consumers", "could", "what", "development", "monitoring", "arbitrary", "logging", "chapter"], "content_type": "tutorial", "domain": "devops|architecture", "complexity": "intermediate", "companies": ["Netflix"], "people": [], "products": [], "technologies": ["application performance monitoring", "business intelligence", "server health statistics", "arbitrary stats and semantic logging", "ISO8601-based timestamps", "unique IDs", "standard log categories and frameworks"], "frameworks": ["Netflix Servo", "StatsD"], "methodologies": ["DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["application performance", "consumer usage"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Netflix Servo", "StatsD"], "topics": ["data from production systems", "application performance monitoring", "business intelligence"], "key_concepts": ["DevOps", "metrics collection", "log analysis"], "problem_statement": "Improving the understanding of application usage and performance to inform development decisions", "solution_approach": "Implementing APM, BI tools, and best practices for log analysis", "extraction_method": "lm"}}
{"segment_id": "00148", "page_num": 148, "segment": "Part 3\n\nApplying metrics to your\nteams, processes, and software\n\nUsing the concepts from part 1 and the rich set of data from part 2, you’re\n\nready to take your metrics collection, analysis, and reporting to the next level. In\npart 3 you’ll learn how to combine the data you’ve been collecting into complex\nmetrics, get the full picture on the quality of your software, and report the data\nthroughout your organization.\n\n Chapter 7 shows you how to combine data across several data sources to create metrics that fit into your processes and your team. You’ll learn how to\nexplore your data, determine what to track, and create formulas that output\nyour own custom metrics.\n\n Chapter 8 shows you how to combine your data to determine how good your\nsoftware products really are. You’ll learn how to measure your software from two\nkey perspectives: usability and maintainability.\n\n Chapter 9 shows you how to publish metrics effectively across your organization. You’ll learn how to build effective dashboards and reports that communicate the right level of information to the right people. We’ll also look at some\npitfalls that can cause your reports to fail and how to avoid them.\n\n Chapter 10 breaks down the agile principles and shows you how to measure\n\nyour team against them.\n\n As in part 2, each chapter ends in a case study so you can see the techniques\n\nyou learned in the chapter applied in a real-world scenario.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "6", "chapter_title": "Data from your production systems", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "Chapter 7 shows you how to combine data across several data sources to create metrics that fit into your processes and your team. Chapter 8 shows you how to combine your data to determine how good your\nsoftware products really are. Chapter 9 shows you how to publish metrics effectively across your...", "l2_summary": "Using the concepts from part 1 and the rich set of data from part 2, you’re In\npart 3 you’ll learn how to combine the data you’ve been collecting into complex\nmetrics, get the full picture on the quality of your software, and report the data\nthroughout your organization. Chapter 7 shows you how to combine data across several data sources to create metrics that fit into your processes and your team. Chapter 8 shows you how to combine your data to determine how good your\nsoftware products really are. Chapter 9 shows you how to publish metrics effectively across your organization. Chapter 10 breaks down the agile principles and shows you how to measure", "prev_page": {"page_num": 147, "segment_id": "00147"}, "next_page": {"page_num": 149, "segment_id": "00149"}, "extended_fields": {"has_code": false, "has_formulas": true, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["data", "metrics", "chapter", "part", "software", "learn", "shows", "combine", "processes", "level"], "content_type": "tutorial", "domain": "management|data_science", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["usability", "maintainability"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["metrics collection and analysis", "software quality assessment", "reporting metrics across organization", "agile measurement"], "key_concepts": ["custom metrics", "data sources combination", "usability and maintainability", "effective dashboards and reports"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00149", "page_num": 149, "segment": "126\n\nCHAPTER\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "6", "chapter_title": "Data from your production systems", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "Licensed to Mark Watson <nordickan@gmail.com>", "l2_summary": "Licensed to Mark Watson <nordickan@gmail.com>", "prev_page": {"page_num": 148, "segment_id": "00148"}, "next_page": {"page_num": 150, "segment_id": "00150"}, "flags": {"continuity_gap": true}}
{"segment_id": "00150", "page_num": 150, "segment": "Working with the\ndata you’re collecting:\nthe sum of the parts\n\nThis chapter covers\n\n■\n\nIdentifying when to use custom metrics\n\n■ Figuring out what you need to create a metric\n\n■ Combining data points to create metrics\n\n■ Building key metrics to track how well your\n\nteam is working\n\nMetrics are measurements or properties that help in making decisions. In agile processes metrics can be created from the data your team is generating to help determine where you need to take action to improve.\n\n7.1\n\nCombining data points to create metrics\nTo create a metric you only need two things:\n\n■ Data to generate the metric from\n■ A function to calculate the metric\n\nIn previous chapters we’ve been focusing on the data you can collect from the different systems in your application lifecycle and what you can learn from it alone or\n\n127\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "6", "chapter_title": "Data from your production systems", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "■ Combining data points to create metrics Combining data points to create metrics\nTo create a metric you only need two things: ■ Data to generate the metric from\n■ A function to calculate the metric", "l2_summary": "Working with the\ndata you’re collecting:\nthe sum of the parts Identifying when to use custom metrics ■ Figuring out what you need to create a metric ■ Combining data points to create metrics Combining data points to create metrics\nTo create a metric you only need two things: ■ Data to generate the metric from\n■ A function to calculate the metric", "prev_page": {"page_num": 149, "segment_id": "00149"}, "next_page": {"page_num": 151, "segment_id": "00151"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["data", "metrics", "create", "metric", "need", "working", "what", "combining", "points", "team"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["custom metrics", "key metrics"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Identifying when to use custom metrics", "Combining data points to create metrics", "Building key metrics to track team performance"], "key_concepts": ["metrics", "data collection", "agile processes"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00151", "page_num": 151, "segment": "128\n\nCHAPTER 7 Working with the data you’re collecting: the sum of the parts\n\nApply\n\nCollect\ndata\n\nAnalyze\n\nMake sure you\nunderstand what you have\nand what it can tell you.\n\nUse the data you\nhave to define actionable\nmetrics you can use.\n\nFigure 7.1 Building metrics in the context of collecting, analyzing, and applying metrics\n\ncombined with other data. Now you can start combining data points to create your\nown metrics using the following steps.\n\n■ Explore your data. Ensure that you know what you have.\n■ Break down your data to determine what to track. Using the knowledge you have\nabout your data, pick out the most useful and telling data points to build metrics with.\n\n■ Build your functions around data points. Adding together multiple data points that\nare related to a behavior will give you metrics that tell you a lot with a simple\nmeasurement.\n\nFigure 7.1 shows these steps in the context of the big picture.\n\n In the analysis phase you want to spend enough time understanding what data you\nhave and how it’s connected so that you can define actionable metrics that become\nuseful in your development cycle.\n\n As you’ve seen throughout this book, there’s so much data generated throughout\nthe application lifecycle that no one or even two data points can give you a clear indicator of performance for your team. To get the best picture in previous chapters, we\nlooked at data points from different systems alone or next to one another. An alternative to that technique is to combine these elements into metrics that shed light on\nlarger questions that can’t be answered by a single data point.\n\n An example of a calculated metric that we’ve already looked at is recidivism. Recidivism tells you how frequently your team is moving the wrong way in your workflow.\nIt’s calculated purely with PTS data with the following formula:\n\nRecidivism = bN / (fN + bN)\n\n■ N = number of tasks\n■ f = moving forward in the workflow\n■ b = moving backward in the workflow\n\nIn many cases you can do some interesting calculations on data from a single system to\nget new insight into recidivism. An example using only SCM data is the Comment To\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "CHAPTER 7 Working with the data you’re collecting: the sum of the parts Use the data you\nhave to define actionable\nmetrics you can use. combined with other data.", "l2_summary": "CHAPTER 7 Working with the data you’re collecting: the sum of the parts Use the data you\nhave to define actionable\nmetrics you can use. combined with other data. ■ Break down your data to determine what to track. Using the knowledge you have\nabout your data, pick out the most useful and telling data points to build metrics with. ■ Build your functions around data points.", "prev_page": {"page_num": 150, "segment_id": "00150"}, "next_page": {"page_num": 152, "segment_id": "00152"}, "extended_fields": {"has_code": false, "has_formulas": true, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "metrics", "points", "what", "have", "recidivism", "using", "moving", "workflow", "collecting"], "content_type": "tutorial", "domain": "data_science|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["PTS", "SCM"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["Recidivism"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Data Collection", "Data Analysis", "Metric Building"], "key_concepts": ["Collecting data", "Analyzing data", "Building metrics", "Recidivism metric"], "problem_statement": "How to effectively collect, analyze and build actionable metrics from the data collected during the application lifecycle.", "solution_approach": "Steps for exploring, breaking down, and building functions around data points to create meaningful metrics.", "extraction_method": "lm"}}
{"segment_id": "00152", "page_num": 152, "segment": "Using your data to define “good”\n\n129\n\nCommit Ratio. If a team is using a workflow that includes code reviews, sometimes\ndevelopers, usually the leads, will end up spending most of their time reviewing other\npeople’s code rather than writing any code themselves. We call this PR Paralysis. This\nis usually a bad sign that there isn’t enough senior technical leadership on the team or\npull requests aren’t being distributed across enough of your team. You can calculate\nComment To Commit Ratio with the following formula:\n\nComment To Commit Ratio = r / (m + c)\n\n■ m = merged pull requests\n■ c = commits\n■ r = reviews\n\nIn chapter 8 we’ll look at the elements of good software, and one of the measures\nwe’ll talk about is Mean Time To Repair (MTTR). This is another important metric\nthat can be calculated simply with data from your APM if you just want it at a high\nlevel. The most simplistic view of MTTR would be\n\nMTTR = f -- s\n\n■ s = start date time when an anomaly is identified\n■ f = date time when the anomaly is fixed\n\nLater in this chapter we’ll be looking at estimate health, or how accurate your team’s\nestimations are. The algorithm for estimate health is outlined in listing 7.1. In a few\nwords it compares the amount of time a task took to complete against the estimated\neffort, and gives a rating of 0 when estimates line up, a rating of greater than 0 when\ntasks are taking longer than estimated (underestimating), and a rating of less than 0\nwhen tasks take less time than estimated (overestimating).\n\n A rather complex metric that we’ll dive into in our case study is release health.\nThis is a combination of PTS, SCM, and release data to find out how healthy releases\nare for a team practicing continuous deployment and releasing software multiple\ntimes a day.\n\n7.2\n\nUsing your data to define “good”\nThere are three “goods” that we’ll be looking at for the remainder of the book.\n\n■ Good software---This is covered in the next chapter. Is what you’re building\n\ndoing what it’s supposed to do and is it built well?\n\n■ A good team---Good teams usually build good software. Because different teams\noperate differently and use their data-collection tools differently, the metrics to\nmeasure how good a team is are often relative to that team. Thus, you need to\nhave the next “good” to measure accurately.\n\n■ Good metrics---You must have good indicators for your team and software that\nprovide trustworthy and consistent data. These are used to measure your team\nand your software.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "Using your data to define “good” ■ A good team---Good teams usually build good software. These are used to measure your team\nand your software.", "l2_summary": "Using your data to define “good” In chapter 8 we’ll look at the elements of good software, and one of the measures\nwe’ll talk about is Mean Time To Repair (MTTR). ■ Good software---This is covered in the next chapter. ■ A good team---Good teams usually build good software. ■ Good metrics---You must have good indicators for your team and software that\nprovide trustworthy and consistent data. These are used to measure your team\nand your software.", "prev_page": {"page_num": 151, "segment_id": "00151"}, "next_page": {"page_num": 153, "segment_id": "00153"}, "extended_fields": {"has_code": false, "has_formulas": true, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["good", "team", "data", "time", "software", "than", "when", "using", "commit", "ratio"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["code reviews", "APM"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["Commit Ratio", "Mean Time To Repair (MTTR)", "Estimate Health", "Release Health"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["APM"], "topics": ["Commit Ratio", "Mean Time To Repair (MTTR)", "Estimate Health", "Release Health"], "key_concepts": ["Good software", "Good team", "Good metrics"], "problem_statement": "Improving development practices and measuring team performance", "solution_approach": "Using data-driven metrics to assess code review efficiency, anomaly resolution time, estimation accuracy, and release health", "extraction_method": "lm"}}
{"segment_id": "00153", "page_num": 153, "segment": "130\n\nCHAPTER 7 Working with the data you’re collecting: the sum of the parts\n\nWhile exploring data in earlier chapters you probably started drilling into data that\nseemed either good or bad based on how you perceived your team’s behaviors. While\nquerying for data points that you know represent good behavior, it may be surprising\nto find how other data points relate. These relationships are important to hone in on\nand discuss across the team to understand why the relationships behave the way they\ndo. Through this exploration and discussion you take action on metrics you know are\nimportant and observe related metrics until patterns emerge that you want to affect.\n\n While measuring your teams and processes it’s important to always keep an open\nmind about the results you get back. Don’t try to jam data into an explanation because\nit’s what you want to see. Assessing what you’re seeing openly and honestly is key to\ngetting your team to their highest possible level of efficiency and productivity.\n\n7.2.1\n\nTurning subjectivity into objectivity\n\nIn chapter 3 we looked at tagging the tasks in your PTS system to subjectively indicate\nwhether or not they went well. If you did this, you can now turn that subjective data\ninto objective data.\n\n Agile development is all about the team. You want to be able to create a happy and\nefficient team that has some ownership of the product they’re working on. Asking the\nteam to mark their tasks with how well they think they went is as good an indicator as\nany to tell you how well your team is doing. A happy team will more likely be a consistently high-performing team. Sad teams may perform well for a time until burnout\nand attrition destroy them.\n\n In this example a team repeatedly brought up in retrospectives that they thought\ndifferent roles on the team weren’t working together closely enough and it was impacting delivery. They wanted to see more information sharing and collaboration between\nteams to be able to deliver their products more efficiently and consistently. They\ndecided that they would have more face-to-face conversations and warm hand-off of\ntasks instead of just assigning tickets to someone else when they thought a task was\ncomplete. To track their success they started labeling tasks in their PTS and SCM with\nthe tag “sharing” if they thought that everyone was working together well through the\ndevelopment and delivery cycle. To them sharing meant that people were sharing their\ntime and information and generally working together well. In this case they knew\ndevelopment time and recidivism should be low but they didn’t have a good idea of\nhow much they should expect their team to comment on their pull requests in source\ncontrol; they called this “code comment count.” Labels, code comment count, recidivism---or the percentage of time tasks move backward in the workflow---and average\ndevelopment time for tasks labeled “sharing” are shown in figure 7.2.\n\n In this case recidivism and development time looked great. Looking at the same\ndata for tasks where the team didn’t think information sharing worked well, they produced the graphs shown in figure 7.3.\n\n In figure 7.3 you see that recidivism and average development time went up significantly. The really interesting thing is the comment counts; they also went way up.\nBuilding on this trend, the team checked to see what happened when comment counts\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "CHAPTER 7 Working with the data you’re collecting: the sum of the parts Agile development is all about the team. A happy team will more likely be a consistently high-performing team.", "l2_summary": "CHAPTER 7 Working with the data you’re collecting: the sum of the parts Agile development is all about the team. Asking the\nteam to mark their tasks with how well they think they went is as good an indicator as\nany to tell you how well your team is doing. A happy team will more likely be a consistently high-performing team. Looking at the same\ndata for tasks where the team didn’t think information sharing worked well, they produced the graphs shown in figure 7.3. In figure 7.3 you see that recidivism and average development time went up significantly.", "prev_page": {"page_num": 152, "segment_id": "00152"}, "next_page": {"page_num": 154, "segment_id": "00154"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["team", "data", "tasks", "well", "time", "development", "sharing", "working", "comment", "good"], "content_type": "theory|practice", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["PTS system", "SCM"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["recidivism", "average development time", "code comment count"], "has_case_study": true, "case_study_company": null, "tools_mentioned": ["PTS system", "SCM"], "topics": ["Working with collected data", "Turning subjectivity into objectivity", "Agile methodology implementation"], "key_concepts": ["data exploration", "open-minded assessment", "subjective to objective data conversion", "team ownership and efficiency", "information sharing and collaboration"], "problem_statement": "Improving team performance through better data analysis and subjective-to-objective data conversion", "solution_approach": "Using PTS system for tagging tasks, implementing Agile methodology for team ownership, and tracking metrics like recidivism and code comment count", "extraction_method": "lm"}}
{"segment_id": "00154", "page_num": 154, "segment": "Using your data to define “good”\n\n131\n\nWhen sharing information was\nworking well between teams they\nused the “sharing” label.\n\nOverall code comments\nseemed pretty low.\n\nAverage development time\nand recidivism looked good.\n\nFigure 7.2 Labels, code\ncomment counts, recidivism,\nand development time for\ntasks labeled “sharing” over\nthe course of a single sprint\n\nThese tasks moved\nbackward once.\n\n0 means tasks never\nmoved backward.\n\nAverage development time\nwas measured in days.\n\nThis graph tracks all the tags\nthat aren’t marked for sharing.\n\nCode comments\nare going up too.\n\nRecidivism goes up.\n\nDevelopment\ntime goes up.\n\nFigure 7.3 Labels for tasks\nnot marked with “sharing” over\nthe course of a sprint\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "Average development time\nand recidivism looked good. Development\ntime goes up. Figure 7.3 Labels for tasks\nnot marked with “sharing” over\nthe course of a sprint", "l2_summary": "Average development time\nand recidivism looked good. Figure 7.2 Labels, code\ncomment counts, recidivism,\nand development time for\ntasks labeled “sharing” over\nthe course of a single sprint These tasks moved\nbackward once. Average development time\nwas measured in days. Development\ntime goes up. Figure 7.3 Labels for tasks\nnot marked with “sharing” over\nthe course of a sprint", "prev_page": {"page_num": 153, "segment_id": "00153"}, "next_page": {"page_num": 155, "segment_id": "00155"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["sharing", "development", "time", "tasks", "code", "recidivism", "good", "comments", "average", "figure"], "content_type": "practice", "domain": "programming|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "Scrum"], "programming_languages": [], "has_best_practices": false, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["code comment counts", "recidivism", "development time"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["team collaboration", "task labeling", "sprint performance"], "key_concepts": ["sharing label", "code comments", "recidivism", "development time"], "problem_statement": "Improving team collaboration and task management during sprints", "solution_approach": "Labeling tasks as 'sharing' to enhance information flow between teams, tracking metrics such as code comments, recidivism, and development time", "extraction_method": "lm"}}
{"segment_id": "00155", "page_num": 155, "segment": "132\n\nCHAPTER 7 Working with the data you’re collecting: the sum of the parts\n\nThese tasks were estimated\nto take very little time.\n\nRecidivism goes up.\n\nDevelopment time skyrockets\nwhen comments go up.\n\nFigure 7.4 The relationship\namong recidivism,\ndevelopment time, code\ncomments, and estimates\n\nwere high in general. That is reflected in figure 7.4. Note that estimates were added\ninto this dashboard as well to show the correlation between how much work the team\nthinks something will take and the amount of time it actually takes.\n\n If the estimates were high, then maybe it would make sense to see the huge jump\nin development time, but in this case the team noticed the opposite: estimates were\nvery low.\n\n For this team, having a lot of code comments seemed to point to poor information\nsharing, tasks taking a lot longer than estimated, and an increase in recidivism. In this\ncase the team was able to turn something that was very subjective---whether they\nthought that sharing was working well---into an objective data relationship they could\nuse as an indicator to see if tasks were going off the rails.\n\n7.2.2 Working backward from good releases\n\nYou want a happy team and you want a team that can deliver. Another way to find out\nwhat good thresholds are for your team is to focus on what went into good software\nreleases to figure out how to tune your process to replicate that success. In this case\nyou can filter your data by anything that went into a release that worked well to find\nthe key data points. As we did with tags in the previous examples, you can then look at\nthe resulting watermarks to see what behavior went into a particular release.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "These tasks were estimated\nto take very little time. Figure 7.4 The relationship\namong recidivism,\ndevelopment time, code\ncomments, and estimates You want a happy team and you want a team that can deliver.", "l2_summary": "CHAPTER 7 Working with the data you’re collecting: the sum of the parts These tasks were estimated\nto take very little time. Development time skyrockets\nwhen comments go up. Figure 7.4 The relationship\namong recidivism,\ndevelopment time, code\ncomments, and estimates 7.2.2 Working backward from good releases You want a happy team and you want a team that can deliver.", "prev_page": {"page_num": 154, "segment_id": "00154"}, "next_page": {"page_num": 156, "segment_id": "00156"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["team", "time", "data", "estimates", "working", "tasks", "very", "recidivism", "development", "comments"], "content_type": "practice", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["recidivism", "development time", "code comments", "estimates"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["team performance metrics", "estimation accuracy", "code quality indicators", "release success factors"], "key_concepts": ["recidivism rate", "development time correlation", "code comments as a proxy for information sharing", "estimates vs actuals"], "problem_statement": "Improving team performance and estimation accuracy", "solution_approach": "Analyzing data relationships to identify key indicators of team success", "extraction_method": "lm"}}
{"segment_id": "00156", "page_num": 156, "segment": "Using your data to define “good”\n\n133\n\nA lot was done in a short time\nto get this out the door.\n\nIt took over a month to fix all\nthe issues with this release; the\ncompletion graph has a long tail.\n\nDevelopment time\nlooks great; on average tasks\nonly took 1 day to complete.\n\nMost tasks moved backward\n0 times or 1 time, which isn't bad.\n\nFigure 7.5 Stats around a painful release. Even though development time looked great, there were nagging bugs\nthat caused hot fixes over a long period.\n\nIn the next example we’ll look at the tale of two releases: a pretty good release and a\npainful release. First, we’ll look at the painful release.\n\n The painful release looked great during the development cycle. The team was\nfocusing on development time and trying to keep it to a minimum. They had a huge\npush to get the release out the door, and when they did the bugs started rolling in.\nNagging issues caused a long support tail of nearly two months of fixing problems.\nThe resulting charts are shown in figure 7.5.\n\n Here are a few notable characteristics of figure 7.5:\n\n■ Average development time of one day looks awesome. If we were to look at only\nthat, it would seem that this team was really good at breaking down their tasks.\n■ Even though the team managed to complete over 100 tasks in a three-week\nperiod to hit the release, there were nagging issues that made up the long tail of\nsupport after the release, which kept the team from completely focusing on the\nnext release.\n\n■ Tasks were moving backward in the workflow around 70% of the time; those are\nall the tasks with recidivism of greater than zero. 20% of tasks got moved backward more than once.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "Development time\nlooks great; on average tasks\nonly took 1 day to complete. Figure 7.5 Stats around a painful release. First, we’ll look at the painful release.", "l2_summary": "Development time\nlooks great; on average tasks\nonly took 1 day to complete. Most tasks moved backward\n0 times or 1 time, which isn't bad. Figure 7.5 Stats around a painful release. In the next example we’ll look at the tale of two releases: a pretty good release and a\npainful release. First, we’ll look at the painful release. The painful release looked great during the development cycle.", "prev_page": {"page_num": 155, "segment_id": "00155"}, "next_page": {"page_num": 157, "segment_id": "00157"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["release", "time", "tasks", "development", "long", "painful", "team", "good", "issues", "tail"], "content_type": "case_study", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": false, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["average development time", "tasks moved backward"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["release management", "task completion", "bug fixing"], "key_concepts": ["development time", "task recidivism", "support tail"], "problem_statement": "Managing a release with short development time but long-term bug issues", "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00157", "page_num": 157, "segment": "134\n\nCHAPTER 7 Working with the data you’re collecting: the sum of the parts\n\nGiven these observations, maybe an average development time of one day isn’t good\nafter all. None of the related metrics around it seem very good, so perhaps this team\nshould start focusing on different metrics to make sure their project is healthy.\n\n In this example, average development time was not enough to help the team get a\ngood release out the door. They paid for shorter development times later with lots of\nbug fixes and additional releases.\n\n As a comparison, in a completely different and much better release the team’s\naverage development time was four full days, much higher than the first example. But\nrecidivism was down and the support tail for the release was much shorter and deliberately phased, as shown in figure 7.6.\n\n In figure 7.6, notice:\n\n■ Tasks were not moving backward in the workflow as frequently. Here they never\nmoved backward 60% of the time, and less than 10% of tasks moved backward\nmore than once.\n\n■ Compared to figure 7.5 the releases in this chart were smaller, between 15 and\n25 tasks instead of the 60 in figure 7.5. Along with the smaller releases came less\nsupport and more consistency.\n\n■ Average development time of tasks went up a lot, from one to four days.\n\nThe spikes indicate releases and show the\nteam is able to consistently get things out the\ndoor with only a week of downtime between.\n\nDevelopment time and recidivism\nlook reasonable but could be better.\n\nFigure 7.6 A very different release with\na better support tail, lower recidivism,\nand longer development time\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "In this example, average development time was not enough to help the team get a\ngood release out the door. In figure 7.6, notice: Figure 7.6 A very different release with\na better support tail, lower recidivism,\nand longer development time", "l2_summary": "Given these observations, maybe an average development time of one day isn’t good\nafter all. In this example, average development time was not enough to help the team get a\ngood release out the door. In figure 7.6, notice: ■ Compared to figure 7.5 the releases in this chart were smaller, between 15 and\n25 tasks instead of the 60 in figure 7.5. ■ Average development time of tasks went up a lot, from one to four days. Figure 7.6 A very different release with\na better support tail, lower recidivism,\nand longer development time", "prev_page": {"page_num": 156, "segment_id": "00156"}, "next_page": {"page_num": 158, "segment_id": "00158"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["development", "time", "figure", "average", "team", "release", "releases", "tasks", "good", "different"], "content_type": "case_study", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "Scrum"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["average development time", "recidivism", "support tail"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["release management", "development time", "recidivism rate"], "key_concepts": ["average development time", "recidivism", "support tail"], "problem_statement": "The team's focus on shortening the average development time led to increased bug fixes and a longer support tail, while focusing on longer development times resulted in better release outcomes.", "solution_approach": "The case study suggests that teams should consider different metrics beyond just average development time to ensure project health and reduce recidivism.", "extraction_method": "lm"}}
{"segment_id": "00158", "page_num": 158, "segment": "How to create metrics\n\n135\n\nIf the team is looking for consistency in their release cycle, certainly the pattern in figure 7.6 is the pattern they want to replicate. If that’s the case, then a longer development time isn’t a bad thing and perhaps the team should consider extremely low\naverage development times a red flag.\n\n7.3\n\nHow to create metrics\n\n“A model is a formal representation of a theory.”\n\n ---Kenneth A. Bollen\n\nThe end goal is to get our team members to perform as well as they can. To get there\nwe want to be able to track what matters most in the context of our team and combine\nthat data into a form that’s easy to track and communicate. In chapter 1 I talked about\nhow to find what matters most for your team by asking questions and mind mapping.\nNow that we’ve gone through all the data that you can use, we’re going to start putting\nit together to answer questions that lead to better team performance.\n\n Before we start creating metrics, we should lay ground rules for what makes a good\n\nmetric. The following guidelines are a good place to start:\n\n■ Metrics should create actionable insight:\n\n■ Do track metrics that you can respond to should they identify a problem.\n■ Don’t track metrics you can’t do anything about or haven’t taken the time to\n\nunderstand the meaning of.\n\n■ Metrics should align with core business and team tenets:\n\n■ Do pick data to track what’s relevant to your end goal. Perhaps security is\nyour number-one goal; for others, feature delivery is more important. Teams\nshould prioritize the most important indicators of their process as it relates\nto what they’re delivering.\n\n■ Don’t track metrics that you can’t somehow track back to something your\n\nteam thinks is important.\n\n■ Metrics should be able to stand alone:\n\n■ Do create metrics that will give you a clear indication of the health of some\n\npart of your process or team by itself.\n\n■ Don’t create a metric that forces you to look at more data to determine if it’s\n\ngood or bad.\n\nWith these points in mind let’s look at how to figure out how well your team is estimating work. You’ll start by breaking the problem down into pieces and identifying all the\ndata points that you can use to track the pieces. You’ll do that by exploring your data\nto make sure you understand what you have and defining metrics that help you see it\nclearly. Once you have good actionable metrics, you can incorporate them into your\ndevelopment cycle.\n\n We’ve already looked at what you can do with your data when you save it in a central place, index it, and create dashboards out of it. As you spend quality time with the\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "How to create metrics How to create metrics ■ Metrics should create actionable insight:", "l2_summary": "How to create metrics How to create metrics ■ Metrics should create actionable insight: ■ Do track metrics that you can respond to should they identify a problem. ■ Metrics should align with core business and team tenets: ■ Don’t track metrics that you can’t somehow track back to something your", "prev_page": {"page_num": 157, "segment_id": "00157"}, "next_page": {"page_num": 159, "segment_id": "00159"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["metrics", "team", "track", "should", "what", "data", "create", "start", "good", "development"], "content_type": "tutorial", "domain": "management|devops", "complexity": "intermediate", "companies": [], "people": ["Kenneth A. Bollen"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": true, "has_metrics": true, "metrics": ["average development times", "estimating work"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["creating metrics", "team performance", "data tracking"], "key_concepts": ["actionable insight", "core business and team tenets", "standalone metrics"], "problem_statement": "how to create effective metrics for a development team", "solution_approach": "defining guidelines for creating actionable, relevant, and standalone metrics", "extraction_method": "lm"}}
{"segment_id": "00159", "page_num": 159, "segment": "136\n\nCHAPTER 7 Working with the data you’re collecting: the sum of the parts\n\ndata your team is generating, you’ll start to think up questions, and more data will\nlead to more questions. Eventually you’ll get to a point where you start to see pieces of\nthe larger puzzle and how they affect each other.\n\n An aspect worth measuring is how well your team decomposes tasks. You can get\nmore done for your customers if you’re frequently delivering lots of small tasks\ninstead of delivering large swaths of functionality at a time. Smaller changes are also\neasier to deliver with tests, to test in general, and to troubleshoot. To measure this let’s\nstart with the question “Is your team breaking their tasks into small enough chunks?”\nIf they are, then you’d expect to see:\n\n■ Distribution of estimates leaning toward smaller numbers and a fairly low average\nestimate---If your tasks are defined well with the intention of delivering small pieces\nof functionality, this should also be reflected in the team’s estimates of the work.\n■ Decreased lead time---Delivery of individual tasks measured by lead time should\n\nbe short.\n\nThat will lead you to:\n\n■ How long do these tasks really take?\n■ Are your estimates accurate?\n\n7.3.1\n\nStep 1: explore your data\n\nTo figure out if your estimates are accurate, you can start by tracking the size of estimates with estimated distribution and average estimate by the amount of time it takes\nto get a task all the way through the development process with lead time. You’d expect\nthat tasks that are well defined and broken down into small pieces will be well understood by the development team and as a result should be able to make it through the\ndevelopment process expediently, thus resulting in fast lead times. Using the tools\nwe’ve been using so far produced the data shown in figure 7.7.\n\nA mean of 3 shows the\nteam is breaking tasks\ninto small chunks.\n\nMost estimates are 3 or 5; apparently\nthe team is breaking tasks down well.\n\nFigure 7.7 The historical estimate distribution and average estimate dashboards for a team\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "That will lead you to: A mean of 3 shows the\nteam is breaking tasks\ninto small chunks. Most estimates are 3 or 5; apparently\nthe team is breaking tasks down well.", "l2_summary": "data your team is generating, you’ll start to think up questions, and more data will\nlead to more questions. An aspect worth measuring is how well your team decomposes tasks. That will lead you to: ■ Are your estimates accurate? A mean of 3 shows the\nteam is breaking tasks\ninto small chunks. Most estimates are 3 or 5; apparently\nthe team is breaking tasks down well.", "prev_page": {"page_num": 158, "segment_id": "00158"}, "next_page": {"page_num": 160, "segment_id": "00160"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["tasks", "team", "lead", "estimates", "data", "well", "small", "time", "start", "estimate"], "content_type": "theory", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["Distribution of estimates", "Lead time"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Task decomposition", "Estimate accuracy", "Lead time"], "key_concepts": ["Decomposing tasks into small chunks", "Accurate estimates", "Short lead times"], "problem_statement": "Improving team efficiency and estimate accuracy through task decomposition", "solution_approach": "Measuring the distribution of estimates, lead time, and using these metrics to improve task management", "extraction_method": "lm"}}
{"segment_id": "00160", "page_num": 160, "segment": "How to create metrics\n\n137\n\nThis team is using the Fibonacci series of estimation with a minimum estimate of 1\nand a maximum estimate of 13 in a two-week, or 10-working-day, sprint. The potential\nvalues in the Fibonacci series are 1, 2, 3, 5, 8, and 13, so an average estimate of 3 is\nfairly low in the spectrum, which is good. Lower estimates show that tasks have been\nbroken down to a workable level.\n\n Overall estimation data looks pretty good; the team seems to be breaking their\ntasks into small, manageable chunks. With that in mind, the next questions that arise\nare these:\n\n■ How long do these tasks really take?\n■ Are our estimations accurate?\n\nEstimating with effort vs. time\n\nStory points are often used to estimate tasks in project tracking systems, but they\ndon’t always have a 1:1 translation to time. For example, 16 story points could mean\n9 days of development on one team and it could mean 14 days on another team. No\nmatter how you estimate, you’ll have to translate them to time to figure out if they’re\naccurate.\n\nIn this case, based on the estimation system used by the team, tasks estimated at 3\npoints should take around 2--3 days to complete. The next thing we can pull in is the\naverage amount of time tasks actually take. That is shown in figure 7.8.\n\n According to our data our average estimate is 3, but on average tasks take 5 days to\ncomplete. In this case a sprint is two weeks or 10 working days and the maximum estimate is 13. That would mean that our average estimate should be closer to 8 than 3.\nThe next question that comes to mind is\n\n■ Why are tasks taking longer than we think they should?\n\nThat doesn’t add up; it seems as if tasks\nare taking much longer than estimated.\n\nFigure 7.8 Adding average time a task takes to get to done. This doesn’t look right; an\naverage estimate of 3 should be 2--3 days.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "According to our data our average estimate is 3, but on average tasks take 5 days to\ncomplete. That would mean that our average estimate should be closer to 8 than 3. This doesn’t look right; an\naverage estimate of 3 should be 2--3 days.", "l2_summary": "This team is using the Fibonacci series of estimation with a minimum estimate of 1\nand a maximum estimate of 13 in a two-week, or 10-working-day, sprint. ■ How long do these tasks really take? According to our data our average estimate is 3, but on average tasks take 5 days to\ncomplete. In this case a sprint is two weeks or 10 working days and the maximum estimate is 13. That would mean that our average estimate should be closer to 8 than 3. This doesn’t look right; an\naverage estimate of 3 should be 2--3 days.", "prev_page": {"page_num": 159, "segment_id": "00159"}, "next_page": {"page_num": 161, "segment_id": "00161"}, "extended_fields": {"has_code": false, "has_formulas": true, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["estimate", "tasks", "average", "days", "team", "time", "take", "should", "estimation", "have"], "content_type": "tutorial", "domain": "programming|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Fibonacci series estimation", "story points"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": true, "has_metrics": true, "metrics": ["average estimate", "actual task completion time"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["estimation techniques", "task management", "metric analysis"], "key_concepts": ["Fibonacci series estimation", "story points", "task breakdown", "time vs. effort estimation"], "problem_statement": "Tasks are taking longer than estimated, leading to discrepancies between average estimates and actual task completion time.", "solution_approach": "Reviewing the estimation process and analyzing the actual time taken for tasks.", "extraction_method": "lm"}}
{"segment_id": "00161", "page_num": 161, "segment": "138\n\nCHAPTER 7 Working with the data you’re collecting: the sum of the parts\n\nBecause this question is a bit more open-ended we can start by querying for the data\nthat we think doesn’t look right to see what properties it has. To do that we can search\nfor all data that has an estimate of 3 and a development time greater than 3 and data\nthat has an estimate of 5 and a development time greater than 5. The query in\nEC/Lucene looks like this:\n\n((devTime:[3 TO *] AND storyPoints:3) OR (devTime:[5 TO *] AND storyPoints:5))\n\nAs mentioned earlier, if you’re tagging or labeling your tasks with as much data as possible, then at this point you can see what tags come back from your search. In this\ncase, a few things jump out at us.\n\n Figure 7.9 shows us that coreteamzero and coreteam1 seem to have this happen\nmore than other teams and that estimates tend to be low when tasks move backward\nin the workflow.\n\n This data will probably lead to even more questions, but by now you should get the\nidea. Rarely do you start off with all the questions that lead you to where you want to\ngo, but spending time exploring your data to see how things are related will help you\ndetermine what to track.\n\nThis seems to happen more frequently\non coreteamzero and coreteam1.\n\nTasks move backward\nin the workflow when\ncompletion estimates are\n3 or 5; maybe those are the\ndeveloper’s equivalent\nof “I don’t know.”\n\nFigure 7.9 Labels show what has been tagged in the cards where estimations are off, and recidivism\nshows that tasks tend to move backward in the workflow more frequently when estimates are 3 or 5.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "CHAPTER 7 Working with the data you’re collecting: the sum of the parts To do that we can search\nfor all data that has an estimate of 3 and a development time greater than 3 and data\nthat has an estimate of 5 and a development time greater than 5. ((devTime:[3 TO *] AND storyPoints:3) OR...", "l2_summary": "CHAPTER 7 Working with the data you’re collecting: the sum of the parts Because this question is a bit more open-ended we can start by querying for the data\nthat we think doesn’t look right to see what properties it has. To do that we can search\nfor all data that has an estimate of 3 and a development time greater than 3 and data\nthat has an estimate of 5 and a development time greater than 5. ((devTime:[3 TO *] AND storyPoints:3) OR (devTime:[5 TO *] AND storyPoints:5)) Figure 7.9 shows us that coreteamzero and coreteam1 seem to have this happen\nmore than other teams and that estimates tend to be low when tasks move backward\nin the workflow. This seems to happen more frequently\non coreteamzero and coreteam1.", "prev_page": {"page_num": 160, "segment_id": "00160"}, "next_page": {"page_num": 162, "segment_id": "00162"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "more", "what", "tasks", "time", "than", "estimates", "when", "move", "backward"], "content_type": "theory", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "Scrum"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["estimations", "development time"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Data Analysis in DevOps", "Workflow Management", "Estimation Accuracy"], "key_concepts": ["Data Exploration", "Tagging and Labeling Tasks", "Recidivism in Task Estimations"], "problem_statement": "Identifying patterns in task estimations and workflow to improve project management.", "solution_approach": "Analyzing data through queries and identifying trends.", "extraction_method": "lm"}}
{"segment_id": "00162", "page_num": 162, "segment": "7.3.2\n\nStep 2: break it down---determine what to track\n\nHow to create metrics\n\n139\n\nHaving a big data mine is great, but sometimes it’s tough to figure out what data\npoints to watch. After exploring your data (a habit that can be very addictive), you\nshould start to get an idea of what data points speak most to how your team works. If\nyou have a small team, whose members sit next to each other, having a high number\nof comments on PTS tickets probably indicates a communication problem, but if you\nhave teams in multiple geographic locations, it could indicate great cross-team communication. If your team doesn’t comment on tickets because they sit next to each\nother and discuss issues, then that data point may never move and therefore will be of\nno use to you. If yours is a globally distributed team that uses comments to push things\nforward, then that data point will be extremely important to you.\n\n A productive way to get to the bottom of what you want to track is to build a mind\n\nmap that helps you identify the questions you’re trying to answer. Follow these steps:\n\n1 Ask the question.\n2 Think of all the dimensions of the answer.\n3 Note where you can obtain data to get your answer.\n\nUsing the scenario in section 7.2.1, the team would have created the mind map shown\nin figure 7.10. Another example of breaking down a big question into smaller, measureable chunks is shown in figures 7.11 and 7.12, which illustrate how to check if\nyour team is estimating accurately.\n\nThe original question\n\nIs teamwork good?\n\nBreaking the question down\ninto measurable pieces\n\nMetrics we can use to\nget the full picture\n\nAre we getting things done?\n\nIs communication good?\n\n--\n\n--\n\nDev time\n\nRecidivism\n\nCode comments\n\nEstimates\n\nFigure 7.10 Breaking down how to measure teamwork with a mind map\n\nStart with your question: what\nare you trying to measure?\n\nHow do we check estimates?\n\nBreak your question down\ninto how you can figure this out.\n\nMake sure they have an expected distribution.\n\nFigure 7.11 Breaking down a problem with a mind map; starting with your question and then breaking\nit down one level\n\nCheck for correlations between real time and estimates.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "Breaking the question down\ninto measurable pieces Figure 7.10 Breaking down how to measure teamwork with a mind map Break your question down\ninto how you can figure this out.", "l2_summary": "Step 2: break it down---determine what to track The original question Breaking the question down\ninto measurable pieces Figure 7.10 Breaking down how to measure teamwork with a mind map Break your question down\ninto how you can figure this out. Figure 7.11 Breaking down a problem with a mind map; starting with your question and then breaking\nit down one level", "prev_page": {"page_num": 161, "segment_id": "00161"}, "next_page": {"page_num": 163, "segment_id": "00163"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["down", "data", "team", "question", "what", "figure", "breaking", "have", "mind", "comments"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["comments on PTS tickets", "communication problem", "cross-team communication", "globally distributed team", "code comments", "estimates"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["tracking metrics in devops", "creating metrics for teams", "mind mapping for tracking"], "key_concepts": ["metrics creation", "teamwork measurement", "data analysis"], "problem_statement": "Determining what data points to track within a team", "solution_approach": "Using mind maps to break down questions into measurable chunks and identifying relevant metrics", "extraction_method": "lm"}}
{"segment_id": "00163", "page_num": 163, "segment": "140\n\nCHAPTER 7 Working with the data you’re collecting: the sum of the parts\n\nMake sure they have an expected distribution.\n\n--\n\nEstimates from PTS\n\nNow you can figure out where\nyou can get your data from.\n\nCheck for correlations between real time and estimates.\n\n--\n\nTask start time --\n\nPTS\n\nTask end time\n\n--\n\nPTS\n\nEstimate of task\n\n--\n\nPTS\n\nAmount of work that went into the task --\n\nPTS\n\nFigure 7.12 Using the mind map to break it down another level and then figuring out where to get data from\n\nOnce you have all the data points that you can use to answer the questions, you’re\nready to start using those data points to build out your metric by adding them\ntogether.\n\n7.3.3\n\nStep 3: create formulas around multiple data points\nto create metrics\n\nOnce you have the relevant data points, you can always stop there and track the individual data. This might work for some people, but it’s useful to have a single data\npoint that allows you to keep a bird’s-eye view on groups of data without having to\nmicromanage every data point. This will also lead to much cleaner, more succinct\ndashboards that can be shared outside the team and make more sense at a glance.\n\n Now it’s time to do a bit of math. If we stick with the examples shown in figures\n7.11 and 7.12, we have several data points that we can roll up to give us an indication\non how good our team’s estimates are. We want to know at a glance if the team is overor underestimating and by how much. If we visualize the association of estimates to\ntime, we’ll see something like figure 7.13.\n\nWork days in the week\nover a two-week period\n\nT\n\nW\n\nT\n\nF\n\nM\n\nT\n\nW\n\nT\n\nF\n\n3\n\n5\n\n8\n\n13\n\nThe estimation scale\nof the Fibonacci series\n\nFigure 7.13 A visual representation of a series of estimates next to time using the\nFibonacci series of estimates over a two-week period\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 4, "cols": 9, "data": [["Work days in the week\nover a two-week period", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", ""], ["T", "W", "T", "F", "M", "T", "W", "T", "F"], ["3 5 8 13\nThe estimation scale\nof the Fibonacci series", "", "", "", "", "", "", "", ""]], "markdown": "| Work days in the week\nover a two-week period |  |  |  |  |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|\n|  |  |  |  |  |  |  |  |  |\n| T | W | T | F | M | T | W | T | F |\n| 3 5 8 13\nThe estimation scale\nof the Fibonacci series |  |  |  |  |  |  |  |  |"}], "table_count": 1, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "CHAPTER 7 Working with the data you’re collecting: the sum of the parts Now you can figure out where\nyou can get your data from. Once you have the relevant data points, you can always stop there and track the individual data.", "l2_summary": "CHAPTER 7 Working with the data you’re collecting: the sum of the parts Now you can figure out where\nyou can get your data from. Check for correlations between real time and estimates. Figure 7.12 Using the mind map to break it down another level and then figuring out where to get data from Step 3: create formulas around multiple data points\nto create metrics Once you have the relevant data points, you can always stop there and track the individual data.", "prev_page": {"page_num": 162, "segment_id": "00162"}, "next_page": {"page_num": 164, "segment_id": "00164"}, "extended_fields": {"has_code": false, "has_formulas": true, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "estimates", "time", "have", "points", "figure", "task", "work", "using", "team"], "content_type": "tutorial", "domain": "data_science|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["estimation accuracy", "task start time", "task end time", "amount of work"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["data collection and analysis", "metric creation", "formula usage"], "key_concepts": ["expected distribution", "correlations", "estimation accuracy", "data points aggregation"], "problem_statement": "Improving the accuracy of task estimates and tracking work progress", "solution_approach": "Collecting data, analyzing correlations, creating metrics through formulas", "extraction_method": "lm"}}
{"segment_id": "00164", "page_num": 164, "segment": "How to create metrics\n\n141\n\nFirst, we’ll name our metric to track estimation accuracy. Let’s call this one estimate\nhealth. Estimate health is some combination of the following data points:\n\n■ Estimates\n■ Task start time\n■ Task end time\n■ Amount of work\n\nBreaking these data points down further, we really care about the amount of time a\ntask took, so we can start by combining start time and end time into elapsed time by\nsubtracting one from the other:\n\nElapsed time (tactual) = task end time - task start time\n\nBecause estimates don’t equal days, the second thing we’ll have to do is correlate time\nto estimates.\n\n Two weeks equals 10 working days, but 1 day in a sprint is used for retrospectives\nand planning, which gives us a total of 9 working days. To deduce time from estimates,\nfirst we’ll take the highest possible estimate and equate it to the highest possible\namount of time, in this case the highest possible estimate of 13 for this team equals 9\nworking days. Using that as our maximum, we can break down the rest of the possible\nestimations. To figure out the exact correlation, use the following formula:\n\nmax(estimateactual) = (estimateworkdays/max(estimateworkdays))*max(tactual)\n\nOnce you have the maximum amount of time, you can figure out the rest of the estimate sweet spots with the following formula:\n\ncorrelation-value = max(tactual)*(max(estimateactual)/estimateworkdays)\n\nWhen you know the correlation between your estimates and time, you can plug in the\ndata you have to find out if your estimates are high or low.\n\n Some example estimation-time mappings using these formulas are shown in table\n7.1. If you’re using the system outlined in appendix A, which uses EC and Lucene, I’ve\nadded the queries you can use to hone in on specific data. These will become more\nuseful when you see them in action in listing 7.1.\n\n Note that in table 7.1 I’ve put the Fibonacci series of estimates along with the series\nof power of 2 estimates, another common estimation series. In the power of 2 series,\neach possible estimate is double the previous estimate.\n\nTable 7.1 Mapping estimates to time ranges and validating with Lucene\n\nEstimate\n\nExact time\n\nTime range\n\nQuery\n\nPower of 2 estimations with two-week sprints\n\n16 points\n\n9 days\n\n7--9 days\n\ndevTime:[7 TO 9] AND storyPoints:16\n\n8 points\n\n4.5 days\n\n4--7 days\n\ndevTime:[4 TO 7] AND storyPoints:8\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 2, "cols": 4, "data": [["Estimate", "Exact time", "Time range", "Query"], ["Power of 2 estimations with two-week sprints", "", "", ""]], "markdown": "| Estimate | Exact time | Time range | Query |\n|---|---|---|---|\n| Power of 2 estimations with two-week sprints |  |  |  |"}], "table_count": 1, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "■ Estimates\n■ Task start time\n■ Task end time\n■ Amount of work Elapsed time (tactual) = task end time - task start time To deduce time from estimates,\nfirst we’ll take the highest possible estimate and equate it to the highest possible\namount of time, in this case the highest possible estimate of...", "l2_summary": "■ Estimates\n■ Task start time\n■ Task end time\n■ Amount of work Breaking these data points down further, we really care about the amount of time a\ntask took, so we can start by combining start time and end time into elapsed time by\nsubtracting one from the other: Elapsed time (tactual) = task end time - task start time Because estimates don’t equal days, the second thing we’ll have to do is correlate time\nto estimates. To deduce time from estimates,\nfirst we’ll take the highest possible estimate and equate it to the highest possible\namount of time, in this case the highest possible estimate of 13 for this team equals 9\nworking days. Table 7.1 Mapping estimates to time ranges and validating with Lucene", "prev_page": {"page_num": 163, "segment_id": "00163"}, "next_page": {"page_num": 165, "segment_id": "00165"}, "extended_fields": {"has_code": false, "has_formulas": true, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["time", "estimates", "estimate", "days", "task", "possible", "data", "points", "start", "amount"], "content_type": "tutorial", "domain": "programming|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["EC", "Lucene"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["estimate health", "elapsed time (tactual)", "correlation-value"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["EC", "Lucene"], "topics": ["metric creation", "estimation accuracy", "time estimation"], "key_concepts": ["estimate health", "elapsed time (tactual)", "correlation between estimates and time"], "problem_statement": "How to create metrics for tracking estimation accuracy", "solution_approach": "Formulas and steps for calculating elapsed time, correlation between estimates and time", "extraction_method": "lm"}}
{"segment_id": "00165", "page_num": 165, "segment": "142\n\nCHAPTER 7 Working with the data you’re collecting: the sum of the parts\n\nTable 7.1 Mapping estimates to time ranges and validating with Lucene (continued)\n\nEstimate\n\nExact time\n\nTime range\n\nQuery\n\nPower of 2 estimations with two-week sprints\n\n4 points\n\n2.25 days\n\n2--4 days\n\ndevTime:[2 TO 4] AND storyPoints:4\n\n2 points\n\n1.125 days\n\n1--2 days\n\ndevTime:[1 TO 2] AND storyPoints:2\n\n1 point\n\n .56 days\n\nless than 1 day devTime:[0 TO 1] AND storyPoints:1\n\nFibonacci series estimations with two-week sprints\n\n13 points\n\n9\n\n8 points\n\n5 points\n\n3 points\n\n2 points\n\n1 points\n\n5.53\n\n3.46\n\n2.07\n\n1.38\n\n.69\n\n6--9 days\n\n4--6 days\n\n3--4 days\n\n2--3 days\n\n1--2 days\n\nless than a day\n\nThis is helpful, but it would be better to have a scale that told you if your estimations\nare high or low and how off they are. For example, a 0 means that estimations meet\nthe time equivalents, a number greater than 0 means your team is underestimating,\nand a value less than 0 means your team is overestimating. Using a scale like this has\nbenefits:\n\n■\n\n■\n\n■\n\n■\n\nIt’s actionable.\nIf you see your estimate health go under 0, you can start adding more time to\nyour estimates to get them back to a level of accuracy.\nIf you see your estimate health go over 0, you can start cutting your estimates a\nbit to get back to a better level of accuracy.\nIt’s easy to communicate.\n\n■ You can easily share this outside your team without a long explanation.\n\n■\n\nIt allows you to digest several data points at a glance.\n\n■ You can put this on your dashboards to keep everyone aware of the latest data.\n\nTo create a rating number you’ll need to write an algorithm that figures out the timeestimate ratio of the estimate in question and checks it against the other estimates in\nyour estimation scale to see if the amount of time a task took matches the corresponding time window for the estimate.\n\n To map estimates to time you’ll need a function that can find the time bounds\nyou’re estimating within, the time scale, and the estimate scale. Figure 7.14, shows\nhow this can work as a function.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 2, "cols": 4, "data": [["Estimate", "Exact time", "Time range", "Query"], ["Power of 2 estimations with two-week sprints", "", "", ""]], "markdown": "| Estimate | Exact time | Time range | Query |\n|---|---|---|---|\n| Power of 2 estimations with two-week sprints |  |  |  |"}], "table_count": 1, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "devTime:[2 TO 4] AND storyPoints:4 devTime:[1 TO 2] AND storyPoints:2 less than 1 day devTime:[0 TO 1] AND storyPoints:1", "l2_summary": "Table 7.1 Mapping estimates to time ranges and validating with Lucene (continued) devTime:[2 TO 4] AND storyPoints:4 devTime:[1 TO 2] AND storyPoints:2 less than 1 day devTime:[0 TO 1] AND storyPoints:1 If you see your estimate health go under 0, you can start adding more time to\nyour estimates to get them back to a level of accuracy. To map estimates to time you’ll need a function that can find the time bounds\nyou’re estimating within, the time scale, and the estimate scale.", "prev_page": {"page_num": 164, "segment_id": "00164"}, "next_page": {"page_num": 166, "segment_id": "00166"}, "extended_fields": {"has_code": false, "has_formulas": true, "has_diagram": false, "has_list": true, "has_citations": false, "key_terms": ["time", "days", "points", "estimate", "estimates", "scale", "estimations", "than", "data", "devtime"], "content_type": "tutorial", "domain": "programming|data_science", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Lucene"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["estimate health"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Lucene"], "topics": ["estimation techniques", "time range mapping", "metric calculation"], "key_concepts": ["power of 2 estimations", "Fibonacci series estimations", "estimate health scale"], "problem_statement": "Improving the accuracy and reliability of estimation in software development", "solution_approach": "Mapping estimates to time ranges, validating with Lucene queries, and calculating estimate health", "extraction_method": "lm"}}
{"segment_id": "00166", "page_num": 166, "segment": "How to create metrics\n\n143\n\n2 Return the associated\n\ntime period for the estimate\n\nT\n\nW\n\nT\n\nF\n\nM\n\nT\n\nW\n\nT\n\nF\n\n3\n\n5\n\n8\n\n13\n\n1 Get the time bounds\nof the estimate\n\n3 Return difference of actual time\nwith estimate association\n\nFigure 7.14 A visual representation of the algorithm to determine estimate health\n\nAn algorithm to do this is outlined in the following listing.\n\nListing 7.1 Algorithm for checking estimate health\n\nstatic def estimateHealth(estimate, actualTime, maxEstimate, maxTime,\n\nestimationValues) {\n\n def result\n def timeEstimateRatio = maxTime / maxEstimate\n def estimateTime = estimate * timeEstimateRatio\n def upperTimeBound = maxTime\n def lowerTimeBound = 0\n\nInitializes the variables\n\n def currentEstimateIndex = estimationValues.findIndexOf { it ==\n ➥ estimate}\n\nFinds the input’s\nindex in the array\n\n if(currentEstimateIndex == 0) {\n lowerTimeBound = 0\n } else {\n lowerTimeBound = estimateTime - ((estimateTime --\n\nLower bound of 0 for\nthe lowest estimate\n\n ➥ * timeEstimateRatio)) / 2)\n }\n\n➥ (estimationValues[estimationValues.findIndexOf { it == estimate} - 1]\n\n if (currentEstimateIndex == estimationValues.size() -1) {\n upperTimeBound = maxTime\n } else {\n upperTimeBound = estimateTime +\n ➥ (((estimationValues[estimationValues.findIndexOf { it == estimate} + 1]\n ➥ * timeEstimateRatio) - estimateTime) / 2)\n }\n\nThe highest estimate\nuses the upper bound\n\n //Calculate the result\n if(upperTimeBound < actualTime) {\n def diff = actualTime -- upperTimeBound\n result = 0 + diff\n\nUnderestimated; it will\nbe greater than 0\n\nLicensed to Mark Watson <nordickan@gmail.com>\n\nCalculates\nthe lower\ntime bound\n\nCalculates\n the upper\ntime bound", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 4, "cols": 9, "data": [["2 Return the associated\ntime period for the estimate", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", ""], ["T", "W", "T", "F", "M", "T", "W", "T", "F"], ["3 5 8 13\n1 Get the time bounds\nof the estimate 3 Return difference of actual time\nwith estimate association", "", "", "", "", "", "", "", ""]], "markdown": "| 2 Return the associated\ntime period for the estimate |  |  |  |  |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|\n|  |  |  |  |  |  |  |  |  |\n| T | W | T | F | M | T | W | T | F |\n| 3 5 8 13\n1 Get the time bounds\nof the estimate 3 Return difference of actual time\nwith estimate association |  |  |  |  |  |  |  |  |"}], "table_count": 1, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "time period for the estimate def currentEstimateIndex = estimationValues.findIndexOf { it ==\n ➥ estimate} ➥ (estimationValues[estimationValues.findIndexOf { it == estimate} - 1]", "l2_summary": "time period for the estimate 1 Get the time bounds\nof the estimate def result\n def timeEstimateRatio = maxTime / maxEstimate\n def estimateTime = estimate * timeEstimateRatio\n def upperTimeBound = maxTime\n def lowerTimeBound = 0 def currentEstimateIndex = estimationValues.findIndexOf { it ==\n ➥ estimate} Lower bound of 0 for\nthe lowest estimate ➥ (estimationValues[estimationValues.findIndexOf { it == estimate} - 1]", "prev_page": {"page_num": 165, "segment_id": "00165"}, "next_page": {"page_num": 167, "segment_id": "00167"}, "extended_fields": {"has_code": true, "has_formulas": false, "has_diagram": true, "has_list": true, "has_citations": false, "key_terms": ["estimate", "estimationvalues", "time", "estimatetime", "uppertimebound", "maxtime", "timeestimateratio", "bound", "algorithm", "actualtime"], "content_type": "tutorial", "domain": "programming|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Java"], "frameworks": [], "methodologies": [], "programming_languages": ["Java"], "has_best_practices": false, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["estimate health"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["algorithm for checking estimate health", "metric creation"], "key_concepts": ["estimate health", "time estimation ratio", "upper and lower time bounds"], "problem_statement": "How to determine the health of an estimate", "solution_approach": "Algorithmic approach using Java", "extraction_method": "lm"}}
{"segment_id": "00167", "page_num": 167, "segment": "144\n\nCHAPTER 7 Working with the data you’re collecting: the sum of the parts\n\n } else if(lowerTimeBound > actualTime) {\n def diff = lowerTimeBound -- actualTime\n result = 0 -- diff\n } else {\n result = 0\n }\n\nReturns 0 within a\nday of the bounds\n\nOverestimated; it will\nbe less than 0\n\n return [ raw:result, result:result.toInteger() ]\n}\n\nRaw result for\nin-depth analysis\n\nThis is a simple example of adding together a few data points to create an actionable\nand easy-to-understand metric that has real value to your team. Estimate health is\nshown alongside lead time, average estimate, and estimate distribution in figure 7.15.\n Using this algorithm you can hone your estimations and thus get to a more predictable cadence of delivery. Using these techniques you can generate a host of useful\nand easy-to-use metrics. Some of the more useful are the ones that help evaluate how\ngood your development team is doing.\n\n7.4\n\nCase study: creating and using a new metric to\nmeasure continuous release quality\nOur case study for this chapter concerns a team that practices CD and is deploying\ncode to production multiple times a day. Before they made this change they were\n\nEstimates are low; this team\nis breaking tasks down.\n\nOverall tasks take 7 days\nfrom definition to completion.\n\nThis team is good at estimating\nin general; on average they\nslightly underestimate.\n\nBased on the distribution,\nestimates are in good shape.\n\nFigure 7.15 Adding estimate health to our other metrics for predictability\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "} else if(lowerTimeBound > actualTime) {\n def diff = lowerTimeBound -- actualTime\n result = 0 -- diff\n } else {\n result = 0\n } return [ raw:result, result:result.toInteger() ]\n} Raw result for\nin-depth analysis", "l2_summary": "CHAPTER 7 Working with the data you’re collecting: the sum of the parts } else if(lowerTimeBound > actualTime) {\n def diff = lowerTimeBound -- actualTime\n result = 0 -- diff\n } else {\n result = 0\n } return [ raw:result, result:result.toInteger() ]\n} Raw result for\nin-depth analysis Estimate health is\nshown alongside lead time, average estimate, and estimate distribution in figure 7.15. Figure 7.15 Adding estimate health to our other metrics for predictability", "prev_page": {"page_num": 166, "segment_id": "00166"}, "next_page": {"page_num": 168, "segment_id": "00168"}, "extended_fields": {"has_code": true, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["result", "team", "estimate", "using", "good", "chapter", "data", "else", "lowertimebound", "actualtime"], "content_type": "theory|practice", "domain": "devops|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Java"], "frameworks": [], "methodologies": ["DevOps"], "programming_languages": ["Java"], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["lead time", "estimate health", "average estimate", "estimate distribution"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["estimation accuracy", "metric generation", "continuous delivery"], "key_concepts": ["estimate health", "lead time", "average estimate", "estimate distribution"], "problem_statement": "Improving estimation accuracy and predictability in development processes", "solution_approach": "Implementing metrics like lead time, average estimate, and estimate distribution to enhance team performance", "extraction_method": "lm"}}
{"segment_id": "00168", "page_num": 168, "segment": "Case study: creating and using a new metric to measure continuous release quality\n\n145\n\nTwo weeks of\ndevelopment\n\nRelease process\nof a few days\n\nDevelopment\ncomplete\n\nManual and\nautomated testing\n\nBig release\n\nProduction\nverification\n\nThe team got as many tasks\nas they could get done in\ntwo weeks into a release.\n\nOnce development\nwas complete there was a\nmultiday regression test.\n\nAll of the code was released\nat the same time during\na release event.\n\nEverything was verified in\nthe production environment\nafter the release was done.\n\nFigure 7.16 A representation of the release process before the team moved to CD\n\nreleasing every few weeks and were able to put a rating on a release based on a few\nfactors:\n\n■ How many bugs they found in the production environment that they didn’t\n\nfind in the test environment.\n\n■ How big the feature they released was, or how many tasks went into the release.\n■ How long the release took measured in hours. Good releases would typically\ntake a few hours to complete, but if there were issues during the deployment,\nreleases could take 8--12 hours.\n\nIn moving to a CD model these metrics didn’t mean much anymore. Deployments\nwere always a single task, they took minutes instead of days, and the team didn’t run a\nfull regression test suite after each release. The before-and-after release process is\nshown in figures 7.16 and 7.17.\n\n Figure 7.16 shows how things were done before CD. The team was agile, but even\nin two-week increments a lot of change went into a release. They modified their process to the representation shown in figure 7.17.\n\nDevelopment\n\nAutomated\ntesting\n\nAutomated\nrelease\n\nProduction\nverification\n\nDevelopment is\non a single task.\n\nTesting is 100%\nautomated.\n\nRelease is automated\nbased on test results.\n\nProduction verification\nis continuous.\n\nFigure 7.17 A representation of the new process they used to deploy code continuously\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "Release process\nof a few days Figure 7.16 A representation of the release process before the team moved to CD The before-and-after release process is\nshown in figures 7.16 and 7.17.", "l2_summary": "Release process\nof a few days Production\nverification Everything was verified in\nthe production environment\nafter the release was done. Figure 7.16 A representation of the release process before the team moved to CD The before-and-after release process is\nshown in figures 7.16 and 7.17. Release is automated\nbased on test results.", "prev_page": {"page_num": 167, "segment_id": "00167"}, "next_page": {"page_num": 169, "segment_id": "00169"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["release", "development", "process", "automated", "production", "team", "test", "figure", "weeks", "complete"], "content_type": "case_study", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["code", "regression test suite"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["bugs found in production", "size of feature released", "release duration"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["continuous delivery process", "release quality metrics", "shift to continuous deployment"], "key_concepts": ["release quality measurement", "continuous integration and deployment (CI/CD)", "agile development"], "problem_statement": "Improving release quality and efficiency in a traditional release process", "solution_approach": "Implementing a CI/CD pipeline with automated testing, continuous verification, and single-task releases", "extraction_method": "lm"}}
{"segment_id": "00169", "page_num": 169, "segment": "146\n\nCHAPTER 7 Working with the data you’re collecting: the sum of the parts\n\nThe most important\naspects of a good release\n\nSmooth delivery\n\nThe original question\n\nWhat is a good release?\n\nFigure 7.18 Part 1 of the mind-mapping exercise\n\nImproves the consumer’s experience\n\nNow that a release was a single task, they had to change their point of view from the\nefficiency of a team to get a group of tasks out to the consumer to the aggregation of\nthe individual health of tasks as they were being worked and deployed. Because they\nwere releasing multiple times a day, they needed a simple indicator that could tell\nthem if their delivery process was working well.\n\n They started off with a mind map to get to the bottom of what they had and what they\nwanted to track. In the context of their new delivery model, they needed to determine\nwhat made up a good release. They started with the elements shown in figure 7.18.\n\n The two most important things the team could think of were these:\n\n■ The release was smooth. Using CD if a release didn’t work well, it slowed down the\nwhole development cycle. A smooth release indicated the team was on track.\n■ The consumer’s experience improved. Instead of asking if a release broke anything,\nthey changed to the consumer’s perspective: whether or not the product was\nimproving.\n\nThen they broke it down further, as shown in figure 7.19.\n\nBreaking aspects\ndown further\n\nGetting to the\ndata points\n\nMeet commitments\n\nSmooth delivery\n\n--\n\nKeep it small\n\n--\n\n--\n\nEstimate health\n\nCLOC\n\nKeep tasks moving forward\n\n--\n\nRecidivism\n\nImproves the consumer’s experience\n\n--\n\nNo bugs after a release\n\n--\n\nEscaped defects\n\nFigure 7.19 Getting to the individual data points for this metric\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "What is a good release? ■ The release was smooth. No bugs after a release", "l2_summary": "The most important\naspects of a good release What is a good release? Improves the consumer’s experience ■ The release was smooth. A smooth release indicated the team was on track. No bugs after a release", "prev_page": {"page_num": 168, "segment_id": "00168"}, "next_page": {"page_num": 170, "segment_id": "00170"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["release", "consumer", "smooth", "delivery", "what", "figure", "data", "good", "experience", "team"], "content_type": "theory", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["CD", "CLOC"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["Smooth delivery", "Improves the consumer’s experience"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Good release criteria", "Release process improvement", "Consumer experience focus"], "key_concepts": ["Smooth delivery", "Consumer experience improvement", "Agile methodology"], "problem_statement": "Improving the efficiency and quality of software releases", "solution_approach": "Defining key metrics for a good release, focusing on smooth delivery and consumer experience", "extraction_method": "lm"}}
{"segment_id": "00170", "page_num": 170, "segment": "Case study: creating and using a new metric to measure continuous release quality\n\n147\n\n For each release they believed the following indicators would tell them if things\n\nwere good or bad:\n\n■ They could make and meet their commitments. Even though we’re talking about a\ndelivery time in days, the team still needs to be able to trust their estimates so\nthey can have some level of predictability.\n■ Measured by estimate health.\n\n■ Tasks continued to move forward. Anything that goes backward in the workflow\ncould be linked to a number of problems, some of which are not understanding\nrequirements, poor quality of work, or lack of tests. It was important that overall\ntasks were progressing and not regressing in the workflow.\n■ Measured by recidivism.\n\n■ Work items were kept intentionally small. To mitigate the risk of integration issues the\nteam had a goal of keeping changes to the codebase minimal in each release.\n■ Measured by CLOC.\n\n■ Releases don’t introduce bugs. The release process was very fast and relied heavily\non automated testing. As a result, any defects that escaped into the production\nenvironment needed to be triaged so the team could figure out how to harden\ntheir testing cycle.\n■ Measured by escaped defects.\n\nThey decided to come up with a formula that combined all of these data points into a\nsingle metric that would indicate the overall health of releases across their development teams, a code health determination (CHD). If this metric started to trend in the\nwrong direction, they could put the brakes on and dig into the problem. If the metric\nstayed in a good place, then teams could continue developing and deploying at will.\n\n In their metric each of the four elements would make up 25% of the total value,\nand each metric would have equal weight in the total calculation. If F represents a formula that normalized each metric, a high level of the formula would be represented\nlike this:\n\nF1(CLOC) + F2(Estimate Health) + F3(Recidivism) + F4(Escaped Defects)\n\nThey decided to create a final number that was a score between 0 and 100 where 0\nmeant everything was going to pieces and 100 meant everything was perfect. To create\nthis number they had to do some math to normalize the inputs and the outputs.\n\nNORMALIZING CHANGED LINES OF CODE\nAfter exploring their data the team concluded that each small change they made was\naround 50 CLOC. To come to their indicator they decided to divide CLOC by 50, chop\noff the decimal, and then multiply it to magnify the effect. Because 25 would be the\nbest possible value, they subtracted the result from 25. To handle the potential of a\nnegative number, they took the maximum between the result and 0 to ensure they\nwould stay within the bounds of 0--25. This gave them the following function:\n\nMAX((25 - ABS((int)(cloc/50)-1)*5), 0)\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "■ Measured by estimate health. In their metric each of the four elements would make up 25% of the total value,\nand each metric would have equal weight in the total calculation. F1(CLOC) + F2(Estimate Health) + F3(Recidivism) + F4(Escaped Defects)", "l2_summary": "For each release they believed the following indicators would tell them if things ■ Measured by estimate health. ■ Measured by recidivism. ■ Measured by escaped defects. In their metric each of the four elements would make up 25% of the total value,\nand each metric would have equal weight in the total calculation. F1(CLOC) + F2(Estimate Health) + F3(Recidivism) + F4(Escaped Defects)", "prev_page": {"page_num": 169, "segment_id": "00169"}, "next_page": {"page_num": 171, "segment_id": "00171"}, "extended_fields": {"has_code": false, "has_formulas": true, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["metric", "would", "each", "could", "cloc", "release", "team", "measured", "health", "number"], "content_type": "case_study", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["automated testing", "CLOC"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["estimate health", "recidivism", "CLOC", "escaped defects"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["release quality metrics", "continuous delivery", "code health determination"], "key_concepts": ["estimate health", "recidivism", "CLOC", "escaped defects", "code health determination (CHD)"], "problem_statement": "Measuring the quality of continuous releases", "solution_approach": "Creating a composite metric combining multiple indicators", "extraction_method": "lm"}}
{"segment_id": "00171", "page_num": 171, "segment": "148\n\nCHAPTER 7 Working with the data you’re collecting: the sum of the parts\n\nWith that formula, example inputs and outputs are shown in table 7.2.\n\nTable 7.2 Example inputs and outputs after normalizing the ideal number of CLOC\n\nInput\n\nOutput\n\nResult\n\n50\n\n135\n\n18\n\n450\n\n25\n\n20\n\n25\n\n0\n\nPerfect.\n\nOver the sweet spot range of 0--100 CLOC results in a lower score.\n\nUnder 50 CLOC but by getting the absolute value it still yields 25.\n\nAn input of 450 results in -15, which is then normalized to 0.\n\nNORMALIZING ESTIMATE HEALTH\nAs you saw earlier, estimate health was a 0 if everything was good, and as the value\nbecame larger or smaller than 0, it was getting worse. To normalize this, the team had\nto get the absolute value, compare the result to 0, and determine what the maximum\ntolerance was for over- or underestimating. If the absolute value was greater than 0, it\nshould count against the total maximum value of 25. The team decided that if they\nwere over- or underestimating by more than three days, then there was a big problem.\nThey made the multiplier 7, which would bring the value really close to 0 if they were\nover or under by three days. For anything more than that, they made the value 0, as\nshown in this formula:\n\nMAX((25 -- (ABS(Estimate Health)* 7)), 0)\n\nExample inputs and outputs are shown in table 7.3.\n\nTable 7.3 Example inputs and outputs for normalizing estimate health data\n\nInput\n\nOutput\n\nResult\n\n0\n\n1\n\n-1\n\n3\n\n4\n\n25\n\n18\n\n18\n\n4\n\n0\n\nPerfect score; estimates are right on track.\n\nEven a day will have a significant effect on the total.\n\nBecause we take the absolute value, -1 and 1 have the same result.\n\nAt this point it’s guaranteed the total rating will be under 80.\n\nAnything greater than 3 results in 0.\n\nNORMALIZING RECIDIVISM\nRecidivism results in a percentage or a decimal. The team was striving to keep tasks\nmoving forward in their workflow so they wanted to have a low recidivism rate.\nRemember that tasks that are completed can have a maximum recidivism of 50% or\n.5, because they would have moved backward as many times as they moved forward. To\ntake the output of the recidivism formula and equate that to a number between 0 and\n25 where 0 is the worst and 25 is the best, they normalized the result by multiplying\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 2, "cols": 3, "data": [["Input", "Output", "Result"], ["", "25\n20\n25\n0", ""]], "markdown": "| Input | Output | Result |\n|---|---|---|\n|  | 25\n20\n25\n0 |  |"}, {"table_id": "table_2", "rows": 2, "cols": 3, "data": [["Input", "Output", "Result"], ["", "25\n18\n18\n4\n0", ""]], "markdown": "| Input | Output | Result |\n|---|---|---|\n|  | 25\n18\n18\n4\n0 |  |"}], "table_count": 2, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "With that formula, example inputs and outputs are shown in table 7.2. For anything more than that, they made the value 0, as\nshown in this formula: Example inputs and outputs are shown in table 7.3.", "l2_summary": "With that formula, example inputs and outputs are shown in table 7.2. If the absolute value was greater than 0, it\nshould count against the total maximum value of 25. For anything more than that, they made the value 0, as\nshown in this formula: Example inputs and outputs are shown in table 7.3. Table 7.3 Example inputs and outputs for normalizing estimate health data Because we take the absolute value, -1 and 1 have the same result.", "prev_page": {"page_num": 170, "segment_id": "00170"}, "next_page": {"page_num": 172, "segment_id": "00172"}, "extended_fields": {"has_code": false, "has_formulas": true, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["value", "result", "than", "have", "recidivism", "example", "inputs", "outputs", "table", "normalizing"], "content_type": "tutorial", "domain": "programming|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["CLOC", "formula"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["Estimate Health", "Recidivism"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Data Normalization", "Estimate Health Calculation", "Recidivism Normalization"], "key_concepts": ["Normalization of data", "Formula for Estimate Health", "Formula for Recidivism"], "problem_statement": "How to normalize and calculate the health of estimates and recidivism in software development projects.", "solution_approach": "Using formulas to normalize inputs into a score between 0 and 25, where lower scores indicate better performance.", "extraction_method": "lm"}}
{"segment_id": "00172", "page_num": 172, "segment": "Case study: creating and using a new metric to measure continuous release quality\n\n149\n\nrecidivism by 50 (25 * 2). Using that formula the highest possible rate of recidivism of\ncompleted tasks would be 0, and the lowest possible result would be 25.\n As a reminder, we used the following formula earlier for recidivism:\n\nRecidivism = Backwards Tasks / (Forward Tasks + Backwards Tasks)\n\nOr\n\nRecidivism = 25 -- ((bN / (fN + bN)) *50)\n\nWith that formula some example inputs and outputs are shown in table 7.4..\n\nTable 7.4 Example inputs and outputs for a normalized recidivism\n\nInput\n\nOutput\n\nResult\n\nbN = 5, fN = 100\n\n22.62\n\nNot bad\n\nbN = 100, fN = 100\n\nbN = 0, fN = 125\n\n 0\n\n25\n\nThe worst possible output\n\nPerfect score\n\nNORMALIZING ESCAPED DEFECTS\nAn escaped defect was a bug that wasn’t caught in the release process; it was found\nafter the release was considered a success. Finding any escaped defects should make\nthe team stop and find out what happened so they could continue to improve their\nautomated test and release. Because of this even a single escaped defect should count\nagainst the total. The formula for escaped defects was pretty simple: multiply them\nand subtract from 25. As with estimate health, we take the larger of 0 or the result to\naccommodate for potentially negative numbers.\n\nMAX((25 -- (Escaped Defects * 10)), 0)\n\nUsing this formula, even a single escaped defect would have a big impact on the overall rating. Some example inputs and outputs are shown in table 7.5.\n\nTable 7.5 Example inputs and outputs from the formula used to normalize escaped defects\n\nInput\n\nOutput\n\nResult\n\n0\n\n1\n\n2\n\n3\n\n25\n\n15\n\n5\n\n-5\n\nPerfect.\n\nA significant impact on the total rating.\n\nThis is the highest tolerance before the number goes below 0.\n\nNegative numbers have a huge impact on the total rating.\n\nADDING THE ELEMENTS TOGETHER\nTo get a number from 0 to 100 with four equally important elements, the calculation\nshould be simply adding the four numbers together. But one final part of the calculation is to create a minimum and maximum for each element. The min and max of\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 2, "cols": 3, "data": [["Input", "Output", "Result"], ["", "22.62\n0\n25", ""]], "markdown": "| Input | Output | Result |\n|---|---|---|\n|  | 22.62\n0\n25 |  |"}, {"table_id": "table_2", "rows": 2, "cols": 3, "data": [["Input", "Output", "Result"], ["", "25\n15\n5\n-5", ""]], "markdown": "| Input | Output | Result |\n|---|---|---|\n|  | 25\n15\n5\n-5 |  |"}], "table_count": 2, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "recidivism by 50 (25 * 2). Recidivism = 25 -- ((bN / (fN + bN)) *50) MAX((25 -- (Escaped Defects * 10)), 0)", "l2_summary": "recidivism by 50 (25 * 2). Recidivism = 25 -- ((bN / (fN + bN)) *50) With that formula some example inputs and outputs are shown in table 7.4.. Table 7.4 Example inputs and outputs for a normalized recidivism MAX((25 -- (Escaped Defects * 10)), 0) Table 7.5 Example inputs and outputs from the formula used to normalize escaped defects", "prev_page": {"page_num": 171, "segment_id": "00171"}, "next_page": {"page_num": 173, "segment_id": "00173"}, "extended_fields": {"has_code": false, "has_formulas": true, "has_diagram": false, "has_list": true, "has_citations": false, "key_terms": ["escaped", "recidivism", "formula", "defects", "release", "tasks", "result", "example", "inputs", "outputs"], "content_type": "case_study", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": false, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["recidivism", "escaped defects"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["normalized recidivism", "escaped defects", "quality metrics"], "key_concepts": ["continuous release quality", "formula for calculating recidivism", "formula for escaped defects"], "problem_statement": "Improving the quality of continuous releases in software development", "solution_approach": "Developing and using new metrics to measure and improve the quality of continuous releases", "extraction_method": "lm"}}
{"segment_id": "00173", "page_num": 173, "segment": "150\n\nCHAPTER 7 Working with the data you’re collecting: the sum of the parts\n\neach number should represent the absolute limits of when the team needs to take corrective action. If there are two or more escaped defects each release, the team needs\nto stop and figure out where the problem is. Everything running smoothly would represent the minimum measurement of each input.\n\n To calculate the min and max, the team decided to use a scale of 0--5 for each\ninput. This would give more weight to each input if they started to trend in the wrong\ndirection. The final metric was calculated with the algorithm shown in the next listing.\n\nListing 7.2 The algorithm as Groovy code\n\nstatic int calculateCHD(cloc, estimateHealth, recidivism, escapedDefects) {\n def chd = 0\n\nMultiplies it\nby 10 to get\nan integer\n\nEscaped\ndefects gets\nmultiplied\n\n def nCloc = ((int)(cloc/50) - 1) * 5\n def nEstimateHealth = Math.abs(estimateHealth)\n def nRecidivism = recidivism * 50\n def nEscapedDefects = escapedDefects * 10\n\n chd = (minMax(nCloc) + minMax(nEstimateHealth) +\n ➥ (minMax(nRecidivism) + minMax(nEscapedDefects)) * 5\n\nNormalizes CLOC\n\nGets the absolute value\n\nReturns a number\nbetween 0 and 100\n\n return chd\n}\n\nprivate static int minMax(val){\n def mm = { v ->\n if (v >= 5) {\n return 5\n } else if (v <= 0) {\n return 0\n } else {\n return v\n }\n }\n return 5 - mm(val)\n}\n\nNormalizes the outputs of the\nindividual inputs by making\nthe maximum 5 and the\nminimum 0. Because the ideal\nof all the inputs is 0, you\nsubtract the result from 5.\n\nUsing this algorithm the team successfully created a metric that gave them an indication of how releases were faring. If they noticed CHD going below 80, they knew they\nhad to check in to see what was not working as expected. If they were above 80, then\nthey could let teams continue to release at will.\n\n Some example inputs and outputs from this formula are shown in table 7.6.\n\nTable 7.6 Example inputs and outputs from the CHD formula with the corresponding ratings\n in parentheses\n\nEstimate Health\n\nCLOC\n\nRecidivism\n\nEscaped Defects\n\nCHD\n\n0 (25)\n\n0 (25)\n\n49 (25)\n\n100 (20)\n\n0 (25)\n\n0 (25)\n\n0 (25)\n\n0 (25)\n\n100\n\n95\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 2, "cols": 5, "data": [["Estimate Health", "CLOC", "Recidivism", "Escaped Defects", "CHD"], ["", "49 (25)\n100 (20)", "0 (25)\n0 (25)", "0 (25)\n0 (25)", ""]], "markdown": "| Estimate Health | CLOC | Recidivism | Escaped Defects | CHD |\n|---|---|---|---|---|\n|  | 49 (25)\n100 (20) | 0 (25)\n0 (25) | 0 (25)\n0 (25) |  |"}], "table_count": 1, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "static int calculateCHD(cloc, estimateHealth, recidivism, escapedDefects) {\n def chd = 0 chd = (minMax(nCloc) + minMax(nEstimateHealth) +\n ➥ (minMax(nRecidivism) + minMax(nEscapedDefects)) * 5 private static int minMax(val){\n def mm = { v ->\n if (v >= 5) {\n return 5\n } else if (v <= 0) {\n return 0\n...", "l2_summary": "To calculate the min and max, the team decided to use a scale of 0--5 for each\ninput. static int calculateCHD(cloc, estimateHealth, recidivism, escapedDefects) {\n def chd = 0 def nCloc = ((int)(cloc/50) - 1) * 5\n def nEstimateHealth = Math.abs(estimateHealth)\n def nRecidivism = recidivism * 50\n def nEscapedDefects = escapedDefects * 10 chd = (minMax(nCloc) + minMax(nEstimateHealth) +\n ➥ (minMax(nRecidivism) + minMax(nEscapedDefects)) * 5 private static int minMax(val){\n def mm = { v ->\n if (v >= 5) {\n return 5\n } else if (v <= 0) {\n return 0\n } else {\n return v\n }\n }\n return 5 - mm(val)\n} Table 7.6 Example inputs and outputs from the CHD formula with the corresponding ratings\n in parentheses", "prev_page": {"page_num": 172, "segment_id": "00172"}, "next_page": {"page_num": 174, "segment_id": "00174"}, "extended_fields": {"has_code": true, "has_formulas": true, "has_diagram": false, "has_list": true, "has_citations": false, "key_terms": ["each", "minmax", "return", "team", "cloc", "inputs", "escaped", "defects", "input", "algorithm"], "content_type": "tutorial", "domain": "programming|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Groovy"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": ["Groovy"], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["CHD"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["metric calculation", "defect management", "release management"], "key_concepts": ["CHD metric", "input normalization", "corrective action thresholds"], "problem_statement": "How to manage and correct defects in software releases", "solution_approach": "Developing a CHD metric using Groovy code for input normalization and corrective action", "extraction_method": "lm"}}
{"segment_id": "00174", "page_num": 174, "segment": "Case study: creating and using a new metric to measure continuous release quality\n\n151\n\nTable 7.6 Example inputs and outputs from the CHD formula with the corresponding ratings\n in parentheses\n\nEstimate Health\n\nCLOC\n\nRecidivism\n\nEscaped Defects\n\nCHD\n\n1 (18)\n\n0 (25)\n\n-2 (11)\n\n3 (4)\n\n-1 (18)\n\n65 (25)\n\n33 (25)\n\n150 (15)\n\n350 (0)\n\n45 (25)\n\n10% (20)\n\n20% (15)\n\n0 (25)\n\n50% (0)\n\n50% (0)\n\n0 (25)\n\n1 (15)\n\n0 (25)\n\n0 (25)\n\n 2 (0)\n\n88\n\n80\n\n76\n\n29\n\n43\n\nWhen the CHD rating dipped below 80 in the span of a 48-hour window, the team\nwould immediately stop releasing code and work to get to the root of the problem.\n\n The final step was to put the data into a dashboard to easily display it to the team.\nThey decided to use Dashing, an open source dashboarding framework, to aggregate\nand display their data easily. The end result is shown in figure 7.20.\n\nThe key metric is\ndisplayed at the top\nof the page.\n\nMTTR over time is also shown; if there\nis a problem, leadership wants to know\nhow long it would take to react.\n\nEach building block metric is broken out below\nto easily identify the source of a problem.\n\nFigure 7.20 Using Dashing to create an easy-to-read dashboard for the larger team\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 2, "cols": 5, "data": [["Estimate Health", "CLOC", "Recidivism", "Escaped Defects", "CHD"], ["", "65 (25)\n33 (25)\n150 (15)\n350 (0)\n45 (25)", "10% (20)\n20% (15)\n0 (25)\n50% (0)\n50% (0)", "0 (25)\n1 (15)\n0 (25)\n0 (25)\n2 (0)", ""]], "markdown": "| Estimate Health | CLOC | Recidivism | Escaped Defects | CHD |\n|---|---|---|---|---|\n|  | 65 (25)\n33 (25)\n150 (15)\n350 (0)\n45 (25) | 10% (20)\n20% (15)\n0 (25)\n50% (0)\n50% (0) | 0 (25)\n1 (15)\n0 (25)\n0 (25)\n2 (0) |  |"}], "table_count": 1, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "The final step was to put the data into a dashboard to easily display it to the team. The end result is shown in figure 7.20. Figure 7.20 Using Dashing to create an easy-to-read dashboard for the larger team", "l2_summary": "When the CHD rating dipped below 80 in the span of a 48-hour window, the team\nwould immediately stop releasing code and work to get to the root of the problem. The final step was to put the data into a dashboard to easily display it to the team. They decided to use Dashing, an open source dashboarding framework, to aggregate\nand display their data easily. The end result is shown in figure 7.20. Each building block metric is broken out below\nto easily identify the source of a problem. Figure 7.20 Using Dashing to create an easy-to-read dashboard for the larger team", "prev_page": {"page_num": 173, "segment_id": "00173"}, "next_page": {"page_num": 175, "segment_id": "00175"}, "extended_fields": {"has_code": false, "has_formulas": true, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["metric", "team", "problem", "easily", "using", "would", "data", "dashboard", "display", "dashing"], "content_type": "case_study", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Dashing", "CHD formula"], "frameworks": [], "methodologies": ["DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["CHD", "MTTR"], "has_case_study": true, "case_study_company": null, "tools_mentioned": ["Dashing"], "topics": ["continuous release quality", "metric creation", "dashboard implementation"], "key_concepts": ["CHD formula", "Estimate Health", "MTTR"], "problem_statement": "Improving continuous release quality and identifying issues promptly", "solution_approach": "Developing a new metric (CHD) to measure release quality, implementing it in a dashboard for real-time monitoring", "extraction_method": "lm"}}
{"segment_id": "00175", "page_num": 175, "segment": "152\n\nCHAPTER 7 Working with the data you’re collecting: the sum of the parts\n\nLeaders and managers could focus on the top-line metrics, and teams could focus on\nthe next level down to make sure they were focusing on the right parts of their lifecycle. In figure 7.20 their estimate health is lower than the other components, so the\nteam could work on their estimation accuracy sprint over sprint.\n\n After this team started publishing their dashboard, other teams wanted to do the\nsame. Another team that wanted to adopt this rating used lead time instead of estimates to track the predictability of their work. They decided to use this rating mostly\nas is; they just swapped out the estimate health rating with the lead time rating. They\nhad an ideal lead time of seven working days, so they crafted their lead time rating to\nfit into the formula:\n\nLead Time Rating = 25 -- ((Lead Time -- 7) * 2.5)\n\nBecause 7 was the ideal lead time, they subtracted that from the current lead time to\nget the difference. When lead time started to go over by a few days, there was usually a\nproblem, so they used a multiplier of 2.5 to enhance the effect of longer lead time relative to the maximum of 25. Their dashboard is shown in figure 7.21.\n\nNote that the rating has gone up by 13%; that indicates that\nthey dipped under 80 but were able to check and adjust.\n\nThe only difference in the dashboard is the\nrating used to measure consistency.\n\nFigure 7.21 A second team decided to use the same rating system but tweak the composition of the\nmetric to fit their process.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "They\nhad an ideal lead time of seven working days, so they crafted their lead time rating to\nfit into the formula: Lead Time Rating = 25 -- ((Lead Time -- 7) * 2.5) Because 7 was the ideal lead time, they subtracted that from the current lead time to\nget the difference.", "l2_summary": "Another team that wanted to adopt this rating used lead time instead of estimates to track the predictability of their work. They decided to use this rating mostly\nas is; they just swapped out the estimate health rating with the lead time rating. They\nhad an ideal lead time of seven working days, so they crafted their lead time rating to\nfit into the formula: Lead Time Rating = 25 -- ((Lead Time -- 7) * 2.5) Because 7 was the ideal lead time, they subtracted that from the current lead time to\nget the difference. When lead time started to go over by a few days, there was usually a\nproblem, so they used a multiplier of 2.5 to enhance the effect of longer lead time relative to the maximum of 25.", "prev_page": {"page_num": 174, "segment_id": "00174"}, "next_page": {"page_num": 176, "segment_id": "00176"}, "extended_fields": {"has_code": false, "has_formulas": true, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["lead", "time", "rating", "team", "could", "figure", "dashboard", "used", "working", "parts"], "content_type": "practice", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "Scrum"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["estimate health", "lead time"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["team focus on metrics", "dashboard implementation", "lead time rating system"], "key_concepts": ["top-line metrics", "sprint estimation accuracy", "lead time predictability"], "problem_statement": "Improving team focus and metric tracking in the development lifecycle", "solution_approach": "Implementing a dashboard with specific ratings for estimate health and lead time to enhance team performance", "extraction_method": "lm"}}
{"segment_id": "00176", "page_num": 176, "segment": "Summary\n\n153\n\nIn this case they were able to take a customized metric, tweak it to meet their needs,\nbut use the same terminology and high-level metrics that other teams in the company\nwere using. This metric was a combination of several data points and was a call to\naction. Using these same techniques you can create metrics of your own based on how\nyour team works and values that have been determined good to keep a pulse on the\nprogress and consistency of your development team.\n\n7.5\n\nSummary\nUsing the data collected through earlier chapters, we crafted custom metrics and used\nthem to give big-picture indicators of complex interactions. In this chapter you\nlearned the following:\n\n■ You can create simple metrics from single data points, or you can use formulas\n\nand algorithms to combine data for more complex and insightful metrics.\n\n■ Some example metrics used in this chapter:\n\n■ Recidivism---Backward Tasks / (Forward Tasks + Backward Tasks)\n■ Comment To Commit Ratio---code reviews / (merged pull requests + commits)\n■ MTTR---Problem Fixed Time -- Problem Identification Time\n■ Continuous Release Quality Rating (CHD)---(25 - ABS((int)(cloc/50)-1) * 5) +\n(25-ABS(Estimate Health) * 7) + (Recidivism = 25 -- ((bN / (fN + bN)) *50))\n+ (25 -- (Escaped Defects * 10))\n\n■ Start off by taking time to explore your data to understand how it represents\n\nyour team and how they work.\n■ Use the data you have already to define what in your development cycle is\n\ngood and worth repeating.\n\n■ Once you have a good understanding of your data, start combining data points\n\nto create metrics to track what matters most to your development cycle.\n\n■ Mind mapping is great way to get to the root of what to measure.\n■ Using the result of your mind map, create the formula to generate custom\n\nmetrics.\n\n■ Adding live data into your formula gives you metrics that you can track in your\n\ndevelopment cycle.\n\n■ When changing your process, looking for new ways to measure your team is a\n\nkey to success.\n\n■ Different teams within the same company can use similar ratings made up of\ndifferent components as long as they’re measuring the same conceptual things.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "■ You can create simple metrics from single data points, or you can use formulas and algorithms to combine data for more complex and insightful metrics. ■ Use the data you have already to define what in your development cycle is", "l2_summary": "■ You can create simple metrics from single data points, or you can use formulas and algorithms to combine data for more complex and insightful metrics. ■ Some example metrics used in this chapter: ■ Use the data you have already to define what in your development cycle is ■ Once you have a good understanding of your data, start combining data points ■ Adding live data into your formula gives you metrics that you can track in your", "prev_page": {"page_num": 175, "segment_id": "00175"}, "next_page": {"page_num": 177, "segment_id": "00177"}, "extended_fields": {"has_code": false, "has_formulas": true, "has_diagram": false, "has_list": true, "has_citations": false, "key_terms": ["metrics", "data", "same", "using", "create", "team", "development", "points", "have", "good"], "content_type": "tutorial", "domain": "devops|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["Recidivism", "Comment To Commit Ratio", "MTTR", "Continuous Release Quality Rating (CHD)"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Custom Metrics Creation", "Data Point Combination", "Development Cycle Tracking"], "key_concepts": ["Custom Metrics", "Data Points Combination", "Development Cycle Tracking"], "problem_statement": "How to create custom metrics for tracking the development cycle and team performance.", "solution_approach": "Using formulas and combining data points to create meaningful metrics.", "extraction_method": "lm"}}
{"segment_id": "00177", "page_num": 177, "segment": "Measuring the technical\nquality of your software\n\nThis chapter covers\n\n■ Measuring software quality\n\n■ Using non-functional requirements, also known\nas the code “ilities,” to measure code quality\n\nIn previous chapters we’ve looked at a lot of data that you can collect throughout\nyour development cycle to gain insight into how your team is performing. In this\nchapter we’ll transition from measuring the process to measuring the product by\nusing that data to determine how good your software products are.\n\n The question we’re asking in this chapter is, “Is your software good?” Before you\ncan answer, you must ask yourself, “What is good software?” Once you know that,\nyou can compare what you have to the ideal picture to determine where your software products stand.\n\n At a high level there are only two dimensions to good software:\n\n■\n\n■\n\nIf it does what it’s supposed to do---These are the functional requirements.\nIf it’s built well---These are the non-functional requirements.\n\nFunctional requirements are what differentiate one software product from another.\nThese all relate to what you’re building and how your consumers are interacting\n\n154\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "Measuring the technical\nquality of your software ■ Measuring software quality Functional requirements are what differentiate one software product from another.", "l2_summary": "Measuring the technical\nquality of your software ■ Measuring software quality In this\nchapter we’ll transition from measuring the process to measuring the product by\nusing that data to determine how good your software products are. At a high level there are only two dimensions to good software: If it does what it’s supposed to do---These are the functional requirements. Functional requirements are what differentiate one software product from another.", "prev_page": {"page_num": 176, "segment_id": "00176"}, "next_page": {"page_num": 178, "segment_id": "00178"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["software", "what", "measuring", "functional", "requirements", "good", "quality", "chapter", "using", "code"], "content_type": "tutorial", "domain": "programming|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["software quality", "code quality"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Measuring software quality", "Non-functional requirements", "Process vs Product measurement"], "key_concepts": ["functional requirements", "non-functional requirements", "code 'ilities'"], "problem_statement": "Determining the technical quality of software products", "solution_approach": "Using data from development cycle to measure software and compare against ideal standards", "extraction_method": "lm"}}
{"segment_id": "00178", "page_num": 178, "segment": "Preparing for analysis: setting up to measure your code\n\n155\n\nwith what you’re producing. In chapter 6 we talked about how to track metrics that\nare product-specific. Metrics around your functional requirements tell you if your software is working and satisfying the consumer.\n\n Non-functional requirements relate to how well your application is built. A wellbuilt application will be easier to update and deploy predictably, which brings us back\nto early and continuous delivery through good architectures, designs, and technical\nexcellence.\n\n The previous chapters give you the tools you need to measure functional and nonfunctional requirements. This chapter uses the tools and data we’ve explored up to\nthis point to measure the technical quality of your software products.\n\n If you drill into the agile principles from the Agile Manifesto, you’ll see a strong\n\nfocus on delivering frequently in the face of change:\n\n■ Your highest priority is to satisfy the customer through early and continuous\n\ndelivery of valuable software.\n\n■ Welcome changing requirements, even late in development. Agile processes\n\nharness change for the customer’s competitive advantage.\n\n■ Deliver working software frequently, from a couple of weeks to a couple of\n\nmonths, with a preference for the shorter timescale.\n\nAll of these are telling you to change things quickly and frequently. Then there are a\nfew that go even deeper:\n\n■ Continuous attention to technical excellence and good design enhances agility.\n■ The best architectures, requirements, and designs emerge from self-organizing\n\nteams.\n\n■ Working software is the primary measure of progress.\n\nAnyone with an IDE and access to the internet can create working software; the really\nimportant thing to dig into is how to create a software product that you can iterate on\nquickly and is stable enough to handle your consumer’s expectations. At a glance,\nmeasuring a codebase across these principles looks a bit nebulous, so to do that we’ll\ndive into non-functional requirements, or the code “ilities.”\n\nMeasuring your team against all the agile tenets\n\nIf you’d like to know how to measure all of the agile tenets, head over to chapter 10\nwhere I’ve broken them down into what you should measure and how to do it.\n\n8.1\n\nPreparing for analysis: setting up to measure your code\nIn earlier chapters we looked into the tools and practices you should be following to get\nthe data you need. In chapter 5 we talked about using static analysis in your CI systems,\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "7", "chapter_title": "Working with the data you’re collecting: the sum of the parts", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "Preparing for analysis: setting up to measure your code delivery of valuable software. ■ Working software is the primary measure of progress.", "l2_summary": "Preparing for analysis: setting up to measure your code In chapter 6 we talked about how to track metrics that\nare product-specific. Metrics around your functional requirements tell you if your software is working and satisfying the consumer. The previous chapters give you the tools you need to measure functional and nonfunctional requirements. delivery of valuable software. ■ Working software is the primary measure of progress.", "prev_page": {"page_num": 177, "segment_id": "00177"}, "next_page": {"page_num": 179, "segment_id": "00179"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["measure", "software", "requirements", "agile", "chapter", "functional", "working", "analysis", "code", "continuous"], "content_type": "tutorial", "domain": "programming|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["IDE", "CI systems"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["functional requirements", "non-functional requirements"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["IDE", "CI systems"], "topics": ["measuring code quality", "agile principles", "technical excellence"], "key_concepts": ["metrics", "functional and non-functional requirements", "continuous delivery", "self-organizing teams"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00179", "page_num": 179, "segment": "156\n\nCHAPTER 8 Measuring the technical quality of your software\n\nand in chapter 6 we walked through using APM tools to gather data on how well your\napplication is functioning and if it’s doing what it’s supposed to do. The data generated\nfrom components in your CI and your APM systems will get you what you need.\n\n As a reference, here’s a list of the tools we’ve used in previous chapters and that\n\nwe’ll be referencing in this chapter.\n\nTool\n\nMeasures\n\nMetrics\n\nNew Relic\n\nApplication monitoring\n\nPage response time, uptime, response time, error rate\n\nHyperSpin\n\nAvailability\n\nUptime, response time\n\nSplunk\n\nReliability\n\nError rate, mean time between failures\n\nOWASP ZAP\n\nSecurity\n\nDynamic analysis issues\n\nSonarQube\n\nMaintainability\n\nCLOC, code coverage, issues, complexity\n\nCheckmarx\n\nSecurity\n\nStatic analysis issues\n\n8.2 Measuring the NFRs through the code “ilities”\n\nThe code “ilities,” or non-functional requirements1 (NFRs), are a set of properties that\ndescribe how well software is built. These aren’t anything new; any software engineer\nshould be familiar with them and how they indicate the quality of software. Here are\nsome examples of non-functional requirements that indicate how well-built software is:\n\n■ Maintainability/extensibility---How easy is it to add features or fix issues and\n\ndeploy your product?\n\n■ Reliability/availability---Can your consumers get what they need from your application consistently?\n\n■ Security---Is your consumer’s information safe when they use your application?\n■ Usability---Is your application intuitive and easy to use?\n\nFigure 8.1 shows the code “ilities” and where they fall in the development lifecycle.\n\nUpdate the\ncode\n\nBuild the\nproduct\n\nDeploy the\nproduct\n\nConsumers\nuse it\n\n• Extensibility\n• Maintainability\n\n• Maintainability\n\n• Maintainability\n\n• Usability\n• Security\n• Scalability\n• Reliability\n\nFigure 8.1 The code “ilities” illustrated through the life of a software product\n\n1 For much more detail, see en.wikipedia.org/wiki/Non-functional_requirement.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 2, "cols": 3, "data": [["Tool", "Measures", "Metrics"], ["", "Application monitoring\nAvailability\nReliability\nSecurity\nMaintainability\nSecurity", ""]], "markdown": "| Tool | Measures | Metrics |\n|---|---|---|\n|  | Application monitoring\nAvailability\nReliability\nSecurity\nMaintainability\nSecurity |  |"}], "table_count": 1, "chapter_num": "8", "chapter_title": "Measuring the technical quality of your software", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "CHAPTER 8 Measuring the technical quality of your software 8.2 Measuring the NFRs through the code “ilities” Figure 8.1 The code “ilities” illustrated through the life of a software product", "l2_summary": "CHAPTER 8 Measuring the technical quality of your software Application monitoring Uptime, response time 8.2 Measuring the NFRs through the code “ilities” • Extensibility\n• Maintainability Figure 8.1 The code “ilities” illustrated through the life of a software product", "prev_page": {"page_num": 178, "segment_id": "00178"}, "next_page": {"page_num": 180, "segment_id": "00180"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": true, "has_citations": false, "key_terms": ["software", "code", "application", "maintainability", "time", "security", "issues", "ilities", "product", "chapter"], "content_type": "theory|practice", "domain": "programming|security|architecture", "complexity": "intermediate", "companies": ["New Relic", "HyperSpin", "Splunk", "OWASP ZAP", "SonarQube", "Checkmarx"], "people": [], "products": [], "technologies": [], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["Page response time", "Uptime", "Response time", "Error rate", "Mean time between failures"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["New Relic", "HyperSpin", "Splunk", "OWASP ZAP", "SonarQube", "Checkmarx"], "topics": ["Measuring technical quality of software", "Non-functional requirements (NFRs)", "Code 'ilities"], "key_concepts": ["Application Performance Management (APM) tools", "CI/CD systems", "Maintainability, extensibility, reliability, security, usability"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00180", "page_num": 180, "segment": "Measuring the NFRs through the code “ilities”\n\n157\n\nManage\ntasks and\nbugs\n\nProject\ntracking\n\nManage\ncode and\ncollaboration\n\nGenerate\nbuilds and\nrun tests\n\nMove code\nacross\nenvironments\n\nSource\ncontrol\n\nContinuous\nintegration\n\nDeployment\ntools\n\nEnsure\neverything\nis working\n\nApplication\nmonitoring\n\nUpdate the\ncode\n\nBuild the\nproduct\n\nDeploy the\nproduct\n\nConsumers\nuse it\n\n• Extensibility\n• Maintainability\n\n• Maintainability\n\n• Maintainability\n\n• Usability\n• Security\n• Scalability\n• Reliability\n\nFigure 8.2 The code “ilities” in relation to the components of the build lifecycle and the tools used\nto measure them\n\nIf you look at this through the lens of the systems you’re using to measure the product\nand the process, you’ll see the chart shown in figure 8.2.\n\n Maintainability encompasses getting changes out to the consumer and all the\n\npieces of the build lifecycle that contribute to that.\n\n Usability tells you if the consumer gets what they need out of the system with the\nleast amount of friction possible. Wrapped up in that is whether the consumer has a\nsecure, reliable experience that can scale to meet demand.\n\n If you wrap extensibility with maintainability and you group security, scalability,\n\nand reliability under usability, you can redraw figure 8.2 as figure 8.3.\n\n With these in mind we’ll focus on the two biggest code “ilities” as we look into how\n\nto measure good software:\n\n■ Maintainability to represent how easily you can get changes out to the consumer\n\n■ Usability to represent customer satisfaction\n\nMaintainability\n\nUsability\n\nProject\ntracking\n\nSource\ncontrol\n\nContinuous\nintegration\n\nDeployment\ntools\n\nUpdate the\ncode\n\nBuild the\nproduct\n\nDeploy the\nproduct\n\nApplication\nmonitoring\n\nConsumers\nuse it\n\nFigure 8.3 Grouping systems used in the development lifecycle together by the two parent code\n“ilities”\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "8", "chapter_title": "Measuring the technical quality of your software", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "Measuring the NFRs through the code “ilities” Manage\ncode and\ncollaboration • Extensibility\n• Maintainability", "l2_summary": "Measuring the NFRs through the code “ilities” Manage\ncode and\ncollaboration Move code\nacross\nenvironments • Extensibility\n• Maintainability Figure 8.2 The code “ilities” in relation to the components of the build lifecycle and the tools used\nto measure them and reliability under usability, you can redraw figure 8.2 as figure 8.3.", "prev_page": {"page_num": 179, "segment_id": "00179"}, "next_page": {"page_num": 181, "segment_id": "00181"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": true, "has_citations": false, "key_terms": ["code", "maintainability", "product", "usability", "figure", "ilities", "build", "consumer", "tools", "lifecycle"], "content_type": "tutorial", "domain": "devops|architecture", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["code", "build lifecycle", "source control", "continuous integration", "deployment tools", "application monitoring"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["Maintainability", "Usability"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["project tracking", "source control", "continuous integration", "deployment tools", "application monitoring"], "topics": ["code 'ilities', maintainability, usability, build lifecycle components"], "key_concepts": ["code 'ilities', maintainability, usability, build lifecycle components"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00181", "page_num": 181, "segment": "158\n\nCHAPTER 8 Measuring the technical quality of your software\n\nMaintainability and extensibility\n\nIn the context of agile delivery, the ability to easily change and deploy your code is\nparamount. Often maintainability and extensibility are treated as separate entities.\nMaintainability refers to the effort needed to keep your software alive and running.\nExtensibility refers to the level of effort required to add new features or extend an application. In today’s agile world where continuous methods are becoming more common,\nthere’s no longer a clear line between maintenance and extension. For that reason I\nlike to combine these two properties in the context of agile projects and certainly in\nthe context of projects that practice CD.\n\n8.3 Measuring maintainability\n\nIn an agile and CD context, maintainability means more than updating code; it also\nmeans everything that goes into delivering changes to your consumers. As you saw in\nfigure 8.3, that includes your build and deploy systems. When you’re looking at maintainability you need to look at all of the properties of your codebase that contribute to\neasier code updates and faster deploy times.\n\n Maintainability encompasses your entire development cycle and as a result is best\nmeasured as the aggregate of data from several different systems. The main components of maintainability are outlined here:\n\n■ Mean time to repair (MTTR)---The measure of time from when you realize something is wrong in production, the issue is triaged, and a fix is determined and\ndeployed.\n\n■ Lead time---The measure of time between the definition of a new feature and\n\nwhen it gets to the consumer.\n\n■ Code coverage---The amount of code measured in LOC that is covered by a unit\n\ntest.\n\n■ Coding standard rules---How well your code adheres to standards of the language\n\nyou’re using.\n\n■ How much code must be changed for features or bug fixes---The CLOC associated with\n\ntasks that go all the way through the development cycle.\n\n■ Bug rates---The number of bugs that are generated as new features are being\n\ndelivered.\n\nIf maintainability is measured by the ability to make frequent changes, then the two\nmost important metrics in this list are MTTR and lead time because they both measure\nthe amount of time it takes to get changes to your consumers.\n\n8.3.1 MTTR and lead time\n\nAs we dig into MTTR and lead time, you’ll see other important metrics that affect\nthem. In figure 8.4 you start to see those other metrics emerging.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "8", "chapter_title": "Measuring the technical quality of your software", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "Maintainability and extensibility 8.3 Measuring maintainability 8.3.1 MTTR and lead time", "l2_summary": "Maintainability and extensibility In the context of agile delivery, the ability to easily change and deploy your code is\nparamount. Often maintainability and extensibility are treated as separate entities. 8.3 Measuring maintainability ■ Lead time---The measure of time between the definition of a new feature and 8.3.1 MTTR and lead time", "prev_page": {"page_num": 180, "segment_id": "00180"}, "next_page": {"page_num": 182, "segment_id": "00182"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["maintainability", "time", "code", "context", "agile", "mttr", "lead", "extensibility", "deploy", "features"], "content_type": "theory", "domain": "devops|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["agile delivery", "continuous deployment", "build and deploy systems", "unit test", "coding standards", "code coverage", "Mean time to repair (MTTR)", "Lead time", "Bug rates"], "frameworks": [], "methodologies": ["Agile", "Continuous Delivery"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["Mean time to repair (MTTR)", "Lead time", "Code coverage", "Coding standard rules", "How much code must be changed for features or bug fixes", "Bug rates"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Maintainability and extensibility in agile projects", "Measuring maintainability", "MTTR and lead time metrics"], "key_concepts": ["agile delivery", "continuous deployment", "maintainability", "extensibility", "Mean time to repair (MTTR)", "Lead time", "code coverage", "coding standards"], "problem_statement": "How to measure the technical quality of software in an agile and continuous deployment context", "solution_approach": "Using metrics such as MTTR, lead time, code coverage, coding standard adherence, and bug rates", "extraction_method": "lm"}}
{"segment_id": "00182", "page_num": 182, "segment": "Measuring maintainability\n\n159\n\n2 How much\n\n3 How long it\n\ncode changed\nfor the fix\n\nMTTR\n\ntakes to build\nand deploy\nchanges\n\nMaintainability\n\n4 End time, fix\nis in front of\nthe consumer\n\n1 Start time---\nsomething\nneeds to be fixed\n\nProject\ntracking\n\nSource\ncontrol\n\nContinuous\nintegration\n\nDeployment\ntools\n\nUpdate the\ncode\n\nBuild the\nproduct\n\nDeploy the\nproduct\n\nUsability\n\nApplication\nmonitoring\n\nConsumers\nuse it\n\nLead\ntime\n\n1 Start time---\n\n2\n\na task is defined\n\nHow much code\nchanged for\nthe update\n\n3\n\nHow long it takes\nto build and deploy\nchanges\n\n4 End time---\n\nupdate is in front\nof the consumer\n\nFigure 8.4 Components of lead time and MTTR as they move through the delivery process\n\nThe two key metrics to measure how fast you’re getting changes to consumers are:\n\n■ MTTR---How long it takes to get a small code change out to the consumer\n■ Lead time---How long it takes to get a new feature out to the consumer\n\nIn figure 8.4 we started breaking out the individual steps of these two metrics. For\nanother point of view, figure 8.5 mind maps what goes into them both.\n\n As illustrated in figure 8.5, the key difference between the two is that MTTR measures triage of the existing system; lead time measures the addition of new pieces to\n\nMaintainability\n\nMTTR\n\n--\n\nLead time\n\n--\n\nTriage the problem\n\nUpdate the code\n\nRun your tests\n\nDeploy\n\nMonitor the change\n\nGet clear requirements\n\nPrioritize the work\n\nUpdate the code\n\nRun your tests\n\nDeploy\n\nMonitor the change\n\nHow well do you\nunderstand your system?\n\nHow good is your\nautomated testing?\n\nHow good is your\ndeploy system?\n\nFigure 8.5 A mind map breakdown of what you need to measure maintainability\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "8", "chapter_title": "Measuring the technical quality of your software", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "How much code\nchanged for\nthe update How long it takes\nto build and deploy\nchanges How good is your\ndeploy system?", "l2_summary": "takes to build\nand deploy\nchanges 4 End time, fix\nis in front of\nthe consumer How much code\nchanged for\nthe update How long it takes\nto build and deploy\nchanges Figure 8.4 Components of lead time and MTTR as they move through the delivery process How good is your\ndeploy system?", "prev_page": {"page_num": 181, "segment_id": "00181"}, "next_page": {"page_num": 183, "segment_id": "00183"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["time", "code", "deploy", "mttr", "update", "lead", "figure", "maintainability", "long", "takes"], "content_type": "tutorial", "domain": "devops|architecture", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Source control", "Continuous integration", "Deployment tools"], "frameworks": [], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["MTTR", "Lead time"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Project tracking", "Source control", "Continuous integration", "Deployment tools"], "topics": ["Maintainability", "MTTR", "Lead time"], "key_concepts": ["MTTR", "Lead time", "Triage the problem", "Update the code", "Run your tests", "Deploy", "Monitor the change"], "problem_statement": "Measuring how fast changes and new features are delivered to consumers", "solution_approach": "Breaking down MTTR and Lead time into individual steps and understanding their components", "extraction_method": "lm"}}
{"segment_id": "00183", "page_num": 183, "segment": "160\n\nCHAPTER 8 Measuring the technical quality of your software\n\nthe system. Even though the systems measure similar things, finding these values\ncomes from different places. Lead time can be found from your project tracking system (PTS) alone, because that measures the time between when something was\ndefined and when something was completed, all of which is tracked with tickets in\nyour PTS. You can get lead time with the following formula:\n\nLead Time = PTS: Task Complete -- PTS: Task Start\n\nThe start time in MTTR is when a problem arises for your consumers. These problems\nare detected in your APM systems, the code changes are made in your SCM system, and\nthe update makes it through your build system and gets deployed. To keep it really\nsimple you can calculate MTTR with the following:\n\nMTTR = APM: Anomaly End - APM: Anomaly Start\n\nThis formula is great for giving you the big picture, but it glosses over a lot of the\ndetails that go into these time ranges. If you end up with an MTTR of 16 hours, the\nnext question will inevitably be, “How can we get faster at fixing our code?” You can\nstart by breaking down the phases of your delivery into the phases of your delivery\ncycle. If you go back to the steps defined in the mind map in figure 8.5, you can start\nvisualizing where you need to focus to continually improve your delivery times.\n\nNOTE MTTR and lead time are measured so similarly that for the rest of this\nsection I’ll focus on MTTR. In the case study at the end of this chapter you’ll\nfind an example of breaking down lead time.\n\nLet’s start with an MTTR of 35 hours over four releases. If you break that out to see the\namount of time it takes for each release, you may end up with something like figure 8.6.\n\nBuild time\nis consistent.\n\nTesting is the biggest\ntarget for time savings.\n\nDeploy time\nis consistent.\n\nTime to repair\n\n2.2.1\n\ns\ne\nx\ni\nf\n\nt\no\nH\n\n2.2.2\n\n2.3.1\n\n2.3.2\n\n0\n\n10\n\n20\nHours\n\n30\n\n40\n\nTriage\n\nDevelopment\n\nBuild\n\nTest\n\nDeploy\n\nTriage takes\na long time.\n\nDevelopment time\nis inconsistent.\n\nThe mean of all hot fix\ndeploys = MTTR of 35.\n\nFigure 8.6 An example breakdown of MTTR over four releases; testing takes much more time than\nanything else in the fix cycle.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 7, "cols": 14, "data": [["", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", "", ""]], "markdown": "|  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |  |"}], "table_count": 1, "chapter_num": "8", "chapter_title": "Measuring the technical quality of your software", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "You can get lead time with the following formula: Build time\nis consistent. Deploy time\nis consistent.", "l2_summary": "You can get lead time with the following formula: Lead Time = PTS: Task Complete -- PTS: Task Start The start time in MTTR is when a problem arises for your consumers. Build time\nis consistent. Deploy time\nis consistent. Development time\nis inconsistent.", "prev_page": {"page_num": 182, "segment_id": "00182"}, "next_page": {"page_num": 184, "segment_id": "00184"}, "extended_fields": {"has_code": false, "has_formulas": true, "has_diagram": true, "has_list": true, "has_citations": false, "key_terms": ["time", "mttr", "start", "lead", "system", "when", "something", "build", "hours", "delivery"], "content_type": "theory|practice", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["PTS", "APM", "SCM", "build system"], "frameworks": [], "methodologies": ["DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["Lead Time", "MTTR"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["PTS", "APM", "SCM", "build system"], "topics": ["Measuring technical quality of software", "Lead Time", "MTTR"], "key_concepts": ["Lead Time", "MTTR", "Project Tracking System (PTS)", "Application Performance Management (APM) systems", "Source Code Management (SCM) system"], "problem_statement": "Improving the speed and efficiency in fixing code issues", "solution_approach": "Breaking down MTTR into phases of delivery to identify areas for improvement", "extraction_method": "lm"}}
{"segment_id": "00184", "page_num": 184, "segment": "Measuring maintainability\n\n161\n\nIn this example the MTTR is over 30 hours, most of which is testing. It also takes a consistently long time to triage problems, and developing fixes doesn’t seem predictable.\nThe best parts of the process to address are the ones that are usually long; that way you\ncan get a consistently measurable benefit from addressing them.\n\n The example in figure 8.6 is a good example of code with poor maintainability:\n\n■ The system appears to be so complex it takes a very long time to triage. In my\nexperience if a problem takes longer than an hour to triage, your system is way\ntoo complex.\n\n■ Development time of fixes is unpredictable. Release 2.3.2 took a much longer\ntime to fix than the other releases; it would be worth drilling in to find out the\ncause.\n\n■ Test cycles that rely heavily on manual testing or a focus on complete regression\nfor small changes are a symptom of a lack of understanding of production\nbehavior.\n\nIn figure 8.6 it’s hard to tell where to start. If you want to get fixes out faster, it looks\nlike you should:\n\n■ Try to improve the test cycle.\n■ Figure out why it takes so long in triage to understand what’s wrong when the\n\nsystem breaks.\n\nFigure 8.7 shows a very different scenario that depicts predictability through a breakdown of MTTR.\n\nThe team\ncan get to the\ncause quickly.\n\nThis looks like a\nvery complex build.\n\nTesting and\ndeploying are\nreally fast.\n\nTime to repair\n\n2.2.1\n\n2.2.2\n\n2.3.1\n\ns\ne\nx\ni\nf\n\nt\no\nH\n\n2.3.2\n\n0\n\n5\n\n10\nHours\n\n15\n\n20\n\nDevelopment time\nseems reasonable.\n\nThe mean MTTR\ndropped to 14.5.\n\nTriage\n\nDevelopment\n\nBuild\n\nTest\n\nDeploy\n\nFigure 8.7 Another example breaking down MTTR over four releases: with testing fully automated, MTTR\ngoes way down. The next focus for improvement should be the build process.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 7, "cols": 13, "data": [["", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", "", "", "", "", "", ""]], "markdown": "|  |  |  |  |  |  |  |  |  |  |  |  |  |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |  |  |  |  |  |"}], "table_count": 1, "chapter_num": "8", "chapter_title": "Measuring the technical quality of your software", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "In this example the MTTR is over 30 hours, most of which is testing. ■ The system appears to be so complex it takes a very long time to triage. ■ Development time of fixes is unpredictable.", "l2_summary": "In this example the MTTR is over 30 hours, most of which is testing. It also takes a consistently long time to triage problems, and developing fixes doesn’t seem predictable. The example in figure 8.6 is a good example of code with poor maintainability: ■ The system appears to be so complex it takes a very long time to triage. ■ Development time of fixes is unpredictable. Figure 8.7 Another example breaking down MTTR over four releases: with testing fully automated, MTTR\ngoes way down.", "prev_page": {"page_num": 183, "segment_id": "00183"}, "next_page": {"page_num": 185, "segment_id": "00185"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": true, "has_citations": false, "key_terms": ["time", "mttr", "triage", "figure", "example", "testing", "takes", "long", "fixes", "system"], "content_type": "theory", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["MTTR", "testing", "triage", "fix development"], "frameworks": [], "methodologies": ["DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["MTTR"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["Maintainability", "MTTR", "Testing and Triage"], "key_concepts": ["MTTR", "Triage time", "Fix development time", "Complexity of the system"], "problem_statement": "Poor maintainability in a software system", "solution_approach": "Improving test cycles, reducing triage time, automating testing and deploying processes", "extraction_method": "lm"}}
{"segment_id": "00185", "page_num": 185, "segment": "162\n\nCHAPTER 8 Measuring the technical quality of your software\n\nIn figure 8.7 the team can get to the root of problems and update the code quickly,\nindicating the codebase is much more maintainable than that shown in figure 8.6. But\nthe build takes a really long time, so reducing build time is an actionable item the\nteam can take away from these measurements. In this case it may also be a good idea\nto cross-reference the shorter test cycle with metrics to ensure that your team has adequate test coverage and they’re not just skipping tests for the sake of speed.\n\n For the teams on the ground adding features and fixing problems, you can break\ninto even finer details like how much code has to change in these time ranges and\nwhat’s taking time inside the build process. For this detail you need to add more data.\n\n8.3.2\n\nAdding SCM and build data\n\nWhat gets built and how often is the next key indicator of a maintainable codebase.\nCommits with a high CLOC (changed lines of code) are a sign that it takes a lot of\nwork to change things. If you start to break down the elements of a change to your\nsoftware products, the number of lines of code it takes to fix things or deliver new features is one of the primary indicators of maintainability. A good target to have is a low\nCLOC, a high number of releases, and a low number of fixes. This ultimately shows that your\ncode is maintainable enough to easily make small tweaks and get in front of the consumer quickly.\n\n Let’s look at an example. A team is practicing CD and releasing updates to their\nconsumers three times a day. They keep track of their CLOC for each release, as shown\nin figure 8.8, which includes a month’s worth of hot-fix release data. Based on their\ntarget, is their code base maintainable?\n\n Figure 8.8 shows the following statistics over the course of a month with 60 releases\n\nmade by the CD system:\n\n■ Ten commits were made and different hotfixes were deployed.\n■ Of those ten commits, on average only six lines of code were added and four\n\nwere removed.\n\n■ Therefore 16% (10 in 60) of the total releases were hot fixes.\n\nIf you assume that the CD system makes 3 releases in an 8-hour day with about 3 hours\nbetween releases, then the following are true:\n\nFigure 8.8 The amount of\ncode that changes for all hot\nfixes in a month\n\n10 total commits\n\nOn average fixes\nare very small.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "8", "chapter_title": "Measuring the technical quality of your software", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "In figure 8.7 the team can get to the root of problems and update the code quickly,\nindicating the codebase is much more maintainable than that shown in figure 8.6. ■ Therefore 16% (10 in 60) of the total releases were hot fixes. Figure 8.8 The amount of\ncode that changes for all hot\nfixes in a...", "l2_summary": "In figure 8.7 the team can get to the root of problems and update the code quickly,\nindicating the codebase is much more maintainable than that shown in figure 8.6. Commits with a high CLOC (changed lines of code) are a sign that it takes a lot of\nwork to change things. Based on their\ntarget, is their code base maintainable? Figure 8.8 shows the following statistics over the course of a month with 60 releases ■ Therefore 16% (10 in 60) of the total releases were hot fixes. Figure 8.8 The amount of\ncode that changes for all hot\nfixes in a month", "prev_page": {"page_num": 184, "segment_id": "00184"}, "next_page": {"page_num": 186, "segment_id": "00186"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["code", "figure", "releases", "team", "maintainable", "build", "time", "commits", "fixes", "takes"], "content_type": "theory|practice", "domain": "devops|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["build process", "SCM", "CD"], "frameworks": [], "methodologies": ["DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["CLOC", "number of releases", "number of fixes"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["measuring technical quality of software", "code maintainability", "build process optimization", "continuous delivery"], "key_concepts": ["CLOC", "high CLOC as a sign of un-maintainable code", "low number of fixes and high number of releases as indicators of maintainability", "cross-referencing test cycle with metrics for adequate coverage"], "problem_statement": "long build times and potential lack of adequate test coverage", "solution_approach": "breaking down the build process, tracking CLOC per release, and ensuring adequate test coverage", "extraction_method": "lm"}}
{"segment_id": "00186", "page_num": 186, "segment": "Measuring maintainability\n\n163\n\n■ For the 10 releases that needed to be fixed, for about 30 hours in the month (out\nof 730 total hours in a month) the code was in a state that needed to be fixed.\n■ That’s just 4% of the entire month when the released version of the product\n\nhad a problem.\n\nPretty good.\n\n As a counter example, let’s take a team that releases their software every two weeks\nwith 1 day of regression testing and a 4-hour release at the end of their development\ncycle. To complete 10 releases would take 10 full days of regression testing and an\nadditional 5 days of release activity, and that’s not even measuring the time it would\ntake to find problems and fix code. That would mean that over half of their time was\nused fixing bugs no matter what the CLOC was on the fixes.\n\n Realistically there are potential outcomes in these scenarios:\n\n■ Teams don’t fix bugs because they know they won’t be able to complete any\n\nmore features if they spend all of their time testing and releasing code.\n\n■ Teams jam as many fixes as possible into a release, and thus you’ll see a very\nhigh CLOC on hot fixes. This results in longer test cycles and bigger releases,\nboth of which are indicators of code with poor maintainability.\n\nTo make sure CLOC and number of fixes are relevant, ensure that you’re also counting total deployments so you can get a percentage of fix to release (FRP). To summarize, the formula for that is\n\nFix Release Percentage (FRP) = Total fixes / releases\n\nYou want your fix release percentage to be as low as possible. A 0 is perfect and anything over .5 is usually pretty bad. By combining MTTR and FRP you can calculate a\nmaintainable release rating with this formula:\n\nMaintainable Release = MTTR(in minutes) * (Total Fixes / Releases)\n\nIn this case the closer to 0 you are the better. Continuing the initial example, if you have\n60 releases and 10 fixes, you have a 16% FRP. If the same team has a 4-hour MTTR, that\nteam then has a maintainable release rating of 40. To compare this to a bad number, if\nyou have a team that has a hot fix with every release and an MTTR of 12 hours, that\nwould give you a maintainability release rating of 720. Due to the broad range of potential values, this metric becomes a good benchmark that your team can focus on improving over time rather than targeting a specific value out of the gate. Table 8.1 shows\nexample inputs and outputs for the maintainable release rating (MRR).\n\nTable 8.1 Example inputs and outputs for a maintainable release rating\n\nInputs\n\nMTTR = 240 minutes\nTotal fixes = 3\nTotal releases = 30\n\nMRR\n\n24\n\nNotes\n\nGiven a 4-hour release time, this team needs to fix only\n1 in every 3 releases, so the rating isn’t bad.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 2, "cols": 3, "data": [["Inputs", "MRR", "Notes"], ["", "24", ""]], "markdown": "| Inputs | MRR | Notes |\n|---|---|---|\n|  | 24 |  |"}], "table_count": 1, "chapter_num": "8", "chapter_title": "Measuring the technical quality of your software", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "Fix Release Percentage (FRP) = Total fixes / releases Maintainable Release = MTTR(in minutes) * (Total Fixes / Releases) MTTR = 240 minutes\nTotal fixes = 3\nTotal releases = 30", "l2_summary": "Fix Release Percentage (FRP) = Total fixes / releases By combining MTTR and FRP you can calculate a\nmaintainable release rating with this formula: Maintainable Release = MTTR(in minutes) * (Total Fixes / Releases) If the same team has a 4-hour MTTR, that\nteam then has a maintainable release rating of 40. Table 8.1 Example inputs and outputs for a maintainable release rating MTTR = 240 minutes\nTotal fixes = 3\nTotal releases = 30", "prev_page": {"page_num": 185, "segment_id": "00185"}, "next_page": {"page_num": 187, "segment_id": "00187"}, "extended_fields": {"has_code": false, "has_formulas": true, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["release", "releases", "fixes", "total", "team", "rating", "time", "mttr", "maintainable", "code"], "content_type": "theory|practice", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["Fix Release Percentage (FRP)", "Maintainable Release"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["Code Maintainability", "Release Management", "Bug Fixing Efficiency"], "key_concepts": ["Fix Release Percentage (FRP)", "Maintainable Release Rating", "MTTR"], "problem_statement": "Improving code maintainability and reducing bug fixing time", "solution_approach": "Calculating FRP and MRR to measure and improve release management", "extraction_method": "lm"}}
{"segment_id": "00187", "page_num": 187, "segment": "164\n\nCHAPTER 8 Measuring the technical quality of your software\n\nTable 8.1 Example inputs and outputs for a maintainable release rating (continued)\n\nInputs\n\nMTTR = 240 minutes\nTotal fixes = 10\nTotal releases = 5\n\nMTTR = 480 minutes\nTotal fixes = 1\nTotal releases = 100\n\n8.3.3\n\nCode coverage\n\nMRR\n\n480\n\nNotes\n\nEven though this team has a 4-hour release time, they\nhave 2 fixes per release on average, which is really bad.\n\n4.8\n\nThis team has a higher MTTR, but they rarely have to fix\nthings (once in 100 releases), so they’re doing great.\n\nCode coverage is the percentage of your codebase that’s covered by automated tests. You\ncan measure code coverage during the build process with a number of tools that run\nduring the build. Some examples are Cobertura, JaCoCo, Clover, NCover, and Gcov.\nI’ll take advantage of the comprehensive dashboards in SonarQube to show you how\nto turn code coverage reports into actionable data. An example coverage report is\nshown in figure 8.9.\n\n In theory, if you have great test coverage, then your project is more maintainable\nbecause developers can find out if their changes affected the rest of the system by simply\nrunning their unit tests. The tricky thing about code coverage is that it tells you how\n\nTotal coverage\n\nCoverage of\neach package in\nthe project\n\nClicking a file\nloads details\nbelow.\n\nCoverage of\neach class in\nthe project\n\nCreated by cwhd7 on 10/2/14.\n\nThis panel will show the detailed\nbreakdown of coverage for a specific class.\n\nFigure 8.9 An example breakdown of code coverage in SonarQube\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 2, "cols": 3, "data": [["Inputs", "MRR", "Notes"], ["", "480\n4.8", ""]], "markdown": "| Inputs | MRR | Notes |\n|---|---|---|\n|  | 480\n4.8 |  |"}], "table_count": 1, "chapter_num": "8", "chapter_title": "Measuring the technical quality of your software", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "An example coverage report is\nshown in figure 8.9. Coverage of\neach class in\nthe project Figure 8.9 An example breakdown of code coverage in SonarQube", "l2_summary": "MTTR = 240 minutes\nTotal fixes = 10\nTotal releases = 5 MTTR = 480 minutes\nTotal fixes = 1\nTotal releases = 100 An example coverage report is\nshown in figure 8.9. Coverage of\neach package in\nthe project Coverage of\neach class in\nthe project Figure 8.9 An example breakdown of code coverage in SonarQube", "prev_page": {"page_num": 186, "segment_id": "00186"}, "next_page": {"page_num": 188, "segment_id": "00188"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["coverage", "code", "total", "example", "release", "mttr", "fixes", "releases", "have", "project"], "content_type": "theory|practice", "domain": "programming|devops", "complexity": "intermediate", "companies": [], "people": ["cwhd7", "Mark Watson"], "products": [], "technologies": ["automated tests", "Cobertura", "JaCoCo", "Clover", "NCover", "Gcov", "SonarQube"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["MTTR", "Total fixes", "Total releases"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["SonarQube"], "topics": ["code coverage", "release quality metrics", "maintainability"], "key_concepts": ["code coverage", "MTTR (Mean Time To Repair)", "total fixes and releases"], "problem_statement": "Measuring the technical quality of software through maintainability ratings, code coverage, and release time", "solution_approach": "Using tools like SonarQube to analyze code coverage and other metrics", "extraction_method": "lm"}}
{"segment_id": "00188", "page_num": 188, "segment": "Measuring maintainability\n\n165\n\nThis is covered and\nsurvived mutation.\n\nThis code has line coverage\nbut wasn’t mutated.\nThis code failed\nmutation tests.\n\nThis code has no\ncoverage at all.\n\nFigure 8.10 An example mutation test report using Pitest\n\nmuch of your codebase is exercised by tests, not how good your tests are. If you write a\ntest that runs a method but doesn’t assert anything to see if the result is the expected\none, then that method is considered covered, even though the test doesn’t check anything. There are two things you can do to your code coverage metric to add more value:\n\n■ Mutation testing---Comparing test results with test results after the underlying\n\ncode has changed or mutated\n\n■ Adding data points to show the value of coverage---Using data from PTS, SCM, and CI\n\nMutation testing is an automated test for your tests. Essentially it messes with your\ncode before your unit tests run. This way, if your tests pass after inserting blatant\nerrors, you know that even though a test is executing your code, it’s not really testing\nyour code. A great tool for this is Pitest (pitest.org/). An example screenshot of a\nPitest analysis is shown in figure 8.10.\n\n Another strategy for vetting your code coverage numbers is to track it alongside\nother metrics. When your codebase has a great suite of automated tests, that usually\ntranslates into a fix release percentage and a maintainable release rating. If you have\ngreat coverage but your releases aren’t going well, that’s a good sign that either you’re\nnot testing the right things or your tests are bad.\n\n Often code coverage gets lumped in with static code analysis because the two\nreports are typically generated at the same time. The two are complementary but different measurements of a maintainable codebase.\n\n8.3.4\n\nAdding static code analysis\n\nStatic code analysis can check your code for best practices against common rule sets\nfor whatever language you’re using. A number of tools can do this for you. SonarQube\nis a great option.\n\n As I mentioned in chapter 5, SonarQube in Action, published by Manning, is a great\nbook with a lot of details, but I’ll give you a quick rundown of maintainability highlights that you can start using today.\n\n The easiest things to look at are these:\n\n■ Lines of code---A large codebase is usually more complicated to build and deploy.\nThis usually also indicates that your codebase does a lot of things, which usually\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "8", "chapter_title": "Measuring the technical quality of your software", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "This code failed\nmutation tests. This code has no\ncoverage at all. code has changed or mutated", "l2_summary": "This code has line coverage\nbut wasn’t mutated. This code failed\nmutation tests. This code has no\ncoverage at all. code has changed or mutated Mutation testing is an automated test for your tests. Adding static code analysis", "prev_page": {"page_num": 187, "segment_id": "00187"}, "next_page": {"page_num": 189, "segment_id": "00189"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["code", "tests", "coverage", "test", "mutation", "codebase", "great", "using", "pitest", "things"], "content_type": "theory", "domain": "programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": ["Pitest", "SonarQube"], "technologies": ["mutation testing", "static code analysis"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["fix release percentage", "maintainable release rating"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Pitest", "SonarQube"], "topics": ["mutation testing", "static code analysis", "code coverage", "maintainability"], "key_concepts": ["line coverage", "mutation tests", "mutation testing", "static code analysis", "best practices", "metrics for maintainability"], "problem_statement": "Improving the quality and reliability of software through better testing practices", "solution_approach": "Implementing mutation testing and static code analysis tools to enhance code coverage and identify potential issues", "extraction_method": "lm"}}
{"segment_id": "00189", "page_num": 189, "segment": "166\n\nCHAPTER 8 Measuring the technical quality of your software\n\nsets your team up for conflicting changes when different developers or teams\nare working on different features. For better maintainability it’s good to keep\nyour modules small and focused.\n\n■\n\n■ Duplications---The classic problem with duplicate code is that if you change it in\none place and not another, you end up with a bug where you didn’t change the\ncode. Duplications also directly conflict with coding practices of modularity and\nreusability (a few more “ilities” that fit under maintainability). Ideally you\nshouldn’t have duplicate code.\nIssues---These will measure your code against the coding standards for whatever\nlanguage you’re using. The better developers are at writing code aligned with\nstandards, the easier it is to make changes and the easier it is for new developers\nor other teams to jump in and make changes. Static analysis tools all have classifications of issues from really important to not so important; at a minimum you\nshould ensure that the really important issues are kept at zero.\n\n■ Complexity---Also known as cyclomatic complexity or the amount of nested code\nyou have. If you have an if statement nested inside a loop and two other if\nstatements, you’ll have a high cyclomatic complexity. This code is hard to read,\ndebug, and test. Ideally you want to keep complexity as low as possible.\n\nFigure 8.11 shows a screenshot of SonarQube and some default key statistics to be\naware of.\n\n All of the metrics are really easy to read, and you can see how they change over\ntime. Figure 8.11 has an interesting anomaly; over the last 30 days no code has\n\nHow big is this codebase;\nsmaller is easier to maintain.\n\nFollowing code standards\nis an indicator of good\nmaintainability.\n\nHigh duplications make the\ncode less maintainable.\n\nLower complexity equates\nto higher maintainability.\n\nFigure 8.11 Key metrics from static analysis that indicate maintainability\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "8", "chapter_title": "Measuring the technical quality of your software", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "Ideally you\nshouldn’t have duplicate code. ■ Complexity---Also known as cyclomatic complexity or the amount of nested code\nyou have. Following code standards\nis an indicator of good\nmaintainability.", "l2_summary": "Ideally you\nshouldn’t have duplicate code. ■ Complexity---Also known as cyclomatic complexity or the amount of nested code\nyou have. This code is hard to read,\ndebug, and test. Figure 8.11 has an interesting anomaly; over the last 30 days no code has Following code standards\nis an indicator of good\nmaintainability. High duplications make the\ncode less maintainable.", "prev_page": {"page_num": 188, "segment_id": "00188"}, "next_page": {"page_num": 190, "segment_id": "00190"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["code", "maintainability", "have", "complexity", "changes", "developers", "duplications", "change", "issues", "standards"], "content_type": "theory", "domain": "programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["SonarQube"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["code duplications", "issues", "complexity"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["SonarQube"], "topics": ["technical quality of software", "maintainability", "code standards", "cyclomatic complexity"], "key_concepts": ["modularity", "reusability", "static analysis", "code duplications", "issues", "complexity"], "problem_statement": "conflicting changes in a codebase", "solution_approach": "using tools like SonarQube to measure and improve maintainability", "extraction_method": "lm"}}
{"segment_id": "00190", "page_num": 190, "segment": "Measuring maintainability\n\n167\n\nchanged but the number of issues and estimated tech debt have decreased. That’s\nprobably because this team ran their analysis and then changed the rule set by which\nthe issues were measured. Updating code standards is a great task for any development team because it forces the team to look at the rules, discuss them, and decide\nwhat makes the most sense for the way they write code.\n\n A metric that I’m not diving into is estimated technical debt. SonarQube has builtin algorithms that will calculate how long it should take your team to fix all the issues\nidentified by static analysis. Even though this is a really cool feature of SonarQube, it\ndoesn’t take into account bigger pictures like potential problems with your build system or architectural issues related to integration between code modules.\n\nBalancing tech debt and delivery\n\nIn chapter 3 our case study addressed identifying tech debt trends using PTS data.\nIt’s important to use this data to find when poor maintainability is starting to creep\ninto your software products because it’s easy to tie to the bottom line. If you’re tracking\nhow the speed of delivery slows down as a result of tech debt, it’s usually not that\ntough to get buy-in from project sponsors to clean it up to ensure you can keep delivering at your best rate instead of constantly slowing down. You can add the data-tracking maintainability into your tech debt impact analysis to break it down into small\nenough chunks for your development team to take action on, and you can use the\nPTS data as an indicator to show your sponsors how it affects their project. You’ll see\nmore on this in chapter 9.\n\nStatic analysis is awesome and every development team should use it. But like everything else, static code analysis alone can’t give you the complete picture. Some problems with using static code analysis alone are these:\n\n■ Even if the code is easy to update and has great tests, that doesn’t make it easy\n\nto deploy.\n\n■ There could be integration issues with the larger systems that you can’t see from\n\nstatic analysis.\n\n■ This doesn’t check the behavior of the application.\n\nAs with the rest of your maintainability metrics, static code analysis is a great tool to\nuse to give you actionable insight into your codebase but doesn’t stand alone as the\nde facto measurement for maintainability.\n\n What static analysis alone doesn’t give you is the correlation between the textbook\nquality of your code and its actual performance, or how it relates to the productivity of\nyour team.\n\n8.3.5\n\nAdding more PTS data\n\nAdding PTS data will help round out the picture to show how all of these metrics affect\nhow your team is delivering. We covered how to collect these metrics and their imporLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "8", "chapter_title": "Measuring the technical quality of your software", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "Static analysis is awesome and every development team should use it. But like everything else, static code analysis alone can’t give you the complete picture. Some problems with using static code analysis alone are these:", "l2_summary": "Measuring maintainability Balancing tech debt and delivery In chapter 3 our case study addressed identifying tech debt trends using PTS data. Static analysis is awesome and every development team should use it. But like everything else, static code analysis alone can’t give you the complete picture. Some problems with using static code analysis alone are these:", "prev_page": {"page_num": 189, "segment_id": "00189"}, "next_page": {"page_num": 191, "segment_id": "00191"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["analysis", "team", "code", "static", "debt", "data", "maintainability", "issues", "tech", "doesn"], "content_type": "theory|practice", "domain": "programming|devops", "complexity": "intermediate", "companies": ["null"], "people": ["Mark Watson"], "products": ["SonarQube"], "technologies": ["static analysis", "PTS data"], "frameworks": ["null"], "methodologies": ["null"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["tech debt", "maintainability"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["SonarQube"], "topics": ["measuring maintainability", "balancing tech debt and delivery", "static analysis limitations"], "key_concepts": ["tech debt", "maintainability metrics", "PTS data"], "problem_statement": "How to measure and balance technical debt with code maintainability in software development.", "solution_approach": "Using tools like SonarQube for static analysis, tracking tech debt trends with PTS data, and understanding the limitations of static analysis.", "extraction_method": "lm"}}
{"segment_id": "00191", "page_num": 191, "segment": "168\n\nCHAPTER 8 Measuring the technical quality of your software\n\ntance in previous chapters, so here I’ll highlight how they tie back into indicating how\nmaintainable your codebase is:\n\n■ Bug rates---If you see lots of bugs being generated as you’re delivering good\ncode, that’s an indication your code isn’t maintainable. You’ll usually see bug\nrates parallel poor code coverage, high issues from static analysis, and high\nMTTR and lead times.\n\n■ Recidivism---Recidivism directly impacts MTTR and lead time because as things\ngo backward in the workflow, you’re effectively doing the same things more\nthan once. If a developer says something is done but it ends up getting rejected\nby QA, that development work needs to be done again, impacting your delivery\ntimes. High recidivism typically parallels high bug rates and can be affected by\nall the other metrics we’ve discussed relating to maintainability.\n\n■ Velocity---All of the metrics discussed in this chapter potentially affect velocity.\nIt’s possible to have a steady cadence of delivery on a terrible codebase. In that\ncase you’ll have a team that’s not reaching their potential output. If you see a\nconsistent velocity with terrible maintainability metrics, then you should think\nabout how you can make your codebase more maintainable to get more done.\n\nIf your application is very maintainable, you should be able to deliver to your consumers consistently and with high quality. Delivery is the first half of the equation; now\nyou have to make sure that what you’re delivering is what your consumers want and\nsomething they can use.\n\n8.4 Measuring usability\n\nThere are several different “ilities” that demonstrate how usable your application is or\nhow good an experience you’re giving to your consumers. The big ones we’ll focus on\nare outlined in figure 8.12.\n\n Most of these metrics measure how well your application is performing. Performance metrics are important because they give you real data on how healthy your\n\nMaintainability\n\nUsability\n\nProject\ntracking\n\nSource\ncontrol\n\nContinuous\nintegration\n\nDeployment\ntools\n\nUpdate the\ncode\n\nBuild the\nproduct\n\nDeploy the\nproduct\n\nApplication\nmonitoring\n\nConsumers\nuse it\n\nFigure 8.12 Once something has been delivered, you need to measure it to make sure it’s good.\n\n• Security\n• Customer interaction\n• Scalability\n• Reliability\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "8", "chapter_title": "Measuring the technical quality of your software", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "tance in previous chapters, so here I’ll highlight how they tie back into indicating how\nmaintainable your codebase is: If you see a\nconsistent velocity with terrible maintainability metrics, then you should think\nabout how you can make your codebase more maintainable to get more done. Most of...", "l2_summary": "tance in previous chapters, so here I’ll highlight how they tie back into indicating how\nmaintainable your codebase is: ■ Bug rates---If you see lots of bugs being generated as you’re delivering good\ncode, that’s an indication your code isn’t maintainable. High recidivism typically parallels high bug rates and can be affected by\nall the other metrics we’ve discussed relating to maintainability. ■ Velocity---All of the metrics discussed in this chapter potentially affect velocity. If you see a\nconsistent velocity with terrible maintainability metrics, then you should think\nabout how you can make your codebase more maintainable to get more done. Most of these metrics measure how well your application is performing.", "prev_page": {"page_num": 190, "segment_id": "00190"}, "next_page": {"page_num": 192, "segment_id": "00192"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": true, "has_citations": false, "key_terms": ["high", "metrics", "maintainable", "code", "application", "consumers", "codebase", "rates", "good", "recidivism"], "content_type": "theory", "domain": "programming|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["codebase", "static analysis", "MTTR", "lead times"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["bug rates", "recidivism", "velocity"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Maintainability", "Usability", "Metrics"], "key_concepts": ["Bug rates", "Recidivism", "Velocity", "MTTR", "Lead times", "Application monitoring"], "problem_statement": "Measuring the technical quality and usability of software", "solution_approach": "Analyzing maintainability metrics, ensuring consistent delivery, and measuring application performance", "extraction_method": "lm"}}
{"segment_id": "00192", "page_num": 192, "segment": "Measuring usability\n\n169\n\nApplication monitoring\n\nUsability\n\nBusiness success metrics\n\n--\n\n--\n\nSecurity\n\nScalability\n\nReliability\n\nConsumer interaction\n\nDoes the\napplication\nwork well?\n\nConsumer interaction\n\nDoes it do what\nit’s supposed to?\n\nFigure 8.13 Mind mapping the parent-child relationships of usability and data\n\napplication is, and combined with business success metrics and customer satisfaction\nthey show you where you can improve your product to make your consumers happy.\n\n To show how to break maintainability into measureable and actionable items, I\nstarted with the high-level indicators---MTTR and lead time---and then dove into the\nelements that affect those metrics. For usability the parent indicators of success are\nthe business success metrics as defined for your application and how consumers interact with your site, as shown in figure 8.13.\n\n In chapter 6 I talked about using arbitrary metrics in your logs or from instrumented code to measure the value your consumers are getting from your application.\nThese metrics reflect your functional requirements, or the specific data points that\ntell you if your application is providing value to your consumers. Because an entire\nchapter is devoted to high-level metrics to track, I’ll jump right into the elements that\naffect consumer satisfaction and business success criteria and where to get them.\n\n8.4.1 Reliability and availability\n\nReliability and availability are often grouped together when the code “ilities” are discussed. I used to think they were the same thing, but when I measure them they definitely point to different but closely related factors.\n\n Availability is the measure of how much time your application is functioning relative to how frequently your consumers want to use it. If you have a web application for\na global user base, then your availability needs to be as close to 100% as possible\nbecause people from different time zones will be logging in and using it as the globe\nspins. In addition, you may have contracts with partners that specify service level\nagreements (SLAs) that bind you to supporting availability times of 99.999%, or about\n4 hours a month. Conversely, if your application is used in a retail location that is\nopen from 7 a.m. to 9 p.m. seven days a week, it probably doesn’t matter if it’s not\navailable from 9:01 p.m. to 6:59 a.m. every day. Availability can be measured by the following metrics:\n\n■ Uptime---What percentage of the time your application is functioning.\n■ Page response time---If your application is so slow that pages aren’t loading in the\n\ntime your consumer expects, you can consider it unavailable.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "8", "chapter_title": "Measuring the technical quality of your software", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "Application monitoring Business success metrics 8.4.1 Reliability and availability", "l2_summary": "Application monitoring Business success metrics Does the\napplication\nwork well? For usability the parent indicators of success are\nthe business success metrics as defined for your application and how consumers interact with your site, as shown in figure 8.13. 8.4.1 Reliability and availability Availability can be measured by the following metrics:", "prev_page": {"page_num": 191, "segment_id": "00191"}, "next_page": {"page_num": 193, "segment_id": "00193"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["application", "metrics", "time", "availability", "success", "consumers", "usability", "business", "consumer", "reliability"], "content_type": "tutorial", "domain": "architecture|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["MTTR", "lead time", "consumer satisfaction", "business success metrics"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["usability", "application monitoring", "reliability and availability"], "key_concepts": ["usability", "business success metrics", "consumer interaction", "MTTR", "lead time", "availability", "uptime", "page response time"], "problem_statement": "Improving application usability and reliability to enhance business success and consumer satisfaction", "solution_approach": "Measuring and improving key performance indicators such as MTTR, lead time, and consumer interaction metrics", "extraction_method": "lm"}}
{"segment_id": "00193", "page_num": 193, "segment": "170\n\nCHAPTER 8 Measuring the technical quality of your software\n\nReliability means how consistently your application does what it’s supposed to do. If\nyou have intermittent issues with your application, it’s not reliable. For example, if you\nhave an e-commerce application where you can’t add items to the shopping cart when\nthe site is under heavy load, it’s not very reliable. You can measure reliability with the\nfollowing metrics:\n\n■ Mean time between failures---How frequently your application breaks for your consumers.\n\n■ Response time---If your response time isn’t consistent, then your application isn’t\n\nreliable.\n\n■ Error rate---By monitoring your logs, you can see how many errors you have over\n\ntime.\n\nYou could have an application that is up all the time (highly available) but doesn’t\nfunction correctly 50% of the time (low reliability). You could also have an application\nthat does what it’s supposed to do all the time (highly reliable) but is down for maintenance for an hour a day (low availability).\n\n Even though the two are different, there’s overlap in how they’re measured. Two\nproducts that help measure uptime are Hyperspin (/en/) and\nNew Relic (/). Example reports from Hyperspin in figure 8.14 show\nthe data you need to measure availability.\n\n As you can see, you have uptime, downtime, and the number of outages, and you\n\ncan click into the report to get details on outage history.\n\n New Relic has a similar feature that creates an availability report. The New Relic\n\nversion is shown in figure 8.15.\n\n As you can see, availability is straightforward to measure and there are plenty of\ntools that can monitor it for you. Figure 8.15 also shows that New Relic can give you\nerror rates for your application.\n\n Another way to get error rate and mean time between failures is through log analysis. Figure 8.16 shows a simple query in Splunk (/) that can get the\nerror rate as output by the logs.\n\nFigure 8.14 Example basic report from Hyperspin\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "8", "chapter_title": "Measuring the technical quality of your software", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "If\nyou have intermittent issues with your application, it’s not reliable. You can measure reliability with the\nfollowing metrics: Figure 8.15 also shows that New Relic can give you\nerror rates for your application.", "l2_summary": "If\nyou have intermittent issues with your application, it’s not reliable. You can measure reliability with the\nfollowing metrics: ■ Mean time between failures---How frequently your application breaks for your consumers. ■ Response time---If your response time isn’t consistent, then your application isn’t You could have an application that is up all the time (highly available) but doesn’t\nfunction correctly 50% of the time (low reliability). Figure 8.15 also shows that New Relic can give you\nerror rates for your application.", "prev_page": {"page_num": 192, "segment_id": "00192"}, "next_page": {"page_num": 194, "segment_id": "00194"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["application", "time", "have", "figure", "reliable", "measure", "error", "availability", "relic", "reliability"], "content_type": "theory|practice", "domain": "devops|architecture", "complexity": "intermediate", "companies": ["Hyperspin", "New Relic"], "people": [], "products": [], "technologies": [], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["Mean time between failures", "Response time", "Error rate"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Hyperspin", "New Relic", "Splunk"], "topics": ["Reliability", "Availability", "Error Rate Measurement"], "key_concepts": ["Mean time between failures", "Response time", "Error rate", "Uptime", "Downtime", "Outages"], "problem_statement": "Measuring the technical quality of software, specifically reliability and availability", "solution_approach": "Using tools like Hyperspin, New Relic, and Splunk for monitoring and analysis", "extraction_method": "lm"}}
{"segment_id": "00194", "page_num": 194, "segment": "Measuring usability\n\n171\n\nThe throughput line represents\ntraffic; this spike could represent\na launch or campaign.\n\nThis report also includes\nerrors and throughput.\n\nThere aren’t many errors\nin this application.\n\nThe background\nrepresents uptime.\n\nFigure 8.15 New Relic’s version of visualizing and reporting availability\n\nSearch for whatever\nyou want.\n\nFigure 8.16 Visualizing error rate using Splunk\n\nYou can see the error\nrate over time.\n\nSplunk and New Relic have rich APIs that offer reliability and availability data to visualize along with the rest of your data. Regardless of which tools you use, measuring the\nhealth of your application by availability and reliability produces core metrics that tell\nyou how well you software is working for your consumer.\n\n8.4.2\n\nSecurity\n\nYour consumers expect to be in a secure place where their personal information isn’t\nin jeopardy of being stolen. I list security under usability because if you don’t have a\nsecure site, then no one will want to use it. Keeping your customers secure and their\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "8", "chapter_title": "Measuring the technical quality of your software", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "Figure 8.15 New Relic’s version of visualizing and reporting availability Figure 8.16 Visualizing error rate using Splunk Keeping your customers secure and their", "l2_summary": "This report also includes\nerrors and throughput. There aren’t many errors\nin this application. Figure 8.15 New Relic’s version of visualizing and reporting availability Figure 8.16 Visualizing error rate using Splunk Splunk and New Relic have rich APIs that offer reliability and availability data to visualize along with the rest of your data. Keeping your customers secure and their", "prev_page": {"page_num": 193, "segment_id": "00193"}, "next_page": {"page_num": 195, "segment_id": "00195"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["availability", "secure", "measuring", "usability", "throughput", "represents", "errors", "application", "figure", "relic"], "content_type": "theory", "domain": "devops|security", "complexity": "intermediate", "companies": ["New Relic", "Splunk"], "people": [], "products": [], "technologies": [], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": false, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["throughput", "errors", "uptime"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["New Relic", "Splunk"], "topics": ["usability", "security", "availability and reliability metrics"], "key_concepts": ["throughput line", "errors", "uptime", "visualizing and reporting availability", "error rate visualization"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00195", "page_num": 195, "segment": "172\n\nCHAPTER 8 Measuring the technical quality of your software\n\ndata safe is one of the most critical things to keep in mind if you’re in the business of\nbuilding consumer-facing software products.\n\n “Hack yourself first” is a good mantra to ensure you’re testing your site for security\n\nholes. The following tools are popular options for ensuring your site is secure:\n\n■ Static code analysis---SonarQube has some rules on ensuring your application is\nsecure, but there are other static analysis tools that specialize in this. A few popular ones are Checkmarx (/), Coverity (www.coverity\n.com/), and Fortify.2\n\n■ Dynamic code analysis---Examples are Veracode3 and WhiteHat (www.whitehatsec\n\n.com/). A great open source option is OWASP Zed Attack Proxy4 (ZAP).\n\nSecurity is a big enough topic for a completely separate book, but if you’re not sure\nwhere to start, here are some tips.\n\n Memorize the Open Web Application Security Project (OWASP) top 10.5 Note that\nthe details on this page sometimes gets out of date but the list remains valid. There’s a\nlist for mobile applications as well as one for web applications. These lists give you the\n10 most important things to keep in mind as you’re designing a secure software product.\n OWASP ZAP is a great way get started doing security scanning on your application.\nZAP will use a spider to crawl through your site and use common hacking techniques to\nattack it and report back on vulnerabilities. An example of ZAP is shown in figure 8.17.\n\nList of requests\nmade by ZAP\n\nDetails on a specific\nrequest and response\n\nSecurity holes are\nflagged as they’re found\n\nFigure 8.17 An example screenshot of OWASP ZAP\n\n2 Fortify static code analyzer, www8.hp.com/us/en/software-solutions/static-code-analysis-sast/index.html?.\n3 Executing data in real time,/products/dynamic-analysis-dast/dynamic-analysis.\n4\n\nIntegrated penetration testing tool for finding vulnerabilities in web applications,/index\n.php/OWASP_Zed_Attack_Proxy_Project.\n\n5 Tips for designing secure software,/index.php/OWASP_Top_Ten_Cheat_Sheet.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "8", "chapter_title": "Measuring the technical quality of your software", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "A great open source option is OWASP Zed Attack Proxy4 (ZAP). Figure 8.17 An example screenshot of OWASP ZAP 2 Fortify static code analyzer, www8.hp.com/us/en/software-solutions/static-code-analysis-sast/index.html?.", "l2_summary": "■ Static code analysis---SonarQube has some rules on ensuring your application is\nsecure, but there are other static analysis tools that specialize in this. ■ Dynamic code analysis---Examples are Veracode3 and WhiteHat (www.whitehatsec A great open source option is OWASP Zed Attack Proxy4 (ZAP). An example of ZAP is shown in figure 8.17. Figure 8.17 An example screenshot of OWASP ZAP 2 Fortify static code analyzer, www8.hp.com/us/en/software-solutions/static-code-analysis-sast/index.html?.", "prev_page": {"page_num": 194, "segment_id": "00194"}, "next_page": {"page_num": 196, "segment_id": "00196"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["analysis", "owasp", "software", "security", "secure", "static", "code", "site", "application", "dynamic"], "content_type": "tutorial", "domain": "security", "complexity": "intermediate", "companies": ["SonarQube", "Checkmarx", "Coverity", "Fortify", "Veracode", "WhiteHat", "OWASP"], "people": [], "products": ["SonarQube", "Checkmarx", "Coverity", "Fortify", "Veracode", "WhiteHat", "OWASP Zed Attack Proxy"], "technologies": [], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["SonarQube", "Checkmarx", "Coverity", "Fortify", "Veracode", "WhiteHat", "OWASP Zed Attack Proxy"], "topics": ["Measuring technical quality of software", "Security testing and tools"], "key_concepts": ["Static code analysis", "Dynamic code analysis", "OWASP top 10", "Security scanning with OWASP ZAP"], "problem_statement": "Ensuring the security of consumer-facing software products", "solution_approach": "Using various static and dynamic code analysis tools for security testing", "extraction_method": "lm"}}
{"segment_id": "00196", "page_num": 196, "segment": "Case study: finding anomalies in lead time\n\n173\n\n8.5\n\nIf your site fails security analysis, you should stop whatever you’re doing and fix the\nproblems. Suffice it to say that a site with poor security has very low usability.\n\nCase study: finding anomalies in lead time\nIn this case study we’ll look at a team that’s practicing Kanban for their task management. They had a steady stream of work, and they paid close attention to lead time to\nmake sure that everything was flowing through their development cycle as consistently\nas possible. They started tagging their tasks so they could see not only overall lead\ntime but also lead time per tagged group of tasks. Once they started tracking tagged\ngroups of tasks, they started to realize that lead time for everything tagged “cam” was\nmuch higher than everything else and was skewing their average. Figure 8.18 shows\nwhat they saw when they filtered by the “cam” tag.\n\n Once the team began digging, they also noticed that everything tagged “cam” also\nhad a much higher CLOC. They broke out their lead time to see where in the process\nthe extra time was coming in. As shown in figure 8.19, they could see that the build and\ndeploy process was fine, but development was taking longer than usual on these tasks.\n\nThere are 63 total\ntasks with a 19-day\nlead time.\n\nFiltering into all\ntasks that have a\nhigh lead time\n\nALOC = added\nlines of code\nquery\n\nCompared to other\nlabels this is a very\nhigh CLOC.\n\nOf the offending tasks\nmost are labeled “cam.”\n\nFigure 8.18 The result of breaking down lead time by tags\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "8", "chapter_title": "Measuring the technical quality of your software", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "Case study: finding anomalies in lead time They started tagging their tasks so they could see not only overall lead\ntime but also lead time per tagged group of tasks. There are 63 total\ntasks with a 19-day\nlead time.", "l2_summary": "Case study: finding anomalies in lead time They started tagging their tasks so they could see not only overall lead\ntime but also lead time per tagged group of tasks. They broke out their lead time to see where in the process\nthe extra time was coming in. There are 63 total\ntasks with a 19-day\nlead time. Filtering into all\ntasks that have a\nhigh lead time Figure 8.18 The result of breaking down lead time by tags", "prev_page": {"page_num": 195, "segment_id": "00195"}, "next_page": {"page_num": 197, "segment_id": "00197"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["time", "lead", "tasks", "everything", "tagged", "case", "study", "started", "also", "figure"], "content_type": "case_study", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Kanban", "build and deploy process"], "frameworks": [], "methodologies": ["Kanban"], "programming_languages": [], "has_best_practices": false, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["lead time", "CLOC"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["anomaly detection in lead time", "Kanban methodology", "task management"], "key_concepts": ["lead time", "tagging tasks", "CLOC (added lines of code)"], "problem_statement": "Finding anomalies in lead time for specific task tags", "solution_approach": "Tracking and analyzing lead time per tagged group of tasks, identifying high CLOC, and pinpointing the development phase as the bottleneck", "extraction_method": "lm"}}
{"segment_id": "00197", "page_num": 197, "segment": "174\n\nCHAPTER 8 Measuring the technical quality of your software\n\nLead time by feature release\n\nDevelopment time is\nthe obvious culprit.\n\ns\ne\ns\na\ne\ne\nR\n\nl\n\ncam.r.5.2\n\ncam.r.5.3\n\ncam.r.5.4\n\naut.r.2.3\n\naut.r.2.4\n\naut.r.2.5\n\nDefine\n\nDevelopment\n\nBuild\n\nTest\n\nDeploy\n\n0\n\n6\n\n12\nHours\n\n18\n\n24\n\nFigure 8.19 Breaking down lead time for tagged tasks to find where the time sink is happening\n\nNo longer called,\nthe old code will\n“starve.”\n\nAbstract inputs\nand outputs\nwith interfaces.\n\nNew code\n\nCode with low\nmaintainability.\n\nProblem code\n\nProblem code\n\nProblem code\n\nAs you add\nnew functionality,\nbuild it out with\nnew code.\n\nNew code\n\nFigure 8.20 The surround and starve refactoring pattern\n\nThey decided to refactor this part of the code base, abstracting the inputs and outputs\nwith interfaces and then writing new code that handled requests, as shown in figure 8.20.\n This allowed them to write new code with good test coverage in smaller modules\n\nand eventually stop calling the problem code in favor of the new code.\n\nRefactoring patterns\n\nThere are a number of patterns you can use to refactor your codebase; using the right\none will depend on the context of your changes. I’ve seen the pattern illustrated in\nfigure 8.20, called “surround and starve,” work well for moving from large monolithic\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 7, "cols": 6, "data": [["", "", "", "", "", ""], ["", "", "", "", "", ""], ["", "", "", "", "", ""], ["", "", "", "", "", ""], ["", "", "", "", "", ""], ["", "", "", "", "", ""], ["", "", "", "", "", ""]], "markdown": "|  |  |  |  |  |  |\n|---|---|---|---|---|---|\n|  |  |  |  |  |  |\n|  |  |  |  |  |  |\n|  |  |  |  |  |  |\n|  |  |  |  |  |  |\n|  |  |  |  |  |  |\n|  |  |  |  |  |  |"}], "table_count": 1, "chapter_num": "8", "chapter_title": "Measuring the technical quality of your software", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "Code with low\nmaintainability. As you add\nnew functionality,\nbuild it out with\nnew code. and eventually stop calling the problem code in favor of the new code.", "l2_summary": "No longer called,\nthe old code will\n“starve.” Code with low\nmaintainability. As you add\nnew functionality,\nbuild it out with\nnew code. Figure 8.20 The surround and starve refactoring pattern They decided to refactor this part of the code base, abstracting the inputs and outputs\nwith interfaces and then writing new code that handled requests, as shown in figure 8.20. and eventually stop calling the problem code in favor of the new code.", "prev_page": {"page_num": 196, "segment_id": "00196"}, "next_page": {"page_num": 198, "segment_id": "00198"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["code", "time", "figure", "problem", "starve", "lead", "development", "build", "test", "called"], "content_type": "practice", "domain": "programming|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["interfaces", "code refactoring", "monolithic codebase"], "frameworks": [], "methodologies": ["refactoring"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["refactoring", "code quality", "lead time"], "key_concepts": ["surround and starve refactoring pattern", "abstract inputs and outputs with interfaces", "breaking down lead time for tagged tasks"], "problem_statement": "Improving the technical quality of software through refactoring and reducing development time", "solution_approach": "Using refactoring patterns like 'surround and starve' to improve code maintainability and reduce lead time", "extraction_method": "lm"}}
{"segment_id": "00198", "page_num": 198, "segment": "Case study: finding anomalies in lead time\n\n175\n\ncodebases to smaller, more modular, and easier-to-maintain codebases, as alluded\nto in this case study. There are entire books dedicated to refactoring. Two good ones\nare Refactoring: Improving the Design of Existing Code by Martin Fowler (Addison-Wesley\nProfessional, 1999) and Refactoring to Patterns by Joshua Kerievsky (Addison-Wesley\nProfessional, 2004).\n\nAfter breaking down their development plan, the team realized that they could refactor as they went along without adding much additional time to their current updates.\nBy focusing on new code that was decoupled from the problematic code, they could\nensure that they had good code coverage and clean code from the start. They created\na new project in SonarQube so they could monitor their new projects and ensure they\nhad good code coverage and were following standards.\n\n Every time they got a new task that would be tagged “cam,” they started developing\nwith their new approach. At first, development time for new tasks went up by a few\ndays, but after only five releases they started to see the benefits of this approach, as\nshown in figure 8.21.\n\nCode with the new\npattern was versioned 6.x.\n\nLead time by feature release\n\nDefine\n\nDevelopment\n\nBuild\n\nTest\n\nDeploy\n\ns\ne\ns\na\ne\ne\nR\n\nl\n\ncam.r.5.2\n\ncam.r.5.3\n\ncam.r.5.4\n\ncam.r.6.0\n\ncam.r.6.1\n\ncam.r.6.2\n\ncam.r.6.3\n\ncam.r.6.4\n\ncam.r.6.5\n\ncam.r.6.6\n\ncam.r.6.7\n\ncam.r.6.8\n\n0\n\n7\n\n14\nHours\n\n21\n\n28\n\nBy the fifth version 6\nrelease, development time\nwas drastically reduced.\n\nFigure 8.21 After five releases development time was reduced by over 60%.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 23, "cols": 7, "data": [["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""], ["", "", "", "", "", "", ""]], "markdown": "|  |  |  |  |  |  |  |\n|---|---|---|---|---|---|---|\n|  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |"}], "table_count": 1, "chapter_num": "8", "chapter_title": "Measuring the technical quality of your software", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "Case study: finding anomalies in lead time Every time they got a new task that would be tagged “cam,” they started developing\nwith their new approach. Code with the new\npattern was versioned 6.x.", "l2_summary": "Case study: finding anomalies in lead time By focusing on new code that was decoupled from the problematic code, they could\nensure that they had good code coverage and clean code from the start. Every time they got a new task that would be tagged “cam,” they started developing\nwith their new approach. Code with the new\npattern was versioned 6.x. By the fifth version 6\nrelease, development time\nwas drastically reduced. Figure 8.21 After five releases development time was reduced by over 60%.", "prev_page": {"page_num": 197, "segment_id": "00197"}, "next_page": {"page_num": 199, "segment_id": "00199"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["time", "code", "development", "refactoring", "good", "could", "case", "study", "lead", "codebases"], "content_type": "case_study", "domain": "programming|devops", "complexity": "intermediate", "companies": [], "people": ["Martin Fowler", "Joshua Kerievsky"], "products": [], "technologies": ["SonarQube"], "frameworks": [], "methodologies": ["Refactoring"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["lead time"], "has_case_study": true, "case_study_company": null, "tools_mentioned": ["SonarQube"], "topics": ["refactoring", "code maintenance", "development process optimization"], "key_concepts": ["modular codebases", "good code coverage", "clean code", "versioning"], "problem_statement": "Finding anomalies in lead time and improving development efficiency through refactoring", "solution_approach": "Breaking down the development plan, focusing on new decoupled code, using SonarQube for monitoring", "extraction_method": "lm"}}
{"segment_id": "00199", "page_num": 199, "segment": "176\n\nCHAPTER 8 Measuring the technical quality of your software\n\nWithout question, this strategy helped this team improve their delivery and their software products by making them more maintainable.\n\n8.6\n\nSummary\nGood software is measured across several dimensions. Using the code “ilities” and several freely available tools, you can measure these dimensions to understand the quality of your code. In this chapter you learned the following:\n\n■ To measure good software in an agile context you can go back to some agile\n\nprinciples that recommend the following:\n■ Continuous and frequent delivery\n■ Technical excellence\n■ Good architectures\n■ Working software\n\n■ Use the code “ilities” or non-functional requirements to measure how well your\n\nsoftware is built.\n\n■ Maintainability and usability are top-level measures.\n■ Maintainability tells you how fast you can iterate, and it can be measured with\n\nthese metrics:\n■ MTTR tells you how fast you can fix issues for your consumers.\n■ Lead time tells you how fast you can get new features in your products.\n■ CLOC ensures code can be changed without huge efforts.\n■ Code coverage shows that your code is well covered by automated tests.\n\n■ Usability tells you how well your application works for your consumers and is\n\nmeasured with these metrics:\n■ Availability tells you how often your consumers can use your application.\n■ Reliability tells you how consistently your application works for your consumers.\n\n■ Security tells you that your application is safe for your consumers.\n\n■ The following tools will help you get better insight into measures of maintainability:\n■ Sonar is a great tool to visually analyze code coverage and rules compliance.\n■ Other code coverage analysis tools include Cobertura, JaCoCo, Clover,\n\nNCover, and Gcov.\n\n■ Pitest is used for mutation testing, which helps test your tests.\n\n■ The following tools will help give you better insight into measures of usability:\n\n■ New Relic for availability and reliability\n■ Splunk for reliability\n■ Coverity, Checkmarx, Fortify, or OWASP Zed Attack Proxy (ZAP) for security\n\nscanning\n\n■ You can measure how maintainable your releases are by combining key metrics:\n\n■ MTTR(in minutes) * (Total Fixes / Total Releases)\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "8", "chapter_title": "Measuring the technical quality of your software", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "■ Maintainability tells you how fast you can iterate, and it can be measured with these metrics:\n■ MTTR tells you how fast you can fix issues for your consumers. measured with these metrics:\n■ Availability tells you how often your consumers can use your application.", "l2_summary": "■ Maintainability tells you how fast you can iterate, and it can be measured with these metrics:\n■ MTTR tells you how fast you can fix issues for your consumers. ■ Usability tells you how well your application works for your consumers and is measured with these metrics:\n■ Availability tells you how often your consumers can use your application. ■ Reliability tells you how consistently your application works for your consumers. ■ You can measure how maintainable your releases are by combining key metrics:", "prev_page": {"page_num": 198, "segment_id": "00198"}, "next_page": {"page_num": 200, "segment_id": "00200"}, "flags": {"continuity_gap": true}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["code", "tells", "software", "consumers", "tools", "measure", "following", "application", "good", "measured"], "content_type": "tutorial", "domain": "programming|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Sonar", "Cobertura", "JaCoCo", "Clover", "NCover", "Gcov", "Pitest", "New Relic", "Splunk", "Coverity", "Checkmarx", "Fortify", "OWASP Zed Attack Proxy (ZAP)"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["MTTR", "Lead time", "CLOC", "Code coverage", "Availability", "Reliability", "Security"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Sonar", "Cobertura", "JaCoCo", "Clover", "NCover", "Gcov", "Pitest", "New Relic", "Splunk", "Coverity", "Checkmarx", "Fortify", "OWASP Zed Attack Proxy (ZAP)"], "topics": ["Measuring software quality", "Agile principles", "Maintainability metrics", "Usability metrics", "Security metrics"], "key_concepts": ["code 'ilities'", "technical excellence", "good architectures", "continuous and frequent delivery"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00200", "page_num": 200, "segment": "Publishing metrics\n\nThis chapter covers\n\n■ How to successfully publish metrics across the\n\norganization\n\n■ Different methods of publishing metrics and\n\ndashboards\n\n■ What types of metrics are important to which\n\nparts of your organization\n\nAs you’ve learned about the different metrics you can build and use throughout\nthis book, you’ve been publishing them along the way. If you create a gigantic dashboard with everything we’ve talked about, you’ll end up with lots of pretty charts\nand graphs that are impossible for anyone except you to understand. Organizing\nthis data for the right audiences is an important way to convey the metrics about\nyour team effectively.\n\n My cardinal rule of designing metrics is to keep them actionable, and the same\nthing applies to publishing metrics. Publish the data to people who can affect the\noutcome of your metrics in a positive way. Giving people metrics they can’t improve\nwill lead to either unnecessary stress or a disregard of the data you’re showing\nthem.\n\n177\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "8", "chapter_title": "Measuring the technical quality of your software", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "■ How to successfully publish metrics across the ■ Different methods of publishing metrics and ■ What types of metrics are important to which", "l2_summary": "■ How to successfully publish metrics across the ■ Different methods of publishing metrics and ■ What types of metrics are important to which As you’ve learned about the different metrics you can build and use throughout\nthis book, you’ve been publishing them along the way. My cardinal rule of designing metrics is to keep them actionable, and the same\nthing applies to publishing metrics. Publish the data to people who can affect the\noutcome of your metrics in a positive way.", "prev_page": {"page_num": 199, "segment_id": "00199"}, "next_page": {"page_num": 201, "segment_id": "00201"}, "flags": {"continuity_gap": true}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["metrics", "publishing", "data", "publish", "organization", "different", "important", "people"], "content_type": "tutorial", "domain": "management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["publishing metrics across the organization", "different methods of publishing metrics and dashboards", "important types of metrics for different parts of the organization"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Publishing Metrics", "Actionable Metrics", "Organizing Data for Audiences"], "key_concepts": ["actionable metrics", "organizing data for the right audiences", "publishing metrics to relevant stakeholders"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00201", "page_num": 201, "segment": "178\n\nCHAPTER 9 Publishing metrics\n\nHow much\nare we\nspending?\n\nAre we hitting\nour strategic\ngoals?\n\nAre our\ncustomers\nhappy?\n\nExecutives\n\nAre\npull requests\ngood?\n\nAre our\nsystems\nworking well?\n\nAre our\nprocesses\nworking well?\n\nAre we\nhitting our\ncommitments?\n\nAre our\nprojects\non track?\n\nTechnical\nmanagers\n\nProgram\nmanagers\n\nDevelopers\n\nFigure 9.1 Different levels/roles in the organization and the questions they ask\n\n9.1\n\nThe right data for the right audience\nPeople at different levels of your organizations will need different types of input in\norder to answer the questions pertaining to their roles. Figure 9.1 shows examples of\nsuch questions.\n\n Data should be distributed across the organization in such a way that everyone can\nget the data they care about at a glance. Here are a few traps that teams fall into when\nthey start publishing metrics:\n\n■ Sending all the data to all stakeholders---When you’re collecting a lot of great metrics, it can be easy to forget that not everyone cares about everything. You may\nbe really excited that your CLOC per hot fix is down to 10, but that may not\nmake sense to your product owners. You don’t want to hide information, but\nyou also don’t want to overreport data.\n\n■ Emailing reports that don’t make sense out of context---Some people spend their\nentire day on a single project; others spend a small amount of their time across\nseveral projects. If a stakeholder receives an email with information about two\ndays of progress toward goals from your last retrospective, they may not understand what you’re talking about or how it affects the bottom line. If you really\nfeel like you need to send certain data out to the entire company, ensure that\nyou tie it back to metrics or information your audience can relate to.\n\n■ Creating web-based dashboards that have too much data for the intended audience---\nAlong the same lines of sending all the data to every stakeholder, creating a\ndashboard that doesn’t have data a viewer can make sense of and potentially\ntake action on immediately will confuse and frustrate people. We’ll talk about\ndesigning dashboards at length later in this chapter.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "8", "chapter_title": "Measuring the technical quality of your software", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "CHAPTER 9 Publishing metrics Figure 9.1 Different levels/roles in the organization and the questions they ask Data should be distributed across the organization in such a way that everyone can\nget the data they care about at a glance.", "l2_summary": "CHAPTER 9 Publishing metrics How much\nare we\nspending? Figure 9.1 Different levels/roles in the organization and the questions they ask Figure 9.1 shows examples of\nsuch questions. Data should be distributed across the organization in such a way that everyone can\nget the data they care about at a glance. You don’t want to hide information, but\nyou also don’t want to overreport data.", "prev_page": {"page_num": 200, "segment_id": "00200"}, "next_page": {"page_num": 202, "segment_id": "00202"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "metrics", "different", "questions", "audience", "people", "make", "sense", "information", "chapter"], "content_type": "theory", "domain": "management|devops", "complexity": "intermediate", "companies": [], "people": ["Executives", "Technical managers", "Program managers", "Developers"], "products": [], "technologies": [], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["CLOC per hot fix"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Publishing metrics", "Data distribution across organization", "Metrics for different roles"], "key_concepts": ["right data for the right audience", "metrics for executives, technical managers, program managers, developers"], "problem_statement": "How to effectively distribute and utilize metrics within an organization", "solution_approach": "Identifying relevant metrics for each role, distributing them appropriately, avoiding overreporting or underreporting", "extraction_method": "lm"}}
{"segment_id": "00202", "page_num": 202, "segment": "The right data for the right audience\n\n179\n\nExecutives\n\nVice presidents\n\nAudience\n\nDirectors\n\nFigure 9.2 A mind map showing the\ndifferent levels of the organization\nand what they can affect\n\nDevelopers\n\nAgile coach\n\nHave the most power to\naffect budgets and strategy\n\nHave the most power to\naffect organizational issues\n\nHave the most power to\naffect tactical execution\n\nHave the most power to affect\ncomponent-level metrics\n\n■ Updating published data without letting anyone know---You may update your data\nonce a sprint, once a day, or once a minute. The interval at which you’re publishing data is important for your audience to know and is ideally visualized on\nyour dashboard. If you’re publishing data every sprint, you may note the sprint\nnumber on your dashboard; if you’re publishing data every minute, you may\nhave a running histogram showing the data changing over time.\n\nAll of these bullets point to the same problem: giving people the wrong information\nor too much of it.\n\n To ensure that you provide data to people who can affect the processes reflected by\nit, start off by looking at what groups or individuals inside your organization can affect\ndirectly. An example of this is shown in figure 9.2.\n\n By giving people data they can act on, you’re helping keep everyone focused\n\nwithin the boundaries of their responsibilities.\n\n If you take figure 9.2 and align the questions with the data we’ve been collecting\nthroughout this book, you’ll start to see that another way to visualize the right type of\ndata for the right audience would be to put everything on a grid, as shown in figure 9.3.\n\nLeadership\n\nHow much are\nwe spending?\n\nManagers\n\nDevelopment\nteam\n\nAre we hitting our\ncommitments?\n\nHow frequently\nare we delivering?\n\nAre our\ncustomers happy?\n\nHow well are we\nmanaging code?\n\nHow is our\ncode quality?\n\nIs our software\nworking well?\n\nProject\ntracking system\n\nSource\ncontrol\n\nContinuous\nintegration\n\nApplication\nperformance monitoring\n\nFigure 9.3 The intersection of the levels of a typical organization, the questions they ask, and the\ndata sources where you can find the answers\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 3, "cols": 5, "data": [["How much are\nLeadership we spending?\nHow frequently Are our\nManagers\nare we delivering? customers happy?\nAre we hitting our\ncommitments?\nHow well are we How is our Is our software\nDevelopment\nmanaging code? code quality? working well?\nteam\nProject Source Continuous Application\ntracking system control integration performance monitoring", "How m\nwe spe", "uch are\nnding?", "", ""], ["", "Are we hitting our", "", "How frequently\nare we delivering?", "Are our\ncustomers happy?"], ["", "commitments?", "How well are we\nmanaging code?", "How is our\ncode quality?", "Is our software\nworking well?"]], "markdown": "| How much are\nLeadership we spending?\nHow frequently Are our\nManagers\nare we delivering? customers happy?\nAre we hitting our\ncommitments?\nHow well are we How is our Is our software\nDevelopment\nmanaging code? code quality? working well?\nteam\nProject Source Continuous Application\ntracking system control integration performance monitoring | How m\nwe spe | uch are\nnding? |  |  |\n|---|---|---|---|---|\n|  | Are we hitting our |  | How frequently\nare we delivering? | Are our\ncustomers happy? |\n|  | commitments? | How well are we\nmanaging code? | How is our\ncode quality? | Is our software\nworking well? |"}], "table_count": 1, "chapter_num": "8", "chapter_title": "Measuring the technical quality of your software", "section_num": "6.1.3", "section_title": "Using logging best practices", "l1_summary": "The right data for the right audience Have the most power to\naffect budgets and strategy Have the most power to\naffect organizational issues", "l2_summary": "The right data for the right audience Figure 9.2 A mind map showing the\ndifferent levels of the organization\nand what they can affect Have the most power to\naffect budgets and strategy Have the most power to\naffect organizational issues Have the most power to\naffect tactical execution Have the most power to affect\ncomponent-level metrics", "prev_page": {"page_num": 201, "segment_id": "00201"}, "next_page": {"page_num": 203, "segment_id": "00203"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "affect", "figure", "have", "right", "audience", "most", "power", "organization", "once"], "content_type": "theory", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Executives", "Vice presidents", "Directors", "Developers", "Agile coach"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["spending", "commitments", "deliveries", "customer happiness", "code management", "code quality", "software performance"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Data for different organizational levels", "Agile methodology", "Data visualization"], "key_concepts": ["right data for the right audience", "organizational hierarchy and influence", "data publishing intervals", "dashboard visualization"], "problem_statement": "Providing incorrect or excessive information to stakeholders", "solution_approach": "Identifying relevant data sources based on organizational levels and aligning them with specific questions and metrics", "extraction_method": "lm"}}
{"segment_id": "00203", "page_num": 203, "segment": "180\n\nCHAPTER 9 Publishing metrics\n\nEven though everyone on the team cares about all of these things, the questions are\nvisually aligned with what each role across the organization can immediately affect.\n\n9.1.1 What to use on your team\n\nThe team on the ground delivering changes to your software is generating the building blocks of your metrics and will care about the details because they reflect the dayto-day work. Realistically, it’s tough to focus on all the data you’re generating all the\ntime. I like to pick key metrics from\neach system we’re pulling data from\nand publish those as defaults so we’re\ngetting a good picture of the entire\nsoftware lifecycle at a glance. A good\nstrategy for your team is to publish all\nyour data but organize your reporting\nso your team is paying attention to the\nmetrics you agree on in your planning\nsessions.\n\nThe team regularly has around 80-90\nbuilds during a development cycle.\n\n Let’s take this scenario as an example: A team found that they had a high\npercentage of failed builds from their\nCI system. They were tracking the\nratio of good/bad builds, as shown in\nfigure 9.4.\n\nJust over half of\ntotal builds pass.\n\nFigure 9.4 A team’s ratio of good and bad builds.\nThey regularly had approximately half of their\nbuilds pass.\n\n When they started to investigate\nthe problem, they found that many of\nthe issues were small mistakes that\ncould have been caught if there were another pair of eyes on the code changes. The\nteam had recently moved from using SVN as their SCM system to Git and was simply\ncommitting code to the master branch, as shown in figure 9.5.\n\nDeveloper checks\na change into SCM.\n\nCI system runs tests and\nvarious other build tasks.\n\nSCM\n\nCI system\n\nThere is a high number of\nbuild errors due to mistakes\nthat could be caught by peer\ncode review.\n\nThe CI system detects\nchanges to source control\nand kicks off builds.\n\nFigure 9.5 How the team put code into their SCM.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": "Publishing metrics", "section_num": "9.1.1", "section_title": "What to use on your team", "l1_summary": "9.1.1 What to use on your team Figure 9.4 A team’s ratio of good and bad builds. Figure 9.5 How the team put code into their SCM.", "l2_summary": "9.1.1 What to use on your team The team regularly has around 80-90\nbuilds during a development cycle. Let’s take this scenario as an example: A team found that they had a high\npercentage of failed builds from their\nCI system. They were tracking the\nratio of good/bad builds, as shown in\nfigure 9.4. Figure 9.4 A team’s ratio of good and bad builds. Figure 9.5 How the team put code into their SCM.", "prev_page": {"page_num": 202, "segment_id": "00202"}, "next_page": {"page_num": 204, "segment_id": "00204"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["team", "builds", "system", "metrics", "good", "figure", "code", "changes", "data", "what"], "content_type": "theory|practice", "domain": "devops|architecture|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Git", "SVN"], "frameworks": [], "methodologies": ["DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["failed builds ratio", "good/bad builds ratio"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["Publishing metrics", "CI/CD pipeline", "Code review"], "key_concepts": ["Key metrics selection", "Team focus on agreed metrics", "Failed builds analysis"], "problem_statement": "High percentage of failed builds due to small mistakes in the code", "solution_approach": "Implementing peer code review and using Git for version control", "extraction_method": "lm"}}
{"segment_id": "00204", "page_num": 204, "segment": "The right data for the right audience\n\n181\n\nDeveloper checks\na change into SCM.\n\nCI system runs tests and\nvarious other build tasks.\n\nSCM\n\nCI system\n\nBy ensuring all commits ask\nfor feedback in a pull request\n(PR), developers should have\nmore accountability for\ntheir changes.\n\nPeers review\nand comment.\n\nFigure 9.6 The proposed updated workflow: moving to pull requests to reduce the number of build\nfailures\n\nThey decided to implement the pull request (PR) workflow to add peer code reviews\ninto their development cycle to help improve the quality of their code. When a developer was ready to commit code, instead of checking it directly into the master branch,\nthey would open a pull request and ask their peers for feedback, as shown in figure 9.6.\n\n In their next retrospective they\nnoticed that their build ratio had\nimproved significantly, as shown in\nfigure 9.7.\n\n When they looked into their build\nresults they found that they had a lot\nfewer builds, the success percentage\nwas much higher, but their total number of builds had gone down.\n\n As they discussed the new process in\ntheir retrospective, PRs came up as a\nproblem repeatedly. Upon further discussion and analysis, they realized that\ndevelopers were using a PR as a way to\nget feedback instead of as a quality\ncheck.\n\nThe total number of builds has\ngone down by close to 60%.\n\nAlmost 75% of builds are passing;\nthat looks like a huge improvement.\n\nFigure 9.7 Good/bad build ratio had improved\nsignificantly, but total number of builds had gone\ndown significantly.\n\n A good PR should include code\ncomplete with tests and should be\nready to ship to consumers. The purpose of the PR would be for peer developers who had deep understanding of the software to ensure that the changes being made were appropriate for the end goal and\nthat they didn’t conflict with anything else under change. A good PR would have few\nor no comments and should get merged fairly quickly.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": "Publishing metrics", "section_num": "9.1.1", "section_title": "What to use on your team", "l1_summary": "In their next retrospective they\nnoticed that their build ratio had\nimproved significantly, as shown in\nfigure 9.7. The total number of builds has\ngone down by close to 60%. Figure 9.7 Good/bad build ratio had improved\nsignificantly, but total number of builds had gone\ndown significantly.", "l2_summary": "Figure 9.6 The proposed updated workflow: moving to pull requests to reduce the number of build\nfailures When a developer was ready to commit code, instead of checking it directly into the master branch,\nthey would open a pull request and ask their peers for feedback, as shown in figure 9.6. In their next retrospective they\nnoticed that their build ratio had\nimproved significantly, as shown in\nfigure 9.7. When they looked into their build\nresults they found that they had a lot\nfewer builds, the success percentage\nwas much higher, but their total number of builds had gone down. The total number of builds has\ngone down by close to 60%. Figure 9.7 Good/bad build ratio had improved\nsignificantly, but total number of builds had gone\ndown significantly.", "prev_page": {"page_num": 203, "segment_id": "00203"}, "next_page": {"page_num": 205, "segment_id": "00205"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["build", "builds", "pull", "should", "figure", "number", "code", "feedback", "request", "developers"], "content_type": "case_study", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["SCM", "CI system"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["build ratio", "success percentage"], "has_case_study": true, "case_study_company": null, "tools_mentioned": ["SCM", "CI system"], "topics": ["pull request workflow", "code review", "build failures"], "key_concepts": ["peer code reviews", "pull requests", "development cycle"], "problem_statement": "Reducing build failures and improving code quality", "solution_approach": "Implementing pull request (PR) workflow for peer code reviews", "extraction_method": "lm"}}
{"segment_id": "00205", "page_num": 205, "segment": "182\n\nCHAPTER 9 Publishing metrics\n\n Instead, developers were checking in half-baked code to start design discussions\nthrough a PR. This was causing the team members to get distracted from their own\nchanges and have lengthy discussions in the comments of the PRs. Developers would\ncommit multiple times to the same PR, which would stay open for days while they\nexchanged comments.\n\n Collaborating through pull requests was apparently ensuring that few small mistakes were being made, but by implementing the PR workflow they ended up getting\nside effects they didn’t expect. If they could go back to the practice of submitting complete code and get the benefit of finding problems before they went into the build\npipeline, they might be able to get better quality and much more frequent delivery.\nThey decided to track the metrics that pointed to negative side effects:\n\n■ High number of comments---They saw that if a PR had more than six comments it\nwas an indication that it was getting tossed around too much. If they saw comments going up, they should investigate to find out why a solution was so controversial.\n\n■ PR duration---If a PR was good, it should get merged within the day. If it needed\na slight tweak, then maybe it would get fixed and merged the day after it was\noriginally submitted. PRs open for multiple days indicated there was a problem\nsomewhere; either the team didn’t agree on the technical implementation and\nthey were spinning on the best solution or for some reason no one was reviewing, approving, or merging the PRs.\n\n■ Number of commits per PR---A PR starts off as a few commits. If there are comments that need to be addressed or the PR gets rejected, then the original developer will add more code through commits to address the issues. The more\ncommits on a PR, the worse the original PR was, so this number should be low.\n\nHigh numbers of PR comments and commits along with long PR durations were contributing to longer lead times and longer development times. In their effort to\nimprove they put those three metrics at the top of their operations dashboard, which\nthe team members looked at and reviewed every day. PR duration and number of commits per PR should stay low. The goal was to keep PRs open for less than a day; by\ntracking that in hours they wanted to make sure it didn’t creep higher than 24. Commits per PR should also stay low; the team noticed that by exploring their data when\ncommits for a PR started to go over 6, there was usually a problem.\n\n The dashboard they configured is shown in figure 9.8.\n Note this is a focused list of metrics that the team decided to concentrate on. They\ncan still pay attention to the other metrics on their dashboard such as lead time and\ndevelopment time, but this targeted list of metrics will help them improve their process where they’ve already identified a problem.\n\n Just as in the previous example, during your team’s retrospective you should identify things that could be done better and metrics that measure those items. Update\nyour dashboard each development period to reflect what the team agreed to be\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": "Publishing metrics", "section_num": "9.1.1", "section_title": "What to use on your team", "l1_summary": "■ Number of commits per PR---A PR starts off as a few commits. The more\ncommits on a PR, the worse the original PR was, so this number should be low. PR duration and number of commits per PR should stay low.", "l2_summary": "CHAPTER 9 Publishing metrics ■ PR duration---If a PR was good, it should get merged within the day. ■ Number of commits per PR---A PR starts off as a few commits. The more\ncommits on a PR, the worse the original PR was, so this number should be low. PR duration and number of commits per PR should stay low. Commits per PR should also stay low; the team noticed that by exploring their data when\ncommits for a PR started to go over 6, there was usually a problem.", "prev_page": {"page_num": 204, "segment_id": "00204"}, "next_page": {"page_num": 206, "segment_id": "00206"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["commits", "metrics", "team", "comments", "should", "more", "number", "dashboard", "code", "would"], "content_type": "case_study", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["High number of comments", "PR duration", "Number of commits per PR"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["Pull Request Workflow", "Code Quality Metrics", "Development Process Improvement"], "key_concepts": ["Pull Request (PR) workflow", "Code quality metrics", "Development process optimization"], "problem_statement": "Developers checking in half-baked code for design discussions through PRs causing distractions and lengthy discussions, leading to longer lead times.", "solution_approach": "Implementing a focused list of metrics on an operations dashboard to track and improve the development process.", "extraction_method": "lm"}}
{"segment_id": "00206", "page_num": 206, "segment": "The right data for the right audience\n\n183\n\nThe top row of their\ndashboard had the key stats\nfor monitoring pull requests.\n\nBy showing the distribution of\nPR commit counts they could find\nanomalies quickly and investigate.\n\nThe main metrics they wanted\nto affect were lead time and\ndevelopment (dev) time.\n\nA PR with a commit\ncount higher than 6 usually\nindicated a problem.\n\nFigure 9.8 An example operational dashboard focusing on metrics the team identified as\nmeasures of their process that need work: (clockwise from top left) comment count per PR, PR\nhours open, PR commits, lead time, and development time. Based on the team’s history, low\nPR comments, commits, and hours open also led to faster lead and development times.\n\nimmediately aware of, but keep the old metrics on it to get a truly holistic picture of\nhow things are working.\n\n If you’re just getting started and aren’t sure what to focus on, the following metrics\nare a good starting point for your team. They will inevitably raise questions that cause\nyou to drill down farther and create and pay attention to a set of measurements you\nuse every day.\n\n■ Tags and labels---These will show you what’s trending day to day. The trends you\nsee should reflect the goals and the immediate tasks the team is working on. If\nyou start to see something that doesn’t feel right, you can address it and react\nquickly.\n\n■ Pull requests and commits---These show that code is changing and the team is\nworking together. I like to set goals for the team to have at least one pull request\nsubmitted and one pull request merged per developer per day. If your team can\nmanage that goal, it usually means that tasks are small enough to make steady\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": "Publishing metrics", "section_num": "9.1.1", "section_title": "What to use on your team", "l1_summary": "The main metrics they wanted\nto affect were lead time and\ndevelopment (dev) time. Figure 9.8 An example operational dashboard focusing on metrics the team identified as\nmeasures of their process that need work: (clockwise from top left) comment count per PR, PR\nhours open, PR commits, lead time...", "l2_summary": "The right data for the right audience The main metrics they wanted\nto affect were lead time and\ndevelopment (dev) time. Figure 9.8 An example operational dashboard focusing on metrics the team identified as\nmeasures of their process that need work: (clockwise from top left) comment count per PR, PR\nhours open, PR commits, lead time, and development time. ■ Tags and labels---These will show you what’s trending day to day. The trends you\nsee should reflect the goals and the immediate tasks the team is working on. ■ Pull requests and commits---These show that code is changing and the team is\nworking together.", "prev_page": {"page_num": 205, "segment_id": "00205"}, "next_page": {"page_num": 207, "segment_id": "00207"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["team", "pull", "metrics", "time", "right", "lead", "development", "commits", "working", "dashboard"], "content_type": "practice", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["dashboard", "pull requests", "commit counts", "lead time", "development time"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["lead time", "development time", "PR commit counts"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["dashboard metrics", "pull request monitoring", "code review process"], "key_concepts": ["anomalies detection", "metrics for process improvement", "team performance indicators"], "problem_statement": "Improving team efficiency and identifying issues in the code review and development process", "solution_approach": "Implementing a dashboard with key metrics to monitor pull requests, commits, lead time, and development time", "extraction_method": "lm"}}
{"segment_id": "00207", "page_num": 207, "segment": "184\n\nCHAPTER 9 Publishing metrics\n\nprogress and your team is pushing changes through the pipeline at a fairly consistent rate.\n\n■ Task flow and recidivism---As with pull requests and commits, I like to see cards\nconsistently moving, preferably forward. This also shows that your team has a\nclear understanding of their tasks and is moving them through the process.\nBecause recidivism points to churn in your workflow, it’s a good metric to look\nat every day. If you start to see that number increase, it’s best to dig into the\nproblematic tasks and get to the bottom of the issue.\n\n■ Good/bad build ratio---Knowing that your team is moving through the workflow\nis important, but you want to make sure they’re moving good code through the\nworkflow. Looking at the good/bad build ratio is a good indicator that your\nteam is publishing changes within the boundaries of your automated test criteria and your code quality rule sets. I firmly believe that when a build breaks,\neveryone should stop what they’re doing and figure out how to fix it. The build\nrules are set up to make sure everything you’ve determined necessary is working. If for some reason it’s not working, it should be the team’s priority to figure\nout why and fix the problem. If you’re following this methodology, then this\nnumber becomes a very important indicator of your team’s ability to develop\nwithin the rules you’ve defined.\n\n■ Consumer-facing quality rating---I’ve seen many teams that separate their production support from their development teams. When this happens, oftentimes\ndevelopers don’t really know how their work is impacting the consumer. That’s\nbad, especially if your team is trying to achieve a continuous delivery workflow.\nIn this context it’s great to have an aggregated metric of quality that combines\nyour most important statistics from your consumer-facing system to show the\nteam how well things are working.\n\nAn example dashboard with all of these metrics is shown in figure 9.9.\n\n This base set of metrics is a great way to keep a daily pulse on the consistent performance of your team. By combining this base data with specific metrics that your team\nhas committed to improve during retrospectives, you have the basics along with specific hot metrics that you need to focus on to bring about positive change.\n\n9.1.2 What managers want to see\n\nManagers can affect how a team interfaces with the rest of the organization, how it operates, and who plays what role on the team. Managers are usually also the conduit\nbetween the development team and the senior leadership team, so they not only need\nto understand the details of how the team is performing, but they also need to know how\nto roll it up in a way that they can show off to the next level of the organizational chart.\n Managers should care about the team’s daily breakdown of metrics. If you have a\nstrong team that has true ownership over their products, then managers should be\nable to let the team handle their day-to-day metrics on their own. Instead of micromanaging the details, managers should look at the data over time to see how the team\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": "Publishing metrics", "section_num": "9.1.1", "section_title": "What to use on your team", "l1_summary": "CHAPTER 9 Publishing metrics This also shows that your team has a\nclear understanding of their tasks and is moving them through the process. Managers should care about the team’s daily breakdown of metrics.", "l2_summary": "CHAPTER 9 Publishing metrics progress and your team is pushing changes through the pipeline at a fairly consistent rate. This also shows that your team has a\nclear understanding of their tasks and is moving them through the process. 9.1.2 What managers want to see Managers can affect how a team interfaces with the rest of the organization, how it operates, and who plays what role on the team. Managers should care about the team’s daily breakdown of metrics.", "prev_page": {"page_num": 206, "segment_id": "00206"}, "next_page": {"page_num": 208, "segment_id": "00208"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["team", "metrics", "managers", "good", "should", "moving", "workflow", "build", "also", "important"], "content_type": "practice", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["task flow and recidivism", "good/bad build ratio", "consumer-facing quality rating"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["publishing metrics", "team performance monitoring", "quality assurance"], "key_concepts": ["task flow and recidivism", "good/bad build ratio", "consumer-facing quality rating"], "problem_statement": "Improving team performance and ensuring code quality in a development pipeline", "solution_approach": "Implementing metrics to monitor task progress, code quality, and consumer impact", "extraction_method": "lm"}}
{"segment_id": "00208", "page_num": 208, "segment": "The right data for the right audience\n\n185\n\nTags show you what is currently\ntrending in the dev cycle.\n\nRecidivism shows you\nhow your workflow is going.\n\nYou can see the volume of work by\nshowing PR and commit data over time.\n\nFigure 9.9 A good example default dashboard to get you started with using metrics in your\ndevelopment cycle\n\nis improving. Managers should also be able to compare data across teams to be able to\nfind similar trends and determine when something is working well for one team and if\nthey should consider trying to implement a similar thing on another.\n\n To be able to accomplish these things managers must have a different view into the\ndata of the organization. Typical elements that I’ve found work well from a management perspective are:\n\n■ Lead time---The time between defining and completing a task gives you the\n\nhigh-level health of how well you can get work through your team.\n\n■ Velocity/volume---Velocity is great to show you how consistently your team is\nworking; volume tells you how many tasks are getting done. Because velocity\nshows your estimates over time and volume shows your total tasks over time, the\ntwo together can give you a good idea of the real amount of work your team\ngets done.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": "Publishing metrics", "section_num": "9.1.1", "section_title": "What to use on your team", "l1_summary": "The right data for the right audience You can see the volume of work by\nshowing PR and commit data over time. high-level health of how well you can get work through your team.", "l2_summary": "The right data for the right audience Recidivism shows you\nhow your workflow is going. You can see the volume of work by\nshowing PR and commit data over time. ■ Lead time---The time between defining and completing a task gives you the high-level health of how well you can get work through your team. ■ Velocity/volume---Velocity is great to show you how consistently your team is\nworking; volume tells you how many tasks are getting done.", "prev_page": {"page_num": 207, "segment_id": "00207"}, "next_page": {"page_num": 209, "segment_id": "00209"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["time", "data", "volume", "work", "team", "shows", "able", "well", "velocity", "right"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["recidivism", "lead time", "velocity/volume"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["dashboard metrics", "team performance monitoring", "management insights"], "key_concepts": ["trending data", "workflow analysis", "work volume tracking"], "problem_statement": "Improving team and workflow management through effective metric visualization", "solution_approach": "Implementing a dashboard with key metrics such as lead time, velocity/volume, and recidivism to monitor and compare team performance", "extraction_method": "lm"}}
{"segment_id": "00209", "page_num": 209, "segment": "186\n\nCHAPTER 9 Publishing metrics\n\n■ Estimate health---Your team’s ability to estimate accurately tells you how well they\n\nunderstand their work and how predictable they are.\n\n■ Committers and pull requestors---Managers are usually also concerned about the\ncareers of their team members. They’re going to care who the top performers\non the team are and who is doing what work. Understanding who is committing\ncode and who is reviewing code can show you trends in contribution across the\nteam that can be used to help coach developers if you see things that shouldn’t\nbe happening.\n\n■ Tags and labels---These help managers slice and dice their reports by what the\nteam is working on and how they identify their work. By giving teams the freedom to label as they see fit, managers can get deep insight into how their team\nthinks, works, and feels.\n\n■ Pull requests and commits over time---As with the development dashboard, managers should want to keep a pulse on the productivity of the team.\n\n■ Consumer-facing quality rating over time---This is also an important statistic for\nmanagers to care about over time because the trends they see here indicate the\nsuccess of the team and in what direction it’s trending. If a team’s consumer\nquality is going up over a long period of time, then the manager should dig in\nto find what’s good and how to replicate that success across other teams.\n\nOne thing to note is that it’s great not only to show stats but also to show terms so you\ncan click into the data that isn’t ideal. Figure 9.10 shows the distribution of lead time\nand estimate health along with the statistical overview of them side by side.\n\n In this case an estimate health of 6 means that tasks are taking on average 6 days\nlonger than developers think they will; that’s bad. By showing the distribution you can\nlook at the data in more detail to see how big this problem is. In this case the data is\nskewed by a few tasks that took a lot longer than estimated; specifically there are 4 values of 197, 124, 91, and 57 days that are skewing the mean. For this team let’s say that\nanything in the queue for longer than a month is a task that was deprioritized and forgotten. With that in mind you can remove all tickets that have an estimate health of\ngreater than 30, thus removing the outliers. After this query modification you’ll see a\nmuch different picture, as shown in figure 9.11.\n\n As you can see, those values were skewing lead time and estimate health significantly. By digging into the details and honing their queries to remove anomalies when\nit makes sense, they could see the true picture and dive into the problem tasks to find\nout why they sat incomplete for two to four months. The potential next step for this\nteam would be to repeat this exercise for lead time to make sure the statistics represent the true work of the team.\n\n Using this technique with the data shown, managers can get a good idea of the big\npicture and drill into trends when they become apparent. Managers care about\nindividuals as well as data across teams. Ultimately managers also care about how their\nteam’s performance affects strategic goals because that’s what they’re responsible for\nshowing to the higher levels of leadership in their organization.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": "Publishing metrics", "section_num": "9.1.1", "section_title": "What to use on your team", "l1_summary": "■ Estimate health---Your team’s ability to estimate accurately tells you how well they They’re going to care who the top performers\non the team are and who is doing what work. As you can see, those values were skewing lead time and estimate health significantly.", "l2_summary": "■ Estimate health---Your team’s ability to estimate accurately tells you how well they understand their work and how predictable they are. ■ Committers and pull requestors---Managers are usually also concerned about the\ncareers of their team members. They’re going to care who the top performers\non the team are and who is doing what work. By giving teams the freedom to label as they see fit, managers can get deep insight into how their team\nthinks, works, and feels. As you can see, those values were skewing lead time and estimate health significantly.", "prev_page": {"page_num": 208, "segment_id": "00208"}, "next_page": {"page_num": 210, "segment_id": "00210"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["team", "managers", "time", "estimate", "health", "what", "data", "work", "also", "care"], "content_type": "theory", "domain": "management|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["estimate health", "committers and pull requestors", "tags and labels", "pull requests and commits over time", "consumer-facing quality rating over time"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["publishing metrics", "team performance indicators", "code contribution analysis", "work distribution visualization", "quality improvement tracking"], "key_concepts": ["estimate accuracy", "developer productivity", "team contribution trends", "work item labeling", "long-term quality metrics"], "problem_statement": "Improving team estimation accuracy and understanding of developer contributions", "solution_approach": "Using various metrics to analyze and improve team performance", "extraction_method": "lm"}}
{"segment_id": "00210", "page_num": 210, "segment": "The right data for the right audience\n\n187\n\nBy showing the distribution (“distro”)\nas terms, you can click into areas\nyou want to investigate.\n\nShowing the stats for these\ngives you a high level idea\nof the health of the statistic.\n\nAn estimate health of 6 means the team\nis taking on average 6 days longer than\nestimated to finish tasks---that's bad.\n\n0 means estimates are spot on. Negative numbers\nindicate overestimating. Positive numbers indicate\nunderestimating. By checking the distribution we\ncan see almost 75% of estimates are correct\nwithin a few days despite the mean of 6.\n\nEven though the mean lead\ntime is 17 almost 50% of tasks\nare done within 3 days. Looking\nat the distribution shows us the\noutliers are killing the mean.\n\nThe very large max is\nprobably skewing the mean,\nso it's important to look at\nthe distribution also.\n\nFigure 9.10 Mean lead time and estimate health don’t properly represent the team because they’re skewed by\nhigh maximum values. By breaking out the distribution next to the statistical overviews, you can get a better\npicture of the outliers and a better representation of the data.\n\nBy removing the biggest anomaly\nmean estimate health is perfect.\n\nThe distributions are largely unchanged\nsince we only removed a few values\nfrom our data source query.\n\nAfter removing the highest outliers from estimate health,\nlead time has also improved. This query could still be\ntweaked to remove the maximum lead time value.\n\nFigure 9.11 After removing the four largest outliers from the estimate health and lead time, the statistics\nlook much closer to the team’s targets. The query to get data for lead time could be tweaked even further.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": "Publishing metrics", "section_num": "9.1.1", "section_title": "What to use on your team", "l1_summary": "Even though the mean lead\ntime is 17 almost 50% of tasks\nare done within 3 days. By removing the biggest anomaly\nmean estimate health is perfect. After removing the highest outliers from estimate health,\nlead time has also improved.", "l2_summary": "Even though the mean lead\ntime is 17 almost 50% of tasks\nare done within 3 days. Looking\nat the distribution shows us the\noutliers are killing the mean. Figure 9.10 Mean lead time and estimate health don’t properly represent the team because they’re skewed by\nhigh maximum values. By removing the biggest anomaly\nmean estimate health is perfect. After removing the highest outliers from estimate health,\nlead time has also improved. The query to get data for lead time could be tweaked even further.", "prev_page": {"page_num": 209, "segment_id": "00209"}, "next_page": {"page_num": 211, "segment_id": "00211"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["health", "mean", "lead", "time", "distribution", "estimate", "data", "outliers", "team", "days"], "content_type": "tutorial", "domain": "devops|data_science", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["estimate health", "lead time"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Estimate Health", "Lead Time", "Data Distribution"], "key_concepts": ["Distribution of Estimates", "Mean Lead Time", "Outliers in Data"], "problem_statement": "Improving the accuracy and efficiency of task estimates and lead times within a team.", "solution_approach": "Analyzing data distribution to identify outliers and adjust statistical measures accordingly.", "extraction_method": "lm"}}
{"segment_id": "00211", "page_num": 211, "segment": "188\n\nCHAPTER 9 Publishing metrics\n\n9.1.3 What executives care about\n\nThe next few levels up the organization chart can change team composition or organization structure, budgets, and strategies. Leadership teams care how things are working currently, but they’re usually more interested in the big picture and how that\naffects the well-being of the company as a whole. If teams are hitting their goals and\nthe organization is successful, you may not even have to worry about rolling reports up\nto this level. When I have to address senior leadership, I make sure that I can demonstrate the information they need as quickly and efficiently as possible. Interestingly\nenough, communication to leadership often happens through presentations instead\nof through dashboards and ad hoc metrics. The size of your organization and how\nhands-on your leadership team is will determine the best way to communicate data to\nthem. Normally they’ll be most interested in data around strategic objectives. If your\nleadership team sets a strategic direction to better engage customers by releasing code\nmore frequently through a DevOps model, you should publish the release frequency\nover time so they can see the progress you’re making on the strategy.\n\n The business metrics we talked about in chapter 6 are great candidates for data\nthat executives would care about. If you created a metric that determined business\nsuccess criteria of a product, then that metric over time is definitely going to be something the people sponsoring your team will care about. If there are several development teams in your organization and each has its own set of business success criteria\nmetrics, the aggregation of all these metrics would produce a great example of a dashboard or report that leadership would care about.\n\n If you’re not sure where to start but want to put an executive dashboard together,\nhere are some metrics that I’ve found leadership teams typically care about regardless\nof strategy:\n\n■ Number of releases/features per release---The frequency and volume with which you\nget changes out that your consumers respond well to show how engaged you\nare with them through your software products.\n\n■ Consumer-facing quality rating over time---This is the one metric that everyone in\nthe organization should care about. From an executive level this tells you if\neverything is moving in the right direction.\n\n■ Development cost---Knowing how much you’re spending on development is something anyone who controls a budget cares about. When you couple this with\nconsumer engagement and satisfaction, it can help everyone understand the\nvalue they’re getting from their investment.\n\nLet’s take an example of a native mobile app. The consumer-facing quality rating\ncomes from the app store---iTunes for iOS apps and Google Play for Android apps.\nThe rating is pretty simple; consumers give the app zero to five stars based on how\nmuch they like it, and the rating is published through the store. A quality check on\nthis number could be the crash percentage for the version of the app released.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": "Publishing metrics", "section_num": "9.1.3", "section_title": "What executives care about", "l1_summary": "CHAPTER 9 Publishing metrics 9.1.3 What executives care about The business metrics we talked about in chapter 6 are great candidates for data\nthat executives would care about.", "l2_summary": "CHAPTER 9 Publishing metrics 9.1.3 What executives care about Leadership teams care how things are working currently, but they’re usually more interested in the big picture and how that\naffects the well-being of the company as a whole. The size of your organization and how\nhands-on your leadership team is will determine the best way to communicate data to\nthem. The business metrics we talked about in chapter 6 are great candidates for data\nthat executives would care about. ■ Consumer-facing quality rating over time---This is the one metric that everyone in\nthe organization should care about.", "prev_page": {"page_num": 210, "segment_id": "00210"}, "next_page": {"page_num": 212, "segment_id": "00212"}, "flags": {"continuity_gap": true}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["care", "leadership", "metrics", "organization", "team", "teams", "rating", "data", "time", "business"], "content_type": "theory", "domain": "management|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": ["native mobile app"], "technologies": ["DevOps model", "iTunes", "Google Play"], "frameworks": [], "methodologies": ["DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["Number of releases/features per release", "Consumer-facing quality rating over time", "Development cost"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Publishing metrics to leadership", "Key business metrics for executives", "Engagement with consumers through software products"], "key_concepts": ["executive reporting", "strategic objectives", "consumer engagement"], "problem_statement": "How to effectively communicate key performance indicators (KPIs) and metrics to senior leadership in a DevOps environment.", "solution_approach": "Providing clear, concise data visualizations and reports that align with strategic goals.", "extraction_method": "lm"}}
{"segment_id": "00212", "page_num": 212, "segment": "The right data for the right audience\n\n189\n\nCorrelation of crash rate, star rating, and cost per release\n\n4.4\n\n4.2\n\ng\nn\ni\nt\na\nr\n\nr\na\nt\nS\n\n5.0\n\n4.6\n\n4.2\n\n3.8\n\n3.4\n\n0\n\n4.3\nCrash rate: 1.3\nStar rating: 4\nFeature: Style updates\nCost: 650000\n\n1\n\n2\nCrash rate\n\n5.0\n\n3\n\n4\n\nEach release is labeled with the\nheadline feature for that release.\n\nLocation tab\n\nStyle updates\n\nMostly fixes\n\nActivity sharing\n\nEach release is represented as a\ncorrelation between key metrics.\n\nThe size of each bubble represents the\ncost of the release in resource hours.\n\nFigure 9.12 An example executive dashboard showing releases sized by cost and measured against\ncrash rate and star rating. Minor releases are much less expensive and tend to be of higher quality.\n\nThe development cost is most easily calculated by adding up the hours of the team\nthat worked on the project. Devices, equipment, and support are usually included in\noverhead costs.\n\n For mobile apps it’s usually easiest to focus on a single feature at a time. In our example each release is tied to a single headline feature that’s published with the metrics.\n\n For the executive team it’s best to get all the data in a single place so they can see\nthe whole picture at a glance. In the example graph shown in figure 9.12 we use a bubble series chart to show all of these metrics at once. There’s a bubble for each release\non an axis, sized by the cost in total hours for the release. The x-axis represents crash\nrate and the y-axis represents the star rating.\n\n As you can see in figure 9.12, there’s a clear correlation between crash rate and star\nrating. When the crash rate is low, releases tend to get better ratings. You can also see\nthat the minor releases (4.2, 4.3, 4.4) cost a lot less than the major release (5.0). From\nan executive point of view it looks like the team is more efficient releasing minor\npieces to the app than full-fledged features.\n\n If your team isn’t doing so well, then more frequent reports to leadership are usually in order. In those updates you should use the key data points you’re tracking to\nget your team back on track to show that you know the problem, know how to fix it,\nand are making progress. Demonstrating how changes you’re making affect your metrics and that you show progress as they’re improving are exactly what leadership wants\nto see; if there’s a problem, at least you understand it, are correcting it, and are tracking it effectively.\n\n9.1.4 Using metrics to prove a point or effect change\n\nBecause you’ve made it this far through this book, you know that metrics are awesome,\nbut you can’t get a clear picture without several of them that show different facets of\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 4, "cols": 6, "data": [["4.4", "4.3", "", "", "", ""], ["4.2", "Crash rat\nStar ratin\nFeature:", "", "e: 1.3\ng: 4\nStyle updates", "", ""], ["", "Cost: 650", "", "000", "", ""], ["", "", "", "", "", "5.0"]], "markdown": "| 4.4 | 4.3 |  |  |  |  |\n|---|---|---|---|---|---|\n| 4.2 | Crash rat\nStar ratin\nFeature: |  | e: 1.3\ng: 4\nStyle updates |  |  |\n|  | Cost: 650 |  | 000 |  |  |\n|  |  |  |  |  | 5.0 |"}], "table_count": 1, "chapter_num": "9", "chapter_title": "Publishing metrics", "section_num": "9.1.3", "section_title": "What executives care about", "l1_summary": "Correlation of crash rate, star rating, and cost per release 4.3\nCrash rate: 1.3\nStar rating: 4\nFeature: Style updates\nCost: 650000 Each release is labeled with the\nheadline feature for that release.", "l2_summary": "Correlation of crash rate, star rating, and cost per release 4.3\nCrash rate: 1.3\nStar rating: 4\nFeature: Style updates\nCost: 650000 Each release is labeled with the\nheadline feature for that release. Each release is represented as a\ncorrelation between key metrics. There’s a bubble for each release\non an axis, sized by the cost in total hours for the release. As you can see in figure 9.12, there’s a clear correlation between crash rate and star\nrating.", "prev_page": {"page_num": 211, "segment_id": "00211"}, "next_page": {"page_num": 213, "segment_id": "00213"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": true, "has_citations": false, "key_terms": ["release", "crash", "rate", "cost", "metrics", "star", "rating", "each", "team", "feature"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["crash rate", "star rating", "cost per release"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["release management", "metric correlation", "executive dashboard"], "key_concepts": ["correlation between crash rate and star rating", "cost-effectiveness of minor releases", "use of metrics for decision-making"], "problem_statement": "Improving the efficiency and quality of mobile app releases", "solution_approach": "Using metrics such as crash rate, star rating, and cost to inform release strategies", "extraction_method": "lm"}}
{"segment_id": "00213", "page_num": 213, "segment": "190\n\nCHAPTER 9 Publishing metrics\n\nHuge jump in bugs midway\nthrough development cycle\n\nBug vs Tasks 2.5 release cycle\nData captured per day\n\n15\n\n10\n\n5\n\n0\n\nTasks\n\nBugs\n\n20\n\n40\n\n60\n\nDay\n\nWhatever gets done in the beginning\nseems to generate lots of bugs.\n\nFigure 9.13 The trends for a team that spent one month on development and one\nmonth on hardening\n\nwhat you’re trying to measure. Sometimes someone will want to make a point and will\npull a specific combination of data to prove their perspective. The problem that can\nstem from pulling only part of the whole data set is that they’ll pull only the data they\nneed to prove a point, which can lead to uninformed and usually poor decisions.\n\n If you want to use data to effect change, you should find a metric that matters to\nthe people you’re trying to convince, but always keep an open mind; your assumptions may be incorrect. Data can be misleading when used in a superficial fashion;\nthat is, a single metric doesn’t tell the whole story. When you’re working to prove or\ndisprove a hypothesis, be sure to have different data points that check each other.\n\n Take, for example, a team that’s developing features for a mobile app that has a\ntwo-month development cycle for each feature, the second half of which is purely dedicated to hardening the codebase and cleaning up tech debt. If you look at some data\nfor this team, you could see trends like those shown in figure 9.13.\n\n At a glance it seemed horrible that there was a one-month hardening period.\nSome of the developers on the project wanted to move to a more continuous delivery\ntype model where they didn’t build up any tech debt as they went along to potentially\nshave time off the release cycle.\n\n The team was able to use the data from their release cycle to convince the stakeholders to change how the team was delivering code with the promise of more frequent releases. After a release they started seeing the benefits. They finished tasks at a\nslower pace, but they were able to work out the kinks and get the release out the door\non time. The updated release cycle is shown in figure 9.14.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 2, "cols": 2, "data": [["", ""], ["", ""]], "markdown": "|  |  |\n|---|---|\n|  |  |"}], "table_count": 1, "chapter_num": "9", "chapter_title": "Publishing metrics", "section_num": "9.1.3", "section_title": "What executives care about", "l1_summary": "Bug vs Tasks 2.5 release cycle\nData captured per day Figure 9.13 The trends for a team that spent one month on development and one\nmonth on hardening The updated release cycle is shown in figure 9.14.", "l2_summary": "Huge jump in bugs midway\nthrough development cycle Bug vs Tasks 2.5 release cycle\nData captured per day Figure 9.13 The trends for a team that spent one month on development and one\nmonth on hardening If you look at some data\nfor this team, you could see trends like those shown in figure 9.13. The team was able to use the data from their release cycle to convince the stakeholders to change how the team was delivering code with the promise of more frequent releases. The updated release cycle is shown in figure 9.14.", "prev_page": {"page_num": 212, "segment_id": "00212"}, "next_page": {"page_num": 214, "segment_id": "00214"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "cycle", "release", "team", "month", "bugs", "development", "tasks", "figure", "hardening"], "content_type": "theory|practice", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["Bug vs Tasks", "Tasks", "Bugs"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["Publishing metrics", "Development cycle", "Hardening period"], "key_concepts": ["Data-driven decision making", "Release cycles", "Tech debt management"], "problem_statement": "Huge jump in bugs midway through development cycle", "solution_approach": "Using data to convince stakeholders for a more frequent release cycle", "extraction_method": "lm"}}
{"segment_id": "00214", "page_num": 214, "segment": "Different publishing methods\n\n191\n\nBugs vs. Tasks 2.6 release cycle\nData captured per day\n\nBugs and tasks are more consistent\nday to day throughout the cycle.\n\nTasks\n\nBugs\n\n6\n\n4\n\n2\n\n0\n\nThe maximum on\nthe x-axis has been\ncut nearly by 66%.\n\n20\n\n40\n\n60\n\nDay\n\nThere is only a slight\ntech debt spike at the\nend of this cycle.\n\nFigure 9.14 The updated velocity and volume for the team after they changed their development\npractices\n\nAt first everyone was really excited that they were able to deliver faster. Counterintuitively, when the release went out they noticed their star ratings on the app store\nstarted to decline. They dug into the results and noticed consumers nitpicking the\nnew features they had released. After meeting and discussing how this could have happened, they pointed back to the shortened development cycle, which included a\nmuch-abridged beta. It turned out that in their previous way of developing software\nthey were able to get a buggy version into their beta users’ hands very quickly. This led\nto an extended hardening period not only for code but also for the features themselves. By focusing on just the data from their project management system and ignoring the data they were collecting from their beta and how it affected the project, they\nended up hurting their consumer when all they wanted was to deliver more efficiently.\n In this example the team narrowed their focus to where they thought the problem\nwas instead of looking at the big picture. They ignored the beta feedback as a data\npoint in their lifecycle and looked only at data from their immediate team. To avoid\nthis problem the team could have acknowledged that the beta cycle was a critical part\nof delivery and figured out how to measure its value before cutting it.\n\n As you dig into the data, make sure you do your analysis with an open mind instead\n\nof manipulating the data to fit your hypothesis.\n\n9.2\n\nDifferent publishing methods\nEvery organization has different quirks around their methods of communication.\nWherever you work there are probably different emails, dashboards, and reports that\ndifferent people rely on for information. The best way to communicate across an organization is to communicate within the boundaries of that organization. If everyone\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": "Publishing metrics", "section_num": "9.1.3", "section_title": "What executives care about", "l1_summary": "Different publishing methods Tasks 2.6 release cycle\nData captured per day Bugs and tasks are more consistent\nday to day throughout the cycle.", "l2_summary": "Different publishing methods Tasks 2.6 release cycle\nData captured per day Bugs and tasks are more consistent\nday to day throughout the cycle. They ignored the beta feedback as a data\npoint in their lifecycle and looked only at data from their immediate team. of manipulating the data to fit your hypothesis. Different publishing methods\nEvery organization has different quirks around their methods of communication.", "prev_page": {"page_num": 213, "segment_id": "00213"}, "next_page": {"page_num": 215, "segment_id": "00215"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "different", "cycle", "beta", "team", "methods", "bugs", "tasks", "only", "organization"], "content_type": "case_study", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["star ratings on the app store"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["release cycle", "beta testing", "development practices"], "key_concepts": ["bugs vs. tasks", "velocity and volume", "tech debt spike"], "problem_statement": "Shortening the development cycle by reducing the beta phase led to a decline in app store ratings due to bugs in new features.", "solution_approach": "Recognizing that the beta feedback is crucial for measuring the overall quality of the product and integrating it into the project lifecycle.", "extraction_method": "lm"}}
{"segment_id": "00215", "page_num": 215, "segment": "192\n\nCHAPTER 9 Publishing metrics\n\nexpects to see a status report every Tuesday, then maybe piggy-backing on that report\nis a great idea. If everyone sends those emails straight to their deleted folder, then\nmaybe using a dashboard where people can pull data when they need it will work better. Following are some tips on using dashboards, emails, and reports; you know what\nworks best in your organization so you should take what works best and apply it.\n\n9.2.1 Building dashboards\n\nWe’ve been using dashboards throughout this book. Web-based dashboards are a\ngreat way to publish results so anyone in your company can check out the data when\nthey want. The following sections provide some dashboarding tips.\n\nDON’T RESTRICT ACCESS INSIDE THE COMPANY, BUT KEEP IT INTERNAL\nSometimes people like to hide data to protect themselves or others. This is usually a\nbad idea within your own company or across your own team. If data is used in a collaborative and open way across your company, then you should allow anyone to access\nthe data. If you work in an environment where culture encourages the use of data in a\nnegative way (for political gain, for example), then it may be best to just keep the data\naccessible inside your team to avoid distraction from outside.\n\n On the flip side, dashboards contain a lot of information about how you work and\nin some cases what you’re working on. It’s always best to think about keeping your\nwebsites and data as secure as possible at all times. Even though you should keep it\nopen inside the office, you should keep it from prying eyes outside your walls.\n\nMAKE IT CUSTOMIZABLE\nManagers, developers, executives, and coaches will all want to see data differently. You\nshould give them the flexibility to see metrics the way they want and when they want.\nIf you’re using the tools we’ve been using throughout this book, you can use Kibana’s\ngreat UI to create your own dashboard and save it. An example of the elements used\nto save dashboards and update widgets is shown in figure 9.15.\n\nName the dashboard\nwhatever you want.\n\nEach component has\ncustomizable settings.\n\nFigure 9.15 The Kibana header and some of its customizable elements\n\nYou can save different dashboard\nconfigurations in Elasticsearch.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": "Publishing metrics", "section_num": "9.2.1", "section_title": "Building dashboards", "l1_summary": "9.2.1 Building dashboards We’ve been using dashboards throughout this book. If data is used in a collaborative and open way across your company, then you should allow anyone to access\nthe data.", "l2_summary": "Following are some tips on using dashboards, emails, and reports; you know what\nworks best in your organization so you should take what works best and apply it. 9.2.1 Building dashboards We’ve been using dashboards throughout this book. Web-based dashboards are a\ngreat way to publish results so anyone in your company can check out the data when\nthey want. If data is used in a collaborative and open way across your company, then you should allow anyone to access\nthe data. You\nshould give them the flexibility to see metrics the way they want and when they want.", "prev_page": {"page_num": 214, "segment_id": "00214"}, "next_page": {"page_num": 216, "segment_id": "00216"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "dashboards", "using", "should", "want", "then", "dashboard", "some", "best", "company"], "content_type": "tutorial", "domain": "management|security", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Kibana", "Elasticsearch"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": true, "has_metrics": true, "metrics": ["publishing metrics", "status report"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Kibana", "Elasticsearch"], "topics": ["dashboard building", "data access control", "customizable dashboards"], "key_concepts": ["internal data sharing", "secure data management", "personalized dashboard views"], "problem_statement": "How to effectively share and manage publishing metrics within an organization", "solution_approach": "Using web-based dashboards, customizing access and views, and ensuring secure internal data sharing", "extraction_method": "lm"}}
{"segment_id": "00216", "page_num": 216, "segment": "Different publishing methods\n\n193\n\nOne of the worst things you can do is give someone a dashboard where the data they\nneed isn’t right where they expect it to be. When this happens, people will be less\nlikely to look at it.\n\nENSURE PEOPLE KNOW THAT DATA IS THERE AS A TOOL, NOT A WEAPON\nThe biggest fear people have around metrics is that someone will use them to prove\nthey’re not good or not good enough. Never use metrics as a weapon. Always communicate that the data is there to help everyone understand how the team is working and\nto help track improvements to their process.\n\n Look for opportunities inside your environment to evangelize these techniques in\na positive way. I’ve found that starting off retrospectives with data ends up leading to\nhonest and open conversations that help everyone understand how their work affects\nmetrics and why they’re important to track.\n\n Ensure that the people generating the data have a say in which metrics they think\nare most important. For example, if you think you should be measuring CLOC, ask\nyour development team what they think of the idea. If they don’t like that metric, ask\nwhy; this will usually lead to them helping you determine which metrics they actually\ncare about. Inclusion in the conversation and collaboration from the team will lead to\nbetter success and broader buy-in.\n\nUSE PAGE TRACKING TO UNDERSTAND HOW YOUR DASHBOARDS ARE USED\nYour dashboard is a product for a consumer, so treat it that way. I’ve found that often\ninternal tools aren’t tracked the same way consumer-facing tools are, which ends up\nleading to things people don’t use or aren’t happy with. By using page tracking on your\ndashboards, you’ll see what people click on the most, what they’re drilling in on, and\nhow frequently they use them. You can then use this information to continue to hone\nwhich metrics mean the most to your team and your company. Go back to chapter 6 and\napply the same techniques you would to track success on your metrics dashboards.\nSome metrics that I’ve found useful in this context are page hits and metric clicks:\n\n■ Page hits---How many people are using your dashboard, which views are they\n\nfrequenting, and how long are they spending on those pages?\n\n■ Metric clicks---What metrics are people clicking on? This shows you what everyone cares about the most and will help you optimize default dashboards.\n\n9.2.2 Using email\n\nEmail can be a great tool but it can also be a curse. When this book was written, Kleiner\nPerkins Caufield & Byers (KPBC)1 Internet Trends,2 a standard in reporting consumer\ntechnology trends, estimated that Americans spend nearly an hour a day checking\nemail or messaging on their phones or tablets, and of total email traffic more than 70%\nof it is spam. If you combine these two facts you’ll see proof of something you probably\nalready know; people spend a lot of time deleting emails. If you spend that much time\n\n1 Kleiner Perkins Caufield & Byers is a venture capital firm in Menlo Park in Silicon Valley, en.wikipedia.org/\n\nwiki/Kleiner_Perkins_Caufield_%26_Byers.\n\n2 A briefing on the Internet Trends 2014---Code Conference,/internet-trends.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": "Publishing metrics", "section_num": "9.2.1", "section_title": "Building dashboards", "l1_summary": "Never use metrics as a weapon. Ensure that the people generating the data have a say in which metrics they think\nare most important. ■ Metric clicks---What metrics are people clicking on?", "l2_summary": "When this happens, people will be less\nlikely to look at it. Never use metrics as a weapon. Ensure that the people generating the data have a say in which metrics they think\nare most important. By using page tracking on your\ndashboards, you’ll see what people click on the most, what they’re drilling in on, and\nhow frequently they use them. ■ Page hits---How many people are using your dashboard, which views are they ■ Metric clicks---What metrics are people clicking on?", "prev_page": {"page_num": 215, "segment_id": "00215"}, "next_page": {"page_num": 217, "segment_id": "00217"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["people", "metrics", "data", "will", "which", "what", "help", "team", "most", "page"], "content_type": "theory|practice", "domain": "management|data_science", "complexity": "intermediate", "companies": ["Kleiner Perkins Caufield & Byers"], "people": [], "products": [], "technologies": [], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["page hits", "metric clicks"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["page tracking"], "topics": ["dashboard design", "data communication", "email usage"], "key_concepts": ["data visibility", "metric weaponization", "user engagement"], "problem_statement": "Improper dashboard design and metric communication", "solution_approach": "Proper dashboard placement, positive metric evangelism, inclusive metric selection, page tracking", "extraction_method": "lm"}}
{"segment_id": "00217", "page_num": 217, "segment": "194\n\nCHAPTER 9 Publishing metrics\n\nfiltering junk you don’t care about, you should definitely make sure that you get information out that your audience cares about when they’re ready for it.\n\n Following are a few email tips.\n\nALLOW PEOPLE TO OPT IN TO YOUR EMAIL REPORTS\nMost people are pretty email savvy, and when they start getting something they don’t\ncare about, they’ll send it to their spam folder. Even worse, if they’re using a spam filter, your nice reports could get caught up in it. If you can give your audience the ability to opt in to an email from your dashboard, then they’ve shown they care enough\nabout the data that they want to have updates periodically.\n\n On the flip side, you should allow people to opt out as well. Make sure to pay attention to how many people are receiving your report as a success metric of whether the\nemail communicates what people need.\n\nGIVE THE LEAST AMOUNT OF DATA NEEDED AND REFERENCE A DYNAMIC DASHBOARD\nWhen you send someone an email, it should contain just the information they need\nnow. Going back to the principle of showing people only what they can affect, keep\nyour emails to a few key metrics that recipients can take direct action on if they choose\nto. At the same time, perhaps they’d like to dig deeper or see more data. In this case,\ninstead of continually adding more data to email reports, add it to the dashboard and\nreference the dashboard in your email.\n\nESTABLISH THE RIGHT CADENCE\nDon’t send emails so frequently that your coworkers get annoyed or, even worse, create an email rule that sends your reports into an abyss. Developers and Scrum masters\nmight want daily reports in their inbox, but higher-level managers may need them\nonly weekly or biweekly. If you have good page-tracking setup in your dashboard, then\nyou should be able to correlate the user activity with your email schedules to see how\neffective your emails are at driving people to the data you’re publishing.\n\nCase study: driving visibility toward a strategic goal\nIn this case study we’ll look at a company that has brick-and-mortar retail and has\nrecently established an e-commerce site to increase sales. Once the site was up and running, they started looking at differences in buying trends between their e-commerce\nsite and their physical stores. One significant trend they noticed was that when consumers purchased items off their site they typically bought one item. Customers in their\nretail locations would typically buy three to four items at a time. Their retail strategy had\nbeen to group related items together so as consumers were shopping for what they\ncame in for, they also found related items.\n\n As the leadership team discussed this trend, they decided they wanted to sponsor a\ndevelopment initiative to increase sales of related items on their e-commerce site.\nAccording to their calculations, if they could achieve a similar trend on their e-commerce site as they were seeing in their retail locations, e-commerce revenue would\nincrease by over 70%. The goal they set for their development team was to increase\nrelated product sales by 100% by the end of their fiscal year.\n\nLicensed to Mark Watson <nordickan@gmail.com>\n\n9.3", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": "Publishing metrics", "section_num": "9.2.1", "section_title": "Building dashboards", "l1_summary": "Following are a few email tips. ALLOW PEOPLE TO OPT IN TO YOUR EMAIL REPORTS\nMost people are pretty email savvy, and when they start getting something they don’t\ncare about, they’ll send it to their spam folder. In this case,\ninstead of continually adding more data to email reports, add it to the...", "l2_summary": "CHAPTER 9 Publishing metrics Following are a few email tips. ALLOW PEOPLE TO OPT IN TO YOUR EMAIL REPORTS\nMost people are pretty email savvy, and when they start getting something they don’t\ncare about, they’ll send it to their spam folder. If you can give your audience the ability to opt in to an email from your dashboard, then they’ve shown they care enough\nabout the data that they want to have updates periodically. On the flip side, you should allow people to opt out as well. In this case,\ninstead of continually adding more data to email reports, add it to the dashboard and\nreference the dashboard in your email.", "prev_page": {"page_num": 216, "segment_id": "00216"}, "next_page": {"page_num": 218, "segment_id": "00218"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["email", "people", "site", "reports", "dashboard", "data", "commerce", "items", "should", "when"], "content_type": "practice", "domain": "management|programming", "complexity": "intermediate", "companies": ["null"], "people": ["Mark Watson"], "products": ["null"], "technologies": ["null"], "frameworks": ["null"], "methodologies": ["null"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["email open rate", "user activity on dashboard"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["Email reporting optimization", "Dashboard integration", "Cadence of publishing metrics"], "key_concepts": ["Opt-in/opt-out mechanisms", "Relevant data in emails", "Dynamic dashboard references", "Right cadence for email reports"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00218", "page_num": 218, "segment": "Case study: driving visibility toward a strategic goal\n\n195\n\nExecutives\n\nProgram\nmanagers\n\nTechnical\nmanagers\n\nData processing team\n\nWeb services team\n\nFront end team\n\nFigure 9.16 An overview of the organization and responsibilities in delivering the feature\n\nThe program managers broke this goal down into why consumers weren’t buying\nrelated products. Following the same model as the retail locations, they decided that\nthey needed some way to show related items to consumers as they were shopping. The\nfeature they decided on was a product recommendation system that would tell a consumer what would complement the products the consumer was looking at. They also\nrealized that in an e-commerce site they could continually improve their site recommendations as they analyzed the patterns of what people bought together.\n\n The delivery team came up with the technical designs and worked with program\nmanagement to flesh out the delivery plan. From the start they decided to create dashboards for everyone involved in the project so all levels of the organization could see\nthe progress as they moved forward. There were a few different delivery teams that\nhad to coordinate efforts to move this feature forward; they’re shown in figure 9.16.\n\n They started with the business success metrics they wanted to get out of the feature\nthey were delivering. In this case the business success metrics they came up with were\nas follows:\n\n■\n\nItems sold---They were already tracking this as a success metric of the site in\ngeneral.\n\n■ Number of items per order---If this feature was successful, they would see this number going up. This was the key metric that related to the strategic goal laid out\nby the leadership team.\n\n■ Recommended items per order---This was the key metric that showed the development team how effective their feature was. If this number was going up, then\nthe product recommender was a success. If they noticed this number increasing\nat the same rate as the number of items per order, then they would know that\nthis was the driving feature for the executive goal.\n\nLike the team in chapter 6, they used StatsD to instrument their code and send these\nmetrics back to their monitoring system, as shown in the following listing.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": "Publishing metrics", "section_num": "9.2.1", "section_title": "Building dashboards", "l1_summary": "Figure 9.16 An overview of the organization and responsibilities in delivering the feature ■ Number of items per order---If this feature was successful, they would see this number going up. This was the key metric that related to the strategic goal laid out\nby the leadership team.", "l2_summary": "Figure 9.16 An overview of the organization and responsibilities in delivering the feature They started with the business success metrics they wanted to get out of the feature\nthey were delivering. In this case the business success metrics they came up with were\nas follows: ■ Number of items per order---If this feature was successful, they would see this number going up. This was the key metric that related to the strategic goal laid out\nby the leadership team. ■ Recommended items per order---This was the key metric that showed the development team how effective their feature was.", "prev_page": {"page_num": 217, "segment_id": "00217"}, "next_page": {"page_num": 219, "segment_id": "00219"}, "extended_fields": {"has_code": true, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["team", "feature", "items", "number", "goal", "would", "success", "program", "managers", "related"], "content_type": "case_study", "domain": "architecture|management", "complexity": "intermediate", "companies": [], "people": ["Executives", "Program managers", "Technical managers", "Data processing team", "Web services team", "Front end team"], "products": [], "technologies": ["StatsD"], "frameworks": [], "methodologies": [], "programming_languages": ["not specified"], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["Items sold", "Number of items per order", "Recommended items per order"], "has_case_study": true, "case_study_company": null, "tools_mentioned": ["StatsD"], "topics": ["product recommendation system", "business metrics", "team collaboration"], "key_concepts": ["feature delivery", "metrics-driven development", "cross-team coordination"], "problem_statement": "Improving product sales through better recommendations on an e-commerce site", "solution_approach": "Developing a product recommendation system and using metrics to track its effectiveness", "extraction_method": "lm"}}
{"segment_id": "00219", "page_num": 219, "segment": "196\n\nCHAPTER 9 Publishing metrics\n\nIncrementing\nthe counter\nfor total\norders\n\nListing 9.1 Using StatsD to add business-specific metrics to the code\n\nprivate static final StatsDClient statsd = new\n\nNonBlockingStatsDClient(\"the.prefix\", \"statsd-host\", 8125);\n\n...\n statsd.incrementCounter(\"orders\");\n statsd.recordGaugeValue(\"itemsPerOrder\", x);\n statsd.recordGaugeValue(\"recommendedItems\", y);\n\nSetting up the\nStatsD client\n\nSetting item count per\norder for this order\n\nHow many items in\nthis order were from\nrecommendations\n\nDifferent teams had different release metrics. The web services team was working on\nimproving their pull request workflow, so they focused on PR comments and pull\nrequests/commits to make sure that was healthy. This is shown in figure 9.17.\n\n The team building a data processor for recommendations knew they had to iterate\nquickly, so they focused on estimate health, lead time, and deploy frequency to ensure\nthey could react to changes predictably. That is shown in figure 9.18.\n\n The program management across the entire project wanted to keep tabs on all of\nthe teams. They wanted to ensure that the workflow was predictable and consistent, so\nthey cared most about velocity, recidivism (how tasks moved through the workflow),\nlead time, and estimate health. They wanted to see stats across all projects but also to\nbe able to drill into individual projects. Their dashboard is shown in figure 9.19.\n\nThey used development time and recidivism\nas primary metrics, so they kept these up\ntop along with their focus metrics.\n\nPR-specific metrics helped\nthe team keep tabs on their\nimmediate concerns.\n\nThey watched PRs and commits over time to ensure\ncode was being updated consistently.\n\nFigure 9.17 The web services team’s dashboard. Development time, PR comments, and PR hours\nopen are right on target. The pulse at the bottom shows that SCM activity over time is healthy.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": "Publishing metrics", "section_num": "9.2.1", "section_title": "Building dashboards", "l1_summary": "Setting up the\nStatsD client This is shown in figure 9.17. That is shown in figure 9.18.", "l2_summary": "CHAPTER 9 Publishing metrics Listing 9.1 Using StatsD to add business-specific metrics to the code statsd.incrementCounter(\"orders\");\n statsd.recordGaugeValue(\"itemsPerOrder\", x);\n statsd.recordGaugeValue(\"recommendedItems\", y); Setting up the\nStatsD client This is shown in figure 9.17. That is shown in figure 9.18.", "prev_page": {"page_num": 218, "segment_id": "00218"}, "next_page": {"page_num": 220, "segment_id": "00220"}, "extended_fields": {"has_code": true, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["statsd", "metrics", "time", "team", "figure", "order", "workflow", "shown", "ensure", "wanted"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["StatsD"], "frameworks": [], "methodologies": [], "programming_languages": ["Java"], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["total orders", "itemsPerOrder", "recommendedItems", "velocity", "recidivism", "lead time", "estimate health"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["StatsD"], "topics": ["publishing metrics", "metrics collection using StatsD", "team-specific release metrics"], "key_concepts": ["incrementing counters", "gauge values", "team focus metrics"], "problem_statement": "How to collect and use metrics for different teams in a project", "solution_approach": "Using StatsD for metric collection and defining team-specific KPIs", "extraction_method": "lm"}}
{"segment_id": "00220", "page_num": 220, "segment": "Case study: driving visibility toward a strategic goal\n\n197\n\nThey want to ensure they understand the work,\nso they track that by checking their estimate health.\n\nBecause this team is practicing CD\nthey want to deploy multiple times a day.\n\nTo ensure the mean doesn't give them a false\nsense of security they break out the distribution\nof lead time and estimate health.\n\nThis team strives for a short lead\ntime to check the relationship between\nrequirements and task completion.\n\nFigure 9.18 The dashboard for the development team working on the data processing\n\nThe clickable list of projects allows the\nviewer to sort this data by project.\n\nLead time and estimate health tell the viewer\nhow predictable these teams can be.\n\nRecidivism shows how healthy\nthe workflows are across teams.\n\nVelocity over bugs shows how much the team is getting\ndone along with how many issues they're creating.\n\nFigure 9.19 The dashboard used by the program and project management team\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": "Publishing metrics", "section_num": "9.2.1", "section_title": "Building dashboards", "l1_summary": "Figure 9.18 The dashboard for the development team working on the data processing Lead time and estimate health tell the viewer\nhow predictable these teams can be. Figure 9.19 The dashboard used by the program and project management team", "l2_summary": "They want to ensure they understand the work,\nso they track that by checking their estimate health. Because this team is practicing CD\nthey want to deploy multiple times a day. This team strives for a short lead\ntime to check the relationship between\nrequirements and task completion. Figure 9.18 The dashboard for the development team working on the data processing Lead time and estimate health tell the viewer\nhow predictable these teams can be. Figure 9.19 The dashboard used by the program and project management team", "prev_page": {"page_num": 219, "segment_id": "00219"}, "next_page": {"page_num": 221, "segment_id": "00221"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["team", "estimate", "health", "lead", "time", "want", "ensure", "figure", "dashboard", "data"], "content_type": "case_study", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["CD", "data processing"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["estimate health", "lead time", "recidivism", "velocity over bugs"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["dashboard metrics", "continuous deployment", "team visibility"], "key_concepts": ["estimate health", "lead time", "recidivism", "velocity over bugs"], "problem_statement": "Ensuring team visibility and understanding of work through tracking metrics", "solution_approach": "Implementing a dashboard with key metrics for continuous improvement", "extraction_method": "lm"}}
{"segment_id": "00221", "page_num": 221, "segment": "198\n\nCHAPTER 9 Publishing metrics\n\nTop line info for the leadership team included total monthly\nrolling revenue and orders and the latest customer tweets.\n\nThe current initiative was\nsupposed to increase items\nper order, so that got top\nbilling on the dashboard.\n\nThis team is using the CHD rating\nintroduced in chapter 7. When it dips\nbelow 80 the team health shows red.\n\nThese projects (code named\nby the teams themselves) are on\ntrack based on the CHD rating.\n\nFigure 9.20 The dashboard used by the leadership team to show the success of the initiative and the\noverall health of the contributing projects. Leadership is always paying attention to the consumer voice\nthrough their Twitter hashtags and the total revenue. They’ve added the average items per order to\ntrack the success of the latest development initiative and show overall ratings for each development\nteam on the project.\n\nThe leadership team trusted their managers to run the factory and wanted data that\nshowed them the bottom line. They wanted to know how sales were going, how projects were progressing, and if their strategies were improving their online business.\nTheir standard metrics were the monthly rolling revenue, customer feedback, and\nrolling monthly orders. For this specific project they added a widget to their dashboard to track average items per order. In addition, they added one widget per project\nto represent the health of the project; if a project was off track it would change to a\nshade of red, and if it was on track it would be green. Their dashboard is shown in figure 9.20.\n\n By showing the key measurement of the current initiative, the leadership team\ncould track the success to potentially check and adjust the approach of the teams if\nnecessary. They didn’t care to get involved in the daily activity of the teams, so they\ndidn’t need the detail on pull requests, estimates, and completed tasks. They did care\nif a project was not healthy, and so to track that they used the code health determination (CHD) rating introduced in chapter 7, which is a combination of workflow, code\nquality, and continuous delivery release efficiency.\n\n As consumers started using the feature, the team watched the business metrics\nstart to be affected as well. Now they had all the data in place that they needed; the\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": "Publishing metrics", "section_num": "9.2.1", "section_title": "Building dashboards", "l1_summary": "This team is using the CHD rating\nintroduced in chapter 7. When it dips\nbelow 80 the team health shows red. Figure 9.20 The dashboard used by the leadership team to show the success of the initiative and the\noverall health of the contributing projects.", "l2_summary": "CHAPTER 9 Publishing metrics This team is using the CHD rating\nintroduced in chapter 7. When it dips\nbelow 80 the team health shows red. Figure 9.20 The dashboard used by the leadership team to show the success of the initiative and the\noverall health of the contributing projects. They’ve added the average items per order to\ntrack the success of the latest development initiative and show overall ratings for each development\nteam on the project. For this specific project they added a widget to their dashboard to track average items per order.", "prev_page": {"page_num": 220, "segment_id": "00220"}, "next_page": {"page_num": 222, "segment_id": "00222"}, "flags": {"continuity_gap": true}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["team", "track", "project", "leadership", "initiative", "dashboard", "health", "chapter", "metrics", "monthly"], "content_type": "case_study", "domain": "management|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["total monthly rolling revenue", "orders", "customer tweets", "average items per order", "CHD rating"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["dashboard metrics", "project health tracking", "leadership team monitoring"], "key_concepts": ["CHD rating", "dashboard visualization", "KPIs for leadership"], "problem_statement": "How to monitor and track the success of development initiatives and project health for a leadership team.", "solution_approach": "Using a dashboard with key metrics like rolling revenue, orders, customer feedback, average items per order, and CHD rating.", "extraction_method": "lm"}}
{"segment_id": "00222", "page_num": 222, "segment": "Summary\n\n199\n\nSource control + project tracking + build and release\n\nDevelopment\nteam\n\nAggregate data across teams\nfor cross-project visibility\n\nSource control + project tracking + build & release for multiple teams\n\nTechnical and\nprogram managers\n\nGenerate combined data metrics for high-level\nvisibility and call out business success metrics\n\nCombined & business-specific metrics\n\nLeadership team\n\nFigure 9.21 A review of the organization and the data being used for each team\n\ndevelopment teams were tracking the details they cared about, the management layer\nwas tracking cross-team data, and they were able to collect data to show the strategic\nobjectives were being met for the leadership team. Figure 9.21 shows the different\nmembers of the teams and the data they were using.\n\n Everyone was happy, Yay!\n\n9.4\n\nSummary\nDifferent levels of the organization need different information from your development teams. Publishing the right information to the relevant audience is a key factor\nin implementing successful metrics reporting across the organization. In this chapter\nyou learned the following:\n\n■ Publish metrics to audiences that can act on them.\n■ Metrics at the team level are the detailed metrics that show you how consistently\n\nyour team is working. Key things to track on your team are these:\n■ Tags and labels\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": "Publishing metrics", "section_num": "9.2.1", "section_title": "Building dashboards", "l1_summary": "Aggregate data across teams\nfor cross-project visibility Figure 9.21 A review of the organization and the data being used for each team your team is working.", "l2_summary": "Aggregate data across teams\nfor cross-project visibility Combined & business-specific metrics Figure 9.21 A review of the organization and the data being used for each team Figure 9.21 shows the different\nmembers of the teams and the data they were using. ■ Metrics at the team level are the detailed metrics that show you how consistently your team is working.", "prev_page": {"page_num": 221, "segment_id": "00221"}, "next_page": {"page_num": 223, "segment_id": "00223"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["team", "metrics", "data", "teams", "tracking", "project", "development", "organization", "different", "summary"], "content_type": "theory", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["source control", "project tracking", "build and release"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["combined data metrics", "business success metrics"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["source control", "project tracking", "build and release"], "topics": ["team-level metrics", "cross-team visibility", "leadership-level metrics"], "key_concepts": ["metrics reporting", "data aggregation", "cross-project visibility"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00223", "page_num": 223, "segment": "200\n\nCHAPTER 9 Publishing metrics\n\n■ Pull requests and commits\n■ Task flow and recidivism\n■ Good/bad build ratio\n■ Consumer-facing quality rating\n\n■ When identifying improvement areas during retrospectives and reflection periods, be sure to also identify corresponding metrics to ensure you’re making\ndaily progress.\n\n■ Metrics at the manager level should show how teams are doing over time and\nhow individuals are doing on a team. Key managerial metrics to focus on\ninclude the following:\n■ Lead time\n■ Velocity/volume\n■ Estimate health\n■ Committers and pull requestors\n■ Tags and labels\n■ Pull requests and commits over time\n■ Consumer-facing quality rating over time\n\n■ Metrics at the executive level should show how the progress of the team affects\n\nstrategic goals. A few default metrics executives should see are these:\n■ Number of releases/features per release\n■ Consumer-facing quality rating over time\n■ Development cost\n\n■ Dashboards are a great way to communicate metrics across the organization.\n\nEffective dashboards have these characteristics:\n■ They don’t restrict access inside the company but keep it internal.\n■ They make it customizable.\n■ They ensure people know that data is there as a tool, not a weapon.\n■ They use page tracking to show how your dashboards are used.\n\n■ Emails are a good communication method under the following conditions:\n\n■ They allow people to opt into your email reports.\n■ They give the least amount of data needed and reference a dynamic dashboard.\n\n■ They establish the right cadence.\n\n■ Showing distributions as well as stats allows you identify anomalies at a glance\n\nand remove them when appropriate.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": "Publishing metrics", "section_num": "9.2.1", "section_title": "Building dashboards", "l1_summary": "CHAPTER 9 Publishing metrics ■ Metrics at the manager level should show how teams are doing over time and\nhow individuals are doing on a team. ■ Metrics at the executive level should show how the progress of the team affects", "l2_summary": "CHAPTER 9 Publishing metrics ■ Metrics at the manager level should show how teams are doing over time and\nhow individuals are doing on a team. Key managerial metrics to focus on\ninclude the following:\n■ Lead time\n■ Velocity/volume\n■ Estimate health\n■ Committers and pull requestors\n■ Tags and labels\n■ Pull requests and commits over time\n■ Consumer-facing quality rating over time ■ Metrics at the executive level should show how the progress of the team affects A few default metrics executives should see are these:\n■ Number of releases/features per release\n■ Consumer-facing quality rating over time\n■ Development cost ■ Dashboards are a great way to communicate metrics across the organization.", "prev_page": {"page_num": 222, "segment_id": "00222"}, "next_page": {"page_num": 224, "segment_id": "00224"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["metrics", "time", "pull", "consumer", "facing", "quality", "rating", "should", "show", "dashboards"], "content_type": "theory", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["lead time", "velocity/volume", "estimate health", "committers and pull requestors", "tags and labels", "pull requests and commits over time", "consumer-facing quality rating over time", "number of releases/features per release", "development cost"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["publishing metrics", "retrospectives", "managerial metrics", "executive metrics", "dashboard characteristics", "email communication methods"], "key_concepts": ["metrics for improvement", "team performance tracking", "strategic goal alignment"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00224", "page_num": 224, "segment": "Measuring your team\nagainst the agile principles\n\nThis chapter covers\n\n■ Breaking down the agile principles into\n\nmeasurable pieces\n\n■ Applying the techniques in this book to the\n\nagile principles\n\n■ Associating metrics with the agile principles\n\n■ Measuring your team’s adherence to the agile\n\nprinciples\n\nIf you ask the CIOs of most Fortune 500 companies if their teams are practicing agile\ndevelopment, they’ll probably all say yes. If you sit on any of the development teams\nin those companies, you’ll notice that they operate differently to varying degrees.\nThat’s okay; in fact, one of the great things about agile development frameworks is\nthat they allow development teams to move quickly in their own context.\n\n Often as teams evolve their agile development processes they’ll start to question\ntheir agility. One way to combat this is to measure your team against the agile principles themselves.\n\n Because we’re talking about continuous improvement in the scope of agile software projects, it seems fitting to complete this book by talking about measuring\n\n201\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": "Publishing metrics", "section_num": "9.2.1", "section_title": "Building dashboards", "l1_summary": "Measuring your team\nagainst the agile principles ■ Breaking down the agile principles into ■ Associating metrics with the agile principles", "l2_summary": "Measuring your team\nagainst the agile principles ■ Breaking down the agile principles into ■ Associating metrics with the agile principles ■ Measuring your team’s adherence to the agile Often as teams evolve their agile development processes they’ll start to question\ntheir agility. One way to combat this is to measure your team against the agile principles themselves.", "prev_page": {"page_num": 223, "segment_id": "00223"}, "next_page": {"page_num": 225, "segment_id": "00225"}, "flags": {"continuity_gap": true}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["agile", "principles", "development", "teams", "measuring", "team", "against", "book", "companies", "talking"], "content_type": "tutorial", "domain": "programming|management", "complexity": "intermediate", "companies": ["Fortune 500"], "people": [], "products": [], "technologies": [], "frameworks": ["Agile"], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["measuring team adherence to agile principles"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Measuring Agile Principles", "Agile Development Processes", "Continuous Improvement"], "key_concepts": ["agile principles", "measurable pieces", "metrics", "team adherence"], "problem_statement": "How to measure a team's adherence to agile principles", "solution_approach": "Applying techniques from the book and associating metrics with agile principles", "extraction_method": "lm"}}
{"segment_id": "00225", "page_num": 225, "segment": "202\n\nCHAPTER 10 Measuring your team against the agile principles\n\nyour project and team against the agile principles. Using the concepts we’ve talked\nabout in this book, we’ll break down the agile principles and show you what to use to\nmeasure your team against each one.\n\n10.1 Breaking the agile principles into measurable\n\ncomponents\nAs a reference point for this chapter let’s start off by reviewing the agile principles as\nthey’re written in the Agile Manifesto.1 Just in case you don’t remember them off the\ntop of your head, here they are in order.\n\n■ Our highest priority is to satisfy the customer through early and continuous\n\ndelivery of valuable software.\n\n■ Welcome changing requirements, even late in development. Agile processes\n\nharness change for the customer’s competitive advantage.\n\n■ Deliver working software frequently, from a couple of weeks to a couple of\n\nmonths, with a preference to the shorter timescale.\n\n■ Business people and developers must work together daily throughout the\n\nproject.\n\n■ Build projects around motivated individuals. Give them the environment and\n\nsupport they need, and trust them to get the job done.\n\n■ The most efficient and effective method of conveying information to and\n\nwithin a development team is face-to-face conversation.\n\n■ Working software is the primary measure of progress.\n■ Agile processes promote sustainable development. The sponsors, developers,\n\nand users should be able to maintain a constant pace indefinitely.\n\n■ Continuous attention to technical excellence and good design enhances agility.\n■ Simplicity---the art of maximizing the amount of work not done---is essential.\n■ The best architectures, requirements, and designs emerge from self-organizing\n\nteams.\n\n■ At regular intervals, the team reflects on how to become more effective, then\n\ntunes and adjusts its behavior accordingly.\n\nTo get started, let’s have a bit of fun with the agile principles. If you put them into a\nword cloud, as shown in figure 10.1, you can get a different perspective on what the\nagile principles are talking about by seeing which words are used the most.\n\n It’s not surprising that the two most frequent words are development and software\nbecause these principles are about developing software. A few other interesting qualities of this viewpoint stand out:\n\n■ The only adjective used more than once is effective.\n■ Requirements, developers, work, team, and process are all focal points.\n\n1\n\n “Principles behind the Agile Manifesto,” agilemanifesto.org/principles.html.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": "Publishing metrics", "section_num": "10.1", "section_title": "Breaking the agile principles into measurable", "l1_summary": "CHAPTER 10 Measuring your team against the agile principles your project and team against the agile principles. 10.1 Breaking the agile principles into measurable", "l2_summary": "CHAPTER 10 Measuring your team against the agile principles your project and team against the agile principles. 10.1 Breaking the agile principles into measurable ■ Agile processes promote sustainable development. To get started, let’s have a bit of fun with the agile principles. “Principles behind the Agile Manifesto,” agilemanifesto.org/principles.html.", "prev_page": {"page_num": 224, "segment_id": "00224"}, "next_page": {"page_num": 226, "segment_id": "00226"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["agile", "principles", "team", "software", "development", "against", "requirements", "developers", "work", "most"], "content_type": "tutorial", "domain": "programming|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Agile Manifesto", "word cloud"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["customer satisfaction", "working software delivery frequency", "daily business and development collaboration", "motivated individuals", "face-to-face communication effectiveness", "technical excellence and design", "sustainable development pace", "team self-organization"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["measuring team against agile principles", "agile manifesto breakdown", "word cloud analysis"], "key_concepts": ["customer satisfaction", "flexible requirements handling", "frequent software delivery", "daily collaboration", "motivated individuals", "face-to-face communication", "working software as progress measure", "sustainable development", "technical excellence", "team self-organization", "regular team reflection"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00226", "page_num": 226, "segment": "Breaking the agile principles into measurable components\n\n203\n\n“Effective” is the\nonly big adjective.\n\nOther things that\nare apparently\nvery important.\n\nWe are talking\nabout developing\nsoftware.\n\nFigure 10.1 The agile principles as a word cloud\n\nIf you start to think about how to measure agile teams, you should definitely think about\nhow you’re measuring the main focal points. If you start breaking these down into questions you can apply measurements to, you’ll get something like the following:\n\n■ Are your development teams effective?\n■ Are your processes effective?\n■ Are your requirements effective?\n\n■\n\nIs your software effective?\n\nThe end goal is great software. You can put these into a simple equation like the one\nshown in figure 10.2.\n\nHOW you’re\ndoing work\n\nTeams + Processes + Requirements = Software\n\nWHO is doing\nthe work\n\nWHAT you’re\nbuilding\n\nFigure 10.2 The core elements of the agile principles\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": "Publishing metrics", "section_num": "10.1", "section_title": "Breaking the agile principles into measurable", "l1_summary": "■ Are your processes effective? ■ Are your requirements effective? Is your software effective?", "l2_summary": "Figure 10.1 The agile principles as a word cloud ■ Are your development teams effective? ■ Are your processes effective? ■ Are your requirements effective? Is your software effective? Teams + Processes + Requirements = Software", "prev_page": {"page_num": 225, "segment_id": "00225"}, "next_page": {"page_num": 227, "segment_id": "00227"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["effective", "agile", "software", "principles", "figure", "teams", "breaking", "start", "think", "like"], "content_type": "tutorial", "domain": "programming|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["effectiveness of development teams", "effectiveness of processes", "effectiveness of requirements", "effectiveness of software"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["measuring agile principles", "agile methodology", "software development effectiveness"], "key_concepts": ["agile principles", "measurement of agile teams", "effectiveness in software development"], "problem_statement": "How to measure the effectiveness of agile teams and their components in software development?", "solution_approach": "Breaking down agile principles into measurable components such as effective development teams, processes, requirements, and resulting software.", "extraction_method": "lm"}}
{"segment_id": "00227", "page_num": 227, "segment": "204\n\nCHAPTER 10 Measuring your team against the agile principles\n\nManage\ntasks and\nbugs\n\nProject\ntracking\n\nManage\ncode and\ncollaboration\n\nSource\ncontrol\n\nGenerate\nbuilds and\nrun tests\n\nContinuous\nintegration\n\nMove code\nacross\nenvironments\n\nDeployment\ntools\n\nEnsure\neverything\nis working\n\nApplication\nmonitoring\n\n• Good designs\n• Architecture\n• Technical excellence\n• Continuous delivery\n• Become more\n effective\n\n• Good designs\n• Architecture\n• Technical excellence\n• Deliver frequently\n• Continuous delivery\n• Become more\n effective\n\n• Good designs\n• Architecture\n• Technical excellence\n• Deliver frequently\n• Continuous delivery\n• Become more\n effective\n\n• Good designs\n• Architecture\n• Technical excellence\n• Working software\n• Satisfy the customer\n\n• Good designs\n• Architecture\n• Technical excellence\n• Simplicity\n• Changing\n requirements\n• Working together\n• Motivated individuals\n• Face to face\n conversation\n\nFigure 10.3 More agile tenets on the delivery lifecycle\n\nThe next step is to use the data you’ve been collecting throughout the book to answer\nyour questions. Using the high-level questions fleshed out previously, you can start\nbreaking the principles into categories in order to measure them. If you look at each\nagile principle in detail, you’ll start to see that of the twelve principles three imply\nmeasuring software, four implicate teamwork, four represent process, and one\nreferences requirements. Let’s start off with aligning the principles with the delivery\nlifecycle.\n\n10.1.1 Aligning the principles with the delivery lifecycle\n\nIf you take keywords from each of the principles and transpose them over the delivery\nlifecycle that you’ve been looking at in previous chapters, you can see where to get the\ndata you need to measure them, as shown in figure 10.3.\n\n Another way to look at these associations is to put all of the key measurements in a\n\nmatrix against the systems you can get data from, as shown in table 10.1.\n\nTable 10.1 Highlights of the agile principles and where to get data to measure them\n\nProject tracking\nsystems\n\nSource control\nmanagement\n\nCI and\ndeployment tools\n\nApplication\nmonitoring\n\nGood designs\n\nGood architectures\n\nTechnical excellence\n\nChanging requirements\n\nWorking together\n\nX\n\nX\n\nX\n\nX\n\nX\n\nX\n\nX\n\nX\n\nX\n\nX\n\nX\n\nX\n\nX\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 2, "cols": 5, "data": [["", "Project tracking\nsystems", "Source control\nmanagement", "CI and\ndeployment tools", "Application\nmonitoring"], ["", "X\nX", "X\nX\nX\nX", "X\nX\nX\nX", ""]], "markdown": "|  | Project tracking\nsystems | Source control\nmanagement | CI and\ndeployment tools | Application\nmonitoring |\n|---|---|---|---|---|\n|  | X\nX | X\nX\nX\nX | X\nX\nX\nX |  |"}], "table_count": 1, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.1", "section_title": "Breaking the agile principles into measurable", "l1_summary": "• Good designs\n• Architecture\n• Technical excellence\n• Continuous delivery\n• Become more\n effective • Good designs\n• Architecture\n• Technical excellence\n• Deliver frequently\n• Continuous delivery\n• Become more\n effective 10.1.1 Aligning the principles with the delivery lifecycle", "l2_summary": "• Good designs\n• Architecture\n• Technical excellence\n• Continuous delivery\n• Become more\n effective • Good designs\n• Architecture\n• Technical excellence\n• Deliver frequently\n• Continuous delivery\n• Become more\n effective • Good designs\n• Architecture\n• Technical excellence\n• Deliver frequently\n• Continuous delivery\n• Become more\n effective • Good designs\n• Architecture\n• Technical excellence\n• Working software\n• Satisfy the customer Figure 10.3 More agile tenets on the delivery lifecycle 10.1.1 Aligning the principles with the delivery lifecycle", "prev_page": {"page_num": 226, "segment_id": "00226"}, "next_page": {"page_num": 228, "segment_id": "00228"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": true, "has_citations": false, "key_terms": ["principles", "good", "delivery", "designs", "technical", "excellence", "architecture", "agile", "continuous", "working"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Project tracking systems", "Source control management", "CI and deployment tools", "Application monitoring"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["Project tracking", "Source control", "CI and deployment tools", "Application monitoring"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Project tracking systems", "Source control management", "CI and deployment tools", "Application monitoring"], "topics": ["Measuring team against agile principles", "Aligning agile principles with delivery lifecycle", "Data collection for measuring agile principles"], "key_concepts": ["Agile principles", "Delivery lifecycle", "Metrics for agile practices"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00228", "page_num": 228, "segment": "Three principles for effective software\n\n205\n\nTable 10.1 Highlights of the agile principles and where to get data to measure them\n\nProject tracking\nsystems\n\nSource control\nmanagement\n\nCI and\ndeployment tools\n\nApplication\nmonitoring\n\nMotivated individuals\n\nFace-to-face\ncommunication\n\nContinuous delivery\n\nBecoming more effective\n\nDelivering frequently\n\nWorking software\n\nSatisfied customers\n\nSimplicity\n\nX\n\nX\n\nX\n\nX\n\nX\n\nX\n\nX\n\nX\n\nX\n\nX\n\nX\n\nX\n\nX\n\nWith the agile principles mapped across the delivery lifecycle and broken down across\nfour key questions, you can start using metrics from previous chapters to start getting\nspecific on mapping metrics to agility.\n\n10.2 Three principles for effective software\n\nThe following three principles all have keywords that make you ask, “Is our software\neffective?” The keywords are italicized in the following list of principles:\n\n■ Working software is the primary measure of progress.\n■ Our highest priority is to satisfy the customer through early and continuous delivery\n\nof valuable software.\n\n■ Continuous attention to technical excellence and good design enhances agility.\n\nWorking software is perhaps the most obvious of the software-related measures but\nmany times the hardest to measure. Chapter 9 is dedicated to measuring how well\nyour software is working.\n\n Satisfying the customer is measured using techniques from chapter 6: using telemetry and business-specific metrics to measure how well your software does what it’s\nsupposed to do.\n\n Early and continuous delivery is covered in chapter 4 in our discussion of CI and\ndeployment systems. This is enabled by build systems that output digestible reports\nand automate enough of your build and deployment cycle to get comprehensive data\naround build and deployment times. You can also measure this through your PTSs covered in chapter 3 by tracking lead time or development time.\n\n You can argue that technical excellence and good design are measured by how fast\nyou can iterate on your code or how maintainable it is, and how well it satisfies the\nconsumer or how usable it is. Both of these are detailed in chapter 9.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 2, "cols": 5, "data": [["", "Project tracking\nsystems", "Source control\nmanagement", "CI and\ndeployment tools", "Application\nmonitoring"], ["", "X\nX\nX\nX\nX", "X\nX\nX", "X\nX\nX", ""]], "markdown": "|  | Project tracking\nsystems | Source control\nmanagement | CI and\ndeployment tools | Application\nmonitoring |\n|---|---|---|---|---|\n|  | X\nX\nX\nX\nX | X\nX\nX | X\nX\nX |  |"}], "table_count": 1, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.1", "section_title": "Breaking the agile principles into measurable", "l1_summary": "Three principles for effective software 10.2 Three principles for effective software of valuable software.", "l2_summary": "Three principles for effective software Table 10.1 Highlights of the agile principles and where to get data to measure them 10.2 Three principles for effective software ■ Working software is the primary measure of progress. of valuable software. Chapter 9 is dedicated to measuring how well\nyour software is working.", "prev_page": {"page_num": 227, "segment_id": "00227"}, "next_page": {"page_num": 229, "segment_id": "00229"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["software", "principles", "measure", "chapter", "effective", "deployment", "continuous", "delivery", "working", "three"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Project tracking systems", "Source control management", "CI and deployment tools", "Application monitoring"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["lead time", "development time"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Project tracking systems", "Source control management", "CI and deployment tools", "Application monitoring"], "topics": ["Agile principles", "Software effectiveness", "Metrics for agility"], "key_concepts": ["Working software as primary measure of progress", "Satisfying the customer through early and continuous delivery", "Continuous attention to technical excellence and good design"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00229", "page_num": 229, "segment": "206\n\nCHAPTER 10 Measuring your team against the agile principles\n\nIs it working?\n\nIs it designed well?\n\nIs your software effective?\n\nIs it delivered continuously?\n\nIs it built well?\n\nIs the customer happy?\n\nFigure 10.4 Mind mapping the big question “Is your software effective?” into smaller bits\nthat represent the principles\n\n10.2.1 Measuring effective software\n\nLet’s start with a mind map to align the principles with the big question you’re trying\nto answer, as shown in figure 10.4.\n\n If you single out just these questions over the delivery lifecycle, you’ll see that these\n\nquestions end up spanning most of it, as shown in figure 10.5.\n\n We talked about measuring CD in chapter 5, and there are concrete metrics you\n\ncan use to ensure that it is working well like the following:\n\n■ Successful versus failed builds:\n\n■ How well is your code review process working?\n■ How good is your local development environment?\nIs your team thinking about quality software?\n\n■\n\n■ How frequently do you get updates in front of your consumers?\n\nEnsuring that your software products are working well and your customers are satisfied\nare the topics we covered in chapter 6 when we talked about production monitoring\nand arbitrary metrics. Here are some metrics that can help you keep an eye on these:\n\n■ Business/application-specific metrics---These are defined based on your application\n\nand tell you how consumers are using your site.\n\nIs our software effective?\n\nManage\ntasks and\nbugs\n\nProject\ntracking\n\nManage\ncode and\ncollaboration\n\nGenerate\nbuilds and\nrun tests\n\nMove code\nacross\nenvironments\n\nSource\ncontrol\n\nContinuous\nintegration\n\nDeployment\ntools\n\nEnsure\neverything\nis working\n\nApplication\nmonitoring\n\n• Continuous\n delivery\n\n• Continuous\n delivery\n\n• Working software\n• Satisfy the customer\n\n• Good designs\n• Architecture\n• Technical\n excellence\n• Continuous\n delivery\n\nFigure 10.5 The systems you can get metrics out of to see if your software is effective\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.1", "section_title": "Breaking the agile principles into measurable", "l1_summary": "Is your software effective? 10.2.1 Measuring effective software Is our software effective?", "l2_summary": "Is your software effective? 10.2.1 Measuring effective software Is our software effective? • Continuous\n delivery • Continuous\n delivery • Working software\n• Satisfy the customer", "prev_page": {"page_num": 228, "segment_id": "00228"}, "next_page": {"page_num": 230, "segment_id": "00230"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": true, "has_citations": false, "key_terms": ["software", "working", "well", "effective", "metrics", "figure", "delivery", "continuous", "chapter", "measuring"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["code review process", "local development environment", "source control", "continuous integration", "deployment tools", "application monitoring"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["successful versus failed builds", "code review process working", "local development environment quality", "updates in front of consumers frequency", "business/application-specific metrics"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["build tools", "code review tools", "source control systems", "continuous integration servers", "deployment automation tools", "application monitoring tools"], "topics": ["measuring software effectiveness", "agile principles", "continuous delivery metrics", "customer satisfaction metrics"], "key_concepts": ["effective software", "Agile principles", "metrics for continuous delivery", "customer satisfaction"], "problem_statement": "Ensuring that the software is effective and customer happy throughout the development lifecycle", "solution_approach": "Using various metrics and tools to measure effectiveness, such as successful builds, code reviews, local environment quality, deployment frequency, and application monitoring", "extraction_method": "lm"}}
{"segment_id": "00230", "page_num": 230, "segment": "Four principles for effective process\n\n207\n\n■ Common site health statistics---These tell you how your application is performing\n\nand how healthy it is. Some key stats to keep an eye on for this would be:\n\n■ Error count\n■ CPU/memory utilization\n■ Response times\n■ Transactions\n■ Disk space\n■ Garbage collection\n■ Thread counts\n\n■ Semantic logging---This can also help you monitor application-specific metrics.\n\nWe also covered some of these metrics in chapter 8 when discussing what makes\nusable software with the following metrics:\n\n■ Usability\n■ Uptime\n■ MTTF\n\nYou can use the metrics we covered in chapter 8 to look at what makes well-built software:\n\n■ Maintainability\n■ MTTR---How fast can you get fixes out to consumers?\n■ Lead time---How fast can you get features out to consumers?\n\nUsing this comprehensive list of metrics you can get a clear picture of how effective\nyour software is, measured across the agile principles. Building software is what you’re\ndoing; your process is how you do it. The following four principles give you an outline\non how to measure your process.\n\n10.3 Four principles for effective process\n\nThe next group of four principles answers the question “Is your process effective?”\n\n■ Simplicity---The art of maximizing the amount of work not done is essential.\n■ Deliver working software frequently, from a couple of weeks to a couple of months,\n\nwith a preference to the shorter timescale.\n\n■ Face-to-face conversation---The most efficient and effective method of conveying\n\ninformation to and within a development team.\n\n■ Maintain a constant pace indefinitely---Agile processes promote sustainable\ndevelopment. The sponsors, developers, and users should be able to maintain a\nconstant pace indefinitely.\n\nAs a software engineer, my initial reaction to simplicity was that it related to software\nmeasurement. After all, you should use simple designs and standard patterns to maximize efficiency on your development team and increase the maintainability of your\nsoftware. Although maximizing the amount of work done relates to maintainable\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.1", "section_title": "Breaking the agile principles into measurable", "l1_summary": "Four principles for effective process and how healthy it is. 10.3 Four principles for effective process", "l2_summary": "Four principles for effective process and how healthy it is. We also covered some of these metrics in chapter 8 when discussing what makes\nusable software with the following metrics: You can use the metrics we covered in chapter 8 to look at what makes well-built software: Building software is what you’re\ndoing; your process is how you do it. 10.3 Four principles for effective process", "prev_page": {"page_num": 229, "segment_id": "00229"}, "next_page": {"page_num": 231, "segment_id": "00231"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["software", "principles", "effective", "process", "metrics", "four", "what", "development", "application", "some"], "content_type": "theory", "domain": "management|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["Error count", "CPU/memory utilization", "Response times", "Transactions", "Disk space", "Garbage collection", "Thread counts", "Usability", "Uptime", "MTTF", "Maintainability", "MTTR", "Lead time"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Four principles for effective process", "Common site health statistics", "Semantic logging"], "key_concepts": ["Simplicity", "Delivering working software frequently", "Face-to-face conversation", "Maintaining a constant pace indefinitely"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00231", "page_num": 231, "segment": "208\n\nCHAPTER 10 Measuring your team against the agile principles\n\ncode, simplicity is most easily measured by looking at data from your PTS, such as task\nvolume or the number of tasks completed. Maximizing the amount of work done\nimplies an effective process: understanding the minimum of what needs to get done\nin order to improve your consumer’s experience.\n\n Delivering working software frequently is easily measured by the amount of time it\ntakes to get tasks through the lifecycle and out to consumers. One caution for measuring the delivery of working software is to measure the time it takes to get working software to the consumer, not to the end of a development cycle if that doesn’t include\ndeployments. I’ve seen teams celebrate the fact that their development time was very\nshort, even though they did gigantic monthly releases to their consumers. Be honest\nwith your delivery time.\n\n Face-to-face conversation is great, but I’ve been in plenty of meetings or planning\nsessions where what is said ends up getting interpreted differently by different people\nin the room. The real key to this principle is ensuring that the team is directly communicating with each other for the highest possible clarity and understanding of what\nyou’re all working on.\n\n Maintaining a constant pace is most commonly measured by that old agile steadfast metric, velocity. Your ability to maintain a constant pace is affected by a host of factors including these:\n\n■ Maintainability of your code---If you have code that’s easy or at least predictable to\n\nchange and deploy, you should be able to keep a constant pace.\n\n■ The consistency of your estimates---By showing that you understand changes and\nyour team is breaking tasks down into manageable chunks, you avoid hiccups in\nyour pace.\n\n10.3.1 Measuring effective processes\n\nFigure 10.6 shows a mind map that helps you take a closer look at measuring your\nprocess.\n\n Again we’ll map the highlights across the delivery lifecycle to see what kind of data\n\nyou should care the most about in the context of your process. See figure 10.7.\n Starting with simplicity, there are a few different metrics you can look at:\n\n■ Use CLOC from your SCM system to see how much code is changing.\n■ Estimates and volume from your PTSs show you the amount of effort and number of tasks completed.\n\nAre you delivering frequently?\n\nIs your process effective?\n\nAre you delivering consistently?\n\nFigure 10.6 Mind mapping “Is your process effective?”\n\nDo you work things out face to face?\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.1", "section_title": "Breaking the agile principles into measurable", "l1_summary": "10.3.1 Measuring effective processes Is your process effective? Figure 10.6 Mind mapping “Is your process effective?”", "l2_summary": "CHAPTER 10 Measuring your team against the agile principles Delivering working software frequently is easily measured by the amount of time it\ntakes to get tasks through the lifecycle and out to consumers. Be honest\nwith your delivery time. 10.3.1 Measuring effective processes Is your process effective? Figure 10.6 Mind mapping “Is your process effective?”", "prev_page": {"page_num": 230, "segment_id": "00230"}, "next_page": {"page_num": 232, "segment_id": "00232"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["process", "measuring", "code", "tasks", "effective", "what", "working", "time", "face", "pace"], "content_type": "theory", "domain": "management|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["PTS", "SCM system"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["task volume", "number of tasks completed", "velocity", "time to get working software to the consumer"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["PTS", "SCM system"], "topics": ["measuring team performance against agile principles", "process effectiveness", "communication and clarity", "maintaining a constant pace"], "key_concepts": ["simplicity", "frequent delivery", "face-to-face communication", "constant pace"], "problem_statement": "Measuring the effectiveness of an agile team's processes", "solution_approach": "Using metrics like task volume, velocity, and time to deliver working software", "extraction_method": "lm"}}
{"segment_id": "00232", "page_num": 232, "segment": "Four principles for effective process\n\n209\n\nAre your processes effective?\n\nManage\ntasks and\nbugs\n\nProject\ntracking\n\nManage\ncode and\ncollaboration\n\nGenerate\nbuilds and\nrun tests\n\nMove code\nacross\nenvironments\n\nSource\ncontrol\n\nContinuous\nintegration\n\nDeployment\ntools\n\nEnsure\neverything\nis working\n\nApplication\nmonitoring\n\n• Constant pace\n• Simplicity\n• Face-to-face\n conversation\n\n• Constant pace\n• Simplicity\n• Face-to-face\n conversation\n\n• Deliver\n frequently\n\n• Deliver\n frequently\n\nFigure 10.7 Are your processes effective?\n\n■ Use estimate health and estimates together to check how predictable your estimates really are.\n\n■ Lead time, development time, and volume give you the high-level picture of\n\nhow fast you can deliver and at what frequency.\n\nFrequent delivery is measured not only by how often you ship code but also the pace\nat which you move. You can measure both frequency and pace with the following metrics from chapters 3 and 5:\n\n■ The old agile metric of velocity is great at tracking consistency.\n■ Use the number of successful deployments from your build and/or deploy\n\nsystems.\n\n■ Lead time will tell you how long it takes to get a task all the way through the lifecycle from concept to delivery.\n\n■ MTTR tells you how quickly you can react and tweak your system when necessary.\n■ Bug counts are a good way to check whether you’re delivering value or you’re\n\nchurning on issues due to poor code quality.\n\n■ As a check to bug count you can add code coverage and static analysis to the\n\nmix to make sure you’re delivering maintainable code.\n\nFace-to-face conversation isn’t so easy to directly measure with agile tools used today.\nBut you can measure the amount of communication through comments captured in\nyour systems at various points in the delivery flow and cross-reference those with arbitrary tags and labels in your PTS to figure out what level of measureable communication works best for your team:\n\n■ PTS and SCM comment counts, as noted previously, imply how well collaboration is going.\n\n■ Cross-reference these comment counts with labels and tags on tasks that your\n\nteam uses to indicate if they think communication is working well.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.1", "section_title": "Breaking the agile principles into measurable", "l1_summary": "Manage\ncode and\ncollaboration • Constant pace\n• Simplicity\n• Face-to-face\n conversation • Constant pace\n• Simplicity\n• Face-to-face\n conversation", "l2_summary": "Are your processes effective? Manage\ncode and\ncollaboration Move code\nacross\nenvironments • Constant pace\n• Simplicity\n• Face-to-face\n conversation • Constant pace\n• Simplicity\n• Face-to-face\n conversation how fast you can deliver and at what frequency.", "prev_page": {"page_num": 231, "segment_id": "00231"}, "next_page": {"page_num": 233, "segment_id": "00233"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": true, "has_citations": false, "key_terms": ["code", "face", "pace", "effective", "conversation", "deliver", "check", "time", "delivery", "measure"], "content_type": "tutorial", "domain": "devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Source control", "Continuous integration", "Deployment tools", "Application monitoring"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["Lead time", "MTTR", "Bug counts", "Code coverage", "Static analysis"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["PTS", "build and/or deploy systems"], "topics": ["Effective process principles", "Process management", "Delivery metrics"], "key_concepts": ["Constant pace", "Simplicity", "Face-to-face conversation", "Frequent delivery"], "problem_statement": "Improving the effectiveness of software development processes", "solution_approach": "Implementing agile methodologies, using tools for source control and continuous integration, measuring lead time and MTTR", "extraction_method": "lm"}}
{"segment_id": "00233", "page_num": 233, "segment": "210\n\nCHAPTER 10 Measuring your team against the agile principles\n\nYour team is building the software through agile processes; measuring how well your\nteam works together is the next set of agile principles we’ll take a look at.\n\n10.4 Four principles for an effective team\n\nThe next group of four answers the question “Is your development team effective?”\n\n■ The best architectures, requirements, and designs emerge from self-organizing\n\nteams.\n\n■ Business people and developers must work together daily throughout the\n\nproject.\n\n■ Build projects around motivated individuals. Give them the environment and\n\nsupport they need, and trust them to get the job done.\n\n■ At regular intervals, the team reflects on how to become more effective and\n\nthen tunes and adjusts its behavior accordingly.\n\nThese principles are all people issues: how well your team works together, how motivated they are, your level of autonomy, and how well you can check and adjust. How\nwell your team is working together affects many of your other metrics. How quickly\ntasks go through your workflow, how much code is written per task, how many bugs\nturn up, and how tasks move through your workflow are all indicators of how well your\nteam is working together.\n\n The first principle in this list ties team autonomy, or self-organization, directly back\n\nto the quality of the software.\n\n You could argue that the fourth item in this list, reflecting at regular intervals, is a\nprocess question because checking and adjusting are part of the agile development\nprocess. But I’ve seen dysfunctional teams take the time to reflect, only to chase the\nwrong goals because they can’t be honest with each other or they don’t engage in the\nimprovement process. It’s better to measure the ability to check and adjust as part of\nmeasuring the effectiveness of your team because it shows your team’s ability to face\nup to their weaknesses and take action to improve them.\n\n The key indicators of good teamwork will be how well things move through your\n\nworkflow and how your team feels about their work.\n\n10.4.1 Measuring an effective development team\n\nWe’ll start off again by creating a mind map for these questions, as shown in figure\n10.8.\n\nAre we working together?\n\nIs the team effective?\n\nIs everybody motivated?\n\nFigure 10.8 Mind mapping “Is the team effective?”\n\nAre we improving?\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.4", "section_title": "Four principles for an effective team", "l1_summary": "10.4 Four principles for an effective team 10.4.1 Measuring an effective development team Is the team effective?", "l2_summary": "CHAPTER 10 Measuring your team against the agile principles 10.4 Four principles for an effective team How\nwell your team is working together affects many of your other metrics. workflow and how your team feels about their work. 10.4.1 Measuring an effective development team Is the team effective?", "prev_page": {"page_num": 232, "segment_id": "00232"}, "next_page": {"page_num": 234, "segment_id": "00234"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": true, "has_citations": false, "key_terms": ["team", "well", "together", "effective", "measuring", "agile", "principles", "take", "development", "motivated"], "content_type": "theory", "domain": "management|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["how quickly tasks go through your workflow", "how much code is written per task", "how many bugs turn up", "how tasks move through your workflow"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Measuring team effectiveness in agile development", "Agile principles for effective teams", "Team motivation and self-organization", "Reflecting on team performance"], "key_concepts": ["self-organizing teams", "daily collaboration between business people and developers", "motivated individuals", "regular reflection and adjustment"], "problem_statement": "How to measure the effectiveness of a development team in an agile environment", "solution_approach": "Using principles such as self-organization, daily collaboration, motivated individuals, and regular reflection", "extraction_method": "lm"}}
{"segment_id": "00234", "page_num": 234, "segment": "Four principles for an effective team\n\n211\n\nIs the development team effective?\n\nManage\ntasks and\nbugs\n\nProject\ntracking\n\n• Work together\n• Motivated\n individuals\n\nManage\ncode and\ncollaboration\n\nGenerate\nbuilds and\nrun tests\n\nMove code\nacross\nenvironments\n\nSource\ncontrol\n\nContinuous\nintegration\n\nDeployment\ntools\n\nEnsure\neverything\nis working\n\nApplication\nmonitoring\n\n• Become more\n effective\n\n• Become more\n effective\n\n• Become more\n effective\n\n• Become more\n effective\n\nFigure 10.9 Transposing the principles regarding the development team over the development lifecycle\n\nTransposing this over the delivery lifecycle, you’ll see that these questions end up\nspanning the entire lifecycle, as shown in figure 10.9.\n\n In chapters 3 and 4 we went through the PTS and looked at data that helps analyze\nhow your team is working together and how motivated they are through the following\npoints:\n\n■ By tagging or labeling tasks in your PTS with how well developers think the task\nwas executed, you can measure how motivated individuals are. We covered this\nin section 3.2.5 with the use of happy and unhappy tags and used the same technique as a mutiny indicator inside the front cover.\n\n■ Teamwork can be measured by comment counts in PTS and SCM data. Keep in\nmind that what is good or bad will vary from team to team, so it’s important to\ncalibrate the numbers based on how well your team is working.\n\n■ Recidivism will also give you a good idea of how well your team is working\ntogether. If tasks are moving through your workflow, that’s usually a good indicator that the team is working well together. You can check this by adding in\nconsumer-facing defects; low recidivism and a high number of consumer-facing\ndefects mean that your team is passing tasks through without properly vetting\nthem---that’s bad. Conversely, high recidivism with low consumer defects means\nthat your team is churning, but at least it’s turning out code of good quality.\n\nIn chapter 2 we talked through how you can use tags and labels as a potential replacement of the Nico-nico calendar, an agile way to measure the motivation of your team.\nIf you encourage your team to label tasks with how they’re feeling as they finish tasks,\nthen you’ll start to build up data around the motivation of your team. What’s better is\nthat you’ll have tasks that relate to the happiness of your team.\n\n If you look at measuring improvement of effectiveness of your development team\nin this context, you can look at frequency of delivery. You can measure that with the\nfollowing metrics:\n\n■ Lead time---The amount of time it takes from inception to delivery of tasks\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.4", "section_title": "Four principles for an effective team", "l1_summary": "Four principles for an effective team Is the development team effective? Figure 10.9 Transposing the principles regarding the development team over the development lifecycle", "l2_summary": "Four principles for an effective team Is the development team effective? • Become more\n effective • Become more\n effective • Become more\n effective Figure 10.9 Transposing the principles regarding the development team over the development lifecycle", "prev_page": {"page_num": 233, "segment_id": "00233"}, "next_page": {"page_num": 235, "segment_id": "00235"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": true, "has_citations": false, "key_terms": ["team", "tasks", "effective", "working", "development", "together", "become", "more", "well", "good"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["PTS", "SCM", "Nico-nico calendar"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["lead time", "recidivism"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["PTS", "SCM"], "topics": ["team effectiveness", "development lifecycle", "motivation metrics"], "key_concepts": ["effective team principles", "task management", "code collaboration", "continuous integration", "deployment tools", "application monitoring"], "problem_statement": "How to measure and improve the effectiveness of a development team", "solution_approach": "Using PTS for task tagging, SCM data analysis, and recidivism metrics", "extraction_method": "lm"}}
{"segment_id": "00235", "page_num": 235, "segment": "212\n\nCHAPTER 10 Measuring your team against the agile principles\n\n■ Development time---The time from when a task enters the development flow to its\n\ncompletion\n\n■ Deploy frequency---How often you get changes out to consumers\n■ Good/failed builds---How frequently you’re delivering working software\n\nMeasuring autonomy with the data you have isn’t very straightforward. I’ve had success with this by checking the counts of people entering and commenting tasks in the\nsystem before they enter the work stream along with the counts of people who are\nassigned the tasks. In very autonomous teams you’ll see that assignees have enough\nownership to collaborate toward the definition of a task before it gets assigned to the\nteam. In teams that simply receive marching orders, you typically see members outside\nthe immediate team doing the bulk of the creation and definition of tasks. Another\npattern to look out for is nonteam members moving tasks through the workflow. A\npictorial representation is shown in figure 10.10.\n\n Of the three different ratios, you should stay away from either extreme (the examples on the left and right in figure 10.10). In the first example, a small intersection\nbetween assignees and creators typically can lead to members of the team who specialize in creating requirements and filling out tasks and other team members who just\nwork the specs they’re given. You’ll see symptoms of this when you start to hear people\ncomplaining about requirements or blaming requirements for poor or incorrect\nimplementations.\n\n In the example on the right in figure 10.10, on teams where developers are responsible for creating their own tasks, you can start to see people entering the bare minimum in the system to indicate they have a task to work on. This extreme also tends to\nlead to poor and incomplete requirements, and this becomes a problem when people\nforget what their minimal cards meant and try to estimate them, or if work ends up\ngetting divided among other team members.\n\n The second example (the one in the middle) is usually a good mix of members\ninside the team and outside stakeholders entering tasks into the work stream. This\nmix typically gives the development team enough investment in their work but also\nhas enough outside influence to keep requirements complete.\n\n An effective team creates effective software using effective processes. The final element is the requirements these teams use to build their software. The last agile\n\n1\n\nA small intersection\nshows that team members\naren’t invested in defining\ntheir own tasks.\n\n2\n\nA large intersection shows\na development team\nheavily invested in the\ndefinition of their work.\n\n3 A complete overlap\nshows that the team\ndefines their own\nwork completely.\n\nAssignees\n\nCreators\n\nAssignees\n\nCreators\n\nAssignees\nCreators\n\nFigure 10.10 A pictorial representation of the overlap between task assignees and creators and how it\nrelates to the level of team autonomy\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.4", "section_title": "Four principles for an effective team", "l1_summary": "CHAPTER 10 Measuring your team against the agile principles A small intersection\nshows that team members\naren’t invested in defining\ntheir own tasks. 3 A complete overlap\nshows that the team\ndefines their own\nwork completely.", "l2_summary": "CHAPTER 10 Measuring your team against the agile principles In the first example, a small intersection\nbetween assignees and creators typically can lead to members of the team who specialize in creating requirements and filling out tasks and other team members who just\nwork the specs they’re given. A small intersection\nshows that team members\naren’t invested in defining\ntheir own tasks. A large intersection shows\na development team\nheavily invested in the\ndefinition of their work. 3 A complete overlap\nshows that the team\ndefines their own\nwork completely. Figure 10.10 A pictorial representation of the overlap between task assignees and creators and how it\nrelates to the level of team autonomy", "prev_page": {"page_num": 234, "segment_id": "00234"}, "next_page": {"page_num": 236, "segment_id": "00236"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["team", "tasks", "work", "members", "assignees", "requirements", "people", "creators", "development", "task"], "content_type": "theory", "domain": "management|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["Development time", "Deploy frequency", "Good/failed builds"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Measuring team autonomy", "Agile principles", "Task management"], "key_concepts": ["Autonomy in teams", "Development time", "Deploy frequency", "Good/failed builds"], "problem_statement": "How to measure and improve team autonomy within the context of Agile principles", "solution_approach": "Analyzing task assignees and creators, balancing the intersection between them for optimal team performance", "extraction_method": "lm"}}
{"segment_id": "00236", "page_num": 236, "segment": "One principle for effective requirements\n\n213\n\nprinciple belongs in a category by itself; although it’s only a single line, it speaks to\nhow your team defines what they’re doing before they do it and can be measured\nfrom a few dimensions.\n\n10.5 One principle for effective requirements\n\nThe final question you need to answer is “Are your requirements effective?”\n\n■ Welcome changing requirements, even late in development. Agile processes\n\nharness change for the customer’s competitive advantage.\n\nChanging requirements is one of the most frustrating things that a team needs to take\nin stride. I’ve been in many sprint retrospectives where teams complain about missing\ngoals due to changing requirements, I’ve seen products make it out the door with a\nfraction of intended functionality because of indecision around requirements\nthroughout the development of the product, and I’ve been a part of several development cycles where our work was scrapped because architectures, features, or experiences changed after we started.\n\n When it comes to figuring out how well your team can handle changing requirements, you need to keep two things in perspective:\n\n■ How do you know if requirements are changing?\n■ Can your team maintain a consistent pace when requirements change?\n\nFor this you’d have to know your current metrics for consistency (for example, lead\ntime, recidivism, and velocity) and you’d need to know when requirements are changing. With both of those in hand you can measure how consistently your team performs\nin the face of change.\n\n10.5.1 Measuring effective requirements\n\nI’ve mind mapped the final principle in figure 10.11.\n\n The interesting thing here is that you need to compare your consistency when\nrequirements are static against when they change. To do that you need to measure\nyour team’s consistency and you need to track when requirements are in flux.\n\n Velocity alone is a terrible metric to measure how well a team can handle change\nand as a result is often a bone of contention when requirements are changing. If a\nteam makes a commitment and starts a sprint, and then midway through the process\nthe end goal moves, the team is set up to miss their commitment. You can reestimate\nor refactor your sprint goal, but in the grand scheme of things it will look like your\nteam went through a patch of low productivity when perhaps they were more productive than ever.\n\nAre you consistent?\n\nWhen requirements change?\n\nWhen requirements don’t change?\n\nFigure 10.11 Mapping\nout “Are you consistent?”\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "10.5 One principle for effective requirements ■ How do you know if requirements are changing? When requirements change?", "l2_summary": "One principle for effective requirements 10.5 One principle for effective requirements ■ How do you know if requirements are changing? 10.5.1 Measuring effective requirements When requirements change? When requirements don’t change?", "prev_page": {"page_num": 235, "segment_id": "00235"}, "next_page": {"page_num": 237, "segment_id": "00237"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["requirements", "team", "when", "changing", "change", "need", "principle", "effective", "development", "things"], "content_type": "theory", "domain": "management|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["lead time", "recidivism", "velocity"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Effective Requirements Management", "Change Management in Agile Development", "Team Consistency Metrics"], "key_concepts": ["Changing requirements", "Consistency metrics", "Velocity as a metric for change handling"], "problem_statement": "Handling changing requirements effectively and measuring team performance under such conditions", "solution_approach": "Comparing consistency when requirements are static versus dynamic, using specific metrics like lead time, recidivism, and velocity", "extraction_method": "lm"}}
{"segment_id": "00237", "page_num": 237, "segment": "214\n\nCHAPTER 10 Measuring your team against the agile principles\n\nVelocity is inconsistent;\nwhen viewed alone\nthis looks bad.\n\nCompleted tasks are increasing,\nwhich shows the team is\nconsistently completing work.\n\nVelocity vs. task volume\n\ni\n\ns\nt\nn\no\np\nt\nn\ni\nr\np\nS\n\n60\n\n40\n\n20\n\n0\n\nVelocity\n\nTask\nvolume\n\n60\n\n40\n\n20\n\n0\n\ns\nk\ns\na\nt\n\nl\n\na\nt\no\nT\n\nSprint 1\n\nSprint 2\n\nSprint 3\n\nSprint 4\n\nSprint 5\n\nSprint 6\n\nSprint 7\n\nSprint 8\n\nFigure 10.12 Velocity for this team is inconsistent, but total completed tasks are actually increasing.\nThis is a typical indicator that the team completes tasks consistently despite changing commitments\nand/or requirements.\n\nBetter metrics to use to track consistency through changing requirements are task volume and average estimates. The number of tasks that your team is able to accomplish\nin the face of change tempered by average estimates will give you a good picture of\nhow consistently your team is completing its work. If your estimates consistently correlate to a few days of work and your task volume stays constant, you should be in good\nshape. If you see these trends along with an inconsistent velocity, it shows you that\nyour team is doing great in the face of change.\n\n Other metrics that help check team consistency while velocity is in flux are recidivism and lead time. Stable lead time and recidivism trends during periods of inconsistent velocity show that through changing commitments the time it takes to get tasks\nout the door and the health of your workflow remain solid. This is demonstrated in\nfigure 10.12.\n\n Effective requirements can most easily be measured at the beginning and the end\n\nof the development cycle, as shown in figure 10.13.\n\nAre the requirements effective?\n\nManage\ncode and\ncollaboration\n\nGenerate\nbuilds and\nrun tests\n\nMove code\nacross\nenvironments\n\nSource\ncontrol\n\nContinuous\nintegration\n\nDeployment\ntools\n\nEnsure\neverything\nis working\n\nApplication\nmonitoring\n\nManage\ntasks and\nbugs\n\nProject\ntracking\n\n • Changing\n requirements\n\nFigure 10.13 Where to find data to measure effective requirements\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 2, "cols": 2, "data": [["", ""], ["", ""]], "markdown": "|  |  |\n|---|---|\n|  |  |"}], "table_count": 1, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "Figure 10.12 Velocity for this team is inconsistent, but total completed tasks are actually increasing. Are the requirements effective? • Changing\n requirements", "l2_summary": "CHAPTER 10 Measuring your team against the agile principles Completed tasks are increasing,\nwhich shows the team is\nconsistently completing work. Figure 10.12 Velocity for this team is inconsistent, but total completed tasks are actually increasing. This is a typical indicator that the team completes tasks consistently despite changing commitments\nand/or requirements. Are the requirements effective? • Changing\n requirements", "prev_page": {"page_num": 236, "segment_id": "00236"}, "next_page": {"page_num": 238, "segment_id": "00238"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["team", "sprint", "velocity", "tasks", "requirements", "inconsistent", "consistently", "task", "volume", "figure"], "content_type": "tutorial", "domain": "management|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Agile", "DevOps"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["Velocity", "Task volume", "Average estimates", "Recidivism", "Lead time"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Measuring team performance against Agile principles", "Consistency in task completion", "Metrics for tracking consistency"], "key_concepts": ["Velocity", "Task volume", "Average estimates", "Recidivism", "Lead time"], "problem_statement": "Inconsistent velocity despite increasing completed tasks", "solution_approach": "Using metrics like task volume and average estimates to track consistent task completion", "extraction_method": "lm"}}
{"segment_id": "00238", "page_num": 238, "segment": "Case study: a new agile team\n\n215\n\nI’ve identified only a single agile principle that directly addresses requirements, but\nyou can also argue that some of the principles I’ve categorized under an effective\nteam also apply here because requirements are often a contract between the business,\ndeveloper, and quality roles on the team. You can look at requirements from a few\nangles. First, does your development team understand them and can they work with\nthem? Second, do your customers actually get what they want? You can measure those\nwith the following metrics:\n\n■ Recidivism---When recidivism is high, that means someone in the workflow\ndidn’t have the same standard as someone downstream. This is usually an indicator of incomplete or inconsistent requirements.\n\n■ Lead time, velocity, and development time---These all measure how long it takes for\n\nyour team to get tasks across the finish line.\n\nYour team translates requirements into software using agile processes. The agile principles\nfrom the Agile Manifesto can be broken down into these four measurable elements that\nyou can track to see how well your team is aligning to the agile principles. Now let’s take\na look at a team in our case study that puts these measurements into practice.\n\n10.6 Case study: a new agile team\n\nMany companies move their development practices to be more agile but don’t get all\nthe way there. Sometimes legacy systems can’t be turned off; mainframes or large\nfinancial reconciliation systems simply can’t be rebuilt or refactored in a reasonable\namount of time and can’t be deployed even as frequently as every few weeks.\n\n This case study takes us to a team that has moved many of their web and mobile\ndevelopment teams to agile practices but has a large financial mainframe that gets\nupdated once a quarter. After seeing the speed at which the agile teams are delivering\ncode, the leadership team sponsors an effort to implement a new system that will take\non the functionality of the mainframe one piece at a time. During this transition\nthey’ll move developers from the mainframe maintenance teams over to the new agile\nteams, train them to work differently, and expect them to start practicing CD.\n\n Technical problems are the easy ones; teaching people new processes when they’ve\nbeen doing the same thing for several years is much harder. To help transition the\nteam, the leaders started by agreeing on target metrics to track so the team could see\nthe progress. The strategy was to start at the top, ensure that everyone understood\nwhat the high-level goals were, and when they established a baseline, then start adding\nmore detailed metrics to continually improve the process.\n\n Each sprint they created headlines; each headline correlated to a demonstrable\npiece of functionality that they committed to finishing by the end of the sprint. The\nheadlines were what was determined would satisfy the customer. They used the headlines\nas their indicators of working software and made sure that they were breaking their tasks\ndown into small enough chunks that they could deliver pieces early and continuously\nthrough their sprint. As they broke tasks down, they used labels to associate headlines\nwith tasks so they could track individual metrics for tasks grouped by headlines. To\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "Case study: a new agile team Your team translates requirements into software using agile processes. 10.6 Case study: a new agile team", "l2_summary": "Case study: a new agile team First, does your development team understand them and can they work with\nthem? You can measure those\nwith the following metrics: your team to get tasks across the finish line. Your team translates requirements into software using agile processes. 10.6 Case study: a new agile team", "prev_page": {"page_num": 237, "segment_id": "00237"}, "next_page": {"page_num": 239, "segment_id": "00239"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["team", "agile", "requirements", "tasks", "headlines", "case", "study", "development", "metrics", "time"], "content_type": "case_study", "domain": "programming|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["agile processes", "mainframes", "financial reconciliation systems", "web and mobile development teams"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["recidivism", "lead time", "velocity", "development time"], "has_case_study": true, "case_study_company": null, "tools_mentioned": [], "topics": ["agile practices", "requirements management", "metric tracking"], "key_concepts": ["agile principles", "recidivism", "lead time", "velocity", "development time"], "problem_statement": "Transitioning mainframe maintenance teams to agile practices", "solution_approach": "Implementing new system, training developers, setting target metrics", "extraction_method": "lm"}}
{"segment_id": "00239", "page_num": 239, "segment": "216\n\nCHAPTER 10 Measuring your team against the agile principles\n\nEach headline that represented major\nfeatures is tracked with a label.\n\nIn Kibana you can click on a term to\nfilter all metrics by your selection.\n\nThe key metric, lead\ntime, shows the team how\nlong it takes to get tasks\nall the way through the\ndevelopment cycle.\n\nThe distribution of estimates\nshows the percentage of tasks\ngrouped by their estimates.\nThis team decided that the\nbulk of estimates should\nbe low numbers.\n\nEstimate health shows the\nteam how close their estimates\nare to actual time. A value of\n4 means on average tasks\ntake four days longer then\nthe original estimate.\n\nFigure 10.14 The team’s dashboard with labels, estimate distribution, estimate health, and lead\ntime. These metrics helped the team stay aware of how well they were estimating and delivering.\n\ntrack success in moving to this new process they decided to measure lead time, estimate health, and estimate distribution. The estimate distribution and estimate health\nwould show them how well they were breaking down tasks, and lead time would show\nhow long it actually took to deliver tasks end to end. The dashboard they started using\nis shown in figure 10.14.\n\n As they were developing, they wanted to ensure they were paying continuous attention to the technical excellence of their software. Before they started developing, the\nteam put all the build and monitoring systems in place to enable easier delivery and\nbetter quality. They were able to adopt tools from other teams in the organization that\nwere already practicing agile and CD such as Sonar, their internal delivery pipeline for\nCI, and APM tools for production monitoring. By using these tools, they would be able\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "The distribution of estimates\nshows the percentage of tasks\ngrouped by their estimates. Estimate health shows the\nteam how close their estimates\nare to actual time. Figure 10.14 The team’s dashboard with labels, estimate distribution, estimate health, and lead\ntime.", "l2_summary": "CHAPTER 10 Measuring your team against the agile principles The key metric, lead\ntime, shows the team how\nlong it takes to get tasks\nall the way through the\ndevelopment cycle. The distribution of estimates\nshows the percentage of tasks\ngrouped by their estimates. Estimate health shows the\nteam how close their estimates\nare to actual time. Figure 10.14 The team’s dashboard with labels, estimate distribution, estimate health, and lead\ntime. track success in moving to this new process they decided to measure lead time, estimate health, and estimate distribution.", "prev_page": {"page_num": 238, "segment_id": "00238"}, "next_page": {"page_num": 240, "segment_id": "00240"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["estimate", "team", "time", "tasks", "lead", "distribution", "estimates", "health", "shows", "would"], "content_type": "case_study", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Sonar", "internal delivery pipeline for CI", "APM tools for production monitoring"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["lead time", "estimate health", "estimate distribution"], "has_case_study": true, "case_study_company": null, "tools_mentioned": ["Kibana", "Sonar", "internal delivery pipeline for CI", "APM tools for production monitoring"], "topics": ["measuring team performance against agile principles", "dashboard metrics", "technical excellence in software development"], "key_concepts": ["lead time", "estimate health", "estimate distribution", "agile methodology", "continuous delivery"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00240", "page_num": 240, "segment": "Case study: a new agile team\n\n217\n\nCode coverage shows how well the team is writing\ntests. Mutant coverage shows how good the tests are.\nIn this case a slight discrepancy in the two isn’t\nbad but still worth investigating.\n\nFigure 10.15 The next set of charts used by the development team included good versus bad\nbuilds, unit test coverage, and mutant coverage.\n\nto better understand the quality of their products as they were developing them\ninstead of through long QA cycles after weeks of development. Because this was a new\nconcept for the development team, they needed some time to become familiar with\nthe tools and get used to using them in their development process. But once they\nwere up and running, they knew from the success of other teams in the company that\nthey could become a more effective team.\n\n To track their success with CI they tracked unit test and mutant coverage to make\nsure the team was writing decent tests and tracked good versus bad build ratio to make\nsure they were iterating on a working product. The dashboard they used for this is\nshown in figure 10.15.\n\n An effective team produces great software, but an effective team is made up of\nmotivated individuals who work together across functions and reflect at regular intervals to become more effective. To measure this, the team tried a few different tactics\nthat other teams across the organization were already doing.\n\n Because they were following Scrum, they had a built-in mechanism to reflect every\ntwo weeks in their retrospective. To ensure this turned into an effective and productive session, they drove the sessions with data. When they discussed what they could\nimprove, they also discussed what metrics they could use to track their improvement.\nThe key metric they used to measure everything was lead time. The thinking was that\nby making the whole team pay attention to the time it took to deliver a feature end-toend, they would bring the business and development teams together toward the common goal of delivery. After tracking their lead time until they delivered something,\nthey could then all sit down and reflect on what they could do to improve. Of course,\nalong the way they collected all the data they needed to look into the details of the\nteam when they were ready to do so.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "Case study: a new agile team Code coverage shows how well the team is writing\ntests. Figure 10.15 The next set of charts used by the development team included good versus bad\nbuilds, unit test coverage, and mutant coverage.", "l2_summary": "Case study: a new agile team Code coverage shows how well the team is writing\ntests. Mutant coverage shows how good the tests are. Figure 10.15 The next set of charts used by the development team included good versus bad\nbuilds, unit test coverage, and mutant coverage. But once they\nwere up and running, they knew from the success of other teams in the company that\nthey could become a more effective team. The dashboard they used for this is\nshown in figure 10.15.", "prev_page": {"page_num": 239, "segment_id": "00239"}, "next_page": {"page_num": 241, "segment_id": "00241"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["team", "coverage", "development", "could", "effective", "used", "time", "tests", "mutant", "good"], "content_type": "case_study", "domain": "devops|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["code coverage", "mutant coverage", "CI", "unit test", "build ratio", "dashboard"], "frameworks": ["Scrum"], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["unit test coverage", "mutant coverage", "good versus bad build ratio", "lead time"], "has_case_study": true, "case_study_company": null, "tools_mentioned": ["dashboard"], "topics": ["Agile development", "Continuous Integration", "Code quality metrics"], "key_concepts": ["code coverage", "mutant coverage", "unit test", "build ratio", "Scrum retrospective", "lead time"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00241", "page_num": 241, "segment": "218\n\nCHAPTER 10 Measuring your team against the agile principles\n\n After a single sprint the team had delivered a partially functional web service. After\nanother sprint they were able to get a functional UI on top of it that met the criteria\nfor their first set of features. They were operating in two-week sprints and at the end\nhad a lead time of 28 days.\n\n In their retrospective they started breaking down lead time to find efficiencies in\ntheir process. They found that there was still a lot of back and forth between the stakeholders, developers, and QA. They correlated that to a lack of good face-to-face conversations and realized that in their case to track improvement they could use\nrecidivism to find the overall churn between the different roles on the team and use\nthe number of bugs to track how well the development and QA teams were communicating, as shown in the dashboard in figure 10.16.\n\n36% of all tasks move backward in the\nworkflow at least once. Of those that move\nbackward nearly 25% move backward more\nthan once. Clearly there is room for\nimprovement with recidivism.\n\nBy tracking bugs completed along with tasks\ncompleted it’s apparent that many of the completed\ntasks are bugs, and it looks like the percentage\nof bugs is increasing over time.\n\nFigure 10.16 The dashboard tracking recidivism and bugs versus total tasks to help the team track\nhow well they’re working together across disciplines.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "36% of all tasks move backward in the\nworkflow at least once. Of those that move\nbackward nearly 25% move backward more\nthan once. By tracking bugs completed along with tasks\ncompleted it’s apparent that many of the completed\ntasks are bugs, and it looks like the percentage\nof bugs is increasing...", "l2_summary": "CHAPTER 10 Measuring your team against the agile principles After a single sprint the team had delivered a partially functional web service. 36% of all tasks move backward in the\nworkflow at least once. Of those that move\nbackward nearly 25% move backward more\nthan once. By tracking bugs completed along with tasks\ncompleted it’s apparent that many of the completed\ntasks are bugs, and it looks like the percentage\nof bugs is increasing over time. Figure 10.16 The dashboard tracking recidivism and bugs versus total tasks to help the team track\nhow well they’re working together across disciplines.", "prev_page": {"page_num": 240, "segment_id": "00240"}, "next_page": {"page_num": 242, "segment_id": "00242"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["bugs", "team", "tasks", "time", "track", "recidivism", "move", "backward", "completed", "sprint"], "content_type": "case_study", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["web service", "UI", "dashboard"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": false, "has_metrics": true, "metrics": ["lead time", "recidivism", "bugs"], "has_case_study": true, "case_study_company": null, "tools_mentioned": ["dashboard"], "topics": ["Agile principles implementation", "team performance metrics", "process improvement"], "key_concepts": ["sprint delivery", "lead time", "recidivism", "bug tracking"], "problem_statement": "Improving team efficiency and communication in Agile sprints", "solution_approach": "Tracking recidivism and bugs to identify process inefficiencies and areas for improvement", "extraction_method": "lm"}}
{"segment_id": "00242", "page_num": 242, "segment": "Summary\n\n219\n\n After another sprint of analyzing data from their PTS, they started to get the hang\nof finding problems in retrospectives, using data to track corrections, and coming up\nwith strategies to improve. The development team was not only new to agile, it was\nalso new to working so quickly, and they had a lot to learn about the best way to manage source control and deployments in this new paradigm. They started paying attention to pull requests versus commits and to build and deploy times from their CI\nsystems and added those metrics to their dashboards.\n\n After several sprints the team was using several metrics based on the agile principles to continually improve their development and delivery processes. They felt\nempowered to deliver changes frequently, and their morale was high after successfully\nmoving toward agile methodologies.\n\n10.7 Summary\n\nAt this point you should have a plethora of tools at your disposal to start measuring\nyour team, incorporating metrics into your development cycle, and communicating\nthem across your team and up the chain.\n\n In this chapter you learned the following:\n\n■ To measure a team against the agile principles you need to answer four big\n\nquestions:\n\n■ Are the requirements effective?\n\n■\n\nIs the development team effective?\n\n■ Are your processes effective?\nIs your software effective?\n\n■\n\n■ You can measure requirements with metrics covered in previous chapters:\n\n■ Recidivism\n■ Lead time\n■ Development time\n■ Velocity\n\n■ You can measure the development team with metrics covered in previous\n\nchapters:\n\n■ Lead time\n■ Development time\n■ Deploy frequency\n■ Good/failed builds\n\n■ You can measure your process with metrics covered in previous chapters:\n\n■ Velocity\n■ PTS and SCM comments\n■ Successful deployments\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "Is the development team effective? ■ You can measure requirements with metrics covered in previous chapters: ■ You can measure the development team with metrics covered in previous", "l2_summary": "■ Are the requirements effective? Is the development team effective? ■ You can measure requirements with metrics covered in previous chapters: ■ Recidivism\n■ Lead time\n■ Development time\n■ Velocity ■ You can measure the development team with metrics covered in previous ■ You can measure your process with metrics covered in previous chapters:", "prev_page": {"page_num": 241, "segment_id": "00241"}, "next_page": {"page_num": 243, "segment_id": "00243"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["development", "team", "metrics", "agile", "measure", "effective", "time", "covered", "previous", "chapters"], "content_type": "tutorial", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["PTS", "CI systems", "source control and deployments"], "frameworks": [], "methodologies": ["agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["recidivism", "lead time", "development time", "velocity", "deploy frequency", "good/failed builds", "PTS and SCM comments", "successful deployments"], "has_case_study": true, "case_study_company": null, "tools_mentioned": ["dashboards", "CI systems"], "topics": ["agile methodologies", "metric tracking", "development process improvement"], "key_concepts": ["requirements effectiveness", "development team effectiveness", "process effectiveness", "software effectiveness"], "problem_statement": "Improving development and delivery processes in a new agile environment", "solution_approach": "Implementing metrics, using data for retrospectives, and improving source control and deployments", "extraction_method": "lm"}}
{"segment_id": "00243", "page_num": 243, "segment": "220\n\nCHAPTER 10 Measuring your team against the agile principles\n\n■ You can measure your software with metrics covered in chapter 8:\n\n■ Successful versus failed builds\n■ Business metrics\n■ Performance health data\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "CHAPTER 10 Measuring your team against the agile principles ■ You can measure your software with metrics covered in chapter 8: ■ Successful versus failed builds\n■ Business metrics\n■ Performance health data", "l2_summary": "CHAPTER 10 Measuring your team against the agile principles ■ You can measure your software with metrics covered in chapter 8: ■ Successful versus failed builds\n■ Business metrics\n■ Performance health data Licensed to Mark Watson <nordickan@gmail.com>", "prev_page": {"page_num": 242, "segment_id": "00242"}, "next_page": {"page_num": 244, "segment_id": "00244"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["chapter", "metrics"], "content_type": "theory", "domain": "management|devops", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": [], "frameworks": [], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["Successful versus failed builds", "Business metrics", "Performance health data"], "has_case_study": false, "case_study_company": null, "tools_mentioned": [], "topics": ["Measuring team performance against Agile principles", "Software metrics"], "key_concepts": ["Agile principles", "Successful builds", "Failed builds", "Business metrics", "Performance health data"], "problem_statement": "How to measure a software development team's adherence to Agile principles", "solution_approach": "Using various types of metrics such as build success rate, business-related KPIs, and performance indicators", "extraction_method": "lm"}}
{"segment_id": "00244", "page_num": 244, "segment": "appendix A\nDIY analytics using ELK\n\nThis appendix covers\n\n■ Reviewing the overall architecture of an agile\n\nmetrics collection and analytics system\n\n■ Setting up an ELK server\n\n■ Building a data-collection application using\n\nGrails\n\n■ Installing the data collector on the ELK server\n\nIn this appendix I’ll walk you through setting up a powerful analytics system using\nthe ELK stack (EC, Logstash, and Kibana). You can download the basic stack from\nthe EC website. You can also get the setup I’ve used for this book on GitHub at\ngithub.com/cwhd/measurementor.\n\n Let’s start by looking at the high-level component diagram in figure A.1 to get an\nidea of the pieces you need to build. You’ll need to write some scripts to collect data,\nset up a database to store it, and put together an indexing, search, and visualization\nengine. Once you have all of these components, you’ll be ready to start building out\nconnectors to external applications used on the development lifecycle to get data\nand analyze it. You’ll use the following technologies for these components:\n\n■ Data collection---Grails (grails.org/)\n■ Database---MongoDB (/)\n■ Data indexing and search---EC (/)\n■ Data visualization---Kibana (/overview/kibana/)\n\n221\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "appendix A\nDIY analytics using ELK ■ Building a data-collection application using ■ Installing the data collector on the ELK server", "l2_summary": "appendix A\nDIY analytics using ELK metrics collection and analytics system ■ Setting up an ELK server ■ Building a data-collection application using ■ Installing the data collector on the ELK server ■ Data collection---Grails (grails.org/)\n■ Database---MongoDB (/)\n■ Data indexing and search---EC (/)\n■ Data visualization---Kibana (/overview/kibana/)", "prev_page": {"page_num": 243, "segment_id": "00243"}, "next_page": {"page_num": 245, "segment_id": "00245"}, "flags": {"continuity_gap": true}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "appendix", "analytics", "using", "collection", "grails", "kibana", "system", "setting", "server"], "content_type": "tutorial", "domain": "data_science|programming", "complexity": "intermediate", "companies": ["EC", "MongoDB"], "people": [], "products": [], "technologies": ["Grails", "ELK stack", "EC", "Logstash", "Kibana", "MongoDB"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": ["Groovy"], "has_best_practices": false, "has_antipatterns": false, "has_instructions": true, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Grails", "ELK stack", "EC", "Logstash", "Kibana", "MongoDB"], "topics": ["DIY analytics using ELK", "Setting up an ELK server", "Building a data-collection application using Grails", "Installing the data collector on the ELK server"], "key_concepts": ["ELK stack", "Agile metrics collection and analytics system", "Data collection with Grails", "MongoDB as database", "EC for indexing and search", "Kibana for visualization"], "problem_statement": null, "solution_approach": "Setting up a DIY analytics system using the ELK stack, including data collection, database storage, indexing, searching, and visualization.", "extraction_method": "lm"}}
{"segment_id": "00245", "page_num": 245, "segment": "222\n\nAPPENDIX A DIY analytics using ELK\n\nScripts query external\nsystems for data.\n\nData visualized in the form\nof charts and graphs.\n\nApplications and\ninteresting data\n\nData collection\nand conversion\n\nData\nvisualization\n\nThe systems\nyou use in your\napplication lifecycle.\n\nDatabase\n\ni d i\nD t\nData indexing\nfor search\n\nWhat we're\nbuilding in\nthis appendix.\n\nFigure A.1 The high-level component diagram of the analytics system\n\nData saved in\nthe database.\n\nIndexing engine makes\ntraversing data easy and fast.\n\nFigure A.2 shows how data flows through the system.\n\n1 Do work\n\nSystems used to manage\nwork in the application\nlifecycle\n\n6 Measure the\n\nwork and tweak\nyour process\n\nApplications and\na\ninteresting data\n\n2 Get data to store\nfor analysis\n\nData\nvisualization\n\n5 Analyze and\n\nvisualize trends\n\nData flow\n\nData\nfetcher\n\nDatabase\n\n3 Store the\ndata\n\nData\nindexing\n\n4 Index data for\n\neasier searching\n\nFigure A.2 A closer look at the flow of data through the analytics system\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 4, "cols": 4, "data": [["Applications an", "", "", ""], ["", "Applications an", "", ""], ["", "", "Applications an", ""], ["", "", "", ""]], "markdown": "| Applications an |  |  |  |\n|---|---|---|---|\n|  | Applications an |  |  |\n|  |  | Applications an |  |\n|  |  |  |  |"}, {"table_id": "table_2", "rows": 5, "cols": 4, "data": [["Applications an", "", "", ""], ["", "Applications an", "", ""], ["", "", "Applications an", ""], ["", "", "", ""], ["", "", "", ""]], "markdown": "| Applications an |  |  |  |\n|---|---|---|---|\n|  | Applications an |  |  |\n|  |  | Applications an |  |\n|  |  |  |  |\n|  |  |  |  |"}, {"table_id": "table_3", "rows": 2, "cols": 2, "data": [["Database", ""], ["", ""]], "markdown": "| Database |  |\n|---|---|\n|  |  |"}], "table_count": 3, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "Applications and\ninteresting data i d i\nD t\nData indexing\nfor search Data saved in\nthe database.", "l2_summary": "Applications and\ninteresting data Data collection\nand conversion i d i\nD t\nData indexing\nfor search Data saved in\nthe database. Applications and\na\ninteresting data 2 Get data to store\nfor analysis", "prev_page": {"page_num": 244, "segment_id": "00244"}, "next_page": {"page_num": 246, "segment_id": "00246"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "analytics", "systems", "database", "indexing", "figure", "system", "work", "appendix", "applications"], "content_type": "tutorial", "domain": "data_science|architecture", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["ELK", "database", "indexing engine"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": false, "has_antipatterns": false, "has_instructions": true, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["ELK", "database"], "topics": ["DIY analytics using ELK", "data flow in analytics system", "components of analytics system"], "key_concepts": ["data collection and conversion", "data visualization", "data indexing for search"], "problem_statement": null, "solution_approach": "using ELK stack to collect, store, index, and visualize data", "extraction_method": "lm"}}
{"segment_id": "00246", "page_num": 246, "segment": "223\n\nFigure A.2 illustrates the data flowing through the analytics system during the application lifecycle. Combining this with the diagram in figure A.1 shows that you’re going\nto do the following:\n\n■ Get MongoDB up and running as your database.\n■ Get EC and Kibana up and running for data indexing, search, and analysis.\n■ Create the application that gets data and saves it to the database.\n■ Generate charts and graphic visualization.\n\nI already have this running for you, so go to GitHub, get the latest version, and run the\nPuppet scripts to set it all up.\n\n If you want to use something other than Grails, it’s okay. I have working code, but\nif you follow along with the examples in appendix B where I go into more detail on\nthe data-gathering application, you can easily translate the system into another\nlanguage.\n\nUsing the examples with Puppet and Vagrant\n\nAs you build the system in this appendix you’ll be installing tools and a database,\nand I’m going to assume you have Java installed on your development machine. But\nwouldn’t it be cool if you didn’t have to worry about any of that? You could open the\nsystem and start developing it without having to worry about installing anything, the\nversion of tools that may already be installed on your computer, or differences between\nOSs. Versioning environments as you version code have become popular along with\nthe recent rise of DevOps (en.wikipedia.org/wiki/DevOps), the concept of combining\ndevelopment and operations to be able to release software faster and more efficiently.\nPuppet (puppetlabs.com/) is a technology that allows you to write code that automates\nthe installation and versioning of the software on your environment. Automating the\ninstallation and versioning of software is great, but it’s even better if you can switch\nbetween development environments if you’re working on different projects or experimenting with different tools and technologies. Using virtual machines (VMs) allows\nyou to keep multiple environments at your fingertips for just that, switching between\nOSs or environment configurations to work across projects with different dependencies. Vagrant () is a development tool that abstracts the details\nof VMs away from the developer and integrates seamlessly with Puppet so that you\ncan automate the installation of software and jump into your development environment\nwith a simple command from your terminal of choice.\n\nAs you implement this system, you’ll do it in a way that can be run on your local machine,\nbut the code examples from the website will be set up using Vagrant and Puppet.\nVagrant makes developing locally easier by wrapping everything in a VM. Puppet will\nconfigure the Vagrant box for you; that way you don’t have to worry about whatever changes you’re making locally and it’s easier to install the solution on different systems.\n\nBefore you start building the system, let’s take a more in-depth look at its design and\nspecification.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "Figure A.2 illustrates the data flowing through the analytics system during the application lifecycle. ■ Get MongoDB up and running as your database. Using the examples with Puppet and Vagrant", "l2_summary": "Figure A.2 illustrates the data flowing through the analytics system during the application lifecycle. ■ Get MongoDB up and running as your database. I already have this running for you, so go to GitHub, get the latest version, and run the\nPuppet scripts to set it all up. Using the examples with Puppet and Vagrant As you build the system in this appendix you’ll be installing tools and a database,\nand I’m going to assume you have Java installed on your development machine. As you implement this system, you’ll do it in a way that can be run on your local machine,\nbut the code examples from the website will be set up using Vagrant and Puppet.", "prev_page": {"page_num": 245, "segment_id": "00245"}, "next_page": {"page_num": 247, "segment_id": "00247"}, "extended_fields": {"has_code": true, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["system", "have", "puppet", "vagrant", "development", "data", "code", "software", "different", "application"], "content_type": "tutorial", "domain": "devops|architecture", "complexity": "intermediate", "companies": ["null"], "people": ["Mark Watson"], "products": ["Grails", "GitHub"], "technologies": ["MongoDB", "EC", "Kibana", "Puppet", "Vagrant"], "frameworks": ["null"], "methodologies": ["DevOps"], "programming_languages": ["Grails"], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Puppet", "Vagrant"], "topics": ["DevOps implementation", "Data analytics system setup", "Automation with Puppet and Vagrant"], "key_concepts": ["Automated environment setup", "Versioning environments", "Virtual machine management"], "problem_statement": "Setting up a data analytics system for an application lifecycle", "solution_approach": "Using Puppet scripts and Vagrant to automate the setup process", "extraction_method": "lm"}}
{"segment_id": "00247", "page_num": 247, "segment": "224\n\nAPPENDIX A DIY analytics using ELK\n\nVagrant manages\nhow to set up the\nvirtual machine.\n\nPuppet installs\nwhat you need and\nconfigures your VM.\n\nWhen complete a\nnew VM is configured\nand ready to use.\n\nThe local development\nmachine acts as a host to\nthe development VM.\n\nFigure A.3 How Vagrant, Puppet, and VirtualBox interact with your development environment\n\nA.1\n\nSetting up your system\nNow that we’ve looked at the high-level design, let’s set up the system. Because the system depends on open source components instead of finding, downloading, and\ninstalling every component, I’ve boiled all the pieces into a Puppet script, which you’ll\nuse to install everything you need in a VM using Vagrant. Using Puppet you can set up\nthis system anywhere you can create a VM. For the purposes of this book I’ll focus on\ngetting you up and running on your localhost.\n\n The interaction of Vagrant, Puppet, and VirtualBox on a local development\n\nmachine is shown in figure A.3.\n\n First, get the latest version of Vagrant from/downloads.html.\nIf you’re using Windows, you’ll also need to install Cygwin with the openssh package\nso you can connect to your Vagrant machine when it boots up.\n\nthe code\n\nthis book\n\n Second, download\n\nfrom github.com/cwhd/\nfor\nmeasurementor. The codebase contains the Vagrant and Puppet configuration to\ninstall all the components you need. The Vagrant file contains a line of code that sets\nup a shared directory between your local machine and the Vagrant box to make\ntweaking the system easier. In the Vagrant file around line 49, change PATH_TO\n_DOWNLOADED_PROJECT_HERE to the path where you downloaded the code. For example, if you downloaded the code to /Users/cwhd/Development/measurementor,\nreplace\n/Users/cwhd/Development/\nmeasurementor. For Windows users, be sure to escape slashes. For example, if you\ndownload the code to C:\\Agile Metrics\\measurementor on a Windows machine,\nchange PATH_TO_DOWNLOADED_PROJECT_HERE to C:\\\\Agile Metrics\\\\measurementor.\n\nPATH_TO_DOWNLOADED_PROJECT_HERE with\n\nNOTE As the versions of these components update, measurementor will\nevolve on GitHub after this book is printed. For the latest setup instructions\nplease check the readme file for the project.\n\nFinally, navigate to the directory where you downloaded the code and run the following commands.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "Vagrant manages\nhow to set up the\nvirtual machine. Figure A.3 How Vagrant, Puppet, and VirtualBox interact with your development environment The interaction of Vagrant, Puppet, and VirtualBox on a local development", "l2_summary": "Vagrant manages\nhow to set up the\nvirtual machine. The local development\nmachine acts as a host to\nthe development VM. Figure A.3 How Vagrant, Puppet, and VirtualBox interact with your development environment The interaction of Vagrant, Puppet, and VirtualBox on a local development In the Vagrant file around line 49, change PATH_TO\n_DOWNLOADED_PROJECT_HERE to the path where you downloaded the code. PATH_TO_DOWNLOADED_PROJECT_HERE with", "prev_page": {"page_num": 246, "segment_id": "00246"}, "next_page": {"page_num": 248, "segment_id": "00248"}, "extended_fields": {"has_code": true, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["vagrant", "machine", "puppet", "development", "code", "measurementor", "downloaded", "system", "using", "need"], "content_type": "tutorial", "domain": "devops|architecture", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Vagrant", "Puppet", "VirtualBox"], "frameworks": [], "methodologies": ["DevOps"], "programming_languages": ["Ruby"], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Vagrant", "Puppet", "VirtualBox"], "topics": ["DIY analytics using ELK stack setup", "DevOps tools configuration", "Local development environment setup"], "key_concepts": ["Vagrant", "Puppet", "VM configuration", "Shared directory setup"], "problem_statement": null, "solution_approach": "Using Vagrant and Puppet to set up a local development VM for DIY analytics using ELK stack", "extraction_method": "lm"}}
{"segment_id": "00248", "page_num": 248, "segment": "Setting up your system\n\n225\n\nListing A.1 Running Vagrant\n\n$> vagrant up\nBringing machine 'default' up with 'virtualbox' provider...\n==> default: Checking if box 'hashicorp/precise64' is up to date...\n==> default: Resuming suspended VM...\n==> default: Booting VM...\n==> default: Waiting for machine to boot. This may take a few minutes...\n default: SSH address: 127.0.0.1:2222\n default: SSH username: vagrant\n default: SSH auth method: private key\n default: Warning: Connection refused. Retrying...\n==> default: Machine booted and ready!\n$> vagrant ssh\nWelcome to Ubuntu 12.04 LTS (GNU/Linux 3.2.0-23-generic x86_64)\n\nCommand to tunnel\ninto the VM\n\nRuns your\nvagrant file\n\nVagrant\noutput as\nit boots\nup your\n local VM\n\n * Documentation: https://help.ubuntu.com/\nNew release '14.04.1 LTS' available.\nRun 'do-release-upgrade' to upgrade to it.\n\nYou’re ready\nto go!\n\nWelcome to your Vagrant-built virtual machine.\nLast login: Mon Nov 24 21:24:23 2014 from 10.0.2.2\nvagrant@precise64:~$\n\nVagrant output\nas you log in.\n\nNow you have a local VM with all the components you need installed on it. In the\nVagrant file I’ve set up ports to share with your localhost, so you can check on EC and\nKibana from your web browser at the following URLs:\n\n■ EC: http://localhost:9200\n■ Kibana: http://localhost:5601\n\nIf those URLs come up, then you’ve successfully gotten your local environment up and\nrunning.\n\nA few more useful Vagrant commands\n\nVagrant makes it very convenient to develop in a sandbox. It’s so easy to set up a\nnew system that often during development you may want to tear the system down\nand start again. To completely wipe away your environment use the vagrant destroy\ncommand, you can always create a new one using vagrant up again.\nOne drawback to using a VM for development is that it will use up a lot of your system’s\nresources, making other programs slow while your VM is booted and running. If you\nwant to halt development and go back to other work, you can pause your Vagrant box\nusing the vagrant suspend command. That will effectively pause the state of your\nVM and return the memory and CPU to your other applications. To resume using it,\nsimply use vagrant up again.\nFor a complete reference visit the Vagrant website.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "Listing A.1 Running Vagrant Runs your\nvagrant file Vagrant output\nas you log in.", "l2_summary": "Listing A.1 Running Vagrant $> vagrant up\nBringing machine 'default' up with 'virtualbox' provider... ==> default: Booting VM... Runs your\nvagrant file Vagrant\noutput as\nit boots\nup your\n local VM Vagrant output\nas you log in.", "prev_page": {"page_num": 247, "segment_id": "00247"}, "next_page": {"page_num": 249, "segment_id": "00249"}, "extended_fields": {"has_code": true, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["vagrant", "default", "system", "machine", "using", "running", "command", "local", "localhost", "development"], "content_type": "tutorial", "domain": "devops", "complexity": "beginner", "companies": ["HashiCorp"], "people": ["Mark Watson"], "products": ["Vagrant"], "technologies": ["VirtualBox", "Ubuntu", "Vagrant"], "frameworks": [], "methodologies": [], "programming_languages": ["Bash"], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Vagrant", "VirtualBox"], "topics": ["Setting up a local VM with Vagrant", "Using Vagrant commands", "Accessing the VM via SSH"], "key_concepts": ["Vagrant setup", "VM boot process", "SSH access to VM"], "problem_statement": null, "solution_approach": "Using Vagrant to set up and manage a local development environment", "extraction_method": "lm"}}
{"segment_id": "00249", "page_num": 249, "segment": "226\n\nAPPENDIX A DIY analytics using ELK\n\nA.1.1 Checking the database\n\nThe previous Puppet script installed Mongo for you. If you want to check directly\nfrom the database, you can do so from the command line in your Vagrant host with\nthe commands in the following listing.\n\nListing A.2 Checking data In MongoDB\n\nStarts the\nMongo\nconsole\n\nInspects\nthe jiraData\ncollection\n\nvagrant@precise64:~$ mongo\nMongoDB shell version: 2.6.5\nconnecting to: test#B\n> show collections\njiraData\njiraData.next_id\njobHistory\njobHistory.next_id\n> db.jiraData.find()\n{ \"_id\" : NumberLong(8805), \"assignees\" : [ ], \"created\" : ISODate(\"2014General informational\noutputs\n\nShows all the\ncollections in\nthe database\n\n11-21T19:19:05Z\"), \"createdBy\" : \"james.lee3_nike.com\", \"dataType\" : \"PTS\",\n\"issuetype\" : \"Bug\", \"key\" : \"ACOE-885\", \"leadTime\" : NumberLong(0),\n\"movedBackward\" : 0, \"movedForward\" : 0, \"storyPoints\" : 0, \"tags\" : [ ],\n\"version\" : 0 }\n> db.jiraData.remove({})\n> exit\n\nDeletes the entire\ncollection\n\nExits MongoDB\n\nNote that if you haven’t yet run the data collector, you won’t see any data in your\ndatabase.\n\n For complete documentation on MongoDB, visit/.\n\nA.1.2 Configuring your data collector\n\nThe application that does everything between the database and the web display (a.k.a.\nthe middle tier) is written with Grails and Groovy. If you want to go in a different\ndirection and write something yourself, you can achieve the same results using other\nlanguages. The key is to use whatever you’re most comfortable with, because that will\nmake the process as painless and seamless as possible. I personally like Groovy because\nit’s a popular language, easy to use, highly supported, gets better with every release,\ndoes a lot of the programming tedium for you, and can run on the Java VM (JVM),\nwhich makes it highly portable and scalable. In short, it’s a Swiss army knife that lets\nyou build stuff fast. If there are examples in certain languages you really want to see,\nfeel free to request help on the Author Online forum,1 and I can help post examples\nin whatever language you’d like to use.\n\n In appendix B I’ll give more information about the details of the Grails app if you\nwant to tweak it. But it should work out of the box with a few configuration tweaks if\nyou don’t want to get under the hood.\n\n I’ve included a shell script with the code that will walk you through the configuration of the system. It will ask you for the URLs and credentials for all the supported sys1/forum.jspa?forumID=924&start=0\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "A.1.1 Checking the database Listing A.2 Checking data In MongoDB Inspects\nthe jiraData\ncollection", "l2_summary": "A.1.1 Checking the database Listing A.2 Checking data In MongoDB Inspects\nthe jiraData\ncollection Shows all the\ncollections in\nthe database Note that if you haven’t yet run the data collector, you won’t see any data in your\ndatabase. A.1.2 Configuring your data collector", "prev_page": {"page_num": 248, "segment_id": "00248"}, "next_page": {"page_num": 250, "segment_id": "00250"}, "extended_fields": {"has_code": true, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["database", "want", "jiradata", "data", "mongodb", "mongo", "will", "appendix", "using", "checking"], "content_type": "tutorial", "domain": "data_science|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["MongoDB", "Grails", "Groovy", "Java VM"], "frameworks": [], "methodologies": [], "programming_languages": ["Groovy"], "has_best_practices": false, "has_antipatterns": false, "has_instructions": true, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["MongoDB", "Grails"], "topics": ["DIY analytics using ELK", "Configuring data collector"], "key_concepts": ["MongoDB database operations", "Grails application configuration"], "problem_statement": "How to perform DIY analytics using ELK stack", "solution_approach": "Using MongoDB for database operations and Grails for middle tier application", "extraction_method": "lm"}}
{"segment_id": "00250", "page_num": 250, "segment": "Setting up your system\n\n227\n\ntems you want to connect to, so you should have those ready. You have to enter\ncredentials as base64-encoded strings that can be used for HTTP basic auth. For example, if your username is UserOne and your password is h0lm3s, then your basic auth\nstring would be UserOne:h0lm3s. You can use a local utility to base64 encode your\nstrings, or you can go to a site like/.\n\n The shell script will copy the file measurementor.properties to a new file called\napplication.properties. Then it will take the parameters you pass into the shell script\nand update the settings for the application. If you don’t run the shell script or want to\nchange anything, you can do the steps manually by copying measurementor.properties\nto a new file called application.properties and adding the URLs to the systems you want\nto connect to. An example for JIRA connectivity should look like the following listing.\n\nListing A.3 Setting up your config file\n\njira.credentials=dXNlcjpwYXNzd29yZA==\njira.url= https\\://jira.whatever.com\n\nThe base64 encoded\nbasic auth\n\nURLs to the root\nof the systems\n\nBecause you’re focusing on getting this running locally, you can now run the Grails\napplication from inside your Vagrant box. It will do the following:\n\n■ Reach out to the systems you’ve specified.\n■ Get data out of them.\n■ Write the data into MongoDB.\n\n■\n\nIndex the data in EC.\n\nSSH into your box and run the app with the commands in this listing.\n\nListing A.4 Running the Grails app\n\nRuns it\n\n>$ vagrant up\nvagrant@precise64:~$ cd /measurementor\nvagrant@precise64:~$ grails run-app\n\nLogs into your\nVagrant box\n\nNavigates to the directory\nwhere the app lives\n\nNow everything is good to go! The app is configured to run once a day to get data out\nof your systems and index it.\n\nBe careful if you’ve hosted JIRA\n\nIf you’re using the hosted version of JIRA, be careful how frequently and at what times\nyou run your data collection. I’ve worked on a fairly large team with a lot of data and\nI’ve noticed that under load JIRA completely dies. If you have a small team, then you\nshould be okay, but if you have a good-size team, you should limit your data collection\nto times when normal usage is low.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "■ Get data out of them. Index the data in EC. >$ vagrant up\nvagrant@precise64:~$ cd /measurementor\nvagrant@precise64:~$ grails run-app", "l2_summary": "jira.credentials=dXNlcjpwYXNzd29yZA==\njira.url= https\\://jira.whatever.com ■ Get data out of them. Index the data in EC. Listing A.4 Running the Grails app >$ vagrant up\nvagrant@precise64:~$ cd /measurementor\nvagrant@precise64:~$ grails run-app Be careful if you’ve hosted JIRA", "prev_page": {"page_num": 249, "segment_id": "00249"}, "next_page": {"page_num": 251, "segment_id": "00251"}, "extended_fields": {"has_code": true, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["jira", "data", "vagrant", "should", "have", "file", "properties", "application", "systems", "listing"], "content_type": "tutorial", "domain": "programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Grails", "Vagrant", "MongoDB", "EC"], "frameworks": [], "methodologies": [], "programming_languages": ["Groovy"], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Grails", "Vagrant"], "topics": ["Setting up a Grails application", "Running the application", "Data collection and indexing"], "key_concepts": ["HTTP basic auth", "Base64 encoding", "Shell script for configuration", "Vagrant for local development"], "problem_statement": null, "solution_approach": "Configuring and running a Grails application to collect data from systems and index it in MongoDB", "extraction_method": "lm"}}
{"segment_id": "00251", "page_num": 251, "segment": "228\n\nAPPENDIX A DIY analytics using ELK\n\nPay attention to your source system logs\n\nAnother lesson learned from doing large-scale data collection is to be wary of the log\nlevel of your source systems. I’ve seen some systems run out of disk space and crash\nbecause no one was paying attention to how the system was logging.\n\nA.2\n\nCreating the dashboard\nThe front end is the open source graphing system Kibana. Once you have data in EC,\nKibana is already wired up for you to start creating nice charts and graphs. Figure A.4\nshows a few ways to create graphs with Kibana.\n\n At the time this book was published Kibana 4 had just been released. Thus, many of\nthe figures have charts from Kibana 3, though the system has been upgraded to use\nKibana 4. For the latest information on how to set up a Kibana dashboard check out my\nblog at/, or read through the Kibana documentation on the EC website.\n\nMouse over the side\nbar to add a panel.\n\nClicking on the data indexed shows you\nthe breakdown of data; you can click\nto create a new graph from here.\n\nOr...\n\nFigure A.4 A few simple ways to create new graphs in Kibana\n\nA.3\n\nSummary\nThe ELK stack is a popular open source set of tools that you can easily use for analyzing your team. Using the code for this book, you should be able to get it running in no\ntime. In this appendix you learned the following:\n\n■ Where to get an open source system to do the measurement we talked about in\n\nthe book\n\n■ About the architecture and operation of an analytics system\n■ How to set up an analytics system that you can use to collect and analyze data\n■ How to create a dashboard interface for displaying data\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 2, "cols": 2, "data": [["Mouse over the side Clicking on the data indexed shows you\nbar to add a panel. the breakdown of data; you can click\nto create a new graph from here.\nOr…", ""], ["", ""]], "markdown": "| Mouse over the side Clicking on the data indexed shows you\nbar to add a panel. the breakdown of data; you can click\nto create a new graph from here.\nOr… |  |\n|---|---|\n|  |  |"}], "table_count": 1, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "Creating the dashboard\nThe front end is the open source graphing system Kibana. Figure A.4\nshows a few ways to create graphs with Kibana. Thus, many of\nthe figures have charts from Kibana 3, though the system has been upgraded to use\nKibana 4.", "l2_summary": "Pay attention to your source system logs Creating the dashboard\nThe front end is the open source graphing system Kibana. Figure A.4\nshows a few ways to create graphs with Kibana. At the time this book was published Kibana 4 had just been released. Thus, many of\nthe figures have charts from Kibana 3, though the system has been upgraded to use\nKibana 4. Figure A.4 A few simple ways to create new graphs in Kibana", "prev_page": {"page_num": 250, "segment_id": "00250"}, "next_page": {"page_num": 252, "segment_id": "00252"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["kibana", "system", "data", "source", "create", "analytics", "dashboard", "open", "graphs", "book"], "content_type": "tutorial", "domain": "data_science|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["ELK stack", "Kibana"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": true, "has_instructions": true, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["ELK stack", "Kibana"], "topics": ["DIY analytics using ELK", "Creating the dashboard with Kibana", "Summary of ELK stack usage"], "key_concepts": ["log level management", "data collection and analysis", "dashboard creation"], "problem_statement": "Managing log data from source systems and creating an analytics dashboard", "solution_approach": "Using the ELK stack for log aggregation, analysis, and visualization with Kibana", "extraction_method": "lm"}}
{"segment_id": "00252", "page_num": 252, "segment": "appendix B\nCollecting data from\nsource systems with Grails\n\nThis appendix covers\n\n■ The architecture of the Grails component in\n\nmeasurementor\n\n■ Structure of the domain objects\n\n■ Using Quartz as a job scheduler\n\nThis appendix goes into the architecture and code of the Grails data collector in\nthe measurementor project. This will be a good chapter to read if you want to fork,\ncontribute to, extend, or customize the project to better fit into your environment.\n This appendix picks up where appendix A left off. In appendix A we talked\nabout using Elasticsearch (EC) to index your data and Kibana to generate graphs\nand do ad hoc analysis of your data. Combined, these platforms are very powerful\nand provide a lot of out-of-the-box functionality. But there are two things missing\nfrom EC and Kibana alone:\n\n■ You need to get data to index. You could set up Logstash for that, but it would be\na lot of work and you’d need access to all of your source system’s installations.\n\n■ EC is for searching data. If you want to calculate metrics based on combinations of data, you’ll have to do that somewhere else.\n\n229\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "appendix B\nCollecting data from\nsource systems with Grails ■ You need to get data to index. ■ EC is for searching data.", "l2_summary": "appendix B\nCollecting data from\nsource systems with Grails This appendix goes into the architecture and code of the Grails data collector in\nthe measurementor project. This appendix picks up where appendix A left off. In appendix A we talked\nabout using Elasticsearch (EC) to index your data and Kibana to generate graphs\nand do ad hoc analysis of your data. ■ You need to get data to index. ■ EC is for searching data.", "prev_page": {"page_num": 251, "segment_id": "00251"}, "next_page": {"page_num": 253, "segment_id": "00253"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["data", "appendix", "grails", "source", "architecture", "measurementor", "using", "project", "want", "index"], "content_type": "tutorial", "domain": "programming|architecture", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Grails", "measurementor", "Elasticsearch (EC)", "Kibana", "Quartz"], "frameworks": ["Grails"], "methodologies": [], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["data indexing", "graph generation", "ad hoc analysis"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Grails", "measurementor", "Elasticsearch (EC)", "Kibana", "Quartz"], "topics": ["data collection with Grails", "architecture of the Grails component in measurementor", "using Quartz as a job scheduler"], "key_concepts": ["data indexing", "job scheduling", "Grails framework"], "problem_statement": "Getting data to index and calculating metrics based on combinations of data", "solution_approach": "Using Grails for data collection, Elasticsearch (EC) for indexing, Kibana for graph generation and ad hoc analysis, and Quartz as a job scheduler", "extraction_method": "lm"}}
{"segment_id": "00253", "page_num": 253, "segment": "230\n\nAPPENDIX B Collecting data from source systems with Grails\n\nTo accommodate for the shortcomings there’s a small Grails-based component to\nmeasurementor that reaches out to various systems via their REST-based APIs, gets the\ndata we’ve been talking about in this book, and sends it to EC for indexing. This is custom code, so you can tweak it do whatever you want, including setting up additional\ndashboards with metrics you calculate yourself.\n\nThe measurementor project\n\nI’ve open sourced the measurementor project and it’s freely available on GitHub:\ngithub.com/cwhd/measurementor. Keep in mind that because it’s a living project,\nsome of the code examples from the print version of this book may be out of date,\nbut the concepts all hold true.\n\nThroughout this appendix we’ll be using the JIRA API in our examples.\n\nUsing JIRA in the examples\n\nThis app has custom code to get data from each source system it needs to connect\nto. To walk through the architecture I’ll use the code that gets data from JIRA, which\nis a very common and popular agile tracking system. The concepts and the structure\nof the code we’ll be looking at can be used with any system that has a REST-based\nAPI; if you crack open the code, you’ll see connectors to other systems. By following\nthe examples in this appendix, you can create your own connectors if you want to get\ndata from other sources.\n\nB.1\n\nArchitectural overview\nFirst, let’s look at the high-level architecture outlined in appendix A to highlight the\narea of focus in this appendix; see figure B.1.\n\n Note that we’ve added a couple of specific technologies to the diagram: Grails and\nMongoDB. You’ll use Grails and MongoDB for their ease of use and flexibility. The\ndata collection and conversion component in figure B.1 breaks out into figure B.2.\n\n Overall the architecture is pretty simple. You’ll use jobs to schedule the collection\nof data, services to parse individual data sources, and domain objects to represent\neach type of data you want to index. Grails has the following out-of-the-box capabilities to help you set up these components:\n\n■ Create jobs with just a few lines of code.\n■ Create simple objects usually called plain old Groovy objects (POGOs) that use\nGrails Object Relational Mapping (GORM) to handle interaction with the database.\n\n■ Create services that you can autowire into other classes with a single line of\n\ncode.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "APPENDIX B Collecting data from source systems with Grails The measurementor project The\ndata collection and conversion component in figure B.1 breaks out into figure B.2.", "l2_summary": "APPENDIX B Collecting data from source systems with Grails The measurementor project This app has custom code to get data from each source system it needs to connect\nto. By following\nthe examples in this appendix, you can create your own connectors if you want to get\ndata from other sources. You’ll use Grails and MongoDB for their ease of use and flexibility. The\ndata collection and conversion component in figure B.1 breaks out into figure B.2.", "prev_page": {"page_num": 252, "segment_id": "00252"}, "next_page": {"page_num": 254, "segment_id": "00254"}, "extended_fields": {"has_code": true, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["data", "code", "grails", "appendix", "measurementor", "examples", "create", "systems", "based", "want"], "content_type": "tutorial", "domain": "programming|architecture", "complexity": "intermediate", "companies": [], "people": ["cwhd"], "products": [], "technologies": ["Grails", "MongoDB", "REST-based APIs", "JIRA API"], "frameworks": [], "methodologies": ["Agile"], "programming_languages": ["Groovy"], "has_best_practices": false, "has_antipatterns": false, "has_instructions": true, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["GitHub"], "topics": ["data collection with Grails", "REST-based APIs", "JIRA API"], "key_concepts": ["Grails-based component", "data indexing", "custom code"], "problem_statement": "Collecting data from source systems", "solution_approach": "Using a small Grails-based component to collect and send data via REST-based APIs", "extraction_method": "lm"}}
{"segment_id": "00254", "page_num": 254, "segment": "Architectural overview\n\n231\n\nScripts query\nexternal systems\nfor data.\n\nData visualized in\nthe form of charts\nand graphs.\n\nApplications and\ninteresting data\n\nData collection\nand conversion\n\nData\nvisualization\n\nThe systems\nyou use in your\napplication lifecycle.\n\nDatabase\n\ni d i\nD t\nData indexing\nfor search\n\nThe focus of\nthis appendix.\n\nData saved in\nthe database.\n\nIndexing engine\nmakes traversing data\neasy and fast.\n\nFigure B.1 The focus of this appendix, the Grails portion of measurementor\n\nGet data - Grails system\n\nWeb service\nquery\n\nSource\nAPI\n\nJobs:\nScheduled jobs\nfetch data\n\nRaw data\nresponse\n\n{\n data: [ ...]\n}\n\nServices:\nSource-specific\nservices map the\ndata to the\ndatabase.\n\nDomain objects:\nDomain objects\ntransfer data\nbetween the source\nand indexer.\n\nData is indexed\nfrom here into\nElasticsearch.\n\nDatabase\n\nFigure B.2 A closer look into the Grails/Mongo-based data collection system\n\nBecause the Grails framework takes care of the tedious plumbing that would go into\nmaking the previous things work in other languages, you’re free to focus on getting\nthe data you want from the source systems without having to focus on only making\nthings work.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 3, "cols": 4, "data": [["", "Applications an", "", ""], ["", "", "Applications an", ""], ["", "", "", ""]], "markdown": "|  | Applications an |  |  |\n|---|---|---|---|\n|  |  | Applications an |  |\n|  |  |  |  |"}, {"table_id": "table_2", "rows": 2, "cols": 2, "data": [["", ""], ["Data\nvisualization", ""]], "markdown": "|  |  |\n|---|---|\n| Data\nvisualization |  |"}, {"table_id": "table_3", "rows": 2, "cols": 2, "data": [["Get data - Grails system\nWeb service\nquery Jobs: Data is indexed\nSource Scheduled jobs from here into\nAPI fetch data Elasticsearch.\nRaw data\nresponse\nServices: Domain objects: Database\n{ Source-specific Domain objects\ndata: [ ...] services map the transfer data\n} data to the between the source\ndatabase. and indexer.", ""], ["", ""]], "markdown": "| Get data - Grails system\nWeb service\nquery Jobs: Data is indexed\nSource Scheduled jobs from here into\nAPI fetch data Elasticsearch.\nRaw data\nresponse\nServices: Domain objects: Database\n{ Source-specific Domain objects\ndata: [ ...] services map the transfer data\n} data to the between the source\ndatabase. and indexer. |  |\n|---|---|\n|  |  |"}], "table_count": 3, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "Data collection\nand conversion i d i\nD t\nData indexing\nfor search Data saved in\nthe database.", "l2_summary": "Scripts query\nexternal systems\nfor data. Applications and\ninteresting data Data collection\nand conversion i d i\nD t\nData indexing\nfor search Data saved in\nthe database. Get data - Grails system", "prev_page": {"page_num": 253, "segment_id": "00253"}, "next_page": {"page_num": 255, "segment_id": "00255"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["data", "database", "focus", "grails", "source", "systems", "query", "collection", "indexing", "appendix"], "content_type": "architecture", "domain": "programming|data_science", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Grails", "MongoDB", "Elasticsearch"], "frameworks": ["Grails"], "methodologies": [], "programming_languages": ["Groovy"], "has_best_practices": false, "has_antipatterns": false, "has_instructions": false, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["MongoDB", "Elasticsearch"], "topics": ["data collection and conversion", "data visualization", "Grails framework"], "key_concepts": ["data indexing", "scheduled jobs", "domain objects"], "problem_statement": "Querying external systems for data and visualizing it in charts and graphs", "solution_approach": "Using the Grails framework to handle data collection, conversion, and visualization", "extraction_method": "lm"}}
{"segment_id": "00255", "page_num": 255, "segment": "232\n\nAPPENDIX B Collecting data from source systems with Grails\n\nA note on using other languages and frameworks\n\nOver the last several years I’ve built a few different versions of this application. The\nfirst rendition used Python instead of Grails. I ended up switching to Grails because\nI prefer working in it. It runs on the JVM, which made it easy to get up and running on\nthe infrastructure I had available at the time, and it was easier to get something running\nfrom scratch. If you hate Grails and can’t imagine why I’m using it, then feel free to\nuse the patterns outlined in this appendix to write something in your framework and\nlanguage of choice. If you do, let me know; I’d love to plug your work on my blog!\n\nIf you check out the project and\nopen it in your favorite IDE, you\ncan navigate to all the interesting\nparts, as shown in figure B.3.\n\n Feel free to poke around the\nrest of the app, but most of the\ninteresting stuff is in the three\nsections noted in figure B.3. We’ll\nlook at each of these sections so\nyou’ll get an idea of what’s going\non and how to extend the app\nfurther.\n\nB.1.1 Domain objects\n\nAll of our domain objects\nto define the data\nwe'll be working with\n\nScheduled tasks that call\nservices that get data and\nsave it for indexing\n\nServices that specialize\nin parsing data from a\nsource and saving it\n\nThe APIs from our source systems\nare so rich that indexing all the\nfields that come back will make\nsearching for patterns and trends\nvery difficult. It’s much easier to get started by focusing on the fields you know\nyou’ll need.\n\nFigure B.3 Where to find the three main components of\nthe app: domain objects, services, and jobs\n\n The nature of this application is to get data from multiple systems and save it in a\ncentral place for indexing. The domain objects will be your contract between the\nsource system and your indexer. The domain objects make it easier to transfer data\nbecause they allow you to take advantage of the database plugins and object relational\nmapping (ORM) provided by the Grails framework. There’s a domain object for each\nsystem you’re getting data from. You can look at one of our domain objects in the following listing to see how simple they are.\n\nListing B.1 The JIRA domain object\n\npackage org.cwhd.measure\n\nclass JiraData {\n static mapWith = \"mongo\"\n\nMaps this object to\nMongoDB via ORM.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "APPENDIX B Collecting data from source systems with Grails All of our domain objects\nto define the data\nwe'll be working with Listing B.1 The JIRA domain object", "l2_summary": "APPENDIX B Collecting data from source systems with Grails All of our domain objects\nto define the data\nwe'll be working with Figure B.3 Where to find the three main components of\nthe app: domain objects, services, and jobs The domain objects will be your contract between the\nsource system and your indexer. There’s a domain object for each\nsystem you’re getting data from. Listing B.1 The JIRA domain object", "prev_page": {"page_num": 254, "segment_id": "00254"}, "next_page": {"page_num": 256, "segment_id": "00256"}, "extended_fields": {"has_code": true, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["domain", "data", "objects", "grails", "source", "object", "systems", "easier", "figure", "services"], "content_type": "tutorial", "domain": "programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Python", "Grails", "JVM"], "frameworks": ["Grails"], "methodologies": [], "programming_languages": ["Groovy"], "has_best_practices": false, "has_antipatterns": false, "has_instructions": true, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Grails", "IDE"], "topics": ["Data Collection with Grails", "Domain Objects", "Services and Jobs"], "key_concepts": ["Domain objects for data transfer", "Scheduled tasks for data indexing", "Parsing and saving data from sources"], "problem_statement": null, "solution_approach": "Using Grails to collect and index data from multiple systems", "extraction_method": "lm"}}
{"segment_id": "00256", "page_num": 256, "segment": "Architectural overview\n\n233\n\nIndexes this object\nonce it’s been saved.\n\nProperties that you’ll\npull out of JIRA’s API.\n\n static searchable = true\n\n String key\n Date created\n String createdBy\n String issuetype\n int movedForward\n int movedBackward\n int storyPoints\n String[] assignees\n String[] tags\n String dataType\n Date finished\n long leadTime\n long devTime\n int commentCount\n String jiraProject\n int estimateHealth\n long rawEstimateHealth\n\n static constraints = {\n finished nullable: true\n created nullable: true\n createdBy nullable: true\n issuetype nullable: true\n storyPoints nullable: true\n assignees nullable: true\n tags nullable: true\n leadTime nullable: true\n devTime nullable: true\n commentCount nullable: true\n jiraProject nullable: true\n estimateHealth nullable: true\n rawEstimateHealth nullable: true\n }\n}\n\nNullable properties\nare easier to manage.\n\nAs you can see, it’s very simple; the domain objects define the data that you’re going\nto move from the source systems to the indexer. The services you’re using for data collection are where most of the work of the application is done, but first let’s look at the\ndata you need to parse.\n\nB.1.2 The data you’re working with\n\nLook at the data you can get back from JIRA’s API in the next listing. In earlier chapters we talked about the who, what, and when you can get from source systems. They’re\nnoted in figure B.2.\n\nListing B.2 Excerpts from the raw data in a typical API response\n\n{\n \"expand\": \"names,schema\",\n \"startAt\": 0,\n \"maxResults\": 50,\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "static searchable = true static constraints = {\n finished nullable: true\n created nullable: true\n createdBy nullable: true\n issuetype nullable: true\n storyPoints nullable: true\n assignees nullable: true\n tags nullable: true\n leadTime nullable: true\n devTime nullable: true\n commentCount nullable...", "l2_summary": "static searchable = true String key\n Date created\n String createdBy\n String issuetype\n int movedForward\n int movedBackward\n int storyPoints\n String[] assignees\n String[] tags\n String dataType\n Date finished\n long leadTime\n long devTime\n int commentCount\n String jiraProject\n int estimateHealth\n long rawEstimateHealth static constraints = {\n finished nullable: true\n created nullable: true\n createdBy nullable: true\n issuetype nullable: true\n storyPoints nullable: true\n assignees nullable: true\n tags nullable: true\n leadTime nullable: true\n devTime nullable: true\n commentCount nullable: true\n jiraProject nullable: true\n estimateHealth nullable: true\n rawEstimateHealth nullable: true\n }\n} Nullable properties\nare easier to manage. B.1.2 The data you’re working with Listing B.2 Excerpts from the raw data in a typical API response", "prev_page": {"page_num": 255, "segment_id": "00255"}, "next_page": {"page_num": 257, "segment_id": "00257"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": true, "has_list": false, "has_citations": false, "key_terms": ["true", "nullable", "string", "data", "long", "properties", "jira", "static", "date", "created"], "content_type": "reference", "domain": "architecture", "complexity": "intermediate", "companies": [], "people": [], "products": [], "technologies": ["JIRA"], "frameworks": [], "methodologies": [], "programming_languages": [], "has_best_practices": false, "has_antipatterns": false, "has_instructions": false, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["JIRA"], "topics": ["Architectural overview", "Domain objects", "Data parsing"], "key_concepts": ["Indexes object once saved", "Properties from JIRA’s API", "Nullable properties management"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00257", "page_num": 257, "segment": "234\n\nAPPENDIX B Collecting data from source systems with Grails\n\nThe when\n\nThe what\n\n \"total\": 1,\n \"issues\": [\n {\n \"expand\": \"editmeta,renderedFields,transitions,changelog,operations\",\n \"id\": \"58496\",\n \"self\": \"https://jira.blastamo.com/rest/api/2/issue/58496\",\n \"key\": \"MSP-3888\",\n \"fields\": {\n \"customfield_17140\": \"55728000\",\n \"created\": \"2012-01-19T14:50:03.000+0000\",\n \"project\": {\n \"key\": \"MOP\",\n \"name\": \"Multi-Operational Platform\",\n }\n creator:\n {\n name: \"jsmit1\",\n emailAddress: \"Joseph.Smith@blastamo.com\",\n displayName: \"Smith, Joseph\",\n },\n aggregatetimeoriginalestimate: null,\n assignee: {\n name: \"jsmit1\",\n emailAddress: \"Joseph.Smith@nike.com\",\n displayName: \"Smith, Joseph\",\n },\n issuetype: {\n name: “Task”,\n subtask: false\n },\n status: {\n name: \"Done\",\n statusCategory: {\n key: \"done\",\n name: \"Complete\"\n }\n },\n},\n\nThe who\n\nTo make it easier to visualize, we turned this JSON response into a group of domain\nobjects in figure B.4.\n\n Figure B.4 paints a rather complex picture of the domain structure that gets sent\nback. The Fields object in the response represents all the data in a task that has a value.\nFields can be as simple as a name-value pair in most cases, but a field can also be an\nobject that has its own set of name-value pairs. The best example is the User object used\nto show the creator and the current assignee of the task; each user has a name, email,\ndisplay name, and other data associated with it. The ChangeLog is another aggregation\nof objects that represent the history of how the issue has changed over time.\n\n To get data from the source systems, you have a service for each source system that\nmaps the data from the source API back to your domain model. That brings us to the\nservices you’ll use to parse the data.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "APPENDIX B Collecting data from source systems with Grails \"total\": 1,\n \"issues\": [\n {\n \"expand\": \"editmeta,renderedFields,transitions,changelog,operations\",\n \"id\": \"58496\",\n \"self\": \"https://jira.blastamo.com/rest/api/2/issue/58496\",\n \"key\": \"MSP-3888\",\n \"fields\": {\n \"customfield_17140\"...", "l2_summary": "APPENDIX B Collecting data from source systems with Grails \"total\": 1,\n \"issues\": [\n {\n \"expand\": \"editmeta,renderedFields,transitions,changelog,operations\",\n \"id\": \"58496\",\n \"self\": \"https://jira.blastamo.com/rest/api/2/issue/58496\",\n \"key\": \"MSP-3888\",\n \"fields\": {\n \"customfield_17140\": \"55728000\",\n \"created\": \"2012-01-19T14:50:03.000+0000\",\n \"project\": {\n \"key\": \"MOP\",\n \"name\": \"Multi-Operational Platform\",\n }\n creator:\n {\n name: \"jsmit1\",\n emailAddress: \"Joseph.Smith@blastamo.com\",\n displayName: \"Smith, Joseph\",\n },\n aggregatetimeoriginalestimate: null,\n assignee: {\n name: \"jsmit1\",\n emailAddress: \"Joseph.Smith@nike.com\",\n displayName: \"Smith, Joseph\",\n },\n issuetype: {\n name: “Task”,\n subtask: false\n },\n status: {\n name: \"Done\",\n statusCategory: {\n key: \"done\",\n name: \"Complete\"\n }\n },\n}, To make it easier to visualize, we turned this JSON response into a group of domain\nobjects in...", "prev_page": {"page_num": 256, "segment_id": "00256"}, "next_page": {"page_num": 258, "segment_id": "00258"}, "extended_fields": {"has_code": true, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["name", "data", "source", "joseph", "smith", "fields", "task", "domain", "object", "value"], "content_type": "tutorial", "domain": "programming|devops", "complexity": "intermediate", "companies": ["Blastamo", "Nike"], "people": ["Joseph Smith"], "products": [], "technologies": ["Grails", "Jira"], "frameworks": [], "methodologies": [], "programming_languages": ["Groovy"], "has_best_practices": false, "has_antipatterns": false, "has_instructions": true, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Grails", "Jira"], "topics": ["Collecting data from source systems with Grails", "JSON response structure", "Domain object mapping"], "key_concepts": ["Grails service for mapping data", "JSON response fields", "Domain objects representation"], "problem_statement": null, "solution_approach": "Mapping data from source API to domain model using services", "extraction_method": "lm"}}
{"segment_id": "00258", "page_num": 258, "segment": "Architectural overview\n\n235\n\nIssue\n\nid\nkey\nfields\nchangelog\n\n1\n\n1\n\n*\n\nChangeLog\n\nmaxResult\ntotal\nhistories\n\n1\n\n*\n\nFields\n\nmaxResult\ntotal\nhistories\nreporter\ncreator\nassignee\n\n1\n\n1\n\n*\n\n(User)Creator\n\nname\nemail\ndisplayName\n\n*\n\n(User)Assignee\n\nname\nemail\ndisplayName\n\n*\n\nHistory\n\nid\nauthor\ncreated\nitems\n\n1\n\n*\n\nHistoryItem\n\nid\nauthor\nhistories\n\nFigure B.4 Representing the response as a grouping of domain objects\n\nB.1.3 Data collection services\n\nThe data collection services will get data from the specific data schemas in your source\nsystems and map them to your domain objects. In some cases this is a simple mapping,\nbut others will include functions that combine or calculate data into fields that aren’t\nin the source system. Sticking with JIRA as an example, the next listing shows excerpts\nof the data collection service for JIRA.\n\nListing B.3 Collecting data from JIRA\n\ndef getData(startAt, maxResults, project, fromDate) {\n def url = grailsApplication.config.jira.url\n def path = \"/rest/api/2/search\"\n def jiraQuery = \"project=$project$fromDate\"\n def query = [jql: jiraQuery, expand:\"changelog\",startAt: startAt,\n ➥ maxResults: maxResults, fields:\"*all\"]\n\nConfigures URL in\napplication.properties.\n\nKey\nparameters.\n\n def json = httpRequestService.callRestfulUrl(url, path, query, true)\n\nThe HTTPRequest calls other services.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": true, "tables": [{"table_id": "table_1", "rows": 6, "cols": 3, "data": [["Issue\nid\nkey\nfields\nchangelog\n1 1\n* *\nFields ChangeLog\nmaxResult maxResult\ntotal total\nhistories histories\nreporter\n1\ncreator\nassignee *\nHistory\n1 1\nid\n* *\nauthor\n(User)Creator (User)Assignee created\nname name items\nemail email\ndisplayName displayName 1\n*\nHistoryItem\nid\nauthor\nhistories", "", ""], ["", "History", ""], ["", "id\nauthor\ncreated\nitems", ""], ["", "", "*"], ["", "HistoryItem", ""], ["", "id\nauthor\nhistories", ""]], "markdown": "| Issue\nid\nkey\nfields\nchangelog\n1 1\n* *\nFields ChangeLog\nmaxResult maxResult\ntotal total\nhistories histories\nreporter\n1\ncreator\nassignee *\nHistory\n1 1\nid\n* *\nauthor\n(User)Creator (User)Assignee created\nname name items\nemail email\ndisplayName displayName 1\n*\nHistoryItem\nid\nauthor\nhistories |  |  |\n|---|---|---|\n|  | History |  |\n|  | id\nauthor\ncreated\nitems |  |\n|  |  | * |\n|  | HistoryItem |  |\n|  | id\nauthor\nhistories |  |"}], "table_count": 1, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "id\nkey\nfields\nchangelog B.1.3 Data collection services Listing B.3 Collecting data from JIRA", "l2_summary": "id\nkey\nfields\nchangelog maxResult\ntotal\nhistories B.1.3 Data collection services The data collection services will get data from the specific data schemas in your source\nsystems and map them to your domain objects. Listing B.3 Collecting data from JIRA def getData(startAt, maxResults, project, fromDate) {\n def url = grailsApplication.config.jira.url\n def path = \"/rest/api/2/search\"\n def jiraQuery = \"project=$project$fromDate\"\n def query = [jql: jiraQuery, expand:\"changelog\",startAt: startAt,\n ➥ maxResults: maxResults, fields:\"*all\"]", "prev_page": {"page_num": 257, "segment_id": "00257"}, "next_page": {"page_num": 259, "segment_id": "00259"}, "extended_fields": {"has_code": true, "has_formulas": false, "has_diagram": false, "has_list": true, "has_citations": false, "key_terms": ["data", "fields", "jira", "changelog", "histories", "collection", "services", "startat", "maxresults", "project"], "content_type": "tutorial", "domain": "architecture", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["JIRA", "Grails"], "frameworks": [], "methodologies": [], "programming_languages": ["Groovy"], "has_best_practices": false, "has_antipatterns": false, "has_instructions": true, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["JIRA", "Grails"], "topics": ["Data Collection Services", "JIRA Data Extraction"], "key_concepts": ["Data Schemas Mapping", "Domain Objects"], "problem_statement": null, "solution_approach": "Collecting data from JIRA using a defined service", "extraction_method": "lm"}}
{"segment_id": "00259", "page_num": 259, "segment": "236\n\nAPPENDIX B Collecting data from source systems with Grails\n\n def keepGoing = false\n\n if(json.issues.size() > 0) {\n keepGoing = true\n }\n\nHandles paged\nrequests.\n\n //NOTE we need to set the map so we know what direction things are\n ➥ moving in; this relates to the moveForward & moveBackward stuff\n\n def taskStatusMap = [\"In Definition\": 1, \"Dev Ready\":2, \"Dev\":3,\n ➥ \"QA Ready\":4, \"QA\":5, \"Deploy Ready\":6, \"Done\":7]\n\nThis map helps set\nrecidivism rate.\n\n for(def i : json.issues) {\n def moveForward = 0\n def moveBackward = 0\n def assignees = []\n def tags = []\n def movedToDev\n def commentCount = 0\n def movedToDevList = []\n\nKey variables.\n\nChecks movement\n history to calculate\nrecidivism.\n\nAll users assigned\nto this ticket.\n\nSee changelog for\nhistorical information.\n\n if (i.changelog) {\n for (def h : i.changelog.histories) {\n for (def t : h.items) {\n if(t.field == \"status\") {\n if(taskStatusMap[t.fromString] > taskStatusMap[t.toString]){\n moveBackward++\n } else {\n moveForward++\n movedToDevList.add(UtilitiesService.cleanJiraDate\n ➥ (h.created))\n }\n } else if(t.field == \"assignee\"){\n if(t.toString) {\n assignees.add(UtilitiesService.makeNonTokenFriendly\n ➥ (t.toString))\n }\n }\n }\n }\n movedToDev = movedToDevList.min()\n } else {\n logger.debug(\"changelog is null!\")\n }\n\n commentCount = i.fields.comment?.total\n\n tags = i.fields.labels\n\n def storyPoints = 0\n if(i.fields.customfield_10013)\n storyPoints = i.fields.customfield_10013.toInteger()\n }\n\nstoryPoints are\na custom field.\n\n def createdDate = UtilitiesService.cleanJiraDate(i.fields.created)\n def fin = UtilitiesService.cleanJiraDate(i.fields.resolutiondate)\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "def keepGoing = false def taskStatusMap = [\"In Definition\": 1, \"Dev Ready\":2, \"Dev\":3,\n ➥ \"QA Ready\":4, \"QA\":5, \"Deploy Ready\":6, \"Done\":7] for(def i : json.issues) {\n def moveForward = 0\n def moveBackward = 0\n def assignees = []\n def tags = []\n def movedToDev\n def commentCount = 0\n def...", "l2_summary": "def keepGoing = false if(json.issues.size() > 0) {\n keepGoing = true\n } def taskStatusMap = [\"In Definition\": 1, \"Dev Ready\":2, \"Dev\":3,\n ➥ \"QA Ready\":4, \"QA\":5, \"Deploy Ready\":6, \"Done\":7] for(def i : json.issues) {\n def moveForward = 0\n def moveBackward = 0\n def assignees = []\n def tags = []\n def movedToDev\n def commentCount = 0\n def movedToDevList = [] def storyPoints = 0\n if(i.fields.customfield_10013)\n storyPoints = i.fields.customfield_10013.toInteger()\n } def createdDate = UtilitiesService.cleanJiraDate(i.fields.created)\n def fin = UtilitiesService.cleanJiraDate(i.fields.resolutiondate)", "prev_page": {"page_num": 258, "segment_id": "00258"}, "next_page": {"page_num": 260, "segment_id": "00260"}, "extended_fields": {"has_code": true, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["fields", "changelog", "utilitiesservice", "moveforward", "movebackward", "taskstatusmap", "ready", "movedtodevlist", "field", "tostring"], "content_type": "tutorial", "domain": "programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Grails", "Jira"], "frameworks": [], "methodologies": [], "programming_languages": ["Groovy"], "has_best_practices": false, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["recidivism rate"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Grails", "Jira"], "topics": ["data collection with Grails", "status tracking", "ticket movement analysis"], "key_concepts": ["taskStatusMap", "recidivism rate", "changelog processing"], "problem_statement": null, "solution_approach": "using Grails to process Jira ticket data for status and movement analysis", "extraction_method": "lm"}}
{"segment_id": "00260", "page_num": 260, "segment": "Architectural overview\n\n237\n\n def leadTime = 0\n def devTime = 0\n if(createdDate && fin) {\n long duration = fin.getTime() - createdDate.getTime()\n leadTime = TimeUnit.MILLISECONDS.toDays(duration)\n }\n\nTime difference\n between created\nand completed.\n\nTime difference\n between dev start\nand completed.\n\n if(movedToDev && fin) {\n long duration = fin.getTime() - movedToDev.getTime()\n devTime = TimeUnit.MILLISECONDS.toDays(duration)\n }\n else if(movedToDev && !fin) {\n long duration = new Date().getTime() - movedToDev.getTime()\n devTime = TimeUnit.MILLISECONDS.toDays(duration)\n }\n\n def estimateHealth = UtilitiesService.estimateHealth\n ➥ (storyPoints, devTime, 13, 9, [1, 2, 3, 5, 8, 13])\n\nEstimate health\ncalculation is in\nUtilitiesService.\n\n def jiraData = JiraData.findByKey(i.key)\n if(jiraData) {\n ...\n } else {\n jiraData = new JiraData(...)\n }\n\n jiraData.save(flush: true, failOnError: true)\n }\n\nCut for brevity;\nsave everything.\n\n if(keepGoing) {\n getData(startAt + maxResults, maxResults, project, fromDate)\n }\n}\n\nRecursive call\nfor paging.\n\nOne thing you may notice is multiple calls to the UtilitiesService, which does things\nlike clean fields to make them easier to index, convert dates from one system to\nanother, and carry out shared complex functionality. I won’t go into the specifics of the\nUtilitiesService in this appendix, so if you want you can check it out on GitHub.1\n The patterns that you see in this service can be applied to a service that gets data\nfrom any other system. Coupled with a domain object that handles persistence and\nindexing, you have the pieces you need to get data from the source and analyze it in\nKibana. The final piece is a set of jobs that update the data on a schedule.\n\nB.1.4 Scheduling jobs for data collection\n\nNormally I’m a big proponent of event-driven systems rather than timer-based systems.\nBut because you’re getting data from several source systems, each of which you may not\nhave control over, it’s easier for you to use timers to update and index the data.\n\n The technology you’ll use to schedule calling the data-collection services is Quartz.\nQuartz allows you to schedule jobs to run at any frequency you define. The Quartz\n\n1\n\nIf you use this\nmeasure/UtilitiesService.groovy, you can get directly to the UtilitiesService class.\n\nlink, github.com/cwhd/measurementor/blob/master/grails-app/services/org/cwhd/\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "def leadTime = 0\n def devTime = 0\n if(createdDate && fin) {\n long duration = fin.getTime() - createdDate.getTime()\n leadTime = TimeUnit.MILLISECONDS.toDays(duration)\n } def jiraData = JiraData.findByKey(i.key)\n if(jiraData) {\n ... } else {\n jiraData = new JiraData(...)\n }", "l2_summary": "def leadTime = 0\n def devTime = 0\n if(createdDate && fin) {\n long duration = fin.getTime() - createdDate.getTime()\n leadTime = TimeUnit.MILLISECONDS.toDays(duration)\n } if(movedToDev && fin) {\n long duration = fin.getTime() - movedToDev.getTime()\n devTime = TimeUnit.MILLISECONDS.toDays(duration)\n }\n else if(movedToDev && !fin) {\n long duration = new Date().getTime() - movedToDev.getTime()\n devTime = TimeUnit.MILLISECONDS.toDays(duration)\n } def estimateHealth = UtilitiesService.estimateHealth\n ➥ (storyPoints, devTime, 13, 9, [1, 2, 3, 5, 8, 13]) def jiraData = JiraData.findByKey(i.key)\n if(jiraData) {\n ... } else {\n jiraData = new JiraData(...)\n } B.1.4 Scheduling jobs for data collection", "prev_page": {"page_num": 259, "segment_id": "00259"}, "next_page": {"page_num": 261, "segment_id": "00261"}, "extended_fields": {"has_code": true, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["data", "duration", "gettime", "utilitiesservice", "jiradata", "devtime", "movedtodev", "long", "timeunit", "milliseconds"], "content_type": "tutorial", "domain": "architecture|programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Groovy", "Quartz"], "frameworks": [], "methodologies": [], "programming_languages": ["Groovy"], "has_best_practices": false, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["leadTime", "devTime"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Quartz"], "topics": ["Lead Time Calculation", "Development Time Calculation", "Jira Data Management", "Scheduling Jobs for Data Collection"], "key_concepts": ["leadTime calculation", "devTime calculation", "UtilitiesService", "JiraData management", "Quartz scheduling"], "problem_statement": "How to calculate lead time and development time, manage Jira data, and schedule jobs for data collection.", "solution_approach": "Using Groovy scripts with UtilitiesService for health estimation, managing Jira data, and Quartz for job scheduling.", "extraction_method": "lm"}}
{"segment_id": "00261", "page_num": 261, "segment": "238\n\nAPPENDIX B Collecting data from source systems with Grails\n\nplugin for Grails is flexible and allows you to define intervals using Cron\n(en.wikipedia.org/wiki/Cron), which is a standard UNIX convention, or use simple\ndeclarations in which you define the repeat interval in milliseconds.\n\nAbout Quartz\n\nQuartz is a flexible, lightweight, fault-tolerant job scheduler written in Java that’s commonly used in applications that need scheduling capabilities. The code for Quartz is\nopen source, so you can compile it on your own if you like, but it’s more commonly\nused in its packaged form. Quartz is very popular and has been around for a long time,\nso the code is hardened and works well for a variety of scenarios. For more in-depth\ninformation check out the documentation on the Quartz website (quartz-scheduler.org/\ndocumentation/).\n\nThe next listing shows an excerpt from my code that defines a Quartz job that calls\none of our services. Take a look at grails-app/jobs/org/cwhd/measure/PopulatorJob.groovy2 for the full code.\n\nListing B.4 Basic job to run our services\n\nclass DataFetchingJob {\n\ndef jiraDataService\nstatic triggers = {\n simple name: 'jobTrig', startDelay: 60000, repeatInterval: 100000\n}\n\nDeclares the service\nto call from the job.\n\nThe method called when the job is\nrun; calls the service from here.\n\ndef execute() {\n def result = \"unknown\"\n try {\n def startDateTime = new Date()\n jiraDataService.getData(0, 100, \"ACOE\")\n stashDataService.getAll()\n def doneDateTime = new Date()\n def difference = doneDateTime.getTime()-startDateTime.getTime()\n def minutesDiff = TimeUnit.MILLISECONDS.toMinutes(difference)\n result = \"success in $difference ms\"\n println \"---------------------------------------------------\"\n println \"ALL DONE IN ~$minutesDiff minutes\"\n println \"---------------------------------------------------\"\n } catch (Exception ex) {\n result = \"FAIL: $ex.message\"\n }\n JobHistory history=new JobHistory(jobDate:new Date(),jobResult:result)\n history.save(failOnError: true)\n}\n\nSets up the\ntrigger to run.\n\nSaves the\njob history\nfor reference.\n\nTimes the\nmethod.\n\n2 github.com/cwhd/measurementor/blob/master/grails-app/jobs/org/cwhd/measure/PopulatorJob.groovy\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "The next listing shows an excerpt from my code that defines a Quartz job that calls\none of our services. Take a look at grails-app/jobs/org/cwhd/measure/PopulatorJob.groovy2 for the full code. Listing B.4 Basic job to run our services", "l2_summary": "For more in-depth\ninformation check out the documentation on the Quartz website (quartz-scheduler.org/\ndocumentation/). The next listing shows an excerpt from my code that defines a Quartz job that calls\none of our services. Take a look at grails-app/jobs/org/cwhd/measure/PopulatorJob.groovy2 for the full code. Listing B.4 Basic job to run our services Declares the service\nto call from the job. Saves the\njob history\nfor reference.", "prev_page": {"page_num": 260, "segment_id": "00260"}, "next_page": {"page_num": 262, "segment_id": "00262"}, "extended_fields": {"has_code": true, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["quartz", "grails", "code", "result", "cwhd", "date", "difference", "println", "history", "source"], "content_type": "tutorial", "domain": "programming", "complexity": "intermediate", "companies": ["cwhd"], "people": [], "products": [], "technologies": ["Grails", "Cron", "Java", "Quartz"], "frameworks": ["Grails"], "methodologies": [], "programming_languages": ["Groovy"], "has_best_practices": false, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["job execution time"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Grails", "Quartz"], "topics": ["Data collection with Grails and Quartz", "Cron scheduling", "Job execution tracking"], "key_concepts": ["Cron intervals", "Quartz job scheduler", "Groovy scripting for jobs"], "problem_statement": null, "solution_approach": "Using Grails plugin with Quartz to schedule data collection tasks", "extraction_method": "lm"}}
{"segment_id": "00262", "page_num": 262, "segment": "Summary\n\n239\n\nThis class could be only a few lines of code because we only need to define the timer and\ncall the services that get the data. The rest of the code times how long it takes to execute, handles errors, and saves the details of the job into the JobHistory domain class.\n The JobHistory class is used to remember when jobs were executed and the\nresult. If for some reason they failed, then the next time the job runs it can try the\nsame query. If it was successful, then we don’t have to get the data a second time.\n\nB.2\n\nSummary\nGroovy, Grails, and MongoDB are simple and fun to work with. Using the open APIs\navailable through source systems, getting data into a single place for more complex\nanalysis is a piece of cake (or pie). In this appendix you learned the following:\n\n■ You can take advantage of the built-in power of Grails to do the following:\n\n■ Manage persistence\n■ Request data from RESTful APIs\nInterface with Elasticsearch\n\n■\n\n■ Set up jobs to update data\n\n■ The measurementor architecture is simple and extendable.\n\n■\n\nIf you don’t like Grails, you can use these patterns to build the same application\nin your language of choice.\n\n■ Quartz is a flexible and lightweight technology used for scheduling jobs.\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "The JobHistory class is used to remember when jobs were executed and the\nresult. If it was successful, then we don’t have to get the data a second time. ■ Set up jobs to update data", "l2_summary": "This class could be only a few lines of code because we only need to define the timer and\ncall the services that get the data. The JobHistory class is used to remember when jobs were executed and the\nresult. If it was successful, then we don’t have to get the data a second time. Summary\nGroovy, Grails, and MongoDB are simple and fun to work with. ■ You can take advantage of the built-in power of Grails to do the following: ■ Set up jobs to update data", "prev_page": {"page_num": 261, "segment_id": "00261"}, "next_page": {"page_num": 263, "segment_id": "00263"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["data", "class", "jobs", "grails", "summary", "only", "code", "jobhistory", "used", "then"], "content_type": "tutorial", "domain": "programming|architecture", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Groovy", "Grails", "MongoDB", "Quartz"], "frameworks": [], "methodologies": [], "programming_languages": ["Groovy"], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Quartz"], "topics": ["Job Scheduling", "Data Management", "Grails Framework"], "key_concepts": ["JobHistory domain class", "RESTful APIs", "Elasticsearch interface"], "problem_statement": "Managing data retrieval and job execution in a scalable manner", "solution_approach": "Using Groovy, Grails, MongoDB, and Quartz for data management and job scheduling", "extraction_method": "lm"}}
{"segment_id": "00263", "page_num": 263, "segment": "240\n\nAPPENDIX B Collecting data from source systems with Grails\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "APPENDIX B Collecting data from source systems with Grails Licensed to Mark Watson <nordickan@gmail.com>", "l2_summary": "APPENDIX B Collecting data from source systems with Grails Licensed to Mark Watson <nordickan@gmail.com>", "prev_page": {"page_num": 262, "segment_id": "00262"}, "next_page": {"page_num": 264, "segment_id": "00264"}, "flags": {"continuity_gap": true}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": [], "content_type": "tutorial", "domain": "programming", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": [], "technologies": ["Grails"], "frameworks": [], "methodologies": [], "programming_languages": ["Groovy"], "has_best_practices": false, "has_antipatterns": false, "has_instructions": true, "has_metrics": false, "metrics": [], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Grails"], "topics": ["Collecting data from source systems with Grails"], "key_concepts": ["data collection", "Grails framework"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00264", "page_num": 264, "segment": "index\n\nA\n\nactionable insight 135\nagile principles\n\naligning with delivery lifecycle 204--205\nmeasuring effectiveness\nprocess 207--210\nrequirements 213--215\nsoftware 205--207\nteam 210--213\noverview 202--204\n\nanalysis\n\napplication performance monitoring. See APM\narbitrary metrics 113\nAtlas framework 110\naudience, for publishing metrics\n\nexecutives 188--189\nmaking point using metrics 189--191\nmanagers 184--186\noverview 178--191\nteam 180--184\n\navailability 156, 169--171\nAxosoft OnTime Scrum 10, 37\n\nin Blastamo Music LLC example 26--27\nCI data\n\nB\n\nBDD 95\nCI server data 91--95\nGatling 95\noverview 96\nReportNG framework 95\nSonarQube 95\nTestNG framework 95\n\ndetermining metrics to track 14\noverview 13--14\npreparing for\n\ndefining completion for tasks 50--52\ndefining done for tasks 49--50\nestimating task duration 47--49\noverview 44--45\ntagging tasks 46--47\nusage of PTS by entire team 45--46\n\npreparing SCM data 64--65\nvisualizing data 14--16\n\nAnsible 88\nAPM (application performance monitoring) 107,\n\n113--115, 156, 160\n\nAppDynamics 13\n\nBamboo 86\nBDD (behavior-driven development) 95\nBI (business intelligence) 107, 110\nBitbucket 11\nBlastamo Music LLC example\nagile pipeline for 20--21\nanalyzing data 26--27\ndata collection 22--26\nfeedback loop 32\nidentifying problems 21\nimproving processes 31--32\nsolving problems 27--28\nvisualization 28--30\n\nbugs\n\ncount measurement 42--44\nfixes for 158, 168\nPTS metrics 53--54\n\nbuild system 11--12\nburn down 39--40\nbusiness intelligence. See BI\n\n241\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "in Blastamo Music LLC example 26--27\nCI data determining metrics to track 14\noverview 13--14\npreparing for preparing SCM data 64--65\nvisualizing data 14--16", "l2_summary": "See APM\narbitrary metrics 113\nAtlas framework 110\naudience, for publishing metrics executives 188--189\nmaking point using metrics 189--191\nmanagers 184--186\noverview 178--191\nteam 180--184 in Blastamo Music LLC example 26--27\nCI data BDD 95\nCI server data 91--95\nGatling 95\noverview 96\nReportNG framework 95\nSonarQube 95\nTestNG framework 95 determining metrics to track 14\noverview 13--14\npreparing for preparing SCM data 64--65\nvisualizing data 14--16", "prev_page": {"page_num": 263, "segment_id": "00263"}, "next_page": {"page_num": 265, "segment_id": "00265"}, "flags": {"continuity_gap": true}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["data", "overview", "metrics", "team", "framework", "tasks", "agile", "application", "performance", "monitoring"], "content_type": "reference", "domain": "devops", "complexity": "intermediate", "companies": ["Blastamo Music LLC"], "people": [], "products": ["PTS", "Axosoft OnTime Scrum", "Gatling", "ReportNG framework", "SonarQube", "TestNG framework", "Ansible", "AppDynamics", "Bamboo", "Bitbucket"], "technologies": ["APM", "CI server data", "application performance monitoring", "behavior-driven development", "business intelligence", "continuous integration", "software metrics", "task management", "version control"], "frameworks": ["Atlas framework", "BDD", "BI", "CI server data", "Gatling", "ReportNG framework", "SonarQube", "TestNG framework"], "methodologies": ["Agile", "DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["count measurement", "fixes for bugs", "PTS metrics"], "has_case_study": true, "case_study_company": "Blastamo Music LLC", "tools_mentioned": ["APM", "Atlas framework", "Axosoft OnTime Scrum", "Bamboo", "Bitbucket", "Gatling", "ReportNG framework", "SonarQube", "TestNG framework"], "topics": ["metrics and KPIs", "agile principles", "continuous integration"], "key_concepts": ["actionable insight", "agile pipeline", "application performance monitoring", "behavior-driven development", "business intelligence", "continuous integration", "task management"], "problem_statement": "Improving software delivery lifecycle, measuring effectiveness of processes and requirements, and enhancing team collaboration.", "solution_approach": "Implementing agile methodologies, using tools for continuous integration and deployment, and tracking metrics to improve performance.", "extraction_method": "lm"}}
{"segment_id": "00265", "page_num": 265, "segment": "242\n\nC\n\nCD (continuous development) 86\n\nanalysis\n\nBDD 95\nCI data 96\nCI server data 91--95\nGatling 95\nReportNG framework 95\nSonarQube 95\nTestNG framework 95\ncontinuous delivery 88--89\ncontinuous integration 86--87\ncontinuous testing 89--90\ndelivery pipeline 90--91\nmeasuring improvements through CI data 101\nmetrics from 97--101\noverview 84--86\n\ncentralized SCM\n\ndata received from 71\nDCVS vs. 65--68\n\nchanged lines of code. See CLOC\nCHD (code health determination) 147, 198\nCheckmarx 156, 172\nCI (continuous integration) 155, 205\n\nanalysis\n\nBDD 95\nCI data 96\nCI server data 91--95\nGatling 95\nReportNG framework 95\nSonarQube 95\nTestNG framework 95\n\ndefined 11\ndelivery pipeline 90--91\noverview 86--87\n\nCLOC (changed lines of code) 5, 24, 147,\n\n162--163, 208\n\nClover 164\nCobertura 91, 164\ncode coverage 164--165\ncode health determination. See CHD\nCodeFlowers 29\ncomments, PR 182\ncommercial off-the-shelf. See COTS\ncommunication, face-to-face 207, 218\ncomplexity 166\nconsumer usage 120\nconsumer-facing quality rating 188\ncontinuous delivery 11, 88--89, 109, 155\ncontinuous integration. See CI\ncontinuous release quality case study\nadding elements together 149\nnormalizing changed lines of code 147--148\n\nINDEX\n\nnormalizing escaped defects 149\nnormalizing estimate health 148\nnormalizing recidivism 148--149\noverview 144--147\n\ncontinuous testing. See CT\nconversion rate 120\ncost, metric 6\nCOTS (commercial off-the-shelf) 77\nCoverity 172\nCrittercism 120, 122\nCrucible 11\nCT (continuous testing) 84\ncumulative flow 41\nCVS 64, 71\ncyclomatic complexity 166\n\nD\n\ndashboards 29\n\ncreating using ELK 228\npublishing metrics using 192\ndata collection services 235--237\ndata points 127--129\nDatadog 10, 113, 122\nDCVS (distributed version control system) 65--68\nDEBUG log level 116\ndelivery lifecycle 204--205\ndelivery pipeline 90--91\nDevOps 109, 122\ndistributed version control system. See DCVS\ndomain objects 232--233\nDSL (domain-specific language) 95\nduplicate code 166\nDynatrace 13\n\nE\n\nEC (Elasticsearch) 24, 221, 229\nElectric Cloud 88\nELK (Elasticsearch/Logstash/Kibana) 21, 115,\n\n121\n\nchecking database 226\nconfiguring data collector 226--227\ncreating dashboard 228\noverview 221--224\nsetting up system 224--226\n\nemail 193--194\nERROR log level 116\nestimations 47--49, 137\nexecutives 188--189\nextensibility 156, 158\nextreme programming. See XP\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "CD (continuous development) 86 BDD 95\nCI data 96\nCI server data 91--95\nGatling 95\nReportNG framework 95\nSonarQube 95\nTestNG framework 95\ncontinuous delivery 88--89\ncontinuous integration 86--87\ncontinuous testing 89--90\ndelivery pipeline 90--91\nmeasuring improvements through CI data 101\nmetrics...", "l2_summary": "CD (continuous development) 86 BDD 95\nCI data 96\nCI server data 91--95\nGatling 95\nReportNG framework 95\nSonarQube 95\nTestNG framework 95\ncontinuous delivery 88--89\ncontinuous integration 86--87\ncontinuous testing 89--90\ndelivery pipeline 90--91\nmeasuring improvements through CI data 101\nmetrics from 97--101\noverview 84--86 data received from 71\nDCVS vs. changed lines of code. BDD 95\nCI data 96\nCI server data 91--95\nGatling 95\nReportNG framework 95\nSonarQube 95\nTestNG framework 95 CLOC (changed lines of code) 5, 24, 147,", "prev_page": {"page_num": 264, "segment_id": "00264"}, "next_page": {"page_num": 266, "segment_id": "00266"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["continuous", "data", "code", "delivery", "framework", "overview", "normalizing", "integration", "testing", "pipeline"], "content_type": "reference", "domain": "devops", "complexity": "intermediate", "companies": ["Electric Cloud", "Checkmarx", "Coverity", "Crittercism", "Dynatrace"], "people": [], "products": ["SonarQube", "TestNG framework", "ReportNG framework", "Gatling", "Checkmarx", "Coverity", "Crittercism", "Crucible"], "technologies": ["continuous integration", "continuous delivery", "continuous testing", "delivery pipeline", "code health determination", "changed lines of code", "code coverage", "cyclomatic complexity"], "frameworks": ["BDD", "SonarQube", "TestNG framework", "ReportNG framework", "Gatling"], "methodologies": ["DevOps"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["improvements through CI data", "metrics from 97--101", "consumer-facing quality rating"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["ELK", "Elasticsearch", "Logstash", "Kibana", "SonarQube", "TestNG framework", "ReportNG framework", "Gatling", "Checkmarx", "Coverity", "Crittercism", "Crucible"], "topics": ["continuous integration and delivery", "code health determination", "metrics and KPIs"], "key_concepts": ["continuous integration", "continuous delivery", "continuous testing", "delivery pipeline", "code coverage", "metrics"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00266", "page_num": 266, "segment": "F\n\nfeedback loop\nanalysis\n\ndetermining metrics to track 14\noverview 13--14\nvisualizing data 14--16\n\nin Blastamo Music LLC example 32\noverview 3--5\nputting changes into practice 16\n\nFibonacci series 137\nFishEye 11, 76--77\nfix to release percentage. See FRP\nFogBugz 10\nFortify 172\nFRP (fix to release percentage) 163\nfunctional requirements 154\n\nG\n\nGantt charts 8\nGatling 95\nGcov 164\nGit 64, 71\nGitHub 11, 68\nGo 88\nGoCD 90\nGoogle Analytics 120--121\nGORM (Grails Object Relational Mapping) 230\nGrafana 113, 121\nGrails 221, 226\n\narchitectural overview 230--232\ndata collection services 235--237\ndomain objects 232--233\nraw data for 233--234\nscheduling jobs for data collection 237--239\nGrails Object Relational Mapping. See GORM\nGraphite 10, 113, 121\nGroovy 226\n\nH\n\nHudson 86--87\nHyperSpin 156, 170\n\nI\n\nINFO log level 116\nISO 8601 115\n\nJ\n\nJaCoCo 91, 164\nJenkins 21, 86--87, 90\n\nINDEX\n\n243\n\nJIRA 10, 37, 227, 230\njobs, Grails 237--239\nJSON (JavaScript Object Notation) 116\nJVM (Java Virtual Machine) 226, 232\n\nK\n\nKanban 4\nKibana 113, 221, 228\nKPBC (Kleiner Perkins Caufield & Byers) 193\nKPIs (key performance indicators)\n\ndefined 5\ndetermining through CI 10\n\nL\n\nlabels 55\nlead time 42\n\nfinding anomalies in 173--176\nMTTR and 158--162\noverview 42, 215\n\nLeanKit 10\nLOC (lines of code) 16, 72, 158\nlogging\n\nrunning out of disk space from 228\nusing formats easy to work with 116\nusing proactively 116\nusing standard categories 116\nusing timestamps 115\nusing unique IDs 115\n\nM\n\nmaintainability 156, 208\n\nCLOC 162--163\ncode coverage 164--165\ndefined 158\nlead time 158--162, 173--176\nMTTR 158--162\noverview 158\nPTS data 167--168\nstatic code analysis 165--167\n\nmaintainable release rating. See MRR\nmanagers 184--186\nmean time between failures. See MTBF\nmean time to repair. See MTTR\nmeasurementor project 230\nmeasurements\n\nbug counts 42--44\nburn down 39--40\ncumulative flow 41\nlead time 42\nvelocity 40--41\n\nMercurial 64\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "determining metrics to track 14\noverview 13--14\nvisualizing data 14--16 labels 55\nlead time 42 finding anomalies in 173--176\nMTTR and 158--162\noverview 42, 215", "l2_summary": "determining metrics to track 14\noverview 13--14\nvisualizing data 14--16 architectural overview 230--232\ndata collection services 235--237\ndomain objects 232--233\nraw data for 233--234\nscheduling jobs for data collection 237--239\nGrails Object Relational Mapping. See GORM\nGraphite 10, 113, 121\nGroovy 226 labels 55\nlead time 42 finding anomalies in 173--176\nMTTR and 158--162\noverview 42, 215 CLOC 162--163\ncode coverage 164--165\ndefined 158\nlead time 158--162, 173--176\nMTTR 158--162\noverview 158\nPTS data 167--168\nstatic code analysis 165--167", "prev_page": {"page_num": 265, "segment_id": "00265"}, "next_page": {"page_num": 267, "segment_id": "00267"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["overview", "data", "time", "using", "grails", "release", "object", "lead", "mttr", "code"], "content_type": "reference", "domain": "devops|architecture", "complexity": "intermediate", "companies": ["Blastamo Music LLC", "KPBC (Kleiner Perkins Caufield & Byers)"], "people": [], "products": ["FogBugz", "Fortify", "Git", "GitHub", "GoCD", "Google Analytics", "GORM", "Grails", "Graphite", "Groovy", "Hudson", "HyperSpin", "JIRA", "Mercurial"], "technologies": ["feedback loop analysis", "determining metrics to track", "visualizing data", "putting changes into practice", "Fibonacci series", "FishEye", "FRP (fix to release percentage)", "functional requirements", "Gantt charts", "Gatling", "Gcov", "Git", "GitHub", "Go", "GoCD", "Google Analytics", "GORM (Grails Object Relational Mapping)", "Grafana", "Grails", "Graphite", "Groovy", "Hudson", "HyperSpin"], "frameworks": ["Kanban", "LeanKit"], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["bug counts", "burn down", "cumulative flow", "lead time", "velocity"], "has_case_study": true, "case_study_company": "Blastamo Music LLC", "tools_mentioned": ["FogBugz", "Fortify", "Git", "GitHub", "GoCD", "Google Analytics", "GORM (Grails Object Relational Mapping)", "Grails", "Graphite", "Groovy", "Hudson", "HyperSpin", "JIRA"], "topics": ["feedback loop analysis", "metrics tracking and visualization", "continuous integration and delivery", "project management tools", "code quality and maintainability"], "key_concepts": ["feedback loop", "metrics tracking", "data visualization", "continuous integration", "release management"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00267", "page_num": 267, "segment": "INDEX\n\n244\n\nmetrics\n\nanswering questions using\nbuild system and 11--12\noverview 9--10\nproject tracking and 10\nsource control and 11\nsystem monitoring and 12--13\n\nfrom CI 97--101\ncombining data points to create 127--129\ncreating\n\ncreating formulas around multiple data\n\npoints 140--144\n\ndetermining what to track 139--140\nexploring data 136--138\noverview 135--136\n\ndefining 129--135\ndetermining which to track 14\nmeasuring continuous release quality case study\n\nadding elements together 149\nnormalizing changed lines of code 147--148\nnormalizing escaped defects 149\nnormalizing estimate health 148\nnormalizing recidivism 148--149\noverview 144--147\n\noverview 5\nfrom PTS\n\nbugs 53--54\nsorting with tags and labels 55\ntask volume 52--53\ntask workflow 54--55\n\nfrom SCM 77--78\nteam difficulties\n\ndefinitions of measurements 6--7\nlack of unified view 8--9\noverview 5--6\nproduct focus vs. project focus 7--8\n\nmind mapping 14\nMongoDB 221, 230\nmonitoring 12--13\nmotivation 217\nMRR (maintainable release rating) 163\nMTBF (mean time between failures) 170, 207\nMTTR (mean time to repair) 129, 158--163, 207,\n\n209\n\nN\n\nNCover 91, 164\nNew Relic 13, 113, 118, 121, 156, 170\nNFRs (non-functional requirements) 156--157\n\nO\n\nobjective data 132\nOctopus 88\n\nOpen Web Analytics 121\nORM (object relational mapping) 232\nOWASP (Open Web Application Security\n\nProject) 172\nOWASP ZAP 156, 172\nownership\n\ngetting support 17--18\nteam members against metrics 18--19\n\nP\n\npds (points/developer/sprint) 28\nperformance, team member 38\nPlanbox 10\nPOGOs (plain old Groovy objects) 230\nPR (pull request) 181\nprivacy 18\nproduct focus vs. project focus 7\nproduction systems\n\nadding arbitrary metrics to development\n\ncycle 113\n\nconsumer usage 120\nlogging best practices\n\nusing formats easy to work with 116\nusing proactively 116\nusing standard categories 116\nusing timestamps 115\nusing unique IDs 115\n\nmoving team to DevOps case study 122\noverview 107--109\nsemantic logging analysis 120--121\nserver health statistics 118--120\nsocial network interaction 116--118\ntools used for collecting data 121--122\nusing features of application performance monitoring system 113--115\n\nproject tracking 10\nPTS (project tracking system) 24, 160, 205, 211\n\nagile measurements using\n\nbug counts 42--44\nburn down 39--40\ncumulative flow 41\nlead time 42\nvelocity 40--41\ndata from 167--168\nmetrics from\n\nbugs 53--54\nsorting with tags and labels 55\ntask volume 52--53\ntask workflow 54--55\n\noverview 37--39\npreparing for analysis\n\ndefining completion for tasks 50--52\ndefining done for tasks 49--50\nestimating task duration 47--49\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "answering questions using\nbuild system and 11--12\noverview 9--10\nproject tracking and 10\nsource control and 11\nsystem monitoring and 12--13 using formats easy to work with 116\nusing proactively 116\nusing standard categories 116\nusing timestamps 115\nusing unique IDs 115 agile measurements using", "l2_summary": "answering questions using\nbuild system and 11--12\noverview 9--10\nproject tracking and 10\nsource control and 11\nsystem monitoring and 12--13 bugs 53--54\nsorting with tags and labels 55\ntask volume 52--53\ntask workflow 54--55 project focus 7\nproduction systems using formats easy to work with 116\nusing proactively 116\nusing standard categories 116\nusing timestamps 115\nusing unique IDs 115 agile measurements using overview 37--39\npreparing for analysis", "prev_page": {"page_num": 266, "segment_id": "00266"}, "next_page": {"page_num": 268, "segment_id": "00268"}, "extended_fields": {"has_code": false, "has_formulas": true, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["using", "overview", "project", "data", "task", "metrics", "system", "normalizing", "team", "focus"], "content_type": "tutorial", "domain": "devops|metrics|monitoring", "complexity": "intermediate", "companies": ["New Relic", "MongoDB"], "people": [], "products": ["NCover", "Octopus"], "technologies": ["CI", "PTS", "SCM", "ORM", "OWASP ZAP"], "frameworks": ["POGOs"], "methodologies": ["Agile"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["MTBF", "MTTR", "MRR"], "has_case_study": true, "case_study_company": null, "tools_mentioned": ["New Relic", "MongoDB", "NCover", "Octopus"], "topics": ["metrics", "monitoring", "CI/CD"], "key_concepts": ["metrics definition", "tracking metrics", "using data points"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00268", "page_num": 268, "segment": "INDEX\n\n245\n\noverview 44--45\ntagging tasks 46--47\nusage of PTS by entire team 45--46\n\ntech debt trending example 57\n\npublishing metrics\n\naudience considerations\nexecutives 188--189\nmaking point using metrics 189--191\nmanagers 184--186\noverview 178--191\nteam 180--184\ndashboards 192\ndriving goal visibility case study 194--200\nemail 193--194\n\npull request workflow 79\npull request. See PR\nPuppet 223--224\nPython 232\n\nQ\n\nQA (quality assurance) 82\nQM (quality management) 79\nQuartz 238\n\nR\n\nRally 37\nrecidivism 23, 52, 128, 148, 184, 196, 215\nrefactoring patterns 174\nrelative terms 50\nreliability 156, 169--171\nReportNG framework 95\n\nS\n\nscheduling jobs 237--239\nSCM (source control management)\n\ncharting activity from 78--79\ndata received\n\nfrom centralized SCM 71\nfrom DCVS 68\nfrom SCMs 71--77\n\ndefined 64\ndistributed system vs. centralized system 65--68\ngeneral discussion 62\nmetrics from 77--78\npreparing for analysis 64--65\npull request workflow 79\nquality management 79\n\nScrum 4\nSDLC (software development lifecycle) 8\nsecurity 156, 171--173\nself-organizing teams 210\n\nsemantic logging analysis 120--121\nserver health statistics 118--120\nservice virtualization 28\nSLAs (service-level agreements) 118, 169\nsocial network interaction 116--118\nsoftware development lifecycle. See SDLC\nSonarQube 91, 95, 156, 172, 175\nsource control management. See SCM\nSplunk 115, 121, 156\nstakeholders 178\nStash 11\nstatic code analysis 165--167\nstatsd framework 110\nstory points 137\nsubjective data 130--132\nSVN (Subversion) 64, 71\n\nT\n\ntags 55\ntasks\n\ndefining completion 50--52\ndefining done 49--50\nestimating duration 47--49\ntagging 46--47\nvolume metric 52--53\nworkflow for 54--55\n\nteams\n\ncase study 215\ndifficulties with measurement in\n\ndefinitions of measurements 6--7\nlack of unified view 8--9\noverview 5--6\nproduct focus vs. project focus 7--8\n\ngetting support 17--18\nmeasuring effectiveness 210--213\nmember performance 38\nmembers against metrics 18--19\npublishing metrics for 180--184\n\ntech debt 57, 167\ntechnical quality analysis\navailability 169--171\ngeneral discussion 154--155\nmaintainability\n\nCLOC 162--163\ncode coverage 164--165\nlead time 158--162, 173--176\nMTTR 158--162\noverview 158\nPTS data 167--168\nstatic code analysis 165--167\n\nNFRs, measuring 156--157\nreliability 169--171\nsecurity 171--173\ntools for 155--156\nusability 168--173\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "pull request workflow 79\npull request. centralized system 65--68\ngeneral discussion 62\nmetrics from 77--78\npreparing for analysis 64--65\npull request workflow 79\nquality management 79 NFRs, measuring 156--157\nreliability 169--171\nsecurity 171--173\ntools for 155--156\nusability 168--173", "l2_summary": "pull request workflow 79\npull request. QA (quality assurance) 82\nQM (quality management) 79\nQuartz 238 centralized system 65--68\ngeneral discussion 62\nmetrics from 77--78\npreparing for analysis 64--65\npull request workflow 79\nquality management 79 Scrum 4\nSDLC (software development lifecycle) 8\nsecurity 156, 171--173\nself-organizing teams 210 See SDLC\nSonarQube 91, 95, 156, 172, 175\nsource control management. NFRs, measuring 156--157\nreliability 169--171\nsecurity 171--173\ntools for 155--156\nusability 168--173", "prev_page": {"page_num": 267, "segment_id": "00267"}, "next_page": {"page_num": 269, "segment_id": "00269"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["metrics", "analysis", "overview", "quality", "management", "pull", "request", "workflow", "data", "code"], "content_type": "reference", "domain": "devops|management", "complexity": "intermediate", "companies": [], "people": ["Mark Watson"], "products": ["PTS", "SonarQube", "Splunk", "Rally", "SVN", "Stash"], "technologies": ["PTS", "Python", "Puppet", "QC", "Scrum", "SDLC", "Agile", "DevOps", "CLOC", "code coverage", "lead time", "MTTR", "static code analysis", "semantic logging", "server health statistics", "service virtualization", "SLAs", "statsd framework"], "frameworks": ["ReportNG framework", "QA (quality assurance)", "QM (quality management)", "Quartz"], "methodologies": ["Scrum"], "programming_languages": [], "has_best_practices": true, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["tech debt trending", "publishing metrics", "pull request workflow", "team performance", "server health statistics", "service-level agreements"], "has_case_study": true, "case_study_company": null, "tools_mentioned": ["PTS", "SonarQube", "Splunk", "Rally", "SVN", "Stash"], "topics": ["metrics and KPIs", "quality management", "team performance measurement"], "key_concepts": ["technical debt", "metrics-driven development", "quality assurance", "DevOps practices"], "problem_statement": "Improving team performance and technical quality through metrics and best practices", "solution_approach": "Implementing various tools and methodologies to track, analyze, and improve software development processes", "extraction_method": "lm"}}
{"segment_id": "00269", "page_num": 269, "segment": "246\n\nINDEX\n\nTelerik TeamPulse 10, 37\ntest engineers 82\nTestNG framework 95\nTFS 10\ntimestamps 115\nTravis CI 86\nTwitter 116, 118\n\nU\n\nVeracode 172\nvisualization 14--16, 28--30\nVMs (virtual machines) 223\n\nW\n\nWARN log level 116\nWhiteHat 172\nworking software 6\n\nusability 156, 168--173\n\nX\n\nV\n\nXMind 14\nXP (extreme programming) 4\n\nVagrant 223--225\nvelocity 168, 196, 215\n\nin Blastamo Music LLC example 26\nas measurement 40--41\n\nY\n\nYammer 116\n\nLicensed to Mark Watson <nordickan@gmail.com>", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "Veracode 172\nvisualization 14--16, 28--30\nVMs (virtual machines) 223 WARN log level 116\nWhiteHat 172\nworking software 6 usability 156, 168--173", "l2_summary": "Telerik TeamPulse 10, 37\ntest engineers 82\nTestNG framework 95\nTFS 10\ntimestamps 115\nTravis CI 86\nTwitter 116, 118 Veracode 172\nvisualization 14--16, 28--30\nVMs (virtual machines) 223 WARN log level 116\nWhiteHat 172\nworking software 6 usability 156, 168--173 XMind 14\nXP (extreme programming) 4 Vagrant 223--225\nvelocity 168, 196, 215", "prev_page": {"page_num": 268, "segment_id": "00268"}, "next_page": {"page_num": 270, "segment_id": "00270"}, "extended_fields": {"has_code": false, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": [], "content_type": "reference", "domain": "programming|devops", "complexity": "intermediate", "companies": ["Blastamo Music LLC", "Veracode", "WhiteHat"], "people": [], "products": ["Telerik TeamPulse", "TestNG framework", "Travis CI", "XMind", "Vagrant"], "technologies": ["timestamps", "VMs (virtual machines)", "WARN log level", "velocity"], "frameworks": ["TestNG framework", "Travis CI"], "methodologies": ["XP (extreme programming)"], "programming_languages": [], "has_best_practices": false, "has_antipatterns": false, "has_instructions": false, "has_metrics": true, "metrics": ["velocity"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["Telerik TeamPulse", "TestNG framework", "Travis CI", "Twitter", "Veracode", "WhiteHat", "XMind", "Vagrant", "Yammer"], "topics": ["Testing and Quality Assurance", "Continuous Integration/Continuous Delivery (CI/CD)", "Project Management"], "key_concepts": ["Test automation", "Build processes", "Performance monitoring", "Virtualization", "Log management"], "problem_statement": null, "solution_approach": null, "extraction_method": "lm"}}
{"segment_id": "00270", "page_num": 270, "segment": "AGILE DEVELOPMENT\n\nAgile Metrics IN ACTION\n\nChristopher W. H. Davis\n\nSEE INSERT\n\non your expedition\n\n“A steadfast companion\n into measurement.”\n\n---From the Foreword by\nOlivier Gaudin, SonarSource\n\n---Sune Lomholt, Nordea Bank\n\nmetrics tailored to your\n\n“The perfect starting point\n for your agile journey.”\n“You’ll be coming up with\n own needs in no time.”\n“Comprehensive and\n easy to follow.”\n\n---Chris Heneghan, SunGard\n\n---Noreen Dertinger\nDertinger Informatics, Inc.\n\nT he iterative nature of agile development is perfect for\n\nexperience-based, continuous improvement. Tracking\nsystems, test and build tools, source control, continuous\n\nintegration, and other built-in parts of a project lifecycle\nthrow off a wealth of data you can use to improve your\nproducts, processes, and teams. The question is, how to do it?\n\nAgile Metrics in Action teaches you how. This practical book is\na rich resource for an agile team that aims to use metrics to\nobjectively measure performance. You’ll learn how to gather\nthe data that really count, along with how to effectively\nanalyze and act upon the results. Along the way, you’ll\ndiscover techniques all team members can use for better\nindividual accountability and team performance.\n\nWhat’s Inside\n\n● Use the data you generate every day from CI and Scrum\n● Improve communication, productivity, transparency,\n and morale\n● Objectively measure performance\n● Make metrics a natural byproduct of your development\n process\n\nPractices in this book will work with any development process\nor tool stack. For code-based examples, this book uses Groovy,\nGrails, and MongoDB.\n\nChristopher Davis has been a software engineer and team leader\nfor over 15 years. He has led numerous teams to successful\ndelivery using agile methodologies.\n\nTo download their free eBook in PDF, ePub, and Kindle formats, owners\nof this book should visit manning.com/AgileMetricsinAction\n\nM A N N I N G\n\n$44.99 / Can $51.99 [INCLUDING eBOOK]", "source_file": "Agile-Metrics-in-Action.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "10", "chapter_title": "Measuring your team against the agile principles", "section_num": "10.5", "section_title": "One principle for effective requirements", "l1_summary": "Agile Metrics IN ACTION metrics tailored to your Agile Metrics in Action teaches you how.", "l2_summary": "Agile Metrics IN ACTION metrics tailored to your T he iterative nature of agile development is perfect for The question is, how to do it? Agile Metrics in Action teaches you how. This practical book is\na rich resource for an agile team that aims to use metrics to\nobjectively measure performance.", "prev_page": {"page_num": 269, "segment_id": "00269"}, "flags": {"continuity_gap": true}, "extended_fields": {"has_code": true, "has_formulas": false, "has_diagram": false, "has_list": false, "has_citations": false, "key_terms": ["agile", "metrics", "development", "book", "team", "data", "performance", "action", "christopher", "davis"], "content_type": "tutorial", "domain": "programming|devops", "complexity": "intermediate", "companies": ["SonarSource", "Nordea Bank", "SunGard", "Dertinger Informatics, Inc."], "people": ["Christopher W. H. Davis"], "products": [], "technologies": ["Groovy", "Grails", "MongoDB"], "frameworks": ["CI", "Scrum"], "methodologies": ["Agile"], "programming_languages": ["Groovy"], "has_best_practices": true, "has_antipatterns": false, "has_instructions": true, "has_metrics": true, "metrics": ["performance", "communication", "productivity", "transparency", "morale"], "has_case_study": false, "case_study_company": null, "tools_mentioned": ["CI", "Scrum"], "topics": ["Agile Metrics", "Data Collection and Analysis", "Team Performance Improvement"], "key_concepts": ["Iterative Development", "Experience-Based Continuous Improvement", "Objective Measurement of Performance"], "problem_statement": "How to use metrics in agile development for continuous improvement", "solution_approach": "Gathering, analyzing, and acting upon data from CI/Scrum processes", "extraction_method": "lm"}}
{"total_cards": 270, "validation": {"valid": true, "errors": 0, "warnings": 4}, "field_coverage": {"chapter_num": {"present": 244, "coverage": 0.9037037037037037}, "chapter_title": {"present": 244, "coverage": 0.9037037037037037}, "section_num": {"present": 262, "coverage": 0.9703703703703703}, "section_title": {"present": 262, "coverage": 0.9703703703703703}, "l1_summary": {"present": 269, "coverage": 0.9962962962962963}, "l2_summary": {"present": 269, "coverage": 0.9962962962962963}, "ocr_used": {"present": 0, "coverage": 0.0}, "has_table": {"present": 43, "coverage": 0.15925925925925927}, "prev_page": {"present": 269, "coverage": 0.9962962962962963}, "next_page": {"present": 269, "coverage": 0.9962962962962963}}, "stage": "finalize", "version": "2.0.0", "created_at": "2025-11-05T10:50:53Z", "segment_id": "__audit__"}
{"segment_id": "__footer__", "created_at": "2025-11-05T10:50:53Z", "version": "2.0.0", "product": "archivist magika", "manifest_sha256": "d47d30baac3cc15c98523adc48c2af2fa75a742754e13f399468e7162da0f310", "card_count": 270}
===DATASET_END===

{"type": "header", "source_dataset": "naked-statistics-pdf.dataset.jsonl", "source_file": null, "book": "naked-statistics-pdf", "title": null, "total_chunks": 372, "chunk_size": 512, "chunk_overlap": 128, "stage": "chunks", "version": "2.0.0", "created_at": "2025-11-05T13:18:48Z"}
{"type": "chunk", "text": "Contents\n\nCover\nTitle Page\nDedication\n\nIntroduction: Why I hated calculus but love statistics\n\n1 What’s the Point? 2 Descriptive Statistics: Who was the best baseball player of all time? Appendix to Chapter 2\n\n3 Deceptive Description: “He’s got a great personality!” and other true but\n\ngrossly misleading statements\n\n4 Correlation: How does Netflix know what movies I like? Appendix to Chapter 4\n\n5 Basic Probability: Don’t buy the extended warranty on your $99 printer\n\n51⁄2 The Monty Hall Problem\n\n6 Problems with Probability: How overconfident math geeks nearly destroyed the\n\nglobal financial system\n\n7 The Importance of Data: “Garbage in, garbage out”\n\n8 The Central Limit Theorem: The Lebron James of statistics\n\n9 Inference: Why my statistics professor thought I might have cheated\n\nAppendix to Chapter 9\n\n10 Polling: How we know that 64 percent of Americans support the death penalty\n\n(with a sampling error ± 3 percent)\n\nAppendix to Chapter 10", "full_text": "Source: naked-statistics-pdf.pdf\n\nContents\n\nCover\nTitle Page\nDedication\n\nIntroduction: Why I hated calculus but love statistics\n\n1 What’s the Point? 2 Descriptive Statistics: Who was the best baseball player of all time? Appendix to Chapter 2\n\n3 Deceptive Description: “He’s got a great personality!” and other true but\n\ngrossly misleading statements\n\n4 Correlation: How does Netflix know what movies I like? Appendix to Chapter 4\n\n5 Basic Probability: Don’t buy the extended warranty on your $99 printer\n\n51⁄2 The Monty Hall Problem\n\n6 Problems with Probability: How overconfident math geeks nearly destroyed the\n\nglobal financial system\n\n7 The Importance of Data: “Garbage in, garbage out”\n\n8 The Central Limit Theorem: The Lebron James of statistics\n\n9 Inference: Why my statistics professor thought I might have cheated\n\nAppendix to Chapter 9\n\n10 Polling: How we know that 64 percent of Americans support the death penalty\n\n(with a sampling error ± 3 percent)\n\nAppendix to Chapter 10", "tokens": 219, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 4, "segment_id": "00004", "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null, "has_table": false}, "context": "Source: naked-statistics-pdf.pdf", "chunk_id": "00000000"}
{"type": "chunk", "text": "Introduction\nWhy I hated calculus but love statistics\n\nI have always had an uncomfortable relationship with math. I don’t like numbers for the sake\n\nof numbers. I am not impressed by fancy formulas that have no real-world application. I\nparticularly disliked high school calculus for the simple reason that no one ever bothered to tell\nme why I needed to learn it. What is the area beneath a parabola? Who cares? In fact, one of the great moments of my life occurred during my senior year of high school, at\nthe end of the first semester of Advanced Placement Calculus. I was working away on the final\nexam, admittedly less prepared for the exam than I ought to have been. (I had been accepted to\nmy first-choice college a few weeks earlier, which had drained away what little motivation I\nhad for the course.) As I stared at the final exam questions, they looked completely unfamiliar. I don’t mean that I was having trouble answering the questions. I mean that I didn’t even\nrecognize what was being asked. I was no stranger to being unprepared for exams, but, to\nparaphrase Donald Rumsfeld, I usually knew what I didn’t know. This exam looked even more\nGreek than usual. I flipped through the pages of the exam for a while and then more or less\nsurrendered. I walked to the front of the classroom, where my calculus teacher, whom we’ll\ncall Carol Smith, was proctoring the exam. “Mrs. Smith,” I said, “I don’t recognize a lot of the\nstuff on the test.”\n\nSuffice it to say that Mrs. Smith did not like me a whole lot more than I liked her. Yes, I can\nnow admit that I sometimes used my limited powers as student association president to\nschedule all-school assemblies just so that Mrs. Smith’s calculus class would be canceled. Yes, my friends and I did have flowers delivered to Mrs. Smith during class from “a secret\nadmirer” just so that we could chortle away in the back of the room as she looked around in\nembarrassment. And yes, I did stop doing any homework at all once I got in to college. So when I walked up to Mrs. Smith in the middle of the exam and said that the material did\nnot look familiar, she was, well, unsympathetic.", "full_text": "Source: naked-statistics-pdf.pdf\n\nIntroduction\nWhy I hated calculus but love statistics\n\nI have always had an uncomfortable relationship with math. I don’t like numbers for the sake\n\nof numbers. I am not impressed by fancy formulas that have no real-world application. I\nparticularly disliked high school calculus for the simple reason that no one ever bothered to tell\nme why I needed to learn it. What is the area beneath a parabola? Who cares? In fact, one of the great moments of my life occurred during my senior year of high school, at\nthe end of the first semester of Advanced Placement Calculus. I was working away on the final\nexam, admittedly less prepared for the exam than I ought to have been. (I had been accepted to\nmy first-choice college a few weeks earlier, which had drained away what little motivation I\nhad for the course.) As I stared at the final exam questions, they looked completely unfamiliar. I don’t mean that I was having trouble answering the questions. I mean that I didn’t even\nrecognize what was being asked. I was no stranger to being unprepared for exams, but, to\nparaphrase Donald Rumsfeld, I usually knew what I didn’t know. This exam looked even more\nGreek than usual. I flipped through the pages of the exam for a while and then more or less\nsurrendered. I walked to the front of the classroom, where my calculus teacher, whom we’ll\ncall Carol Smith, was proctoring the exam. “Mrs. Smith,” I said, “I don’t recognize a lot of the\nstuff on the test.”\n\nSuffice it to say that Mrs. Smith did not like me a whole lot more than I liked her. Yes, I can\nnow admit that I sometimes used my limited powers as student association president to\nschedule all-school assemblies just so that Mrs. Smith’s calculus class would be canceled. Yes, my friends and I did have flowers delivered to Mrs. Smith during class from “a secret\nadmirer” just so that we could chortle away in the back of the room as she looked around in\nembarrassment. And yes, I did stop doing any homework at all once I got in to college. So when I walked up to Mrs. Smith in the middle of the exam and said that the material did\nnot look familiar, she was, well, unsympathetic.", "tokens": 496, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 6, "segment_id": "00006", "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null, "has_table": false}, "context": "Source: naked-statistics-pdf.pdf", "chunk_id": "00000001"}
{"type": "chunk", "text": "Smith’s calculus class would be canceled. Yes, my friends and I did have flowers delivered to Mrs. Smith during class from “a secret\nadmirer” just so that we could chortle away in the back of the room as she looked around in\nembarrassment. And yes, I did stop doing any homework at all once I got in to college. So when I walked up to Mrs. Smith in the middle of the exam and said that the material did\nnot look familiar, she was, well, unsympathetic. “Charles,” she said loudly, ostensibly to me\nbut facing the rows of desks to make certain that the whole class could hear, “if you had\nstudied, the material would look a lot more familiar.” This was a compelling point. So I slunk back to my desk. After a few minutes, Brian Arbetter, a far better calculus\nstudent than I, walked to the front of the room and whispered a few things to Mrs. Smith. She\nwhispered back and then a truly extraordinary thing happened. “Class, I need your attention,”\nMrs. Smith announced. “It appears that I have given you the second semester exam by\nmistake.” We were far enough into the test period that the whole exam had to be aborted and\nrescheduled. I cannot fully describe my euphoria. I would go on in life to marry a wonderful woman. We\nhave three healthy children. I’ve published books and visited places like the Taj Mahal and\nAngkor Wat. Still, the day that my calculus teacher got her comeuppance is a top five life", "full_text": "Source: naked-statistics-pdf.pdf\n\nSmith’s calculus class would be canceled. Yes, my friends and I did have flowers delivered to Mrs. Smith during class from “a secret\nadmirer” just so that we could chortle away in the back of the room as she looked around in\nembarrassment. And yes, I did stop doing any homework at all once I got in to college. So when I walked up to Mrs. Smith in the middle of the exam and said that the material did\nnot look familiar, she was, well, unsympathetic. “Charles,” she said loudly, ostensibly to me\nbut facing the rows of desks to make certain that the whole class could hear, “if you had\nstudied, the material would look a lot more familiar.” This was a compelling point. So I slunk back to my desk. After a few minutes, Brian Arbetter, a far better calculus\nstudent than I, walked to the front of the room and whispered a few things to Mrs. Smith. She\nwhispered back and then a truly extraordinary thing happened. “Class, I need your attention,”\nMrs. Smith announced. “It appears that I have given you the second semester exam by\nmistake.” We were far enough into the test period that the whole exam had to be aborted and\nrescheduled. I cannot fully describe my euphoria. I would go on in life to marry a wonderful woman. We\nhave three healthy children. I’ve published books and visited places like the Taj Mahal and\nAngkor Wat. Still, the day that my calculus teacher got her comeuppance is a top five life", "tokens": 336, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 6, "segment_id": "00006", "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null, "has_table": false}, "context": "Source: naked-statistics-pdf.pdf", "chunk_id": "00000002"}
{"type": "chunk", "text": "moment. (The fact that I nearly failed the makeup final exam did not significantly diminish this\nwonderful life experience.)\n\nThe calculus exam incident tells you much of what you need to know about my relationship\nwith mathematics---but not everything. Curiously, I loved physics in high school, even though\nphysics relies very heavily on the very same calculus that I refused to do in Mrs. Smith’s\nclass. Why? Because physics has a clear purpose. I distinctly remember my high school\nphysics teacher showing us during the World Series how we could use the basic formula for\nacceleration to estimate how far a home run had been hit. That’s cool---and the same formula\nhas many more socially significant applications. Once I arrived in college, I thoroughly enjoyed probability, again because it offered insight\ninto interesting real-life situations. In hindsight, I now recognize that it wasn’t the math that\nbothered me in calculus class; it was that no one ever saw fit to explain the point of it. If\nyou’re not fascinated by the elegance of formulas alone---which I am most emphatically not---\nthen it is just a lot of tedious and mechanistic formulas, at least the way it was taught to me. That brings me to statistics (which, for the purposes of this book, includes probability). I\nlove statistics. Statistics can be used to explain everything from DNA testing to the idiocy of\nplaying the lottery. Statistics can help us identify the factors associated with diseases like\ncancer and heart disease; it can help us spot cheating on standardized tests. Statistics can even\nhelp you win on game shows. There was a famous program during my childhood called Let’s\nMake a Deal, with its equally famous host, Monty Hall. At the end of each day’s show, a\nsuccessful player would stand with Monty facing three big doors: Door no. 1, Door no. 2, and\nDoor no. 3. Monty Hall explained to the player that there was a highly desirable prize behind\none of the doors---something like a new car---and a goat behind the other two. The idea was\nstraightforward: the player chose one of the doors and would get the contents behind that door. As each player stood facing the doors with Monty Hall, he or she had a 1 in 3 chance of\nchoosing the door that would be opened to reveal the valuable prize.", "full_text": "Source: naked-statistics-pdf.pdf\n\nmoment. (The fact that I nearly failed the makeup final exam did not significantly diminish this\nwonderful life experience.)\n\nThe calculus exam incident tells you much of what you need to know about my relationship\nwith mathematics---but not everything. Curiously, I loved physics in high school, even though\nphysics relies very heavily on the very same calculus that I refused to do in Mrs. Smith’s\nclass. Why? Because physics has a clear purpose. I distinctly remember my high school\nphysics teacher showing us during the World Series how we could use the basic formula for\nacceleration to estimate how far a home run had been hit. That’s cool---and the same formula\nhas many more socially significant applications. Once I arrived in college, I thoroughly enjoyed probability, again because it offered insight\ninto interesting real-life situations. In hindsight, I now recognize that it wasn’t the math that\nbothered me in calculus class; it was that no one ever saw fit to explain the point of it. If\nyou’re not fascinated by the elegance of formulas alone---which I am most emphatically not---\nthen it is just a lot of tedious and mechanistic formulas, at least the way it was taught to me. That brings me to statistics (which, for the purposes of this book, includes probability). I\nlove statistics. Statistics can be used to explain everything from DNA testing to the idiocy of\nplaying the lottery. Statistics can help us identify the factors associated with diseases like\ncancer and heart disease; it can help us spot cheating on standardized tests. Statistics can even\nhelp you win on game shows. There was a famous program during my childhood called Let’s\nMake a Deal, with its equally famous host, Monty Hall. At the end of each day’s show, a\nsuccessful player would stand with Monty facing three big doors: Door no. 1, Door no. 2, and\nDoor no. 3. Monty Hall explained to the player that there was a highly desirable prize behind\none of the doors---something like a new car---and a goat behind the other two. The idea was\nstraightforward: the player chose one of the doors and would get the contents behind that door. As each player stood facing the doors with Monty Hall, he or she had a 1 in 3 chance of\nchoosing the door that would be opened to reveal the valuable prize.", "tokens": 498, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 7, "segment_id": "00007", "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null, "has_table": false}, "context": "Source: naked-statistics-pdf.pdf", "chunk_id": "00000003"}
{"type": "chunk", "text": "1, Door no. 2, and\nDoor no. 3. Monty Hall explained to the player that there was a highly desirable prize behind\none of the doors---something like a new car---and a goat behind the other two. The idea was\nstraightforward: the player chose one of the doors and would get the contents behind that door. As each player stood facing the doors with Monty Hall, he or she had a 1 in 3 chance of\nchoosing the door that would be opened to reveal the valuable prize. But Let’s Make a Deal\nhad a twist, which has delighted statisticians ever since (and perplexed everyone else). After\nthe player chose a door, Monty Hall would open one of the two remaining doors, always\nrevealing a goat. For the sake of example, assume that the player has chosen Door no. 1. Monty would then open Door no. 3; the live goat would be standing there on stage. Two doors\nwould still be closed, nos. 1 and 2. If the valuable prize was behind no. 1, the contestant would\nwin; if it was behind no. 2, he would lose. But then things got more interesting: Monty would\nturn to the player and ask whether he would like to change his mind and switch doors (from no. 1 to no. 2 in this case). Remember, both doors were still closed, and the only new information\nthe contestant had received was that a goat showed up behind one of the doors that he didn’t\npick. Should he switch? The answer is yes. Why? That’s in Chapter 51⁄2. The paradox of statistics is that they are everywhere---from batting averages to presidential\npolls---but the discipline itself has a reputation for being uninteresting and inaccessible. Many\nstatistics books and classes are overly laden with math and jargon. Believe me, the technical", "full_text": "Source: naked-statistics-pdf.pdf\n\n1, Door no. 2, and\nDoor no. 3. Monty Hall explained to the player that there was a highly desirable prize behind\none of the doors---something like a new car---and a goat behind the other two. The idea was\nstraightforward: the player chose one of the doors and would get the contents behind that door. As each player stood facing the doors with Monty Hall, he or she had a 1 in 3 chance of\nchoosing the door that would be opened to reveal the valuable prize. But Let’s Make a Deal\nhad a twist, which has delighted statisticians ever since (and perplexed everyone else). After\nthe player chose a door, Monty Hall would open one of the two remaining doors, always\nrevealing a goat. For the sake of example, assume that the player has chosen Door no. 1. Monty would then open Door no. 3; the live goat would be standing there on stage. Two doors\nwould still be closed, nos. 1 and 2. If the valuable prize was behind no. 1, the contestant would\nwin; if it was behind no. 2, he would lose. But then things got more interesting: Monty would\nturn to the player and ask whether he would like to change his mind and switch doors (from no. 1 to no. 2 in this case). Remember, both doors were still closed, and the only new information\nthe contestant had received was that a goat showed up behind one of the doors that he didn’t\npick. Should he switch? The answer is yes. Why? That’s in Chapter 51⁄2. The paradox of statistics is that they are everywhere---from batting averages to presidential\npolls---but the discipline itself has a reputation for being uninteresting and inaccessible. Many\nstatistics books and classes are overly laden with math and jargon. Believe me, the technical", "tokens": 403, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 7, "segment_id": "00007", "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null, "has_table": false}, "context": "Source: naked-statistics-pdf.pdf", "chunk_id": "00000004"}
{"type": "chunk", "text": "details are crucial (and interesting)---but it’s just Greek if you don’t understand the intuition. And you may not even care about the intuition if you’re not convinced that there is any reason\nto learn it. Every chapter in this book promises to answer the basic question that I asked (to no\neffect) of my high school calculus teacher: What is the point of this? This book is about the intuition. It is short on math, equations, and graphs; when they are\nused, I promise that they will have a clear and enlightening purpose. Meanwhile, the book is\nlong on examples to convince you that there are great reasons to learn this stuff. Statistics can\nbe really interesting, and most of it isn’t that difficult. The idea for this book was born not terribly long after my unfortunate experience in Mrs. Smith’s AP Calculus class. I went to graduate school to study economics and public policy. Before the program even started, I was assigned (not surprisingly) to “math camp” along with\nthe bulk of my classmates to prepare us for the quantitative rigors that were to follow. For\nthree weeks, we learned math all day in a windowless, basement classroom (really). On one of those days, I had something very close to a career epiphany. Our instructor was\ntrying to teach us the circumstances under which the sum of an infinite series converges to a\nfinite number. Stay with me here for a minute because this concept will become clear. (Right\nnow you’re probably feeling the way I did in that windowless classroom.) An infinite series is\na pattern of numbers that goes on forever, such as 1 + 1⁄2 + 1⁄4 + 1⁄8 . . . The three dots means\nthat the pattern continues to infinity. This is the part we were having trouble wrapping our heads around. Our instructor was\ntrying to convince us, using some proof I’ve long since forgotten, that a series of numbers can\ngo on forever and yet still add up (roughly) to a finite number. One of my classmates, Will\nWarshauer, would have none of it, despite the impressive mathematical proof. (To be honest, I\nwas a bit skeptical myself.) How can something that is infinite add up to something that is\nfinite? Then I got an inspiration, or more accurately, the intuition of what the instructor was trying to\nexplain.", "full_text": "Source: naked-statistics-pdf.pdf\n\ndetails are crucial (and interesting)---but it’s just Greek if you don’t understand the intuition. And you may not even care about the intuition if you’re not convinced that there is any reason\nto learn it. Every chapter in this book promises to answer the basic question that I asked (to no\neffect) of my high school calculus teacher: What is the point of this? This book is about the intuition. It is short on math, equations, and graphs; when they are\nused, I promise that they will have a clear and enlightening purpose. Meanwhile, the book is\nlong on examples to convince you that there are great reasons to learn this stuff. Statistics can\nbe really interesting, and most of it isn’t that difficult. The idea for this book was born not terribly long after my unfortunate experience in Mrs. Smith’s AP Calculus class. I went to graduate school to study economics and public policy. Before the program even started, I was assigned (not surprisingly) to “math camp” along with\nthe bulk of my classmates to prepare us for the quantitative rigors that were to follow. For\nthree weeks, we learned math all day in a windowless, basement classroom (really). On one of those days, I had something very close to a career epiphany. Our instructor was\ntrying to teach us the circumstances under which the sum of an infinite series converges to a\nfinite number. Stay with me here for a minute because this concept will become clear. (Right\nnow you’re probably feeling the way I did in that windowless classroom.) An infinite series is\na pattern of numbers that goes on forever, such as 1 + 1⁄2 + 1⁄4 + 1⁄8 . . . The three dots means\nthat the pattern continues to infinity. This is the part we were having trouble wrapping our heads around. Our instructor was\ntrying to convince us, using some proof I’ve long since forgotten, that a series of numbers can\ngo on forever and yet still add up (roughly) to a finite number. One of my classmates, Will\nWarshauer, would have none of it, despite the impressive mathematical proof. (To be honest, I\nwas a bit skeptical myself.) How can something that is infinite add up to something that is\nfinite? Then I got an inspiration, or more accurately, the intuition of what the instructor was trying to\nexplain.", "tokens": 504, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 8, "segment_id": "00008", "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null, "has_table": false}, "context": "Source: naked-statistics-pdf.pdf", "chunk_id": "00000005"}
{"type": "chunk", "text": "This is the part we were having trouble wrapping our heads around. Our instructor was\ntrying to convince us, using some proof I’ve long since forgotten, that a series of numbers can\ngo on forever and yet still add up (roughly) to a finite number. One of my classmates, Will\nWarshauer, would have none of it, despite the impressive mathematical proof. (To be honest, I\nwas a bit skeptical myself.) How can something that is infinite add up to something that is\nfinite? Then I got an inspiration, or more accurately, the intuition of what the instructor was trying to\nexplain. I turned to Will and talked him through what I had just worked out in my head. Imagine\nthat you have positioned yourself exactly 2 feet from a wall. Now move half the distance to that wall (1 foot), so that you are left standing 1 foot away. From 1 foot away, move half the distance to the wall once again (6 inches, or 1⁄2 a foot). And\nfrom 6 inches away, do it again (move 3 inches, or 1⁄4 of a foot). Then do it again (move 11⁄2\ninches, or 1⁄8 of a foot). And so on. You will gradually get pretty darn close to the wall. (For example, when you are 1/1024th of\nan inch from the wall, you will move half the distance, or another 1/2048th of an inch.) But you\nwill never hit the wall, because by definition each move takes you only half the remaining\ndistance. In other words, you will get infinitely close to the wall but never hit it. If we measure\nyour moves in feet, the series can be described as 1 + 1⁄2 + 1⁄4 + 1⁄8 . . . Therein lies the insight: Even though you will continue moving forever---with each move\ntaking you half the remaining distance to the wall---the total distance you travel can never be\nmore than 2 feet, which is your starting distance from the wall. For mathematical purposes, the\ntotal distance you travel can be approximated as 2 feet, which turns out to be very handy for", "full_text": "Source: naked-statistics-pdf.pdf\n\nThis is the part we were having trouble wrapping our heads around. Our instructor was\ntrying to convince us, using some proof I’ve long since forgotten, that a series of numbers can\ngo on forever and yet still add up (roughly) to a finite number. One of my classmates, Will\nWarshauer, would have none of it, despite the impressive mathematical proof. (To be honest, I\nwas a bit skeptical myself.) How can something that is infinite add up to something that is\nfinite? Then I got an inspiration, or more accurately, the intuition of what the instructor was trying to\nexplain. I turned to Will and talked him through what I had just worked out in my head. Imagine\nthat you have positioned yourself exactly 2 feet from a wall. Now move half the distance to that wall (1 foot), so that you are left standing 1 foot away. From 1 foot away, move half the distance to the wall once again (6 inches, or 1⁄2 a foot). And\nfrom 6 inches away, do it again (move 3 inches, or 1⁄4 of a foot). Then do it again (move 11⁄2\ninches, or 1⁄8 of a foot). And so on. You will gradually get pretty darn close to the wall. (For example, when you are 1/1024th of\nan inch from the wall, you will move half the distance, or another 1/2048th of an inch.) But you\nwill never hit the wall, because by definition each move takes you only half the remaining\ndistance. In other words, you will get infinitely close to the wall but never hit it. If we measure\nyour moves in feet, the series can be described as 1 + 1⁄2 + 1⁄4 + 1⁄8 . . . Therein lies the insight: Even though you will continue moving forever---with each move\ntaking you half the remaining distance to the wall---the total distance you travel can never be\nmore than 2 feet, which is your starting distance from the wall. For mathematical purposes, the\ntotal distance you travel can be approximated as 2 feet, which turns out to be very handy for", "tokens": 476, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 8, "segment_id": "00008", "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null, "has_table": false}, "context": "Source: naked-statistics-pdf.pdf", "chunk_id": "00000006"}
{"type": "chunk", "text": "computation purposes. A mathematician would say that the sum of this infinite series 1 ft + 1⁄2\nft + 1⁄4 ft + 1⁄8 ft . . . converges to 2 feet, which is what our instructor was trying to teach us that\nday. The point is that I convinced Will. I convinced myself. I can’t remember the math proving\nthat the sum of an infinite series can converge to a finite number, but I can always look that up\nonline. And when I do, it will probably make sense. In my experience, the intuition makes the\nmath and other technical details more understandable---but not necessarily the other way\naround. The point of this book is to make the most important statistical concepts more intuitive and\nmore accessible, not just for those of us forced to study them in windowless classrooms but for\nanyone interested in the extraordinary power of numbers and data. Now, having just made the case that the core tools of statistics are less intuitive and accessible\nthan they ought to be, I’m going to make a seemingly contradictory point: Statistics can be\noverly accessible in the sense that anyone with data and a computer can do sophisticated\nstatistical procedures with a few keystrokes. The problem is that if the data are poor, or if the\nstatistical techniques are used improperly, the conclusions can be wildly misleading and even\npotentially dangerous. Consider the following hypothetical Internet news flash: People Who\nTake Short Breaks at Work Are Far More Likely to Die of Cancer. Imagine that headline\npopping up while you are surfing the Web. According to a seemingly impressive study of\n36,000 office workers (a huge data set!), those workers who reported leaving their offices to\ntake regular ten-minute breaks during the workday were 41 percent more likely to develop\ncancer over the next five years than workers who don’t leave their offices during the workday. Clearly we need to act on this kind of finding---perhaps some kind of national awareness\ncampaign to prevent short breaks on the job. Or maybe we just need to think more clearly about what many workers are doing during that\nten-minute break. My professional experience suggests that many of those workers who report\nleaving their offices for short breaks are huddled outside the entrance of the building smoking\ncigarettes (creating a haze of smoke through which the rest of us have to walk in order to get in\nor out).", "full_text": "Source: naked-statistics-pdf.pdf\n\ncomputation purposes. A mathematician would say that the sum of this infinite series 1 ft + 1⁄2\nft + 1⁄4 ft + 1⁄8 ft . . . converges to 2 feet, which is what our instructor was trying to teach us that\nday. The point is that I convinced Will. I convinced myself. I can’t remember the math proving\nthat the sum of an infinite series can converge to a finite number, but I can always look that up\nonline. And when I do, it will probably make sense. In my experience, the intuition makes the\nmath and other technical details more understandable---but not necessarily the other way\naround. The point of this book is to make the most important statistical concepts more intuitive and\nmore accessible, not just for those of us forced to study them in windowless classrooms but for\nanyone interested in the extraordinary power of numbers and data. Now, having just made the case that the core tools of statistics are less intuitive and accessible\nthan they ought to be, I’m going to make a seemingly contradictory point: Statistics can be\noverly accessible in the sense that anyone with data and a computer can do sophisticated\nstatistical procedures with a few keystrokes. The problem is that if the data are poor, or if the\nstatistical techniques are used improperly, the conclusions can be wildly misleading and even\npotentially dangerous. Consider the following hypothetical Internet news flash: People Who\nTake Short Breaks at Work Are Far More Likely to Die of Cancer. Imagine that headline\npopping up while you are surfing the Web. According to a seemingly impressive study of\n36,000 office workers (a huge data set!), those workers who reported leaving their offices to\ntake regular ten-minute breaks during the workday were 41 percent more likely to develop\ncancer over the next five years than workers who don’t leave their offices during the workday. Clearly we need to act on this kind of finding---perhaps some kind of national awareness\ncampaign to prevent short breaks on the job. Or maybe we just need to think more clearly about what many workers are doing during that\nten-minute break. My professional experience suggests that many of those workers who report\nleaving their offices for short breaks are huddled outside the entrance of the building smoking\ncigarettes (creating a haze of smoke through which the rest of us have to walk in order to get in\nor out).", "tokens": 506, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 9, "segment_id": "00009", "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null, "has_table": false}, "context": "Source: naked-statistics-pdf.pdf", "chunk_id": "00000007"}
{"type": "chunk", "text": "Clearly we need to act on this kind of finding---perhaps some kind of national awareness\ncampaign to prevent short breaks on the job. Or maybe we just need to think more clearly about what many workers are doing during that\nten-minute break. My professional experience suggests that many of those workers who report\nleaving their offices for short breaks are huddled outside the entrance of the building smoking\ncigarettes (creating a haze of smoke through which the rest of us have to walk in order to get in\nor out). I would further infer that it’s probably the cigarettes, and not the short breaks from\nwork, that are causing the cancer. I’ve made up this example just so that it would be\nparticularly absurd, but I can assure you that many real-life statistical abominations are nearly\nthis absurd once they are deconstructed. Statistics is like a high-caliber weapon: helpful when used correctly and potentially\ndisastrous in the wrong hands. This book will not make you a statistical expert; it will teach\nyou enough care and respect for the field that you don’t do the statistical equivalent of blowing\nsomeone’s head off. This is not a textbook, which is liberating in terms of the topics that have to be covered and\nthe ways in which they can be explained. The book has been designed to introduce the\nstatistical concepts with the most relevance to everyday life. How do scientists conclude that\nsomething causes cancer? How does polling work (and what can go wrong)? Who “lies with", "full_text": "Source: naked-statistics-pdf.pdf\n\nClearly we need to act on this kind of finding---perhaps some kind of national awareness\ncampaign to prevent short breaks on the job. Or maybe we just need to think more clearly about what many workers are doing during that\nten-minute break. My professional experience suggests that many of those workers who report\nleaving their offices for short breaks are huddled outside the entrance of the building smoking\ncigarettes (creating a haze of smoke through which the rest of us have to walk in order to get in\nor out). I would further infer that it’s probably the cigarettes, and not the short breaks from\nwork, that are causing the cancer. I’ve made up this example just so that it would be\nparticularly absurd, but I can assure you that many real-life statistical abominations are nearly\nthis absurd once they are deconstructed. Statistics is like a high-caliber weapon: helpful when used correctly and potentially\ndisastrous in the wrong hands. This book will not make you a statistical expert; it will teach\nyou enough care and respect for the field that you don’t do the statistical equivalent of blowing\nsomeone’s head off. This is not a textbook, which is liberating in terms of the topics that have to be covered and\nthe ways in which they can be explained. The book has been designed to introduce the\nstatistical concepts with the most relevance to everyday life. How do scientists conclude that\nsomething causes cancer? How does polling work (and what can go wrong)? Who “lies with", "tokens": 310, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 9, "segment_id": "00009", "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null, "has_table": false}, "context": "Source: naked-statistics-pdf.pdf", "chunk_id": "00000008"}
{"type": "chunk", "text": "statistics,” and how do they do it? How does your credit card company use data on what you\nare buying to predict if you are likely to miss a payment? (Seriously, they can do that.)\n\nIf you want to understand the numbers behind the news and to appreciate the extraordinary\n(and growing) power of data, this is the stuff you need to know. In the end, I hope to persuade\nyou of the observation first made by Swedish mathematician and writer Andrejs Dunkels: It’s\neasy to lie with statistics, but it’s hard to tell the truth without them. But I have even bolder aspirations than that. I think you might actually enjoy statistics. The\nunderlying ideas are fabulously interesting and relevant. The key is to separate the important\nideas from the arcane technical details that can get in the way. That is Naked Statistics.", "full_text": "Source: naked-statistics-pdf.pdf\n\nstatistics,” and how do they do it? How does your credit card company use data on what you\nare buying to predict if you are likely to miss a payment? (Seriously, they can do that.)\n\nIf you want to understand the numbers behind the news and to appreciate the extraordinary\n(and growing) power of data, this is the stuff you need to know. In the end, I hope to persuade\nyou of the observation first made by Swedish mathematician and writer Andrejs Dunkels: It’s\neasy to lie with statistics, but it’s hard to tell the truth without them. But I have even bolder aspirations than that. I think you might actually enjoy statistics. The\nunderlying ideas are fabulously interesting and relevant. The key is to separate the important\nideas from the arcane technical details that can get in the way. That is Naked Statistics.", "tokens": 176, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 10, "segment_id": "00010", "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null, "has_table": false}, "context": "Source: naked-statistics-pdf.pdf", "chunk_id": "00000009"}
{"type": "chunk", "text": "CHAPTER 1\n\nWhat’s the Point? I’ve noticed a curious phenomenon. Students will complain that statistics is confusing and\n\nirrelevant. Then the same students will leave the classroom and happily talk over lunch about\nbatting averages (during the summer) or the windchill factor (during the winter) or grade point\naverages (always). They will recognize that the National Football League’s “passer rating”---a\nstatistic that condenses a quarterback’s performance into a single number---is a somewhat\nflawed and arbitrary measure of a quarterback’s game day performance. The same data\n(completion rate, average yards per pass attempt, percentage of touchdown passes per pass\nattempt, and interception rate) could be combined in a different way, such as giving greater or\nlesser weight to any of those inputs, to generate a different but equally credible measure of\nperformance. Yet anyone who has watched football recognizes that it’s handy to have a single\nnumber that can be used to encapsulate a quarterback’s performance. Is the quarterback rating perfect? No. Statistics rarely offers a single “right” way of doing\nanything. Does it provide meaningful information in an easily accessible way? Absolutely. It’s\na nice tool for making a quick comparison between the performances of two quarterbacks on a\ngiven day. I am a Chicago Bears fan. During the 2011 playoffs, the Bears played the Packers;\nthe Packers won. There are a lot of ways I could describe that game, including pages and pages\nof analysis and raw data. But here is a more succinct analysis. Chicago Bears quarterback Jay\nCutler had a passer rating of 31.8. In contrast, Green Bay quarterback Aaron Rodgers had a\npasser rating of 55.4. Similarly, we can compare Jay Cutler’s performance to that in a game\nearlier in the season against Green Bay, when he had a passer rating of 85.6. That tells you a\nlot of what you need to know in order to understand why the Bears beat the Packers earlier in\nthe season but lost to them in the playoffs. That is a very helpful synopsis of what happened on the field. Does it simplify things? Yes,\nthat is both the strength and the weakness of any descriptive statistic. One number tells you\nthat Jay Cutler was outgunned by Aaron Rodgers in the Bears’ playoff loss.", "full_text": "Source: naked-statistics-pdf.pdf\n\nCHAPTER 1\n\nWhat’s the Point? I’ve noticed a curious phenomenon. Students will complain that statistics is confusing and\n\nirrelevant. Then the same students will leave the classroom and happily talk over lunch about\nbatting averages (during the summer) or the windchill factor (during the winter) or grade point\naverages (always). They will recognize that the National Football League’s “passer rating”---a\nstatistic that condenses a quarterback’s performance into a single number---is a somewhat\nflawed and arbitrary measure of a quarterback’s game day performance. The same data\n(completion rate, average yards per pass attempt, percentage of touchdown passes per pass\nattempt, and interception rate) could be combined in a different way, such as giving greater or\nlesser weight to any of those inputs, to generate a different but equally credible measure of\nperformance. Yet anyone who has watched football recognizes that it’s handy to have a single\nnumber that can be used to encapsulate a quarterback’s performance. Is the quarterback rating perfect? No. Statistics rarely offers a single “right” way of doing\nanything. Does it provide meaningful information in an easily accessible way? Absolutely. It’s\na nice tool for making a quick comparison between the performances of two quarterbacks on a\ngiven day. I am a Chicago Bears fan. During the 2011 playoffs, the Bears played the Packers;\nthe Packers won. There are a lot of ways I could describe that game, including pages and pages\nof analysis and raw data. But here is a more succinct analysis. Chicago Bears quarterback Jay\nCutler had a passer rating of 31.8. In contrast, Green Bay quarterback Aaron Rodgers had a\npasser rating of 55.4. Similarly, we can compare Jay Cutler’s performance to that in a game\nearlier in the season against Green Bay, when he had a passer rating of 85.6. That tells you a\nlot of what you need to know in order to understand why the Bears beat the Packers earlier in\nthe season but lost to them in the playoffs. That is a very helpful synopsis of what happened on the field. Does it simplify things? Yes,\nthat is both the strength and the weakness of any descriptive statistic. One number tells you\nthat Jay Cutler was outgunned by Aaron Rodgers in the Bears’ playoff loss.", "tokens": 488, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 11, "segment_id": "00011", "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null, "has_table": false}, "context": "Source: naked-statistics-pdf.pdf", "chunk_id": "00000010"}
{"type": "chunk", "text": "Similarly, we can compare Jay Cutler’s performance to that in a game\nearlier in the season against Green Bay, when he had a passer rating of 85.6. That tells you a\nlot of what you need to know in order to understand why the Bears beat the Packers earlier in\nthe season but lost to them in the playoffs. That is a very helpful synopsis of what happened on the field. Does it simplify things? Yes,\nthat is both the strength and the weakness of any descriptive statistic. One number tells you\nthat Jay Cutler was outgunned by Aaron Rodgers in the Bears’ playoff loss. On the other hand,\nthat number won’t tell you whether a quarterback had a bad break, such as throwing a perfect\npass that was bobbled by the receiver and then intercepted, or whether he “stepped up” on\ncertain key plays (since every completion is weighted the same, whether it is a crucial third\ndown or a meaningless play at the end of the game), or whether the defense was terrible. And\nso on. The curious thing is that the same people who are perfectly comfortable discussing statistics\nin the context of sports or the weather or grades will seize up with anxiety when a researcher\nstarts to explain something like the Gini index, which is a standard tool in economics for\nmeasuring income inequality. I’ll explain what the Gini index is in a moment, but for now the\nmost important thing to recognize is that the Gini index is just like the passer rating. It’s a\nhandy tool for collapsing complex information into a single number. As such, it has the", "full_text": "Source: naked-statistics-pdf.pdf\n\nSimilarly, we can compare Jay Cutler’s performance to that in a game\nearlier in the season against Green Bay, when he had a passer rating of 85.6. That tells you a\nlot of what you need to know in order to understand why the Bears beat the Packers earlier in\nthe season but lost to them in the playoffs. That is a very helpful synopsis of what happened on the field. Does it simplify things? Yes,\nthat is both the strength and the weakness of any descriptive statistic. One number tells you\nthat Jay Cutler was outgunned by Aaron Rodgers in the Bears’ playoff loss. On the other hand,\nthat number won’t tell you whether a quarterback had a bad break, such as throwing a perfect\npass that was bobbled by the receiver and then intercepted, or whether he “stepped up” on\ncertain key plays (since every completion is weighted the same, whether it is a crucial third\ndown or a meaningless play at the end of the game), or whether the defense was terrible. And\nso on. The curious thing is that the same people who are perfectly comfortable discussing statistics\nin the context of sports or the weather or grades will seize up with anxiety when a researcher\nstarts to explain something like the Gini index, which is a standard tool in economics for\nmeasuring income inequality. I’ll explain what the Gini index is in a moment, but for now the\nmost important thing to recognize is that the Gini index is just like the passer rating. It’s a\nhandy tool for collapsing complex information into a single number. As such, it has the", "tokens": 335, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 11, "segment_id": "00011", "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null, "has_table": false}, "context": "Source: naked-statistics-pdf.pdf", "chunk_id": "00000011"}
{"type": "chunk", "text": "strengths of most descriptive statistics, namely that it provides an easy way to compare the\nincome distribution in two countries, or in a single country at different points in time. The Gini index measures how evenly wealth (or income) is shared within a country on a\nscale from zero to one. The statistic can be calculated for wealth or for annual income, and it\ncan be calculated at the individual level or at the household level. (All of these statistics will\nbe highly correlated but not identical.) The Gini index, like the passer rating, has no intrinsic\nmeaning; it’s a tool for comparison. A country in which every household had identical wealth\nwould have a Gini index of zero. By contrast, a country in which a single household held the\ncountry’s entire wealth would have a Gini index of one. As you can probably surmise, the\ncloser a country is to one, the more unequal its distribution of wealth. The United States has a\nGini index of .45, according to the Central Intelligence Agency (a great collector of statistics,\nby the way).1 So what? Once that number is put into context, it can tell us a lot. For example, Sweden has a Gini\nindex of .23. Canada’s is .32. China’s is .42. Brazil’s is .54. South Africa’s is .65. * As we\nlook across those numbers, we get a sense of where the United States falls relative to the rest\nof the world when it comes to income inequality. We can also compare different points in time. The Gini index for the United States was .41 in 1997 and grew to .45 over the next decade. (The most recent CIA data are for 2007.) This tells us in an objective way that while the\nUnited States grew richer over that period of time, the distribution of wealth grew more\nunequal. Again, we can compare the changes in the Gini index across countries over roughly\nthe same time period. Inequality in Canada was basically unchanged over the same stretch. Sweden has had significant economic growth over the past two decades, but the Gini index in\nSweden actually fell from .25 in 1992 to .23 in 2005, meaning that Sweden grew richer and\nmore equal over that period. Is the Gini index the perfect measure of inequality? Absolutely not---just as the passer rating\nis not a perfect measure of quarterback performance.", "full_text": "Source: naked-statistics-pdf.pdf\n\nstrengths of most descriptive statistics, namely that it provides an easy way to compare the\nincome distribution in two countries, or in a single country at different points in time. The Gini index measures how evenly wealth (or income) is shared within a country on a\nscale from zero to one. The statistic can be calculated for wealth or for annual income, and it\ncan be calculated at the individual level or at the household level. (All of these statistics will\nbe highly correlated but not identical.) The Gini index, like the passer rating, has no intrinsic\nmeaning; it’s a tool for comparison. A country in which every household had identical wealth\nwould have a Gini index of zero. By contrast, a country in which a single household held the\ncountry’s entire wealth would have a Gini index of one. As you can probably surmise, the\ncloser a country is to one, the more unequal its distribution of wealth. The United States has a\nGini index of .45, according to the Central Intelligence Agency (a great collector of statistics,\nby the way).1 So what? Once that number is put into context, it can tell us a lot. For example, Sweden has a Gini\nindex of .23. Canada’s is .32. China’s is .42. Brazil’s is .54. South Africa’s is .65. * As we\nlook across those numbers, we get a sense of where the United States falls relative to the rest\nof the world when it comes to income inequality. We can also compare different points in time. The Gini index for the United States was .41 in 1997 and grew to .45 over the next decade. (The most recent CIA data are for 2007.) This tells us in an objective way that while the\nUnited States grew richer over that period of time, the distribution of wealth grew more\nunequal. Again, we can compare the changes in the Gini index across countries over roughly\nthe same time period. Inequality in Canada was basically unchanged over the same stretch. Sweden has had significant economic growth over the past two decades, but the Gini index in\nSweden actually fell from .25 in 1992 to .23 in 2005, meaning that Sweden grew richer and\nmore equal over that period. Is the Gini index the perfect measure of inequality? Absolutely not---just as the passer rating\nis not a perfect measure of quarterback performance.", "tokens": 508, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 12, "segment_id": "00012", "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null, "has_table": false}, "context": "Source: naked-statistics-pdf.pdf", "chunk_id": "00000012"}
{"type": "chunk", "text": "Again, we can compare the changes in the Gini index across countries over roughly\nthe same time period. Inequality in Canada was basically unchanged over the same stretch. Sweden has had significant economic growth over the past two decades, but the Gini index in\nSweden actually fell from .25 in 1992 to .23 in 2005, meaning that Sweden grew richer and\nmore equal over that period. Is the Gini index the perfect measure of inequality? Absolutely not---just as the passer rating\nis not a perfect measure of quarterback performance. But it certainly gives us some valuable\ninformation on a socially significant phenomenon in a convenient format. We have also slowly backed our way into answering the question posed in the chapter title:\nWhat is the point? The point is that statistics helps us process data, which is really just a fancy\nname for information. Sometimes the data are trivial in the grand scheme of things, as with\nsports statistics. Sometimes they offer insight into the nature of human existence, as with the\nGini index. But, as any good infomercial would point out, That’s not all! Hal Varian, chief economist at\nGoogle, told the New York Times that being a statistician will be “the sexy job” over the next\ndecade.2 I’ll be the first to concede that economists sometimes have a warped definition of\n“sexy.” Still, consider the following disparate questions:\n\nHow can we catch schools that are cheating on their standardized tests? How does Netflix know what kind of movies you like? How can we figure out what substances or behaviors cause cancer, given that we cannot\n\nconduct cancer-causing experiments on humans? Does praying for surgical patients improve their outcomes?", "full_text": "Source: naked-statistics-pdf.pdf\n\nAgain, we can compare the changes in the Gini index across countries over roughly\nthe same time period. Inequality in Canada was basically unchanged over the same stretch. Sweden has had significant economic growth over the past two decades, but the Gini index in\nSweden actually fell from .25 in 1992 to .23 in 2005, meaning that Sweden grew richer and\nmore equal over that period. Is the Gini index the perfect measure of inequality? Absolutely not---just as the passer rating\nis not a perfect measure of quarterback performance. But it certainly gives us some valuable\ninformation on a socially significant phenomenon in a convenient format. We have also slowly backed our way into answering the question posed in the chapter title:\nWhat is the point? The point is that statistics helps us process data, which is really just a fancy\nname for information. Sometimes the data are trivial in the grand scheme of things, as with\nsports statistics. Sometimes they offer insight into the nature of human existence, as with the\nGini index. But, as any good infomercial would point out, That’s not all! Hal Varian, chief economist at\nGoogle, told the New York Times that being a statistician will be “the sexy job” over the next\ndecade.2 I’ll be the first to concede that economists sometimes have a warped definition of\n“sexy.” Still, consider the following disparate questions:\n\nHow can we catch schools that are cheating on their standardized tests? How does Netflix know what kind of movies you like? How can we figure out what substances or behaviors cause cancer, given that we cannot\n\nconduct cancer-causing experiments on humans? Does praying for surgical patients improve their outcomes?", "tokens": 349, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 12, "segment_id": "00012", "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null, "has_table": false}, "context": "Source: naked-statistics-pdf.pdf", "chunk_id": "00000013"}
{"type": "chunk", "text": "Is there really an economic benefit to getting a degree from a highly selective college or\n\nuniversity? What is causing the rising incidence of autism? Statistics can help answer these questions (or, we hope, can soon). The world is producing\nmore and more data, ever faster and faster. Yet, as the New York Times has noted, “Data is\nmerely the raw material of knowledge.”3* Statistics is the most powerful tool we have for\nusing information to some meaningful end, whether that is identifying underrated baseball\nplayers or paying teachers more fairly. Here is a quick tour of how statistics can bring meaning\nto raw data. Description and Comparison\nA bowling score is a descriptive statistic. So is a batting average. Most American sports fans\nover the age of five are already conversant in the field of descriptive statistics. We use\nnumbers, in sports and everywhere else in life, to summarize information. How good a baseball\nplayer was Mickey Mantle? He was a career .298 hitter. To a baseball fan, that is a meaningful\nstatement, which is remarkable when you think about it, because it encapsulates an eighteenseason career.4 (There is, I suppose, something mildly depressing about having one’s lifework\ncollapsed into a single number.) Of course, baseball fans have also come to recognize that\ndescriptive statistics other than batting average may better encapsulate a player’s value on the\nfield. We evaluate the academic performance of high school and college students by means of a\ngrade point average, or GPA. A letter grade is assigned a point value; typically an A is worth 4\npoints, a B is worth 3, a C is worth 2, and so on. By graduation, when high school students are\napplying to college and college students are looking for jobs, the grade point average is a\nhandy tool for assessing their academic potential. Someone who has a 3.7 GPA is clearly a\nstronger student than someone at the same school with a 2.5 GPA. That makes it a nice\ndescriptive statistic. It’s easy to calculate, it’s easy to understand, and it’s easy to compare\nacross students. But it’s not perfect . The GPA does not reflect the difficulty of the courses that different\nstudents may have taken.", "full_text": "Source: naked-statistics-pdf.pdf\n\nIs there really an economic benefit to getting a degree from a highly selective college or\n\nuniversity? What is causing the rising incidence of autism? Statistics can help answer these questions (or, we hope, can soon). The world is producing\nmore and more data, ever faster and faster. Yet, as the New York Times has noted, “Data is\nmerely the raw material of knowledge.”3* Statistics is the most powerful tool we have for\nusing information to some meaningful end, whether that is identifying underrated baseball\nplayers or paying teachers more fairly. Here is a quick tour of how statistics can bring meaning\nto raw data. Description and Comparison\nA bowling score is a descriptive statistic. So is a batting average. Most American sports fans\nover the age of five are already conversant in the field of descriptive statistics. We use\nnumbers, in sports and everywhere else in life, to summarize information. How good a baseball\nplayer was Mickey Mantle? He was a career .298 hitter. To a baseball fan, that is a meaningful\nstatement, which is remarkable when you think about it, because it encapsulates an eighteenseason career.4 (There is, I suppose, something mildly depressing about having one’s lifework\ncollapsed into a single number.) Of course, baseball fans have also come to recognize that\ndescriptive statistics other than batting average may better encapsulate a player’s value on the\nfield. We evaluate the academic performance of high school and college students by means of a\ngrade point average, or GPA. A letter grade is assigned a point value; typically an A is worth 4\npoints, a B is worth 3, a C is worth 2, and so on. By graduation, when high school students are\napplying to college and college students are looking for jobs, the grade point average is a\nhandy tool for assessing their academic potential. Someone who has a 3.7 GPA is clearly a\nstronger student than someone at the same school with a 2.5 GPA. That makes it a nice\ndescriptive statistic. It’s easy to calculate, it’s easy to understand, and it’s easy to compare\nacross students. But it’s not perfect . The GPA does not reflect the difficulty of the courses that different\nstudents may have taken.", "tokens": 477, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 13, "segment_id": "00013", "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null, "has_table": false}, "context": "Source: naked-statistics-pdf.pdf", "chunk_id": "00000014"}
{"type": "chunk", "text": "By graduation, when high school students are\napplying to college and college students are looking for jobs, the grade point average is a\nhandy tool for assessing their academic potential. Someone who has a 3.7 GPA is clearly a\nstronger student than someone at the same school with a 2.5 GPA. That makes it a nice\ndescriptive statistic. It’s easy to calculate, it’s easy to understand, and it’s easy to compare\nacross students. But it’s not perfect . The GPA does not reflect the difficulty of the courses that different\nstudents may have taken. How can we compare a student with a 3.4 GPA in classes that\nappear to be relatively nonchallenging and a student with a 2.9 GPA who has taken calculus,\nphysics, and other tough subjects? I went to a high school that attempted to solve this problem\nby giving extra weight to difficult classes, so that an A in an “honors” class was worth five\npoints instead of the usual four. This caused its own problems. My mother was quick to\nrecognize the distortion caused by this GPA “fix.” For a student taking a lot of honors classes\n(me), any A in a nonhonors course, such as gym or health education, would actually pull my\nGPA down, even though it is impossible to do better than an A in those classes. As a result,\nmy parents forbade me to take driver’s education in high school, lest even a perfect\nperformance diminish my chances of getting into a competitive college and going on to write\npopular books. Instead, they paid to send me to a private driving school, at nights over the", "full_text": "Source: naked-statistics-pdf.pdf\n\nBy graduation, when high school students are\napplying to college and college students are looking for jobs, the grade point average is a\nhandy tool for assessing their academic potential. Someone who has a 3.7 GPA is clearly a\nstronger student than someone at the same school with a 2.5 GPA. That makes it a nice\ndescriptive statistic. It’s easy to calculate, it’s easy to understand, and it’s easy to compare\nacross students. But it’s not perfect . The GPA does not reflect the difficulty of the courses that different\nstudents may have taken. How can we compare a student with a 3.4 GPA in classes that\nappear to be relatively nonchallenging and a student with a 2.9 GPA who has taken calculus,\nphysics, and other tough subjects? I went to a high school that attempted to solve this problem\nby giving extra weight to difficult classes, so that an A in an “honors” class was worth five\npoints instead of the usual four. This caused its own problems. My mother was quick to\nrecognize the distortion caused by this GPA “fix.” For a student taking a lot of honors classes\n(me), any A in a nonhonors course, such as gym or health education, would actually pull my\nGPA down, even though it is impossible to do better than an A in those classes. As a result,\nmy parents forbade me to take driver’s education in high school, lest even a perfect\nperformance diminish my chances of getting into a competitive college and going on to write\npopular books. Instead, they paid to send me to a private driving school, at nights over the", "tokens": 348, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 13, "segment_id": "00013", "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null, "has_table": false}, "context": "Source: naked-statistics-pdf.pdf", "chunk_id": "00000015"}
{"type": "chunk", "text": "summer. Was that insane? Yes. But one theme of this book will be that an overreliance on any\ndescriptive statistic can lead to misleading conclusions, or cause undesirable behavior. My\noriginal draft of that sentence used the phrase “oversimplified descriptive statistic,” but I\nstruck the word “oversimplified” because it’s redundant. Descriptive statistics exist to\nsimplify, which always implies some loss of nuance or detail. Anyone working with numbers\nneeds to recognize as much. Inference\nHow many homeless people live on the streets of Chicago? How often do married people have\nsex? These may seem like wildly different kinds of questions; in fact, they both can be\nanswered (not perfectly) by the use of basic statistical tools. One key function of statistics is\nto use the data we have to make informed conjectures about larger questions for which we do\nnot have full information. In short, we can use data from the “known world” to make informed\ninferences about the “unknown world.”\n\nLet’s begin with the homeless question. It is expensive and logistically difficult to count the\nhomeless population in a large metropolitan area. Yet it is important to have a numerical\nestimate of this population for purposes of providing social services, earning eligibility for\nstate and federal revenues, and gaining congressional representation. One important statistical\npractice is sampling, which is the process of gathering data for a small area, say, a handful of\ncensus tracts, and then using those data to make an informed judgment, or inference, about the\nhomeless population for the city as a whole. Sampling requires far less resources than trying to\ncount an entire population; done properly, it can be every bit as accurate. A political poll is one form of sampling. A research organization will attempt to contact a\nsample of households that are broadly representative of the larger population and ask them\ntheir views about a particular issue or candidate. This is obviously much cheaper and faster\nthan trying to contact every household in an entire state or country. The polling and research\nfirm Gallup reckons that a methodologically sound poll of 1,000 households will produce\nroughly the same results as a poll that attempted to contact every household in America. That’s how we figured out how often Americans are having sex, with whom, and what kind. In the mid-1990s, the National Opinion Research Center at the University of Chicago carried\nout a remarkably ambitious study of American sexual behavior.", "full_text": "Source: naked-statistics-pdf.pdf\n\nsummer. Was that insane? Yes. But one theme of this book will be that an overreliance on any\ndescriptive statistic can lead to misleading conclusions, or cause undesirable behavior. My\noriginal draft of that sentence used the phrase “oversimplified descriptive statistic,” but I\nstruck the word “oversimplified” because it’s redundant. Descriptive statistics exist to\nsimplify, which always implies some loss of nuance or detail. Anyone working with numbers\nneeds to recognize as much. Inference\nHow many homeless people live on the streets of Chicago? How often do married people have\nsex? These may seem like wildly different kinds of questions; in fact, they both can be\nanswered (not perfectly) by the use of basic statistical tools. One key function of statistics is\nto use the data we have to make informed conjectures about larger questions for which we do\nnot have full information. In short, we can use data from the “known world” to make informed\ninferences about the “unknown world.”\n\nLet’s begin with the homeless question. It is expensive and logistically difficult to count the\nhomeless population in a large metropolitan area. Yet it is important to have a numerical\nestimate of this population for purposes of providing social services, earning eligibility for\nstate and federal revenues, and gaining congressional representation. One important statistical\npractice is sampling, which is the process of gathering data for a small area, say, a handful of\ncensus tracts, and then using those data to make an informed judgment, or inference, about the\nhomeless population for the city as a whole. Sampling requires far less resources than trying to\ncount an entire population; done properly, it can be every bit as accurate. A political poll is one form of sampling. A research organization will attempt to contact a\nsample of households that are broadly representative of the larger population and ask them\ntheir views about a particular issue or candidate. This is obviously much cheaper and faster\nthan trying to contact every household in an entire state or country. The polling and research\nfirm Gallup reckons that a methodologically sound poll of 1,000 households will produce\nroughly the same results as a poll that attempted to contact every household in America. That’s how we figured out how often Americans are having sex, with whom, and what kind. In the mid-1990s, the National Opinion Research Center at the University of Chicago carried\nout a remarkably ambitious study of American sexual behavior.", "tokens": 511, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 14, "segment_id": "00014", "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null, "has_table": false}, "context": "Source: naked-statistics-pdf.pdf", "chunk_id": "00000016"}
{"type": "chunk", "text": "This is obviously much cheaper and faster\nthan trying to contact every household in an entire state or country. The polling and research\nfirm Gallup reckons that a methodologically sound poll of 1,000 households will produce\nroughly the same results as a poll that attempted to contact every household in America. That’s how we figured out how often Americans are having sex, with whom, and what kind. In the mid-1990s, the National Opinion Research Center at the University of Chicago carried\nout a remarkably ambitious study of American sexual behavior. The results were based on\ndetailed surveys conducted in person with a large, representative sample of American adults. If\nyou read on, Chapter 10 will tell you what they learned. How many other statistics books can\npromise you that? Assessing Risk and Other Probability-Related Events\nCasinos make money in the long run---always. That does not mean that they are making money\nat any given moment. When the bells and whistles go off, some high roller has just won", "full_text": "Source: naked-statistics-pdf.pdf\n\nThis is obviously much cheaper and faster\nthan trying to contact every household in an entire state or country. The polling and research\nfirm Gallup reckons that a methodologically sound poll of 1,000 households will produce\nroughly the same results as a poll that attempted to contact every household in America. That’s how we figured out how often Americans are having sex, with whom, and what kind. In the mid-1990s, the National Opinion Research Center at the University of Chicago carried\nout a remarkably ambitious study of American sexual behavior. The results were based on\ndetailed surveys conducted in person with a large, representative sample of American adults. If\nyou read on, Chapter 10 will tell you what they learned. How many other statistics books can\npromise you that? Assessing Risk and Other Probability-Related Events\nCasinos make money in the long run---always. That does not mean that they are making money\nat any given moment. When the bells and whistles go off, some high roller has just won", "tokens": 212, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 14, "segment_id": "00014", "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null, "has_table": false}, "context": "Source: naked-statistics-pdf.pdf", "chunk_id": "00000017"}
{"type": "chunk", "text": "thousands of dollars. The whole gambling industry is built on games of chance, meaning that\nthe outcome of any particular roll of the dice or turn of the card is uncertain. At the same time,\nthe underlying probabilities for the relevant events---drawing 21 at blackjack or spinning red in\nroulette---are known. When the underlying probabilities favor the casinos (as they always do),\nwe can be increasingly certain that the “house” is going to come out ahead as the number of\nbets wagered gets larger and larger, even as those bells and whistles keep going off. This turns out to be a powerful phenomenon in areas of life far beyond casinos. Many\nbusinesses must assess the risks associated with assorted adverse outcomes. They cannot\nmake those risks go away entirely, just as a casino cannot guarantee that you won’t win every\nhand of blackjack that you play. However, any business facing uncertainty can manage these\nrisks by engineering processes so that the probability of an adverse outcome, anything from an\nenvironmental catastrophe to a defective product, becomes acceptably low. Wall Street firms\nwill often evaluate the risks posed to their portfolios under different scenarios, with each of\nthose scenarios weighted based on its probability. The financial crisis of 2008 was precipitated\nin part by a series of market events that had been deemed extremely unlikely, as if every\nplayer in a casino drew blackjack all night. I will argue later in the book that these Wall Street\nmodels were flawed and that the data they used to assess the underlying risks were too limited,\nbut the point here is that any model to deal with risk must have probability as its foundation. When individuals and firms cannot make unacceptable risks go away, they seek protection in\nother ways. The entire insurance industry is built upon charging customers to protect them\nagainst some adverse outcome, such as a car crash or a house fire. The insurance industry\ndoes not make money by eliminating these events; cars crash and houses burn every day. Sometimes cars even crash into houses, causing them to burn. Instead, the insurance industry\nmakes money by charging premiums that are more than sufficient to pay for the expected\npayouts from car crashes and house fires. (The insurance company may also try to lower its\nexpected payouts by encouraging safe driving, fences around swimming pools, installation of\nsmoke detectors in every bedroom, and so on.)\n\nProbability can even be used to catch cheats in some situations.", "full_text": "Source: naked-statistics-pdf.pdf\n\nthousands of dollars. The whole gambling industry is built on games of chance, meaning that\nthe outcome of any particular roll of the dice or turn of the card is uncertain. At the same time,\nthe underlying probabilities for the relevant events---drawing 21 at blackjack or spinning red in\nroulette---are known. When the underlying probabilities favor the casinos (as they always do),\nwe can be increasingly certain that the “house” is going to come out ahead as the number of\nbets wagered gets larger and larger, even as those bells and whistles keep going off. This turns out to be a powerful phenomenon in areas of life far beyond casinos. Many\nbusinesses must assess the risks associated with assorted adverse outcomes. They cannot\nmake those risks go away entirely, just as a casino cannot guarantee that you won’t win every\nhand of blackjack that you play. However, any business facing uncertainty can manage these\nrisks by engineering processes so that the probability of an adverse outcome, anything from an\nenvironmental catastrophe to a defective product, becomes acceptably low. Wall Street firms\nwill often evaluate the risks posed to their portfolios under different scenarios, with each of\nthose scenarios weighted based on its probability. The financial crisis of 2008 was precipitated\nin part by a series of market events that had been deemed extremely unlikely, as if every\nplayer in a casino drew blackjack all night. I will argue later in the book that these Wall Street\nmodels were flawed and that the data they used to assess the underlying risks were too limited,\nbut the point here is that any model to deal with risk must have probability as its foundation. When individuals and firms cannot make unacceptable risks go away, they seek protection in\nother ways. The entire insurance industry is built upon charging customers to protect them\nagainst some adverse outcome, such as a car crash or a house fire. The insurance industry\ndoes not make money by eliminating these events; cars crash and houses burn every day. Sometimes cars even crash into houses, causing them to burn. Instead, the insurance industry\nmakes money by charging premiums that are more than sufficient to pay for the expected\npayouts from car crashes and house fires. (The insurance company may also try to lower its\nexpected payouts by encouraging safe driving, fences around swimming pools, installation of\nsmoke detectors in every bedroom, and so on.)\n\nProbability can even be used to catch cheats in some situations.", "tokens": 497, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 15, "segment_id": "00015", "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null, "has_table": false}, "context": "Source: naked-statistics-pdf.pdf", "chunk_id": "00000018"}
{"type": "chunk", "text": "The insurance industry\ndoes not make money by eliminating these events; cars crash and houses burn every day. Sometimes cars even crash into houses, causing them to burn. Instead, the insurance industry\nmakes money by charging premiums that are more than sufficient to pay for the expected\npayouts from car crashes and house fires. (The insurance company may also try to lower its\nexpected payouts by encouraging safe driving, fences around swimming pools, installation of\nsmoke detectors in every bedroom, and so on.)\n\nProbability can even be used to catch cheats in some situations. The firm Caveon Test\nSecurity specializes in what it describes as “data forensics” to find patterns that suggest\ncheating.5 For example, the company (which was founded by a former test developer for the\nSAT) will flag exams at a school or test site on which the number of identical wrong answers\nis highly unlikely, usually a pattern that would happen by chance less than one time in a\nmillion. The mathematical logic stems from the fact that we cannot learn much when a large\ngroup of students all answer a question correctly. That’s what they are supposed to do; they\ncould be cheating, or they could be smart. But when those same test takers get an answer\nwrong, they should not all consistently have the same wrong answer. If they do, it suggests\nthat they are copying from one another (or sharing answers via text). The company also looks\nfor exams in which a test taker does significantly better on hard questions than on easy\nquestions (suggesting that he or she had answers in advance) and for exams on which the\nnumber of “wrong to right” erasures is significantly higher than the number of “right to wrong”\nerasures (suggesting that a teacher or administrator changed the answer sheets after the test).", "full_text": "Source: naked-statistics-pdf.pdf\n\nThe insurance industry\ndoes not make money by eliminating these events; cars crash and houses burn every day. Sometimes cars even crash into houses, causing them to burn. Instead, the insurance industry\nmakes money by charging premiums that are more than sufficient to pay for the expected\npayouts from car crashes and house fires. (The insurance company may also try to lower its\nexpected payouts by encouraging safe driving, fences around swimming pools, installation of\nsmoke detectors in every bedroom, and so on.)\n\nProbability can even be used to catch cheats in some situations. The firm Caveon Test\nSecurity specializes in what it describes as “data forensics” to find patterns that suggest\ncheating.5 For example, the company (which was founded by a former test developer for the\nSAT) will flag exams at a school or test site on which the number of identical wrong answers\nis highly unlikely, usually a pattern that would happen by chance less than one time in a\nmillion. The mathematical logic stems from the fact that we cannot learn much when a large\ngroup of students all answer a question correctly. That’s what they are supposed to do; they\ncould be cheating, or they could be smart. But when those same test takers get an answer\nwrong, they should not all consistently have the same wrong answer. If they do, it suggests\nthat they are copying from one another (or sharing answers via text). The company also looks\nfor exams in which a test taker does significantly better on hard questions than on easy\nquestions (suggesting that he or she had answers in advance) and for exams on which the\nnumber of “wrong to right” erasures is significantly higher than the number of “right to wrong”\nerasures (suggesting that a teacher or administrator changed the answer sheets after the test).", "tokens": 374, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 15, "segment_id": "00015", "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null, "has_table": false}, "context": "Source: naked-statistics-pdf.pdf", "chunk_id": "00000019"}
{"type": "chunk", "text": "Of course, you can see the limitations of using probability. A large group of test takers\nmight have the same wrong answers by coincidence; in fact, the more schools we evaluate, the\nmore likely it is that we will observe such patterns just as a matter of chance. A statistical\nanomaly does not prove wrongdoing. Delma Kinney, a fifty-year-old Atlanta man, won $1\nmillion in an instant lottery game in 2008 and then another $1 million in an instant game in\n2011.6 The probability of that happening to the same person is somewhere in the range of 1 in\n25 trillion. We cannot arrest Mr. Kinney for fraud on the basis of that calculation alone (though\nwe might inquire whether he has any relatives who work for the state lottery). Probability is\none weapon in an arsenal that requires good judgment. Identifying Important Relationships\n(Statistical Detective Work)\nDoes smoking cigarettes cause cancer? We have an answer for that question---but the process\nof answering it was not nearly as straightforward as one might think. The scientific method\ndictates that if we are testing a scientific hypothesis, we should conduct a controlled\nexperiment in which the variable of interest (e.g., smoking) is the only thing that differs\nbetween the experimental group and the control group. If we observe a marked difference in\nsome outcome between the two groups (e.g., lung cancer), we can safely infer that the variable\nof interest is what caused that outcome. We cannot do that kind of experiment on humans. If\nour working hypothesis is that smoking causes cancer, it would be unethical to assign recent\ncollege graduates to two groups, smokers and nonsmokers, and then see who has cancer at the\ntwentieth reunion. (We can conduct controlled experiments on humans when our hypothesis is\nthat a new drug or treatment may improve their health; we cannot knowingly expose human\nsubjects when we expect an adverse outcome.)*\n\nNow, you might point out that we do not need to conduct an ethically dubious experiment to\nobserve the effects of smoking. Couldn’t we just skip the whole fancy methodology and\ncompare cancer rates at the twentieth reunion between those who have smoked since\ngraduation and those who have not? No. Smokers and nonsmokers are likely to be different in ways other than their smoking\nbehavior. For example, smokers may be more likely to have other habits, such as drinking\nheavily or eating badly, that cause adverse health outcomes.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nOf course, you can see the limitations of using probability. A large group of test takers\nmight have the same wrong answers by coincidence; in fact, the more schools we evaluate, the\nmore likely it is that we will observe such patterns just as a matter of chance. A statistical\nanomaly does not prove wrongdoing. Delma Kinney, a fifty-year-old Atlanta man, won $1\nmillion in an instant lottery game in 2008 and then another $1 million in an instant game in\n2011.6 The probability of that happening to the same person is somewhere in the range of 1 in\n25 trillion. We cannot arrest Mr. Kinney for fraud on the basis of that calculation alone (though\nwe might inquire whether he has any relatives who work for the state lottery). Probability is\none weapon in an arsenal that requires good judgment. Identifying Important Relationships\n(Statistical Detective Work)\nDoes smoking cigarettes cause cancer? We have an answer for that question---but the process\nof answering it was not nearly as straightforward as one might think. The scientific method\ndictates that if we are testing a scientific hypothesis, we should conduct a controlled\nexperiment in which the variable of interest (e.g., smoking) is the only thing that differs\nbetween the experimental group and the control group. If we observe a marked difference in\nsome outcome between the two groups (e.g., lung cancer), we can safely infer that the variable\nof interest is what caused that outcome. We cannot do that kind of experiment on humans. If\nour working hypothesis is that smoking causes cancer, it would be unethical to assign recent\ncollege graduates to two groups, smokers and nonsmokers, and then see who has cancer at the\ntwentieth reunion. (We can conduct controlled experiments on humans when our hypothesis is\nthat a new drug or treatment may improve their health; we cannot knowingly expose human\nsubjects when we expect an adverse outcome.)*\n\nNow, you might point out that we do not need to conduct an ethically dubious experiment to\nobserve the effects of smoking. Couldn’t we just skip the whole fancy methodology and\ncompare cancer rates at the twentieth reunion between those who have smoked since\ngraduation and those who have not? No. Smokers and nonsmokers are likely to be different in ways other than their smoking\nbehavior. For example, smokers may be more likely to have other habits, such as drinking\nheavily or eating badly, that cause adverse health outcomes.", "tokens": 512, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 16, "segment_id": "00016", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000020"}
{"type": "chunk", "text": "Couldn’t we just skip the whole fancy methodology and\ncompare cancer rates at the twentieth reunion between those who have smoked since\ngraduation and those who have not? No. Smokers and nonsmokers are likely to be different in ways other than their smoking\nbehavior. For example, smokers may be more likely to have other habits, such as drinking\nheavily or eating badly, that cause adverse health outcomes. If the smokers are particularly\nunhealthy at the twentieth reunion, we would not know whether to attribute this outcome to\nsmoking or to other unhealthy things that many smokers happen to do. We would also have a\nserious problem with the data on which we are basing our analysis. Smokers who have become\nseriously ill with cancer are less likely to attend the twentieth reunion. (The dead smokers\ndefinitely won’t show up.) As a result, any analysis of the health of the attendees at the\ntwentieth reunion (related to smoking or anything else) will be seriously flawed by the fact that\nthe healthiest members of the class are the most likely to show up. The further the class gets\nfrom graduation, say, a fortieth or a fiftieth reunion, the more serious this bias will be. We cannot treat humans like laboratory rats. As a result, statistics is a lot like good", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nCouldn’t we just skip the whole fancy methodology and\ncompare cancer rates at the twentieth reunion between those who have smoked since\ngraduation and those who have not? No. Smokers and nonsmokers are likely to be different in ways other than their smoking\nbehavior. For example, smokers may be more likely to have other habits, such as drinking\nheavily or eating badly, that cause adverse health outcomes. If the smokers are particularly\nunhealthy at the twentieth reunion, we would not know whether to attribute this outcome to\nsmoking or to other unhealthy things that many smokers happen to do. We would also have a\nserious problem with the data on which we are basing our analysis. Smokers who have become\nseriously ill with cancer are less likely to attend the twentieth reunion. (The dead smokers\ndefinitely won’t show up.) As a result, any analysis of the health of the attendees at the\ntwentieth reunion (related to smoking or anything else) will be seriously flawed by the fact that\nthe healthiest members of the class are the most likely to show up. The further the class gets\nfrom graduation, say, a fortieth or a fiftieth reunion, the more serious this bias will be. We cannot treat humans like laboratory rats. As a result, statistics is a lot like good", "tokens": 274, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 16, "segment_id": "00016", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000021"}
{"type": "chunk", "text": "detective work. The data yield clues and patterns that can ultimately lead to meaningful\nconclusions. You have probably watched one of those impressive police procedural shows like\nCSI: New York in which very attractive detectives and forensic experts pore over minute clues\n---DNA from a cigarette butt, teeth marks on an apple, a single fiber from a car floor mat---and\nthen use the evidence to catch a violent criminal. The appeal of the show is that these experts\ndo not have the conventional evidence used to find the bad guy, such as an eyewitness or a\nsurveillance videotape. So they turn to scientific inference instead. Statistics does basically the\nsame thing. The data present unorganized clues---the crime scene. Statistical analysis is the\ndetective work that crafts the raw data into some meaningful conclusion. After Chapter 11, you will appreciate the television show I hope to pitch: CSI: Regression\nAnalysis, which would be only a small departure from those other action-packed police\nprocedurals. Regression analysis is the tool that enables researchers to isolate a relationship\nbetween two variables, such as smoking and cancer, while holding constant (or “controlling\nfor”) the effects of other important variables, such as diet, exercise, weight, and so on. When\nyou read in the newspaper that eating a bran muffin every day will reduce your chances of\ngetting colon cancer, you need not fear that some unfortunate group of human experimental\nsubjects has been force-fed bran muffins in the basement of a federal laboratory somewhere\nwhile the control group in the next building gets bacon and eggs. Instead, researchers will\ngather detailed information on thousands of people, including how frequently they eat bran\nmuffins, and then use regression analysis to do two crucial things: (1) quantify the association\nobserved between eating bran muffins and contracting colon cancer (e.g., a hypothetical finding\nthat people who eat bran muffins have a 9 percent lower incidence of colon cancer, controlling\nfor other factors that may affect the incidence of the disease); and (2) quantify the likelihood\nthat the association between bran muffins and a lower rate of colon cancer observed in this\nstudy is merely a coincidence---a quirk in the data for this sample of people---rather than a\nmeaningful insight about the relationship between diet and health. Of course, CSI: Regression Analysis will star actors and actresses who are much better\nlooking than the academics who typically pore over such data.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\ndetective work. The data yield clues and patterns that can ultimately lead to meaningful\nconclusions. You have probably watched one of those impressive police procedural shows like\nCSI: New York in which very attractive detectives and forensic experts pore over minute clues\n---DNA from a cigarette butt, teeth marks on an apple, a single fiber from a car floor mat---and\nthen use the evidence to catch a violent criminal. The appeal of the show is that these experts\ndo not have the conventional evidence used to find the bad guy, such as an eyewitness or a\nsurveillance videotape. So they turn to scientific inference instead. Statistics does basically the\nsame thing. The data present unorganized clues---the crime scene. Statistical analysis is the\ndetective work that crafts the raw data into some meaningful conclusion. After Chapter 11, you will appreciate the television show I hope to pitch: CSI: Regression\nAnalysis, which would be only a small departure from those other action-packed police\nprocedurals. Regression analysis is the tool that enables researchers to isolate a relationship\nbetween two variables, such as smoking and cancer, while holding constant (or “controlling\nfor”) the effects of other important variables, such as diet, exercise, weight, and so on. When\nyou read in the newspaper that eating a bran muffin every day will reduce your chances of\ngetting colon cancer, you need not fear that some unfortunate group of human experimental\nsubjects has been force-fed bran muffins in the basement of a federal laboratory somewhere\nwhile the control group in the next building gets bacon and eggs. Instead, researchers will\ngather detailed information on thousands of people, including how frequently they eat bran\nmuffins, and then use regression analysis to do two crucial things: (1) quantify the association\nobserved between eating bran muffins and contracting colon cancer (e.g., a hypothetical finding\nthat people who eat bran muffins have a 9 percent lower incidence of colon cancer, controlling\nfor other factors that may affect the incidence of the disease); and (2) quantify the likelihood\nthat the association between bran muffins and a lower rate of colon cancer observed in this\nstudy is merely a coincidence---a quirk in the data for this sample of people---rather than a\nmeaningful insight about the relationship between diet and health. Of course, CSI: Regression Analysis will star actors and actresses who are much better\nlooking than the academics who typically pore over such data.", "tokens": 505, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 17, "segment_id": "00017", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000022"}
{"type": "chunk", "text": "Of course, CSI: Regression Analysis will star actors and actresses who are much better\nlooking than the academics who typically pore over such data. These hotties (all of whom\nwould have PhDs, despite being only twenty-three years old) would study large data sets and\nuse the latest statistical tools to answer important social questions: What are the most effective\ntools for fighting violent crime? What individuals are most likely to become terrorists? Later in\nthe book we will discuss the concept of a “statistically significant” finding, which means that\nthe analysis has uncovered an association between two variables that is not likely to be the\nproduct of chance alone. For academic researchers, this kind of statistical finding is the\n“smoking gun.” On CSI: Regression Analysis, I envision a researcher working late at night in\nthe computer lab because of her daytime commitment as a member of the U.S. Olympic beach\nvolleyball team. When she gets the printout from her statistical analysis, she sees exactly what\nshe has been looking for: a large and statistically significant relationship in her data set\nbetween some variable that she had hypothesized might be important and the onset of autism. She must share this breakthrough immediately! The researcher takes the printout and runs down the hall, slowed somewhat by the fact that", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nOf course, CSI: Regression Analysis will star actors and actresses who are much better\nlooking than the academics who typically pore over such data. These hotties (all of whom\nwould have PhDs, despite being only twenty-three years old) would study large data sets and\nuse the latest statistical tools to answer important social questions: What are the most effective\ntools for fighting violent crime? What individuals are most likely to become terrorists? Later in\nthe book we will discuss the concept of a “statistically significant” finding, which means that\nthe analysis has uncovered an association between two variables that is not likely to be the\nproduct of chance alone. For academic researchers, this kind of statistical finding is the\n“smoking gun.” On CSI: Regression Analysis, I envision a researcher working late at night in\nthe computer lab because of her daytime commitment as a member of the U.S. Olympic beach\nvolleyball team. When she gets the printout from her statistical analysis, she sees exactly what\nshe has been looking for: a large and statistically significant relationship in her data set\nbetween some variable that she had hypothesized might be important and the onset of autism. She must share this breakthrough immediately! The researcher takes the printout and runs down the hall, slowed somewhat by the fact that", "tokens": 265, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 17, "segment_id": "00017", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000023"}
{"type": "chunk", "text": "she is wearing high heels and a relatively small, tight black skirt. She finds her male partner,\nwho is inexplicably fit and tan for a guy who works fourteen hours a day in a basement\ncomputer lab, and shows him the results. He runs his fingers through his neatly trimmed goatee,\ngrabs his Glock 9-mm pistol from the desk drawer, and slides it into the shoulder holster\nbeneath his $5,000 Hugo Boss suit (also inexplicable given his starting academic salary of\n$38,000 a year). Together the regression analysis experts walk briskly to see their boss, a\ngrizzled veteran who has overcome failed relationships and a drinking problem . . . Okay, you don’t have to buy into the television drama to appreciate the importance of this\nkind of statistical research. Just about every social challenge that we care about has been\ninformed by the systematic analysis of large data sets. (In many cases, gathering the relevant\ndata, which is expensive and time-consuming, plays a crucial role in this process as will be\nexplained in Chapter 7.) I may have embellished my characters in CSI: Regression Analysis\nbut not the kind of significant questions they could examine. There is an academic literature on\nterrorists and suicide bombers---a subject that would be difficult to study by means of human\nsubjects (or lab rats for that matter). One such book, What Makes a Terrorist , was written by\none of my graduate school statistics professors. The book draws its conclusions from data\ngathered on terrorist attacks around the world. A sample finding: Terrorists are not desperately\npoor, or poorly educated. The author, Princeton economist Alan Krueger, concludes,\n“Terrorists tend to be drawn from well-educated, middle-class or high-income families.”7\n\nWhy? Well, that exposes one of the limitations of regression analysis. We can isolate a\nstrong association between two variables by using statistical analysis, but we cannot\nnecessarily explain why that relationship exists, and in some cases, we cannot know for certain\nthat the relationship is causal, meaning that a change in one variable is really causing a change\nin the other. In the case of terrorism, Professor Krueger hypothesizes that since terrorists are\nmotivated by political goals, those who are most educated and affluent have the strongest\nincentive to change society. These individuals may also be particularly rankled by suppression\nof freedom, another factor associated with terrorism.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nshe is wearing high heels and a relatively small, tight black skirt. She finds her male partner,\nwho is inexplicably fit and tan for a guy who works fourteen hours a day in a basement\ncomputer lab, and shows him the results. He runs his fingers through his neatly trimmed goatee,\ngrabs his Glock 9-mm pistol from the desk drawer, and slides it into the shoulder holster\nbeneath his $5,000 Hugo Boss suit (also inexplicable given his starting academic salary of\n$38,000 a year). Together the regression analysis experts walk briskly to see their boss, a\ngrizzled veteran who has overcome failed relationships and a drinking problem . . . Okay, you don’t have to buy into the television drama to appreciate the importance of this\nkind of statistical research. Just about every social challenge that we care about has been\ninformed by the systematic analysis of large data sets. (In many cases, gathering the relevant\ndata, which is expensive and time-consuming, plays a crucial role in this process as will be\nexplained in Chapter 7.) I may have embellished my characters in CSI: Regression Analysis\nbut not the kind of significant questions they could examine. There is an academic literature on\nterrorists and suicide bombers---a subject that would be difficult to study by means of human\nsubjects (or lab rats for that matter). One such book, What Makes a Terrorist , was written by\none of my graduate school statistics professors. The book draws its conclusions from data\ngathered on terrorist attacks around the world. A sample finding: Terrorists are not desperately\npoor, or poorly educated. The author, Princeton economist Alan Krueger, concludes,\n“Terrorists tend to be drawn from well-educated, middle-class or high-income families.”7\n\nWhy? Well, that exposes one of the limitations of regression analysis. We can isolate a\nstrong association between two variables by using statistical analysis, but we cannot\nnecessarily explain why that relationship exists, and in some cases, we cannot know for certain\nthat the relationship is causal, meaning that a change in one variable is really causing a change\nin the other. In the case of terrorism, Professor Krueger hypothesizes that since terrorists are\nmotivated by political goals, those who are most educated and affluent have the strongest\nincentive to change society. These individuals may also be particularly rankled by suppression\nof freedom, another factor associated with terrorism.", "tokens": 509, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 18, "segment_id": "00018", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000024"}
{"type": "chunk", "text": "We can isolate a\nstrong association between two variables by using statistical analysis, but we cannot\nnecessarily explain why that relationship exists, and in some cases, we cannot know for certain\nthat the relationship is causal, meaning that a change in one variable is really causing a change\nin the other. In the case of terrorism, Professor Krueger hypothesizes that since terrorists are\nmotivated by political goals, those who are most educated and affluent have the strongest\nincentive to change society. These individuals may also be particularly rankled by suppression\nof freedom, another factor associated with terrorism. In Krueger’s study, countries with high\nlevels of political repression have more terrorist activity (holding other factors constant). This discussion leads me back to the question posed by the chapter title: What is the point? The point is not to do math, or to dazzle friends and colleagues with advanced statistical\ntechniques. The point is to learn things that inform our lives. Lies, Damned Lies, and Statistics\nEven in the best of circumstances, statistical analysis rarely unveils “the truth.” We are usually\nbuilding a circumstantial case based on imperfect data. As a result, there are numerous reasons\nthat intellectually honest individuals may disagree about statistical results or their implications. At the most basic level, we may disagree on the question that is being answered. Sports\nenthusiasts will be arguing for all eternity over “the best baseball player ever” because there is\nno objective definition of “best.” Fancy descriptive statistics can inform this question, but they\nwill never answer it definitively. As the next chapter will point out, more socially significant", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nWe can isolate a\nstrong association between two variables by using statistical analysis, but we cannot\nnecessarily explain why that relationship exists, and in some cases, we cannot know for certain\nthat the relationship is causal, meaning that a change in one variable is really causing a change\nin the other. In the case of terrorism, Professor Krueger hypothesizes that since terrorists are\nmotivated by political goals, those who are most educated and affluent have the strongest\nincentive to change society. These individuals may also be particularly rankled by suppression\nof freedom, another factor associated with terrorism. In Krueger’s study, countries with high\nlevels of political repression have more terrorist activity (holding other factors constant). This discussion leads me back to the question posed by the chapter title: What is the point? The point is not to do math, or to dazzle friends and colleagues with advanced statistical\ntechniques. The point is to learn things that inform our lives. Lies, Damned Lies, and Statistics\nEven in the best of circumstances, statistical analysis rarely unveils “the truth.” We are usually\nbuilding a circumstantial case based on imperfect data. As a result, there are numerous reasons\nthat intellectually honest individuals may disagree about statistical results or their implications. At the most basic level, we may disagree on the question that is being answered. Sports\nenthusiasts will be arguing for all eternity over “the best baseball player ever” because there is\nno objective definition of “best.” Fancy descriptive statistics can inform this question, but they\nwill never answer it definitively. As the next chapter will point out, more socially significant", "tokens": 338, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 18, "segment_id": "00018", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000025"}
{"type": "chunk", "text": "questions fall prey to the same basic challenge. What is happening to the economic health of\nthe American middle class? That answer depends on how one defines both “middle class” and\n“economic health.”\n\nThere are limits on the data we can gather and the kinds of experiments we can perform. Alan Krueger’s study of terrorists did not follow thousands of youth over multiple decades to\nobserve which of them evolved into terrorists. It’s just not possible. Nor can we create two\nidentical nations---except that one is highly repressive and the other is not---and then compare\nthe number of suicide bombers that emerge in each. Even when we can conduct large,\ncontrolled experiments on human beings, they are neither easy nor cheap. Researchers did a\nlarge-scale study on whether or not prayer reduces postsurgical complications, which was one\nof the questions raised earlier in this chapter. That study cost $2.4 million. (For the results,\nyou’ll have to wait until Chapter 13.)\n\nSecretary of Defense Donald Rumsfeld famously said, “You go to war with the army you\nhave---not the army you might want or wish to have at a later time.” Whatever you may think\nof Rumsfeld (and the Iraq war that he was explaining), that aphorism applies to research, too. We conduct statistical analysis using the best data and methodologies and resources available. The approach is not like addition or long division, in which the correct technique yields the\n“right” answer and a computer is always more precise and less fallible than a human. Statistical analysis is more like good detective work (hence the commercial potential of CSI:\nRegression Analysis). Smart and honest people will often disagree about what the data are\ntrying to tell us. But who says that everyone using statistics is smart or honest? As mentioned, this book\nbegan as an homage to How to Lie with Statistics, which was first published in 1954 and has\nsold over a million copies. The reality is that you can lie with statistics. Or you can make\ninadvertent errors. In either case, the mathematical precision attached to statistical analysis can\ndress up some serious nonsense. This book will walk through many of the most common\nstatistical errors and misrepresentations (so that you can recognize them, not put them to use). So, to return to the title chapter, what is the point of learning statistics? To summarize huge quantities of data. To make better decisions. To answer important social questions.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nquestions fall prey to the same basic challenge. What is happening to the economic health of\nthe American middle class? That answer depends on how one defines both “middle class” and\n“economic health.”\n\nThere are limits on the data we can gather and the kinds of experiments we can perform. Alan Krueger’s study of terrorists did not follow thousands of youth over multiple decades to\nobserve which of them evolved into terrorists. It’s just not possible. Nor can we create two\nidentical nations---except that one is highly repressive and the other is not---and then compare\nthe number of suicide bombers that emerge in each. Even when we can conduct large,\ncontrolled experiments on human beings, they are neither easy nor cheap. Researchers did a\nlarge-scale study on whether or not prayer reduces postsurgical complications, which was one\nof the questions raised earlier in this chapter. That study cost $2.4 million. (For the results,\nyou’ll have to wait until Chapter 13.)\n\nSecretary of Defense Donald Rumsfeld famously said, “You go to war with the army you\nhave---not the army you might want or wish to have at a later time.” Whatever you may think\nof Rumsfeld (and the Iraq war that he was explaining), that aphorism applies to research, too. We conduct statistical analysis using the best data and methodologies and resources available. The approach is not like addition or long division, in which the correct technique yields the\n“right” answer and a computer is always more precise and less fallible than a human. Statistical analysis is more like good detective work (hence the commercial potential of CSI:\nRegression Analysis). Smart and honest people will often disagree about what the data are\ntrying to tell us. But who says that everyone using statistics is smart or honest? As mentioned, this book\nbegan as an homage to How to Lie with Statistics, which was first published in 1954 and has\nsold over a million copies. The reality is that you can lie with statistics. Or you can make\ninadvertent errors. In either case, the mathematical precision attached to statistical analysis can\ndress up some serious nonsense. This book will walk through many of the most common\nstatistical errors and misrepresentations (so that you can recognize them, not put them to use). So, to return to the title chapter, what is the point of learning statistics? To summarize huge quantities of data. To make better decisions. To answer important social questions.", "tokens": 511, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 19, "segment_id": "00019", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000026"}
{"type": "chunk", "text": "The reality is that you can lie with statistics. Or you can make\ninadvertent errors. In either case, the mathematical precision attached to statistical analysis can\ndress up some serious nonsense. This book will walk through many of the most common\nstatistical errors and misrepresentations (so that you can recognize them, not put them to use). So, to return to the title chapter, what is the point of learning statistics? To summarize huge quantities of data. To make better decisions. To answer important social questions. To recognize patterns that can refine how we do everything from selling diapers to catching\n\ncriminals. To catch cheaters and prosecute criminals. To evaluate the effectiveness of policies, programs, drugs, medical procedures, and other\n\ninnovations. And to spot the scoundrels who use these very same powerful tools for nefarious ends. If you can do all of that while looking great in a Hugo Boss suit or a short black skirt, then\n\nyou might also be the next star of CSI: Regression Analysis. * The Gini index is sometimes multiplied by 100 to make it a whole number. In that case, the United States would have a\nGini Index of 45.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThe reality is that you can lie with statistics. Or you can make\ninadvertent errors. In either case, the mathematical precision attached to statistical analysis can\ndress up some serious nonsense. This book will walk through many of the most common\nstatistical errors and misrepresentations (so that you can recognize them, not put them to use). So, to return to the title chapter, what is the point of learning statistics? To summarize huge quantities of data. To make better decisions. To answer important social questions. To recognize patterns that can refine how we do everything from selling diapers to catching\n\ncriminals. To catch cheaters and prosecute criminals. To evaluate the effectiveness of policies, programs, drugs, medical procedures, and other\n\ninnovations. And to spot the scoundrels who use these very same powerful tools for nefarious ends. If you can do all of that while looking great in a Hugo Boss suit or a short black skirt, then\n\nyou might also be the next star of CSI: Regression Analysis. * The Gini index is sometimes multiplied by 100 to make it a whole number. In that case, the United States would have a\nGini Index of 45.", "tokens": 245, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 19, "segment_id": "00019", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000027"}
{"type": "chunk", "text": "* The word “data” has historically been considered plural (e.g., “The data are very encouraging.”) The singular is “datum,”\nwhich would refer to a single data point, such as one person’s response to a single question on a poll. Using the word “data”\nas a plural noun is a quick way to signal to anyone who does serious research that you are conversant with statistics. That\nsaid, many authorities on grammar and many publications, such as the New York Times , now accept that “data” can be\nsingular or plural, as the passage that I’ve quoted from the Times demonstrates. * This is a gross simplification of the fascinating and complex field of medical ethics.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\n* The word “data” has historically been considered plural (e.g., “The data are very encouraging.”) The singular is “datum,”\nwhich would refer to a single data point, such as one person’s response to a single question on a poll. Using the word “data”\nas a plural noun is a quick way to signal to anyone who does serious research that you are conversant with statistics. That\nsaid, many authorities on grammar and many publications, such as the New York Times , now accept that “data” can be\nsingular or plural, as the passage that I’ve quoted from the Times demonstrates. * This is a gross simplification of the fascinating and complex field of medical ethics.", "tokens": 144, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 20, "segment_id": "00020", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000028"}
{"type": "chunk", "text": "CHAPTER 2\nDescriptive Statistics\nWho was the best baseball player of all time? Let us ponder for a moment two seemingly unrelated questions: (1) What is happening to the\n\neconomic health of America’s middle class? and (2) Who was the greatest baseball player of\nall time? The first question is profoundly important. It tends to be at the core of presidential\ncampaigns and other social movements. The middle class is the heart of America, so the\neconomic well-being of that group is a crucial indicator of the nation’s overall economic\nhealth. The second question is trivial (in the literal sense of the word), but baseball enthusiasts\ncan argue about it endlessly. What the two questions have in common is that they can be used\nto illustrate the strengths and limitations of descriptive statistics, which are the numbers and\ncalculations we use to summarize raw data. If I want to demonstrate that Derek Jeter is a great baseball player, I can sit you down and\ndescribe every at bat in every Major League game that he’s played. That would be raw data,\nand it would take a while to digest, given that Jeter has played seventeen seasons with the New\nYork Yankees and taken 9,868 at bats. Or I can just tell you that at the end of the 2011 season Derek Jeter had a career batting\n\naverage of .313. That is a descriptive statistic, or a “summary statistic.”\n\nThe batting average is a gross simplification of Jeter’s seventeen seasons. It is easy to\nunderstand, elegant in its simplicity---and limited in what it can tell us. Baseball experts have a\nbevy of descriptive statistics that they consider to be more valuable than the batting average. I\ncalled Steve Moyer, president of Baseball Info Solutions (a firm that provides a lot of the raw\ndata for the Moneyball types), to ask him, (1) What are the most important statistics for\nevaluating baseball talent? and (2) Who was the greatest player of all time? I’ll share his\nanswer once we have more context. Meanwhile, let’s return to the less trivial subject, the economic health of the middle class. Ideally we would like to find the economic equivalent of a batting average, or something even\nbetter. We would like a simple but accurate measure of how the economic well-being of the\ntypical American worker has been changing in recent years.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nCHAPTER 2\nDescriptive Statistics\nWho was the best baseball player of all time? Let us ponder for a moment two seemingly unrelated questions: (1) What is happening to the\n\neconomic health of America’s middle class? and (2) Who was the greatest baseball player of\nall time? The first question is profoundly important. It tends to be at the core of presidential\ncampaigns and other social movements. The middle class is the heart of America, so the\neconomic well-being of that group is a crucial indicator of the nation’s overall economic\nhealth. The second question is trivial (in the literal sense of the word), but baseball enthusiasts\ncan argue about it endlessly. What the two questions have in common is that they can be used\nto illustrate the strengths and limitations of descriptive statistics, which are the numbers and\ncalculations we use to summarize raw data. If I want to demonstrate that Derek Jeter is a great baseball player, I can sit you down and\ndescribe every at bat in every Major League game that he’s played. That would be raw data,\nand it would take a while to digest, given that Jeter has played seventeen seasons with the New\nYork Yankees and taken 9,868 at bats. Or I can just tell you that at the end of the 2011 season Derek Jeter had a career batting\n\naverage of .313. That is a descriptive statistic, or a “summary statistic.”\n\nThe batting average is a gross simplification of Jeter’s seventeen seasons. It is easy to\nunderstand, elegant in its simplicity---and limited in what it can tell us. Baseball experts have a\nbevy of descriptive statistics that they consider to be more valuable than the batting average. I\ncalled Steve Moyer, president of Baseball Info Solutions (a firm that provides a lot of the raw\ndata for the Moneyball types), to ask him, (1) What are the most important statistics for\nevaluating baseball talent? and (2) Who was the greatest player of all time? I’ll share his\nanswer once we have more context. Meanwhile, let’s return to the less trivial subject, the economic health of the middle class. Ideally we would like to find the economic equivalent of a batting average, or something even\nbetter. We would like a simple but accurate measure of how the economic well-being of the\ntypical American worker has been changing in recent years.", "tokens": 495, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 21, "segment_id": "00021", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000029"}
{"type": "chunk", "text": "and (2) Who was the greatest player of all time? I’ll share his\nanswer once we have more context. Meanwhile, let’s return to the less trivial subject, the economic health of the middle class. Ideally we would like to find the economic equivalent of a batting average, or something even\nbetter. We would like a simple but accurate measure of how the economic well-being of the\ntypical American worker has been changing in recent years. Are the people we define as\nmiddle class getting richer, poorer, or just running in place? A reasonable answer---though by\nno means the “right” answer---would be to calculate the change in per capita income in the\nUnited States over the course of a generation, which is roughly thirty years. Per capita income\nis a simple average: total income divided by the size of the population. By that measure,\naverage income in the United States climbed from $7,787 in 1980 to $26,487 in 2010 (the\nlatest year for which the government has data).1 Voilà! Congratulations to us. There is just one problem. My quick calculation is technically correct and yet totally wrong", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nand (2) Who was the greatest player of all time? I’ll share his\nanswer once we have more context. Meanwhile, let’s return to the less trivial subject, the economic health of the middle class. Ideally we would like to find the economic equivalent of a batting average, or something even\nbetter. We would like a simple but accurate measure of how the economic well-being of the\ntypical American worker has been changing in recent years. Are the people we define as\nmiddle class getting richer, poorer, or just running in place? A reasonable answer---though by\nno means the “right” answer---would be to calculate the change in per capita income in the\nUnited States over the course of a generation, which is roughly thirty years. Per capita income\nis a simple average: total income divided by the size of the population. By that measure,\naverage income in the United States climbed from $7,787 in 1980 to $26,487 in 2010 (the\nlatest year for which the government has data).1 Voilà! Congratulations to us. There is just one problem. My quick calculation is technically correct and yet totally wrong", "tokens": 240, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 21, "segment_id": "00021", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000030"}
{"type": "chunk", "text": "in terms of the question I set out to answer. To begin with, the figures above are not adjusted\nfor inflation. (A per capita income of $7,787 in 1980 is equal to about $19,600 when converted\nto 2010 dollars.) That’s a relatively quick fix. The bigger problem is that the average income\nin America is not equal to the income of the average American. Let’s unpack that clever little\nphrase. Per capita income merely takes all of the income earned in the country and divides by the\nnumber of people, which tells us absolutely nothing about who is earning how much of that\nincome---in 1980 or in 2010. As the Occupy Wall Street folks would point out, explosive\ngrowth in the incomes of the top 1 percent can raise per capita income significantly without\nputting any more money in the pockets of the other 99 percent. In other words, average income\ncan go up without helping the average American. As with the baseball statistic query, I have sought outside expertise on how we ought to\nmeasure the health of the American middle class. I asked two prominent labor economists,\nincluding President Obama’s top economic adviser, what descriptive statistics they would use\nto assess the economic well-being of a typical American. Yes, you will get that answer, too,\nonce we’ve taken a quick tour of descriptive statistics to give it more meaning. From baseball to income, the most basic task when working with data is to summarize a\ngreat deal of information. There are some 330 million residents in the United States. A\nspreadsheet with the name and income history of every American would contain all the\ninformation we could ever want about the economic health of the country---yet it would also be\nso unwieldy as to tell us nothing at all. The irony is that more data can often present less\nclarity. So we simplify. We perform calculations that reduce a complex array of data into a\nhandful of numbers that describe those data, just as we might encapsulate a complex,\nmultifaceted Olympic gymnastics performance with one number: 9.8. The good news is that these descriptive statistics give us a manageable and meaningful\nsummary of the underlying phenomenon. That’s what this chapter is about. The bad news is\nthat any simplification invites abuse. Descriptive statistics can be like online dating profiles:\ntechnically accurate and yet pretty darn misleading.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nin terms of the question I set out to answer. To begin with, the figures above are not adjusted\nfor inflation. (A per capita income of $7,787 in 1980 is equal to about $19,600 when converted\nto 2010 dollars.) That’s a relatively quick fix. The bigger problem is that the average income\nin America is not equal to the income of the average American. Let’s unpack that clever little\nphrase. Per capita income merely takes all of the income earned in the country and divides by the\nnumber of people, which tells us absolutely nothing about who is earning how much of that\nincome---in 1980 or in 2010. As the Occupy Wall Street folks would point out, explosive\ngrowth in the incomes of the top 1 percent can raise per capita income significantly without\nputting any more money in the pockets of the other 99 percent. In other words, average income\ncan go up without helping the average American. As with the baseball statistic query, I have sought outside expertise on how we ought to\nmeasure the health of the American middle class. I asked two prominent labor economists,\nincluding President Obama’s top economic adviser, what descriptive statistics they would use\nto assess the economic well-being of a typical American. Yes, you will get that answer, too,\nonce we’ve taken a quick tour of descriptive statistics to give it more meaning. From baseball to income, the most basic task when working with data is to summarize a\ngreat deal of information. There are some 330 million residents in the United States. A\nspreadsheet with the name and income history of every American would contain all the\ninformation we could ever want about the economic health of the country---yet it would also be\nso unwieldy as to tell us nothing at all. The irony is that more data can often present less\nclarity. So we simplify. We perform calculations that reduce a complex array of data into a\nhandful of numbers that describe those data, just as we might encapsulate a complex,\nmultifaceted Olympic gymnastics performance with one number: 9.8. The good news is that these descriptive statistics give us a manageable and meaningful\nsummary of the underlying phenomenon. That’s what this chapter is about. The bad news is\nthat any simplification invites abuse. Descriptive statistics can be like online dating profiles:\ntechnically accurate and yet pretty darn misleading.", "tokens": 499, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 22, "segment_id": "00022", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000031"}
{"type": "chunk", "text": "The irony is that more data can often present less\nclarity. So we simplify. We perform calculations that reduce a complex array of data into a\nhandful of numbers that describe those data, just as we might encapsulate a complex,\nmultifaceted Olympic gymnastics performance with one number: 9.8. The good news is that these descriptive statistics give us a manageable and meaningful\nsummary of the underlying phenomenon. That’s what this chapter is about. The bad news is\nthat any simplification invites abuse. Descriptive statistics can be like online dating profiles:\ntechnically accurate and yet pretty darn misleading. Suppose you are at work, idly surfing the Web when you stumble across a riveting day-by-day\naccount of Kim Kardashian’s failed seventy-two-day marriage to professional basketball\nplayer Kris Humphries. You have finished reading about day seven of the marriage when your\nboss shows up with two enormous files of data. One file has warranty claim information for\neach of the 57,334 laser printers that your firm sold last year. (For each printer sold, the file\ndocuments the number of quality problems that were reported during the warranty period.) The\nother file has the same information for each of the 994,773 laser printers that your chief\ncompetitor sold during the same stretch. Your boss wants to know how your firm’s printers\ncompare in terms of quality with the competition. Fortunately the computer you’ve been using to read about the Kardashian marriage has a\nbasics statistics package, but where do you begin? Your instincts are probably correct: The\nfirst descriptive task is often to find some measure of the “middle” of a set of data, or what\nstatisticians might describe as its “central tendency.” What is the typical quality experience for", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThe irony is that more data can often present less\nclarity. So we simplify. We perform calculations that reduce a complex array of data into a\nhandful of numbers that describe those data, just as we might encapsulate a complex,\nmultifaceted Olympic gymnastics performance with one number: 9.8. The good news is that these descriptive statistics give us a manageable and meaningful\nsummary of the underlying phenomenon. That’s what this chapter is about. The bad news is\nthat any simplification invites abuse. Descriptive statistics can be like online dating profiles:\ntechnically accurate and yet pretty darn misleading. Suppose you are at work, idly surfing the Web when you stumble across a riveting day-by-day\naccount of Kim Kardashian’s failed seventy-two-day marriage to professional basketball\nplayer Kris Humphries. You have finished reading about day seven of the marriage when your\nboss shows up with two enormous files of data. One file has warranty claim information for\neach of the 57,334 laser printers that your firm sold last year. (For each printer sold, the file\ndocuments the number of quality problems that were reported during the warranty period.) The\nother file has the same information for each of the 994,773 laser printers that your chief\ncompetitor sold during the same stretch. Your boss wants to know how your firm’s printers\ncompare in terms of quality with the competition. Fortunately the computer you’ve been using to read about the Kardashian marriage has a\nbasics statistics package, but where do you begin? Your instincts are probably correct: The\nfirst descriptive task is often to find some measure of the “middle” of a set of data, or what\nstatisticians might describe as its “central tendency.” What is the typical quality experience for", "tokens": 363, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 22, "segment_id": "00022", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000032"}
{"type": "chunk", "text": "your printers compared with those of the competition? The most basic measure of the “middle”\nof a distribution is the mean, or average. In this case, we want to know the average number of\nquality problems per printer sold for your firm and for your competitor. You would simply\ntally the total number of quality problems reported for all printers during the warranty period\nand then divide by the total number of printers sold. (Remember, the same printer can have\nmultiple problems while under warranty.) You would do that for each firm, creating an\nimportant descriptive statistic: the average number of quality problems per printer sold. Suppose it turns out that your competitor’s printers have an average of 2.8 quality-related\nproblems per printer during the warranty period compared with your firm’s average of 9.1\nreported defects. That was easy. You’ve just taken information on a million printers sold by\ntwo different companies and distilled it to the essence of the problem: your printers break a lot. Clearly it’s time to send a short e-mail to your boss quantifying this quality gap and then get\nback to day eight of Kim Kardashian’s marriage. Or maybe not. I was deliberately vague earlier when I referred to the “middle” of a\ndistribution. The mean, or average, turns out to have some problems in that regard, namely, that\nit is prone to distortion by “outliers,” which are observations that lie farther from the center. To\nget your mind around this concept, imagine that ten guys are sitting on bar stools in a middleclass drinking establishment in Seattle; each of these guys earns $35,000 a year, which makes\nthe mean annual income for the group $35,000. Bill Gates walks into the bar with a talking\nparrot perched on his shoulder. (The parrot has nothing to do with the example, but it kind of\nspices things up.) Let’s assume for the sake of the example that Bill Gates has an annual\nincome of $1 billion. When Bill sits down on the eleventh bar stool, the mean annual income\nfor the bar patrons rises to about $91 million. Obviously none of the original ten drinkers is\nany richer (though it might be reasonable to expect Bill Gates to buy a round or two). If I were\nto describe the patrons of this bar as having an average annual income of $91 million, the\nstatement would be both statistically correct and grossly misleading.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nyour printers compared with those of the competition? The most basic measure of the “middle”\nof a distribution is the mean, or average. In this case, we want to know the average number of\nquality problems per printer sold for your firm and for your competitor. You would simply\ntally the total number of quality problems reported for all printers during the warranty period\nand then divide by the total number of printers sold. (Remember, the same printer can have\nmultiple problems while under warranty.) You would do that for each firm, creating an\nimportant descriptive statistic: the average number of quality problems per printer sold. Suppose it turns out that your competitor’s printers have an average of 2.8 quality-related\nproblems per printer during the warranty period compared with your firm’s average of 9.1\nreported defects. That was easy. You’ve just taken information on a million printers sold by\ntwo different companies and distilled it to the essence of the problem: your printers break a lot. Clearly it’s time to send a short e-mail to your boss quantifying this quality gap and then get\nback to day eight of Kim Kardashian’s marriage. Or maybe not. I was deliberately vague earlier when I referred to the “middle” of a\ndistribution. The mean, or average, turns out to have some problems in that regard, namely, that\nit is prone to distortion by “outliers,” which are observations that lie farther from the center. To\nget your mind around this concept, imagine that ten guys are sitting on bar stools in a middleclass drinking establishment in Seattle; each of these guys earns $35,000 a year, which makes\nthe mean annual income for the group $35,000. Bill Gates walks into the bar with a talking\nparrot perched on his shoulder. (The parrot has nothing to do with the example, but it kind of\nspices things up.) Let’s assume for the sake of the example that Bill Gates has an annual\nincome of $1 billion. When Bill sits down on the eleventh bar stool, the mean annual income\nfor the bar patrons rises to about $91 million. Obviously none of the original ten drinkers is\nany richer (though it might be reasonable to expect Bill Gates to buy a round or two). If I were\nto describe the patrons of this bar as having an average annual income of $91 million, the\nstatement would be both statistically correct and grossly misleading.", "tokens": 503, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 23, "segment_id": "00023", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000033"}
{"type": "chunk", "text": "When Bill sits down on the eleventh bar stool, the mean annual income\nfor the bar patrons rises to about $91 million. Obviously none of the original ten drinkers is\nany richer (though it might be reasonable to expect Bill Gates to buy a round or two). If I were\nto describe the patrons of this bar as having an average annual income of $91 million, the\nstatement would be both statistically correct and grossly misleading. This isn’t a bar where\nmultimillionaires hang out; it’s a bar where a bunch of guys with relatively low incomes happen\nto be sitting next to Bill Gates and his talking parrot. The sensitivity of the mean to outliers is\nwhy we should not gauge the economic health of the American middle class by looking at per\ncapita income. Because there has been explosive growth in incomes at the top end of the\ndistribution---CEOs, hedge fund managers, and athletes like Derek Jeter---the average income\nin the United States could be heavily skewed by the megarich, making it look a lot like the bar\nstools with Bill Gates at the end. For this reason, we have another statistic that also signals the “middle” of a distribution,\nalbeit differently: the median. The median is the point that divides a distribution in half,\nmeaning that half of the observations lie above the median and half lie below. (If there is an\neven number of observations, the median is the midpoint between the two middle\nobservations.) If we return to the bar stool example, the median annual income for the ten guys\noriginally sitting in the bar is $35,000. When Bill Gates walks in with his parrot and perches on\na stool, the median annual income for the eleven of them is still $35,000. If you literally\nenvision lining up the bar patrons on stools in ascending order of their incomes, the income of", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nWhen Bill sits down on the eleventh bar stool, the mean annual income\nfor the bar patrons rises to about $91 million. Obviously none of the original ten drinkers is\nany richer (though it might be reasonable to expect Bill Gates to buy a round or two). If I were\nto describe the patrons of this bar as having an average annual income of $91 million, the\nstatement would be both statistically correct and grossly misleading. This isn’t a bar where\nmultimillionaires hang out; it’s a bar where a bunch of guys with relatively low incomes happen\nto be sitting next to Bill Gates and his talking parrot. The sensitivity of the mean to outliers is\nwhy we should not gauge the economic health of the American middle class by looking at per\ncapita income. Because there has been explosive growth in incomes at the top end of the\ndistribution---CEOs, hedge fund managers, and athletes like Derek Jeter---the average income\nin the United States could be heavily skewed by the megarich, making it look a lot like the bar\nstools with Bill Gates at the end. For this reason, we have another statistic that also signals the “middle” of a distribution,\nalbeit differently: the median. The median is the point that divides a distribution in half,\nmeaning that half of the observations lie above the median and half lie below. (If there is an\neven number of observations, the median is the midpoint between the two middle\nobservations.) If we return to the bar stool example, the median annual income for the ten guys\noriginally sitting in the bar is $35,000. When Bill Gates walks in with his parrot and perches on\na stool, the median annual income for the eleven of them is still $35,000. If you literally\nenvision lining up the bar patrons on stools in ascending order of their incomes, the income of", "tokens": 392, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 23, "segment_id": "00023", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000034"}
{"type": "chunk", "text": "the guy sitting on the sixth stool represents the median income for the group. If Warren Buffett\ncomes in and sits down on the twelfth stool next to Bill Gates, the median still does not\nchange.*\n\nFor distributions without serious outliers, the median and the mean will be similar. I’ve\nincluded a hypothetical summary of the quality data for the competitor’s printers. In particular,\nI’ve laid out the data in what is known as a frequency distribution. The number of quality\nproblems per printer is arrayed along the bottom; the height of each bar represents the\npercentages of printers sold with that number of quality problems. For example, 36 percent of\nthe competitor’s printers had two quality defects during the warranty period. Because the\ndistribution includes all possible quality outcomes, including zero defects, the proportions\nmust sum to 1 (or 100 percent). Frequency Distribution of Quality Complaints for Competitor’s Printers\n\nBecause the distribution is nearly symmetrical, the mean and median are relatively close to\none another. The distribution is slightly skewed to the right by the small number of printers\nwith many reported quality defects. These outliers move the mean slightly rightward but have\nno impact on the median. Suppose that just before you dash off the quality report to your boss\nyou decide to calculate the median number of quality problems for your firm’s printers and the\ncompetition’s. With a few keystrokes, you get the result. The median number of quality\ncomplaints for the competitor’s printers is 2; the median number of quality complaints for your\ncompany’s printers is 1. Huh? Your firm’s median number of quality complaints per printer is actually lower than\nyour competitor’s. Because the Kardashian marriage is getting monotonous, and because you\nare intrigued by this finding, you print a frequency distribution for your own quality problems. Frequency Distribution of Quality Complaints at Your Company", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nthe guy sitting on the sixth stool represents the median income for the group. If Warren Buffett\ncomes in and sits down on the twelfth stool next to Bill Gates, the median still does not\nchange.*\n\nFor distributions without serious outliers, the median and the mean will be similar. I’ve\nincluded a hypothetical summary of the quality data for the competitor’s printers. In particular,\nI’ve laid out the data in what is known as a frequency distribution. The number of quality\nproblems per printer is arrayed along the bottom; the height of each bar represents the\npercentages of printers sold with that number of quality problems. For example, 36 percent of\nthe competitor’s printers had two quality defects during the warranty period. Because the\ndistribution includes all possible quality outcomes, including zero defects, the proportions\nmust sum to 1 (or 100 percent). Frequency Distribution of Quality Complaints for Competitor’s Printers\n\nBecause the distribution is nearly symmetrical, the mean and median are relatively close to\none another. The distribution is slightly skewed to the right by the small number of printers\nwith many reported quality defects. These outliers move the mean slightly rightward but have\nno impact on the median. Suppose that just before you dash off the quality report to your boss\nyou decide to calculate the median number of quality problems for your firm’s printers and the\ncompetition’s. With a few keystrokes, you get the result. The median number of quality\ncomplaints for the competitor’s printers is 2; the median number of quality complaints for your\ncompany’s printers is 1. Huh? Your firm’s median number of quality complaints per printer is actually lower than\nyour competitor’s. Because the Kardashian marriage is getting monotonous, and because you\nare intrigued by this finding, you print a frequency distribution for your own quality problems. Frequency Distribution of Quality Complaints at Your Company", "tokens": 389, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 24, "segment_id": "00024", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000035"}
{"type": "chunk", "text": "What becomes clear is that your firm does not have a uniform quality problem; you have a\n“lemon” problem; a small number of printers have a huge number of quality complaints. These\noutliers inflate the mean but not the median. More important from a production standpoint, you\ndo not need to retool the whole manufacturing process; you need only figure out where the\negregiously low-quality printers are coming from and fix that.*\n\nNeither the median nor the mean is hard to calculate; the key is determining which measure\nof the “middle” is more accurate in a particular situation (a phenomenon that is easily\nexploited). Meanwhile, the median has some useful relatives. As we’ve already discussed, the\nmedian divides a distribution in half. The distribution can be further divided into quarters, or\nquartiles. The first quartile consists of the bottom 25 percent of the observations; the second\nquartile consists of the next 25 percent of the observations; and so on. Or the distribution can\nbe divided into deciles, each with 10 percent of the observations. (If your income is in the top\ndecile of the American income distribution, you would be earning more than 90 percent of your\nfellow workers.) We can go even further and divide the distribution into hundredths, or\npercentiles. Each percentile represents 1 percent of the distribution, so that the 1st percentile\nrepresents the bottom 1 percent of the distribution and the 99th percentile represents the top 1\npercent of the distribution. The benefit of these kinds of descriptive statistics is that they describe where a particular\nobservation lies compared with everyone else. If I tell you that your child scored in the 3rd\npercentile on a reading comprehension test, you should know immediately that the family\nshould be logging more time at the library. You don’t need to know anything about the test\nitself, or the number of questions that your child got correct. The percentile score provides a\nranking of your child’s score relative to that of all the other test takers. If the test was easy,\nthen most test takers will have a high number of answers correct, but your child will have\nfewer correct than most of the others. If the test was extremely difficult, then all the test takers\nwill have a low number of correct answers, but your child’s score will be lower still. Here is a good point to introduce some useful terminology.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nWhat becomes clear is that your firm does not have a uniform quality problem; you have a\n“lemon” problem; a small number of printers have a huge number of quality complaints. These\noutliers inflate the mean but not the median. More important from a production standpoint, you\ndo not need to retool the whole manufacturing process; you need only figure out where the\negregiously low-quality printers are coming from and fix that.*\n\nNeither the median nor the mean is hard to calculate; the key is determining which measure\nof the “middle” is more accurate in a particular situation (a phenomenon that is easily\nexploited). Meanwhile, the median has some useful relatives. As we’ve already discussed, the\nmedian divides a distribution in half. The distribution can be further divided into quarters, or\nquartiles. The first quartile consists of the bottom 25 percent of the observations; the second\nquartile consists of the next 25 percent of the observations; and so on. Or the distribution can\nbe divided into deciles, each with 10 percent of the observations. (If your income is in the top\ndecile of the American income distribution, you would be earning more than 90 percent of your\nfellow workers.) We can go even further and divide the distribution into hundredths, or\npercentiles. Each percentile represents 1 percent of the distribution, so that the 1st percentile\nrepresents the bottom 1 percent of the distribution and the 99th percentile represents the top 1\npercent of the distribution. The benefit of these kinds of descriptive statistics is that they describe where a particular\nobservation lies compared with everyone else. If I tell you that your child scored in the 3rd\npercentile on a reading comprehension test, you should know immediately that the family\nshould be logging more time at the library. You don’t need to know anything about the test\nitself, or the number of questions that your child got correct. The percentile score provides a\nranking of your child’s score relative to that of all the other test takers. If the test was easy,\nthen most test takers will have a high number of answers correct, but your child will have\nfewer correct than most of the others. If the test was extremely difficult, then all the test takers\nwill have a low number of correct answers, but your child’s score will be lower still. Here is a good point to introduce some useful terminology.", "tokens": 511, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 25, "segment_id": "00025", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000036"}
{"type": "chunk", "text": "You don’t need to know anything about the test\nitself, or the number of questions that your child got correct. The percentile score provides a\nranking of your child’s score relative to that of all the other test takers. If the test was easy,\nthen most test takers will have a high number of answers correct, but your child will have\nfewer correct than most of the others. If the test was extremely difficult, then all the test takers\nwill have a low number of correct answers, but your child’s score will be lower still. Here is a good point to introduce some useful terminology. An “absolute” score, number, or\nfigure has some intrinsic meaning. If I shoot 83 for eighteen holes of golf, that is an absolute\nfigure. I may do that on a day that is 58 degrees, which is also an absolute figure. Absolute\nfigures can usually be interpreted without any context or additional information. When I tell\nyou that I shot 83, you don’t need to know what other golfers shot that day in order to evaluate\nmy performance. (The exception might be if the conditions are particularly awful, or if the\ncourse is especially difficult or easy.) If I place ninth in the golf tournament, that is a relative", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nYou don’t need to know anything about the test\nitself, or the number of questions that your child got correct. The percentile score provides a\nranking of your child’s score relative to that of all the other test takers. If the test was easy,\nthen most test takers will have a high number of answers correct, but your child will have\nfewer correct than most of the others. If the test was extremely difficult, then all the test takers\nwill have a low number of correct answers, but your child’s score will be lower still. Here is a good point to introduce some useful terminology. An “absolute” score, number, or\nfigure has some intrinsic meaning. If I shoot 83 for eighteen holes of golf, that is an absolute\nfigure. I may do that on a day that is 58 degrees, which is also an absolute figure. Absolute\nfigures can usually be interpreted without any context or additional information. When I tell\nyou that I shot 83, you don’t need to know what other golfers shot that day in order to evaluate\nmy performance. (The exception might be if the conditions are particularly awful, or if the\ncourse is especially difficult or easy.) If I place ninth in the golf tournament, that is a relative", "tokens": 263, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 25, "segment_id": "00025", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000037"}
{"type": "chunk", "text": "statistic. A “relative” value or figure has meaning only in comparison to something else, or in\nsome broader context, such as compared with the eight golfers who shot better than I did. Most\nstandardized tests produce results that have meaning only as a relative statistic. If I tell you\nthat a third grader in an Illinois elementary school scored 43 out of 60 on the mathematics\nportion of the Illinois State Achievement Test, that absolute score doesn’t have much meaning. But when I convert it to a percentile---meaning that I put that raw score into a distribution with\nthe math scores for all other Illinois third graders---then it acquires a great deal of meaning. If\n43 correct answers falls into the 83rd percentile, then this student is doing better than most of\nhis peers statewide. If he’s in the 8th percentile, then he’s really struggling. In this case, the\npercentile (the relative score) is more meaningful than the number of correct answers (the\nabsolute score). Another statistic that can help us describe what might otherwise be a jumble of numbers is\nthe standard deviation, which is a measure of how dispersed the data are from their mean. In\nother words, how spread out are the observations? Suppose I collected data on the weights of\n250 people on an airplane headed for Boston, and I also collected the weights of a sample of\n250 qualifiers for the Boston Marathon. Now assume that the mean weight for both groups is\nroughly the same, say 155 pounds. Anyone who has been squeezed into a row on a crowded\nflight, fighting for the armrest, knows that many people on a typical commercial flight weigh\nmore than 155 pounds. But you may recall from those same unpleasant, overcrowded flights\nthat there were lots of crying babies and poorly behaved children, all of whom have enormous\nlung capacity but not much mass. When it comes to calculating the average weight on the\nflight, the heft of the 320-pound football players on either side of your middle seat is likely\noffset by the tiny screaming infant across the row and the six-year-old kicking the back of your\nseat from the row behind. On the basis of the descriptive tools introduced so far, the weights of the airline passengers\nand the marathoners are nearly identical. But they’re not.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nstatistic. A “relative” value or figure has meaning only in comparison to something else, or in\nsome broader context, such as compared with the eight golfers who shot better than I did. Most\nstandardized tests produce results that have meaning only as a relative statistic. If I tell you\nthat a third grader in an Illinois elementary school scored 43 out of 60 on the mathematics\nportion of the Illinois State Achievement Test, that absolute score doesn’t have much meaning. But when I convert it to a percentile---meaning that I put that raw score into a distribution with\nthe math scores for all other Illinois third graders---then it acquires a great deal of meaning. If\n43 correct answers falls into the 83rd percentile, then this student is doing better than most of\nhis peers statewide. If he’s in the 8th percentile, then he’s really struggling. In this case, the\npercentile (the relative score) is more meaningful than the number of correct answers (the\nabsolute score). Another statistic that can help us describe what might otherwise be a jumble of numbers is\nthe standard deviation, which is a measure of how dispersed the data are from their mean. In\nother words, how spread out are the observations? Suppose I collected data on the weights of\n250 people on an airplane headed for Boston, and I also collected the weights of a sample of\n250 qualifiers for the Boston Marathon. Now assume that the mean weight for both groups is\nroughly the same, say 155 pounds. Anyone who has been squeezed into a row on a crowded\nflight, fighting for the armrest, knows that many people on a typical commercial flight weigh\nmore than 155 pounds. But you may recall from those same unpleasant, overcrowded flights\nthat there were lots of crying babies and poorly behaved children, all of whom have enormous\nlung capacity but not much mass. When it comes to calculating the average weight on the\nflight, the heft of the 320-pound football players on either side of your middle seat is likely\noffset by the tiny screaming infant across the row and the six-year-old kicking the back of your\nseat from the row behind. On the basis of the descriptive tools introduced so far, the weights of the airline passengers\nand the marathoners are nearly identical. But they’re not.", "tokens": 483, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 26, "segment_id": "00026", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000038"}
{"type": "chunk", "text": "But you may recall from those same unpleasant, overcrowded flights\nthat there were lots of crying babies and poorly behaved children, all of whom have enormous\nlung capacity but not much mass. When it comes to calculating the average weight on the\nflight, the heft of the 320-pound football players on either side of your middle seat is likely\noffset by the tiny screaming infant across the row and the six-year-old kicking the back of your\nseat from the row behind. On the basis of the descriptive tools introduced so far, the weights of the airline passengers\nand the marathoners are nearly identical. But they’re not. Yes, the weights of the two groups\nhave roughly the same “middle,” but the airline passengers have far more dispersion around\nthat midpoint, meaning that their weights are spread farther from the midpoint. My eight-yearold son might point out that the marathon runners look like they all weigh the same amount,\nwhile the airline passengers have some tiny people and some bizarrely large people. The\nweights of the airline passengers are “more spread out,” which is an important attribute when it\ncomes to describing the weights of these two groups. The standard deviation is the descriptive\nstatistic that allows us to assign a single number to this dispersion around the mean. The\nformulas for calculating the standard deviation and the variance (another common measure of\ndispersion from which the standard deviation is derived) are included in an appendix at the end\nof the chapter. For now, let’s think about why the measuring of dispersion matters. Suppose you walk into the doctor’s office. You’ve been feeling fatigued ever since your\npromotion to head of North American printer quality. Your doctor draws blood, and a few days\nlater her assistant leaves a message on your answering machine to inform you that your HCb2\ncount (a fictitious blood chemical) is 134. You rush to the Internet and discover that the mean\nHCb2 count for a person your age is 122 (and the median is about the same). Holy crap! If", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nBut you may recall from those same unpleasant, overcrowded flights\nthat there were lots of crying babies and poorly behaved children, all of whom have enormous\nlung capacity but not much mass. When it comes to calculating the average weight on the\nflight, the heft of the 320-pound football players on either side of your middle seat is likely\noffset by the tiny screaming infant across the row and the six-year-old kicking the back of your\nseat from the row behind. On the basis of the descriptive tools introduced so far, the weights of the airline passengers\nand the marathoners are nearly identical. But they’re not. Yes, the weights of the two groups\nhave roughly the same “middle,” but the airline passengers have far more dispersion around\nthat midpoint, meaning that their weights are spread farther from the midpoint. My eight-yearold son might point out that the marathon runners look like they all weigh the same amount,\nwhile the airline passengers have some tiny people and some bizarrely large people. The\nweights of the airline passengers are “more spread out,” which is an important attribute when it\ncomes to describing the weights of these two groups. The standard deviation is the descriptive\nstatistic that allows us to assign a single number to this dispersion around the mean. The\nformulas for calculating the standard deviation and the variance (another common measure of\ndispersion from which the standard deviation is derived) are included in an appendix at the end\nof the chapter. For now, let’s think about why the measuring of dispersion matters. Suppose you walk into the doctor’s office. You’ve been feeling fatigued ever since your\npromotion to head of North American printer quality. Your doctor draws blood, and a few days\nlater her assistant leaves a message on your answering machine to inform you that your HCb2\ncount (a fictitious blood chemical) is 134. You rush to the Internet and discover that the mean\nHCb2 count for a person your age is 122 (and the median is about the same). Holy crap! If", "tokens": 420, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 26, "segment_id": "00026", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000039"}
{"type": "chunk", "text": "you’re like me, you would finally draft a will. You’d write tearful letters to your parents,\nspouse, children, and close friends. You might take up skydiving or try to write a novel very\nfast. You would send your boss a hastily composed e-mail comparing him to a certain part of\nthe human anatomy---IN ALL CAPS. None of these things may be necessary (and the e-mail to your boss could turn out very\nbadly). When you call the doctor’s office back to arrange for your hospice care, the\nphysician’s assistant informs you that your count is within the normal range. But how could\nthat be? “My count is 12 points higher than average!” you yell repeatedly into the receiver. “The standard deviation for the HCb2 count is 18,” the technician informs you curtly. What the heck does that mean? There is natural variation in the HCb2 count, as there is with most biological phenomena\n(e.g., height). While the mean count for the fake chemical might be 122, plenty of healthy\npeople have counts that are higher or lower. The danger arises only when the HCb2 count gets\nexcessively high or low. So how do we figure out what “excessively” means in this context? As we’ve already noted, the standard deviation is a measure of dispersion, meaning that it\nreflects how tightly the observations cluster around the mean. For many typical distributions of\ndata, a high proportion of the observations lie within one standard deviation of the mean\n(meaning that they are in the range from one standard deviation below the mean to one standard\ndeviation above the mean). To illustrate with a simple example, the mean height for American\nadult men is 5 feet 10 inches. The standard deviation is roughly 3 inches. A high proportion of\nadult men are between 5 feet 7 inches and 6 feet 1 inch. Or, to put it slightly differently, any man in this height range would not be considered\nabnormally short or tall. Which brings us back to your troubling HCb2 results. Yes, your count\nis 12 above the mean, but that’s less than one standard deviation, which is the blood chemical\nequivalent of being about 6 feet tall---not particularly unusual. Of course, far fewer\nobservations lie two standard deviations from the mean, and fewer still lie three or four\nstandard deviations away.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nyou’re like me, you would finally draft a will. You’d write tearful letters to your parents,\nspouse, children, and close friends. You might take up skydiving or try to write a novel very\nfast. You would send your boss a hastily composed e-mail comparing him to a certain part of\nthe human anatomy---IN ALL CAPS. None of these things may be necessary (and the e-mail to your boss could turn out very\nbadly). When you call the doctor’s office back to arrange for your hospice care, the\nphysician’s assistant informs you that your count is within the normal range. But how could\nthat be? “My count is 12 points higher than average!” you yell repeatedly into the receiver. “The standard deviation for the HCb2 count is 18,” the technician informs you curtly. What the heck does that mean? There is natural variation in the HCb2 count, as there is with most biological phenomena\n(e.g., height). While the mean count for the fake chemical might be 122, plenty of healthy\npeople have counts that are higher or lower. The danger arises only when the HCb2 count gets\nexcessively high or low. So how do we figure out what “excessively” means in this context? As we’ve already noted, the standard deviation is a measure of dispersion, meaning that it\nreflects how tightly the observations cluster around the mean. For many typical distributions of\ndata, a high proportion of the observations lie within one standard deviation of the mean\n(meaning that they are in the range from one standard deviation below the mean to one standard\ndeviation above the mean). To illustrate with a simple example, the mean height for American\nadult men is 5 feet 10 inches. The standard deviation is roughly 3 inches. A high proportion of\nadult men are between 5 feet 7 inches and 6 feet 1 inch. Or, to put it slightly differently, any man in this height range would not be considered\nabnormally short or tall. Which brings us back to your troubling HCb2 results. Yes, your count\nis 12 above the mean, but that’s less than one standard deviation, which is the blood chemical\nequivalent of being about 6 feet tall---not particularly unusual. Of course, far fewer\nobservations lie two standard deviations from the mean, and fewer still lie three or four\nstandard deviations away.", "tokens": 508, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 27, "segment_id": "00027", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000040"}
{"type": "chunk", "text": "A high proportion of\nadult men are between 5 feet 7 inches and 6 feet 1 inch. Or, to put it slightly differently, any man in this height range would not be considered\nabnormally short or tall. Which brings us back to your troubling HCb2 results. Yes, your count\nis 12 above the mean, but that’s less than one standard deviation, which is the blood chemical\nequivalent of being about 6 feet tall---not particularly unusual. Of course, far fewer\nobservations lie two standard deviations from the mean, and fewer still lie three or four\nstandard deviations away. (In the case of height, an American man who is three standard\ndeviations above average in height would be 6 feet 7 inches or taller.)\n\nSome distributions are more dispersed than others. Hence, the standard deviation of the\nweights of the 250 airline passengers will be higher than the standard deviation of the weights\nof the 250 marathon runners. A frequency distribution with the weights of the airline\npassengers would literally be fatter (more spread out) than a frequency distribution of the\nweights of the marathon runners. Once we know the mean and standard deviation for any\ncollection of data, we have some serious intellectual traction. For example, suppose I tell you\nthat the mean score on the SAT math test is 500 with a standard deviation of 100. As with\nheight, the bulk of students taking the test will be within one standard deviation of the mean, or\nbetween 400 and 600. How many students do you think score 720 or higher? Probably not very\nmany, since that is more than two standard deviations above the mean. In fact, we can do even better than “not very many.” This is a good time to introduce one of\nthe most important, helpful, and common distributions in statistics: the normal distribution. Data that are distributed normally are symmetrical around their mean in a bell shape that will", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nA high proportion of\nadult men are between 5 feet 7 inches and 6 feet 1 inch. Or, to put it slightly differently, any man in this height range would not be considered\nabnormally short or tall. Which brings us back to your troubling HCb2 results. Yes, your count\nis 12 above the mean, but that’s less than one standard deviation, which is the blood chemical\nequivalent of being about 6 feet tall---not particularly unusual. Of course, far fewer\nobservations lie two standard deviations from the mean, and fewer still lie three or four\nstandard deviations away. (In the case of height, an American man who is three standard\ndeviations above average in height would be 6 feet 7 inches or taller.)\n\nSome distributions are more dispersed than others. Hence, the standard deviation of the\nweights of the 250 airline passengers will be higher than the standard deviation of the weights\nof the 250 marathon runners. A frequency distribution with the weights of the airline\npassengers would literally be fatter (more spread out) than a frequency distribution of the\nweights of the marathon runners. Once we know the mean and standard deviation for any\ncollection of data, we have some serious intellectual traction. For example, suppose I tell you\nthat the mean score on the SAT math test is 500 with a standard deviation of 100. As with\nheight, the bulk of students taking the test will be within one standard deviation of the mean, or\nbetween 400 and 600. How many students do you think score 720 or higher? Probably not very\nmany, since that is more than two standard deviations above the mean. In fact, we can do even better than “not very many.” This is a good time to introduce one of\nthe most important, helpful, and common distributions in statistics: the normal distribution. Data that are distributed normally are symmetrical around their mean in a bell shape that will", "tokens": 404, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 27, "segment_id": "00027", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000041"}
{"type": "chunk", "text": "look familiar to you. The normal distribution describes many common phenomena. Imagine a frequency\ndistribution describing popcorn popping on a stove top. Some kernels start to pop early, maybe\none or two pops per second; after ten or fifteen seconds, the kernels are exploding frenetically. Then gradually the number of kernels popping per second fades away at roughly the same rate\nat which the popping began. The heights of American men are distributed more or less\nnormally, meaning that they are roughly symmetrical around the mean of 5 feet 10 inches. Each\nSAT test is specifically designed to produce a normal distribution of scores with mean 500 and\nstandard deviation of 100. According to the Wall Street Journal , Americans even tend to park\nin a normal distribution at shopping malls; most cars park directly opposite the mall entrance---\nthe “peak” of the normal curve---with “tails” of cars going off to the right and left of the\nentrance. The beauty of the normal distribution---its Michael Jordan power, finesse, and elegance---\ncomes from the fact that we know by definition exactly what proportion of the observations in\na normal distribution lie within one standard deviation of the mean (68.2 percent), within two\nstandard deviations of the mean (95.4 percent), within three standard deviations (99.7 percent),\nand so on. This may sound like trivia. In fact, it is the foundation on which much of statistics is\nbuilt. We will come back to this point in much great depth later in the book. The Normal Distribution\n\nThe mean is the middle line which is often represented by the Greek letter μ. The standard\ndeviation is often represented by the Greek letter σ. Each band represents one standard\ndeviation. Descriptive statistics are often used to compare two figures or quantities. I’m one inch taller\nthan my brother; today’s temperature is nine degrees above the historical average for this date;\nand so on. Those comparisons make sense because most of us recognize the scale of the units\ninvolved. One inch does not amount to much when it comes to a person’s height, so you can\ninfer that my brother and I are roughly the same height. Conversely, nine degrees is a\nsignificant temperature deviation in just about any climate at any time of year, so nine degrees", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nlook familiar to you. The normal distribution describes many common phenomena. Imagine a frequency\ndistribution describing popcorn popping on a stove top. Some kernels start to pop early, maybe\none or two pops per second; after ten or fifteen seconds, the kernels are exploding frenetically. Then gradually the number of kernels popping per second fades away at roughly the same rate\nat which the popping began. The heights of American men are distributed more or less\nnormally, meaning that they are roughly symmetrical around the mean of 5 feet 10 inches. Each\nSAT test is specifically designed to produce a normal distribution of scores with mean 500 and\nstandard deviation of 100. According to the Wall Street Journal , Americans even tend to park\nin a normal distribution at shopping malls; most cars park directly opposite the mall entrance---\nthe “peak” of the normal curve---with “tails” of cars going off to the right and left of the\nentrance. The beauty of the normal distribution---its Michael Jordan power, finesse, and elegance---\ncomes from the fact that we know by definition exactly what proportion of the observations in\na normal distribution lie within one standard deviation of the mean (68.2 percent), within two\nstandard deviations of the mean (95.4 percent), within three standard deviations (99.7 percent),\nand so on. This may sound like trivia. In fact, it is the foundation on which much of statistics is\nbuilt. We will come back to this point in much great depth later in the book. The Normal Distribution\n\nThe mean is the middle line which is often represented by the Greek letter μ. The standard\ndeviation is often represented by the Greek letter σ. Each band represents one standard\ndeviation. Descriptive statistics are often used to compare two figures or quantities. I’m one inch taller\nthan my brother; today’s temperature is nine degrees above the historical average for this date;\nand so on. Those comparisons make sense because most of us recognize the scale of the units\ninvolved. One inch does not amount to much when it comes to a person’s height, so you can\ninfer that my brother and I are roughly the same height. Conversely, nine degrees is a\nsignificant temperature deviation in just about any climate at any time of year, so nine degrees", "tokens": 469, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 28, "segment_id": "00028", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000042"}
{"type": "chunk", "text": "above average makes for a day that is much hotter than usual. But suppose that I told you that\nGranola Cereal A contains 31 milligrams more sodium than Granola Cereal B. Unless you\nknow an awful lot about sodium (and the serving sizes for granola cereal), that statement is not\ngoing to be particularly informative. Or what if I told you that my cousin Al earned $53,000\nless this year than last year? Should we be worried about Al? Or is he a hedge fund manager\nfor whom $53,000 is a rounding error in his annual compensation? In both the sodium and the income examples, we’re missing context. The easiest way to give\nmeaning to these relative comparisons is by using percentages. It would mean something if I\ntold you that Granola Bar A has 50 percent more sodium than Granola Bar B, or that Uncle\nAl’s income fell 47 percent last year. Measuring change as a percentage gives us some sense\nof scale. You probably learned how to calculate percentages in fourth grade and will be tempted to\nskip the next few paragraphs. Fair enough. But first do one simple exercise for me. Assume\nthat a department store is selling a dress for $100. The assistant manager marks down all\nmerchandise by 25 percent. But then that assistant manager is fired for hanging out in a bar\nwith Bill Gates,* and the new assistant manager raises all prices by 25 percent. What is the\nfinal price of the dress? If you said (or thought) $100, then you had better not skip any\nparagraphs. The final price of the dress is actually $93.75. This is not merely a fun parlor trick that will\nwin you applause and adulation at cocktail parties. Percentages are useful---but also\npotentially confusing or even deceptive. The formula for calculating a percentage difference\n(or change) is the following: (new figure -- original figure)/original figure. The numerator (the\npart on the top of the fraction) gives us the size of the change in absolute terms; the\ndenominator (the bottom of the fraction) is what puts this change in context by comparing it\nwith our starting point. At first, this seems straightforward, as when the assistant store manager\ncuts the price of the $100 dress by 25 percent.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nabove average makes for a day that is much hotter than usual. But suppose that I told you that\nGranola Cereal A contains 31 milligrams more sodium than Granola Cereal B. Unless you\nknow an awful lot about sodium (and the serving sizes for granola cereal), that statement is not\ngoing to be particularly informative. Or what if I told you that my cousin Al earned $53,000\nless this year than last year? Should we be worried about Al? Or is he a hedge fund manager\nfor whom $53,000 is a rounding error in his annual compensation? In both the sodium and the income examples, we’re missing context. The easiest way to give\nmeaning to these relative comparisons is by using percentages. It would mean something if I\ntold you that Granola Bar A has 50 percent more sodium than Granola Bar B, or that Uncle\nAl’s income fell 47 percent last year. Measuring change as a percentage gives us some sense\nof scale. You probably learned how to calculate percentages in fourth grade and will be tempted to\nskip the next few paragraphs. Fair enough. But first do one simple exercise for me. Assume\nthat a department store is selling a dress for $100. The assistant manager marks down all\nmerchandise by 25 percent. But then that assistant manager is fired for hanging out in a bar\nwith Bill Gates,* and the new assistant manager raises all prices by 25 percent. What is the\nfinal price of the dress? If you said (or thought) $100, then you had better not skip any\nparagraphs. The final price of the dress is actually $93.75. This is not merely a fun parlor trick that will\nwin you applause and adulation at cocktail parties. Percentages are useful---but also\npotentially confusing or even deceptive. The formula for calculating a percentage difference\n(or change) is the following: (new figure -- original figure)/original figure. The numerator (the\npart on the top of the fraction) gives us the size of the change in absolute terms; the\ndenominator (the bottom of the fraction) is what puts this change in context by comparing it\nwith our starting point. At first, this seems straightforward, as when the assistant store manager\ncuts the price of the $100 dress by 25 percent.", "tokens": 486, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 29, "segment_id": "00029", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000043"}
{"type": "chunk", "text": "Percentages are useful---but also\npotentially confusing or even deceptive. The formula for calculating a percentage difference\n(or change) is the following: (new figure -- original figure)/original figure. The numerator (the\npart on the top of the fraction) gives us the size of the change in absolute terms; the\ndenominator (the bottom of the fraction) is what puts this change in context by comparing it\nwith our starting point. At first, this seems straightforward, as when the assistant store manager\ncuts the price of the $100 dress by 25 percent. Twenty-five percent of the original $100 price\nis $25; that’s the discount, which takes the price down to $75. You can plug the numbers into\nthe formula above and do some simple manipulation to get to the same place: ($100 --\n$75)/$100 = .25, or 25 percent. The dress is selling for $75 when the new assistant manager demands that the price be raised\n25 percent. That’s where many of the people reading this paragraph probably made a mistake. The 25 percent markup is calculated as a percentage of the dress’s new reduced price, which is\n$75. The increase will be .25($75), or $18.75, which is how the final price ends up at $93.75\n(and not $100). The point is that a percentage change always gives the value of some figure\nrelative to something else. Therefore, we had better understand what that something else is. I once invested some money in a company that my college roommate started. Since it was a\nprivate venture, there were no requirements as to what information had to be provided to\nshareholders. A number of years went by without any information on the fate of my investment;\nmy former roommate was fairly tight-lipped on the subject. Finally, I received a letter in the\nmail informing me that the firm’s profits were 46 percent higher than the year before. There\nwas no information on the size of those profits in absolute terms, meaning that I still had", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nPercentages are useful---but also\npotentially confusing or even deceptive. The formula for calculating a percentage difference\n(or change) is the following: (new figure -- original figure)/original figure. The numerator (the\npart on the top of the fraction) gives us the size of the change in absolute terms; the\ndenominator (the bottom of the fraction) is what puts this change in context by comparing it\nwith our starting point. At first, this seems straightforward, as when the assistant store manager\ncuts the price of the $100 dress by 25 percent. Twenty-five percent of the original $100 price\nis $25; that’s the discount, which takes the price down to $75. You can plug the numbers into\nthe formula above and do some simple manipulation to get to the same place: ($100 --\n$75)/$100 = .25, or 25 percent. The dress is selling for $75 when the new assistant manager demands that the price be raised\n25 percent. That’s where many of the people reading this paragraph probably made a mistake. The 25 percent markup is calculated as a percentage of the dress’s new reduced price, which is\n$75. The increase will be .25($75), or $18.75, which is how the final price ends up at $93.75\n(and not $100). The point is that a percentage change always gives the value of some figure\nrelative to something else. Therefore, we had better understand what that something else is. I once invested some money in a company that my college roommate started. Since it was a\nprivate venture, there were no requirements as to what information had to be provided to\nshareholders. A number of years went by without any information on the fate of my investment;\nmy former roommate was fairly tight-lipped on the subject. Finally, I received a letter in the\nmail informing me that the firm’s profits were 46 percent higher than the year before. There\nwas no information on the size of those profits in absolute terms, meaning that I still had", "tokens": 426, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 29, "segment_id": "00029", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000044"}
{"type": "chunk", "text": "absolutely no idea how my investment was performing. Suppose that last year the firm earned\n27 cents---essentially nothing. This year the firm earned 39 cents---also essentially nothing. Yet the company’s profits grew from 27 cents to 39 cents, which is technically a 46 percent\nincrease. Obviously the shareholder letter would have been more of a downer if it pointed out\nthat the firm’s cumulative profits over two years were less than the cost of a cup of Starbucks\ncoffee. To be fair to my roommate, he eventually sold the company for hundreds of millions of\ndollars, earning me a 100 percent return on my investment. (Since you have no idea how much\nI invested, you also have no idea how much money I made---which reinforces my point here\nvery nicely!)\n\nLet me make one additional distinction. Percentage change must not be confused with a\nchange in percentage points. Rates are often expressed in percentages. The sales tax rate in\nIllinois is 6.75 percent. I pay my agent 15 percent of my book royalties. These rates are levied\nagainst some quantity, such as income in the case of the income tax rate. Obviously the rates\ncan go up or down; less intuitively, the changes in the rates can be described in vastly\ndissimilar ways. The best example of this was a recent change in the Illinois personal income\ntax, which was raised from 3 percent to 5 percent. There are two ways to express this tax\nchange, both of which are technically accurate. The Democrats, who engineered this tax\nincrease, pointed out (correctly) that the state income tax rate was increased by 2 percentage\npoints (from 3 percent to 5 percent). The Republicans pointed out (also correctly) that the state\nincome tax had been raised by 67 percent. [This is a handy test of the formula from a few\nparagraphs back: (5 -- 3)/3 = 2/3, which rounds up to 67 percent.]\n\nThe Democrats focused on the absolute change in the tax rate; Republicans focused on the\npercentage change in the tax burden.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nabsolutely no idea how my investment was performing. Suppose that last year the firm earned\n27 cents---essentially nothing. This year the firm earned 39 cents---also essentially nothing. Yet the company’s profits grew from 27 cents to 39 cents, which is technically a 46 percent\nincrease. Obviously the shareholder letter would have been more of a downer if it pointed out\nthat the firm’s cumulative profits over two years were less than the cost of a cup of Starbucks\ncoffee. To be fair to my roommate, he eventually sold the company for hundreds of millions of\ndollars, earning me a 100 percent return on my investment. (Since you have no idea how much\nI invested, you also have no idea how much money I made---which reinforces my point here\nvery nicely!)\n\nLet me make one additional distinction. Percentage change must not be confused with a\nchange in percentage points. Rates are often expressed in percentages. The sales tax rate in\nIllinois is 6.75 percent. I pay my agent 15 percent of my book royalties. These rates are levied\nagainst some quantity, such as income in the case of the income tax rate. Obviously the rates\ncan go up or down; less intuitively, the changes in the rates can be described in vastly\ndissimilar ways. The best example of this was a recent change in the Illinois personal income\ntax, which was raised from 3 percent to 5 percent. There are two ways to express this tax\nchange, both of which are technically accurate. The Democrats, who engineered this tax\nincrease, pointed out (correctly) that the state income tax rate was increased by 2 percentage\npoints (from 3 percent to 5 percent). The Republicans pointed out (also correctly) that the state\nincome tax had been raised by 67 percent. [This is a handy test of the formula from a few\nparagraphs back: (5 -- 3)/3 = 2/3, which rounds up to 67 percent.]\n\nThe Democrats focused on the absolute change in the tax rate; Republicans focused on the\npercentage change in the tax burden.", "tokens": 444, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 30, "segment_id": "00030", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000045"}
{"type": "chunk", "text": "The Democrats, who engineered this tax\nincrease, pointed out (correctly) that the state income tax rate was increased by 2 percentage\npoints (from 3 percent to 5 percent). The Republicans pointed out (also correctly) that the state\nincome tax had been raised by 67 percent. [This is a handy test of the formula from a few\nparagraphs back: (5 -- 3)/3 = 2/3, which rounds up to 67 percent.]\n\nThe Democrats focused on the absolute change in the tax rate; Republicans focused on the\npercentage change in the tax burden. As noted, both descriptions are technically correct, though\nI would argue that the Republican description more accurately conveys the impact of the tax\nchange, since what I’m going to have to pay to the government---the amount that I care about,\nas opposed to the way it is calculated---really has gone up by 67 percent. Many phenomena defy perfect description with a single statistic. Suppose quarterback Aaron\nRodgers throws for 365 yards but no touchdowns. Meanwhile, Peyton Manning throws for a\nmeager 127 yards but three touchdowns. Manning generated more points, but presumably\nRodgers set up touchdowns by marching his team down the field and keeping the other team’s\noffense off the field. Who played better? In Chapter 1, I discussed the NFL passer rating,\nwhich is the league’s reasonable attempt to deal with this statistical challenge. The passer\nrating is an example of an index, which is a descriptive statistic made up of other descriptive\nstatistics. Once these different measures of performance are consolidated into a single number,\nthat statistic can be used to make comparisons, such as ranking quarterbacks on a particular\nday, or even over a whole career. If baseball had a similar index, then the question of the best\nplayer ever would be solved. Or would it? The advantage of any index is that it consolidates lots of complex information into a single\nnumber. We can then rank things that otherwise defy simple comparison---anything from\nquarterbacks to colleges to beauty pageant contestants. In the Miss America pageant, the", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThe Democrats, who engineered this tax\nincrease, pointed out (correctly) that the state income tax rate was increased by 2 percentage\npoints (from 3 percent to 5 percent). The Republicans pointed out (also correctly) that the state\nincome tax had been raised by 67 percent. [This is a handy test of the formula from a few\nparagraphs back: (5 -- 3)/3 = 2/3, which rounds up to 67 percent.]\n\nThe Democrats focused on the absolute change in the tax rate; Republicans focused on the\npercentage change in the tax burden. As noted, both descriptions are technically correct, though\nI would argue that the Republican description more accurately conveys the impact of the tax\nchange, since what I’m going to have to pay to the government---the amount that I care about,\nas opposed to the way it is calculated---really has gone up by 67 percent. Many phenomena defy perfect description with a single statistic. Suppose quarterback Aaron\nRodgers throws for 365 yards but no touchdowns. Meanwhile, Peyton Manning throws for a\nmeager 127 yards but three touchdowns. Manning generated more points, but presumably\nRodgers set up touchdowns by marching his team down the field and keeping the other team’s\noffense off the field. Who played better? In Chapter 1, I discussed the NFL passer rating,\nwhich is the league’s reasonable attempt to deal with this statistical challenge. The passer\nrating is an example of an index, which is a descriptive statistic made up of other descriptive\nstatistics. Once these different measures of performance are consolidated into a single number,\nthat statistic can be used to make comparisons, such as ranking quarterbacks on a particular\nday, or even over a whole career. If baseball had a similar index, then the question of the best\nplayer ever would be solved. Or would it? The advantage of any index is that it consolidates lots of complex information into a single\nnumber. We can then rank things that otherwise defy simple comparison---anything from\nquarterbacks to colleges to beauty pageant contestants. In the Miss America pageant, the", "tokens": 436, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 30, "segment_id": "00030", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000046"}
{"type": "chunk", "text": "overall winner is a combination of five separate competitions: personal interview, swimsuit,\nevening wear, talent, and onstage question. (Miss Congeniality is voted on separately by the\nparticipants themselves.)\n\nAlas, the disadvantage of any index is that it consolidates lots of complex information into a\nsingle number. There are countless ways to do that; each has the potential to produce a\ndifferent outcome. Malcolm Gladwell makes this point brilliantly in a New Yorker piece\ncritiquing our compelling need to rank things.2 (He comes down particularly hard on the\ncollege rankings.) Gladwell offers the example of Car and Driver’s ranking of three sports\ncars: the Porsche Cayman, the Chevrolet Corvette, and the Lotus Evora. Using a formula that\nincludes twenty-one different variables, Car and Driver ranked the Porsche number one. But\nGladwell points out that “exterior styling” counts for only 4 percent of the total score in the\nCar and Driver formula, which seems ridiculously low for a sports car. If styling is given\nmore weight in the overall ranking (25 percent), then the Lotus comes out on top. But wait. Gladwell also points out that the sticker price of the car gets relatively little weight\nin the Car and Driver formula. If value is weighted more heavily (so that the ranking is based\nequally on price, exterior styling, and vehicle characteristics), the Chevy Corvette is ranked\nnumber one. Any index is highly sensitive to the descriptive statistics that are cobbled together to build it,\nand to the weight given to each of those components. As a result, indices range from useful but\nimperfect tools to complete charades. An example of the former is the United Nations Human\nDevelopment Index, or HDI. The HDI was created as a measure of economic well-being that is\nbroader than income alone. The HDI uses income as one of its components but also includes\nmeasures of life expectancy and educational attainment. The United States ranks eleventh in\nthe world in terms of per capita economic output (behind several oil-rich nations like Qatar,\nBrunei, and Kuwait) but fourth in the world in human development.3 It’s true that the HDI\nrankings would change slightly if the component parts of the index were reconfigured, but no\nreasonable change is going to make Zimbabwe zoom up the rankings past Norway. The HDI\nprovides a handy and reasonably accurate snapshot of living standards around the globe.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\noverall winner is a combination of five separate competitions: personal interview, swimsuit,\nevening wear, talent, and onstage question. (Miss Congeniality is voted on separately by the\nparticipants themselves.)\n\nAlas, the disadvantage of any index is that it consolidates lots of complex information into a\nsingle number. There are countless ways to do that; each has the potential to produce a\ndifferent outcome. Malcolm Gladwell makes this point brilliantly in a New Yorker piece\ncritiquing our compelling need to rank things.2 (He comes down particularly hard on the\ncollege rankings.) Gladwell offers the example of Car and Driver’s ranking of three sports\ncars: the Porsche Cayman, the Chevrolet Corvette, and the Lotus Evora. Using a formula that\nincludes twenty-one different variables, Car and Driver ranked the Porsche number one. But\nGladwell points out that “exterior styling” counts for only 4 percent of the total score in the\nCar and Driver formula, which seems ridiculously low for a sports car. If styling is given\nmore weight in the overall ranking (25 percent), then the Lotus comes out on top. But wait. Gladwell also points out that the sticker price of the car gets relatively little weight\nin the Car and Driver formula. If value is weighted more heavily (so that the ranking is based\nequally on price, exterior styling, and vehicle characteristics), the Chevy Corvette is ranked\nnumber one. Any index is highly sensitive to the descriptive statistics that are cobbled together to build it,\nand to the weight given to each of those components. As a result, indices range from useful but\nimperfect tools to complete charades. An example of the former is the United Nations Human\nDevelopment Index, or HDI. The HDI was created as a measure of economic well-being that is\nbroader than income alone. The HDI uses income as one of its components but also includes\nmeasures of life expectancy and educational attainment. The United States ranks eleventh in\nthe world in terms of per capita economic output (behind several oil-rich nations like Qatar,\nBrunei, and Kuwait) but fourth in the world in human development.3 It’s true that the HDI\nrankings would change slightly if the component parts of the index were reconfigured, but no\nreasonable change is going to make Zimbabwe zoom up the rankings past Norway. The HDI\nprovides a handy and reasonably accurate snapshot of living standards around the globe.", "tokens": 509, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 31, "segment_id": "00031", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000047"}
{"type": "chunk", "text": "The HDI uses income as one of its components but also includes\nmeasures of life expectancy and educational attainment. The United States ranks eleventh in\nthe world in terms of per capita economic output (behind several oil-rich nations like Qatar,\nBrunei, and Kuwait) but fourth in the world in human development.3 It’s true that the HDI\nrankings would change slightly if the component parts of the index were reconfigured, but no\nreasonable change is going to make Zimbabwe zoom up the rankings past Norway. The HDI\nprovides a handy and reasonably accurate snapshot of living standards around the globe. Descriptive statistics give us insight into phenomena that we care about. In that spirit, we can\nreturn to the questions posed at the beginning of the chapter. Who is the best baseball player of\nall time? More important for the purposes of this chapter, what descriptive statistics would be\nmost helpful in answering that question? According to Steve Moyer, president of Baseball Info\nSolutions, the three most valuable statistics (other than age) for evaluating any player who is\nnot a pitcher would be the following:\n\n1. On-base percentage (OBP), sometimes called the on-base average (OBA): Measures the\nproportion of the time that a player reaches base successfully, including walks (which\nare not counted in the batting average). 2. Slugging percentage (SLG): Measures power hitting by calculating the total bases\nreached per at bat. A single counts as 1, a double is 2, a triple is 3, and a home run is 4. Thus, a batter who hit a single and a triple in five at bats would have a slugging", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThe HDI uses income as one of its components but also includes\nmeasures of life expectancy and educational attainment. The United States ranks eleventh in\nthe world in terms of per capita economic output (behind several oil-rich nations like Qatar,\nBrunei, and Kuwait) but fourth in the world in human development.3 It’s true that the HDI\nrankings would change slightly if the component parts of the index were reconfigured, but no\nreasonable change is going to make Zimbabwe zoom up the rankings past Norway. The HDI\nprovides a handy and reasonably accurate snapshot of living standards around the globe. Descriptive statistics give us insight into phenomena that we care about. In that spirit, we can\nreturn to the questions posed at the beginning of the chapter. Who is the best baseball player of\nall time? More important for the purposes of this chapter, what descriptive statistics would be\nmost helpful in answering that question? According to Steve Moyer, president of Baseball Info\nSolutions, the three most valuable statistics (other than age) for evaluating any player who is\nnot a pitcher would be the following:\n\n1. On-base percentage (OBP), sometimes called the on-base average (OBA): Measures the\nproportion of the time that a player reaches base successfully, including walks (which\nare not counted in the batting average). 2. Slugging percentage (SLG): Measures power hitting by calculating the total bases\nreached per at bat. A single counts as 1, a double is 2, a triple is 3, and a home run is 4. Thus, a batter who hit a single and a triple in five at bats would have a slugging", "tokens": 349, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 31, "segment_id": "00031", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000048"}
{"type": "chunk", "text": "percentage of (1 + 3)/5, or .800. 3. At bats (AB): Puts the above in context. Any mope can have impressive statistics for a\ngame or two. A superstar compiles impressive “numbers” over thousands of plate\nappearances. In Moyer’s view (without hesitation, I might add), the best baseball player of all time was Babe\nRuth because of his unique ability to hit and to pitch. Babe Ruth still holds the Major League\ncareer record for slugging percentage at .690.4\n\nWhat about the economic health of the American middle class? Again, I deferred to the\nexperts. I e-mailed Jeff Grogger (a colleague of mine at the University of Chicago) and Alan\nKrueger (the same Princeton economist who studied terrorists and is now serving as chair of\nPresident Obama’s Council of Economic Advisers). Both gave variations on the same basic\nanswer. To assess the economic health of America’s “middle class,” we should examine\nchanges in the median wage (adjusted for inflation) over the last several decades. They also\nrecommended examining changes to wages at the 25th and 75th percentiles (which can\nreasonably be interpreted as the upper and lower bounds for the middle class). One more distinction is in order. When assessing economic health, we can examine income\nor wages. They are not the same thing. A wage is what we are paid for some fixed amount of\nlabor, such as an hourly or weekly wage. Income is the sum of all payments from different\nsources. If workers take a second job or work more hours, their income can go up without a\nchange in the wage. (For that matter, income can go up even if the wage is falling, provided a\nworker logs enough hours on the job.) However, if individuals have to work more in order to\nearn more, it’s hard to evaluate the overall effect on their well-being. The wage is a less\nambiguous measure of how Americans are being compensated for the work they do; the higher\nthe wage, the more workers take home for every hour on the job. Having said all that, here is a graph of American wages over the past three decades. I’ve\nalso added the 90th percentile to illustrate changes in the wages for middle-class workers\ncompared over this time frame to those workers at the top of the distribution.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\npercentage of (1 + 3)/5, or .800. 3. At bats (AB): Puts the above in context. Any mope can have impressive statistics for a\ngame or two. A superstar compiles impressive “numbers” over thousands of plate\nappearances. In Moyer’s view (without hesitation, I might add), the best baseball player of all time was Babe\nRuth because of his unique ability to hit and to pitch. Babe Ruth still holds the Major League\ncareer record for slugging percentage at .690.4\n\nWhat about the economic health of the American middle class? Again, I deferred to the\nexperts. I e-mailed Jeff Grogger (a colleague of mine at the University of Chicago) and Alan\nKrueger (the same Princeton economist who studied terrorists and is now serving as chair of\nPresident Obama’s Council of Economic Advisers). Both gave variations on the same basic\nanswer. To assess the economic health of America’s “middle class,” we should examine\nchanges in the median wage (adjusted for inflation) over the last several decades. They also\nrecommended examining changes to wages at the 25th and 75th percentiles (which can\nreasonably be interpreted as the upper and lower bounds for the middle class). One more distinction is in order. When assessing economic health, we can examine income\nor wages. They are not the same thing. A wage is what we are paid for some fixed amount of\nlabor, such as an hourly or weekly wage. Income is the sum of all payments from different\nsources. If workers take a second job or work more hours, their income can go up without a\nchange in the wage. (For that matter, income can go up even if the wage is falling, provided a\nworker logs enough hours on the job.) However, if individuals have to work more in order to\nearn more, it’s hard to evaluate the overall effect on their well-being. The wage is a less\nambiguous measure of how Americans are being compensated for the work they do; the higher\nthe wage, the more workers take home for every hour on the job. Having said all that, here is a graph of American wages over the past three decades. I’ve\nalso added the 90th percentile to illustrate changes in the wages for middle-class workers\ncompared over this time frame to those workers at the top of the distribution.", "tokens": 501, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 32, "segment_id": "00032", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000049"}
{"type": "chunk", "text": "The wage is a less\nambiguous measure of how Americans are being compensated for the work they do; the higher\nthe wage, the more workers take home for every hour on the job. Having said all that, here is a graph of American wages over the past three decades. I’ve\nalso added the 90th percentile to illustrate changes in the wages for middle-class workers\ncompared over this time frame to those workers at the top of the distribution. Source: “Changes in the Distribution of Workers’ Hourly Wages between 1979 and 2009,” Congressional Budget Office,\nFebruary\nat\nfor\ndata\nhttp://www.cbo.gov/sites/default/files/cbofiles/ftpdocs/120xx/doc12051/02-16-wagedispersion.pdf. 2011. found\n\nchart\n\nThe\n\ncan\n\n16,\n\nthe\n\nbe", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThe wage is a less\nambiguous measure of how Americans are being compensated for the work they do; the higher\nthe wage, the more workers take home for every hour on the job. Having said all that, here is a graph of American wages over the past three decades. I’ve\nalso added the 90th percentile to illustrate changes in the wages for middle-class workers\ncompared over this time frame to those workers at the top of the distribution. Source: “Changes in the Distribution of Workers’ Hourly Wages between 1979 and 2009,” Congressional Budget Office,\nFebruary\nat\nfor\ndata\nhttp://www.cbo.gov/sites/default/files/cbofiles/ftpdocs/120xx/doc12051/02-16-wagedispersion.pdf. 2011. found\n\nchart\n\nThe\n\ncan\n\n16,\n\nthe\n\nbe", "tokens": 175, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 32, "segment_id": "00032", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000050"}
{"type": "chunk", "text": "A variety of conclusions can be drawn from these data. They do not present a single “right”\nanswer with regard to the economic fortunes of the middle class. They do tell us that the\ntypical worker, an American worker earning the median wage, has been “running in place” for\nnearly thirty years. Workers at the 90th percentile have done much, much better. Descriptive\nstatistics help to frame the issue. What we do about it, if anything, is an ideological and\npolitical question. APPENDIX TO CHAPTER 2\n\nData for the printer defects graphics\n\nFormula for variance and standard deviation\nVariance and standard deviation are the most common statistical mechanisms for measuring\nand describing the dispersion of a distribution. The variance, which is often represented by the\n2, is calculated by determining how far the observations within a distribution lie from\nsymbol σ\nthe mean. However, the twist is that the difference between each observation and the mean is\nsquared; the sum of those squared terms is then divided by the number of observations. Specifically:\n\nBecause the difference between each term and the mean is squared, the formula for\ncalculating variance puts particular weight on observations that lie far from the mean, or\noutliers, as the following table of student heights illustrates.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nA variety of conclusions can be drawn from these data. They do not present a single “right”\nanswer with regard to the economic fortunes of the middle class. They do tell us that the\ntypical worker, an American worker earning the median wage, has been “running in place” for\nnearly thirty years. Workers at the 90th percentile have done much, much better. Descriptive\nstatistics help to frame the issue. What we do about it, if anything, is an ideological and\npolitical question. APPENDIX TO CHAPTER 2\n\nData for the printer defects graphics\n\nFormula for variance and standard deviation\nVariance and standard deviation are the most common statistical mechanisms for measuring\nand describing the dispersion of a distribution. The variance, which is often represented by the\n2, is calculated by determining how far the observations within a distribution lie from\nsymbol σ\nthe mean. However, the twist is that the difference between each observation and the mean is\nsquared; the sum of those squared terms is then divided by the number of observations. Specifically:\n\nBecause the difference between each term and the mean is squared, the formula for\ncalculating variance puts particular weight on observations that lie far from the mean, or\noutliers, as the following table of student heights illustrates.", "tokens": 263, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 33, "segment_id": "00033", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000051"}
{"type": "chunk", "text": "* Absolute value is the distance between two figures, regardless of direction, so that it is always positive. In this case, it\nrepresents the number of inches between the height of the individual and the mean. Both groups of students have a mean height of 70 inches. The heights of students in both\ngroups also differ from the mean by the same number of total inches: 14. By that measure of\ndispersion, the two distributions are identical. However, the variance for Group 2 is higher\nbecause of the weight given in the variance formula to values that lie particularly far from the\nmean---Sahar and Narciso in this case. Variance is rarely used as a descriptive statistic on its own. Instead, the variance is most\nuseful as a step toward calculating the standard deviation of a distribution, which is a more\nintuitive tool as a descriptive statistic. The standard deviation for a set of observations is the square root of the variance:\n\nFor any set of n observations x1, x2, x3 . . . xn with mean μ,\nstandard deviation = σ = square root of this whole quantity =\n\n* With twelve bar patrons, the median would be the midpoint between the income of the guy on the sixth stool and the\nincome of the guy on the seventh stool. Since they both make $35,000, the median is $35,000. If one made $35,000 and\nthe other made $36,000, the median for the whole group would be $35,500. * Manufacturing update: It turns out that nearly all of the defective printers were being manufactured at a plant in Kentucky\nwhere workers had stripped parts off the assembly line in order to build a bourbon distillery. Both the perpetually drunk\nemployees and the random missing pieces on the assembly line appear to have compromised the quality of the printers\nbeing produced there. * Remarkably, this person was one of the ten people with annual incomes of $35,000 who were sitting on bar stools when\nBill Gates walked in with his parrot. Go figure!", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\n* Absolute value is the distance between two figures, regardless of direction, so that it is always positive. In this case, it\nrepresents the number of inches between the height of the individual and the mean. Both groups of students have a mean height of 70 inches. The heights of students in both\ngroups also differ from the mean by the same number of total inches: 14. By that measure of\ndispersion, the two distributions are identical. However, the variance for Group 2 is higher\nbecause of the weight given in the variance formula to values that lie particularly far from the\nmean---Sahar and Narciso in this case. Variance is rarely used as a descriptive statistic on its own. Instead, the variance is most\nuseful as a step toward calculating the standard deviation of a distribution, which is a more\nintuitive tool as a descriptive statistic. The standard deviation for a set of observations is the square root of the variance:\n\nFor any set of n observations x1, x2, x3 . . . xn with mean μ,\nstandard deviation = σ = square root of this whole quantity =\n\n* With twelve bar patrons, the median would be the midpoint between the income of the guy on the sixth stool and the\nincome of the guy on the seventh stool. Since they both make $35,000, the median is $35,000. If one made $35,000 and\nthe other made $36,000, the median for the whole group would be $35,500. * Manufacturing update: It turns out that nearly all of the defective printers were being manufactured at a plant in Kentucky\nwhere workers had stripped parts off the assembly line in order to build a bourbon distillery. Both the perpetually drunk\nemployees and the random missing pieces on the assembly line appear to have compromised the quality of the printers\nbeing produced there. * Remarkably, this person was one of the ten people with annual incomes of $35,000 who were sitting on bar stools when\nBill Gates walked in with his parrot. Go figure!", "tokens": 424, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 34, "segment_id": "00034", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000052"}
{"type": "chunk", "text": "CHAPTER 3\nDeceptive Description\n“He’s got a great personality!” and other\ntrue but grossly misleading statements\n\nTo anyone who has ever contemplated dating, the phrase “he’s got a great personality”\n\nusually sets off alarm bells, not because the description is necessarily wrong, but for what it\nmay not reveal, such as the fact that the guy has a prison record or that his divorce is “not\nentirely final.” We don’t doubt that this guy has a great personality; we are wary that a true\nstatement, the great personality, is being used to mask or obscure other information in a way\nthat is seriously misleading (assuming that most of us would prefer not to date ex-felons who\nare still married). The statement is not a lie per se, meaning that it wouldn’t get you convicted\nof perjury, but it still could be so inaccurate as to be untruthful. And so it is with statistics. Although the field of statistics is rooted in mathematics, and\nmathematics is exact, the use of statistics to describe complex phenomena is not exact. That\nleaves plenty of room for shading the truth. Mark Twain famously remarked that there are three\nkinds of lies: lies, damned lies, and statistics.* As the last chapter explained, most phenomena\nthat we care about can be described in multiple ways. Once there are multiple ways of\ndescribing the same thing (e.g., “he’s got a great personality” or “he was convicted of\nsecurities fraud”), the descriptive statistics that we choose to use (or not to use) will have a\nprofound impact on the impression that we leave. Someone with nefarious motives can use\nperfectly good facts and figures to support entirely disputable or illegitimate conclusions. We ought to begin with the crucial distinction between “precision” and “accuracy.” These\nwords are not interchangeable. Precision reflects the exactitude with which we can express\nsomething. In a description of the length of your commute, “41.6 miles” is more precise than\n“about 40 miles,” which is more precise than “a long f------ing way.” If you ask me how far it\nis to the nearest gas station, and I tell you that it’s 1.265 miles to the east, that’s a precise\nanswer. Here is the problem: That answer may be entirely inaccurate if the gas station happens\nto be in the other direction.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nCHAPTER 3\nDeceptive Description\n“He’s got a great personality!” and other\ntrue but grossly misleading statements\n\nTo anyone who has ever contemplated dating, the phrase “he’s got a great personality”\n\nusually sets off alarm bells, not because the description is necessarily wrong, but for what it\nmay not reveal, such as the fact that the guy has a prison record or that his divorce is “not\nentirely final.” We don’t doubt that this guy has a great personality; we are wary that a true\nstatement, the great personality, is being used to mask or obscure other information in a way\nthat is seriously misleading (assuming that most of us would prefer not to date ex-felons who\nare still married). The statement is not a lie per se, meaning that it wouldn’t get you convicted\nof perjury, but it still could be so inaccurate as to be untruthful. And so it is with statistics. Although the field of statistics is rooted in mathematics, and\nmathematics is exact, the use of statistics to describe complex phenomena is not exact. That\nleaves plenty of room for shading the truth. Mark Twain famously remarked that there are three\nkinds of lies: lies, damned lies, and statistics.* As the last chapter explained, most phenomena\nthat we care about can be described in multiple ways. Once there are multiple ways of\ndescribing the same thing (e.g., “he’s got a great personality” or “he was convicted of\nsecurities fraud”), the descriptive statistics that we choose to use (or not to use) will have a\nprofound impact on the impression that we leave. Someone with nefarious motives can use\nperfectly good facts and figures to support entirely disputable or illegitimate conclusions. We ought to begin with the crucial distinction between “precision” and “accuracy.” These\nwords are not interchangeable. Precision reflects the exactitude with which we can express\nsomething. In a description of the length of your commute, “41.6 miles” is more precise than\n“about 40 miles,” which is more precise than “a long f------ing way.” If you ask me how far it\nis to the nearest gas station, and I tell you that it’s 1.265 miles to the east, that’s a precise\nanswer. Here is the problem: That answer may be entirely inaccurate if the gas station happens\nto be in the other direction.", "tokens": 505, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 35, "segment_id": "00035", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000053"}
{"type": "chunk", "text": "Precision reflects the exactitude with which we can express\nsomething. In a description of the length of your commute, “41.6 miles” is more precise than\n“about 40 miles,” which is more precise than “a long f------ing way.” If you ask me how far it\nis to the nearest gas station, and I tell you that it’s 1.265 miles to the east, that’s a precise\nanswer. Here is the problem: That answer may be entirely inaccurate if the gas station happens\nto be in the other direction. On the other hand, if I tell you, “Drive ten minutes or so until you\nsee a hot dog stand. The gas station will be a couple hundred yards after that on the right. If\nyou pass the Hooters, you’ve gone too far,” my answer is less precise than “1.265 miles to the\neast” but significantly better because I am sending you in the direction of the gas station. Accuracy is a measure of whether a figure is broadly consistent with the truth---hence the\ndanger of confusing precision with accuracy. If an answer is accurate, then more precision is\nusually better. But no amount of precision can make up for inaccuracy. In fact, precision can mask inaccuracy by giving us a false sense of certainty, either\ninadvertently or quite deliberately. Joseph McCarthy, the Red-baiting senator from Wisconsin,", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nPrecision reflects the exactitude with which we can express\nsomething. In a description of the length of your commute, “41.6 miles” is more precise than\n“about 40 miles,” which is more precise than “a long f------ing way.” If you ask me how far it\nis to the nearest gas station, and I tell you that it’s 1.265 miles to the east, that’s a precise\nanswer. Here is the problem: That answer may be entirely inaccurate if the gas station happens\nto be in the other direction. On the other hand, if I tell you, “Drive ten minutes or so until you\nsee a hot dog stand. The gas station will be a couple hundred yards after that on the right. If\nyou pass the Hooters, you’ve gone too far,” my answer is less precise than “1.265 miles to the\neast” but significantly better because I am sending you in the direction of the gas station. Accuracy is a measure of whether a figure is broadly consistent with the truth---hence the\ndanger of confusing precision with accuracy. If an answer is accurate, then more precision is\nusually better. But no amount of precision can make up for inaccuracy. In fact, precision can mask inaccuracy by giving us a false sense of certainty, either\ninadvertently or quite deliberately. Joseph McCarthy, the Red-baiting senator from Wisconsin,", "tokens": 292, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 35, "segment_id": "00035", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000054"}
{"type": "chunk", "text": "reached the apogee of his reckless charges in 1950 when he alleged not only that the U.S. State\nDepartment was infiltrated with communists, but that he had a list of their names. During a\nspeech in Wheeling, West Virginia, McCarthy waved in the air a piece of paper and declared,\n“I have here in my hand a list of 205---a list of names that were made known to the Secretary\nof State as being members of the Communist Party and who nevertheless are still working and\nshaping policy in the State Department.”1 It turns out that the paper had no names on it at all,\nbut the specificity of the charge gave it credibility, despite the fact that it was a bald-faced lie. I learned the important distinction between precision and accuracy in a less malicious\ncontext. For Christmas one year my wife bought me a golf range finder to calculate distances\non the course from my golf ball to the hole. The device works with some kind of laser; I stand\nnext to my ball in the fairway (or rough) and point the range finder at the flag on the green, at\nwhich point the device calculates the exact distance that I’m supposed to hit the ball. This is an\nimprovement upon the standard yardage markers, which give distances only to the center of the\ngreen (and are therefore accurate but less precise). With my Christmas-gift range finder I was\nable to know that I was 147.2 yards from the hole. I expected the precision of this nifty\ntechnology to improve my golf game. Instead, it got appreciably worse. There were two problems. First, I used the stupid device for three months before I realized\nthat it was set to meters rather than to yards; every seemingly precise calculation (147.2) was\nwrong. Second, I would sometimes inadvertently aim the laser beam at the trees behind the\ngreen, rather than at the flag marking the hole, so that my “perfect” shot would go exactly the\ndistance it was supposed to go---right over the green into the forest. The lesson for me, which\napplies to all statistical analysis, is that even the most precise measurements or calculations\nshould be checked against common sense. To take an example with more serious implications, many of the Wall Street risk\nmanagement models prior to the 2008 financial crisis were quite precise.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nreached the apogee of his reckless charges in 1950 when he alleged not only that the U.S. State\nDepartment was infiltrated with communists, but that he had a list of their names. During a\nspeech in Wheeling, West Virginia, McCarthy waved in the air a piece of paper and declared,\n“I have here in my hand a list of 205---a list of names that were made known to the Secretary\nof State as being members of the Communist Party and who nevertheless are still working and\nshaping policy in the State Department.”1 It turns out that the paper had no names on it at all,\nbut the specificity of the charge gave it credibility, despite the fact that it was a bald-faced lie. I learned the important distinction between precision and accuracy in a less malicious\ncontext. For Christmas one year my wife bought me a golf range finder to calculate distances\non the course from my golf ball to the hole. The device works with some kind of laser; I stand\nnext to my ball in the fairway (or rough) and point the range finder at the flag on the green, at\nwhich point the device calculates the exact distance that I’m supposed to hit the ball. This is an\nimprovement upon the standard yardage markers, which give distances only to the center of the\ngreen (and are therefore accurate but less precise). With my Christmas-gift range finder I was\nable to know that I was 147.2 yards from the hole. I expected the precision of this nifty\ntechnology to improve my golf game. Instead, it got appreciably worse. There were two problems. First, I used the stupid device for three months before I realized\nthat it was set to meters rather than to yards; every seemingly precise calculation (147.2) was\nwrong. Second, I would sometimes inadvertently aim the laser beam at the trees behind the\ngreen, rather than at the flag marking the hole, so that my “perfect” shot would go exactly the\ndistance it was supposed to go---right over the green into the forest. The lesson for me, which\napplies to all statistical analysis, is that even the most precise measurements or calculations\nshould be checked against common sense. To take an example with more serious implications, many of the Wall Street risk\nmanagement models prior to the 2008 financial crisis were quite precise.", "tokens": 492, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 36, "segment_id": "00036", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000055"}
{"type": "chunk", "text": "Second, I would sometimes inadvertently aim the laser beam at the trees behind the\ngreen, rather than at the flag marking the hole, so that my “perfect” shot would go exactly the\ndistance it was supposed to go---right over the green into the forest. The lesson for me, which\napplies to all statistical analysis, is that even the most precise measurements or calculations\nshould be checked against common sense. To take an example with more serious implications, many of the Wall Street risk\nmanagement models prior to the 2008 financial crisis were quite precise. The concept of\n“value at risk” allowed firms to quantify with precision the amount of the firm’s capital that\ncould be lost under different scenarios. The problem was that the supersophisticated models\nwere the equivalent of setting my range finder to meters rather than to yards. The math was\ncomplex and arcane. The answers it produced were reassuringly precise. But the assumptions\nabout what might happen to global markets that were embedded in the models were just plain\nwrong, making the conclusions wholly inaccurate in ways that destabilized not only Wall Street\nbut the entire global economy. Even the most precise and accurate descriptive statistics can suffer from a more fundamental\nproblem: a lack of clarity over what exactly we are trying to define, describe, or explain. Statistical arguments have much in common with bad marriages; the disputants often talk past\nis American\none another. Consider an\nmanufacturing? One often hears that American manufacturing jobs are being lost in huge\nnumbers to China, India, and other low-wage countries. One also hears that high-tech\nmanufacturing still thrives in the United States and that America remains one of the world’s top\nexporters of manufactured goods. Which is it? This would appear to be a case in which sound\nanalysis of good data could reconcile these competing narratives. Is U.S. manufacturing\n\nimportant economic question: How healthy", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nSecond, I would sometimes inadvertently aim the laser beam at the trees behind the\ngreen, rather than at the flag marking the hole, so that my “perfect” shot would go exactly the\ndistance it was supposed to go---right over the green into the forest. The lesson for me, which\napplies to all statistical analysis, is that even the most precise measurements or calculations\nshould be checked against common sense. To take an example with more serious implications, many of the Wall Street risk\nmanagement models prior to the 2008 financial crisis were quite precise. The concept of\n“value at risk” allowed firms to quantify with precision the amount of the firm’s capital that\ncould be lost under different scenarios. The problem was that the supersophisticated models\nwere the equivalent of setting my range finder to meters rather than to yards. The math was\ncomplex and arcane. The answers it produced were reassuringly precise. But the assumptions\nabout what might happen to global markets that were embedded in the models were just plain\nwrong, making the conclusions wholly inaccurate in ways that destabilized not only Wall Street\nbut the entire global economy. Even the most precise and accurate descriptive statistics can suffer from a more fundamental\nproblem: a lack of clarity over what exactly we are trying to define, describe, or explain. Statistical arguments have much in common with bad marriages; the disputants often talk past\nis American\none another. Consider an\nmanufacturing? One often hears that American manufacturing jobs are being lost in huge\nnumbers to China, India, and other low-wage countries. One also hears that high-tech\nmanufacturing still thrives in the United States and that America remains one of the world’s top\nexporters of manufactured goods. Which is it? This would appear to be a case in which sound\nanalysis of good data could reconcile these competing narratives. Is U.S. manufacturing\n\nimportant economic question: How healthy", "tokens": 392, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 36, "segment_id": "00036", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000056"}
{"type": "chunk", "text": "profitable and globally competitive, or is it shrinking in the face of intense foreign competition? Both. The British news magazine the Economist reconciled the two seemingly contradictory\n\nviews of American manufacturing with the following graph. “The Rustbelt Recovery,” March 10, 2011\n\nThe seeming contradiction lies in how one defines the “health” of U.S. manufacturing. In\nterms of output---the total value of goods produced and sold---the U.S. manufacturing sector\ngrew steadily in the 2000s, took a big hit during the Great Recession, and has since bounced\nback robustly. This is consistent with data from the CIA’s World Factbook showing that the\nUnited States is the third-largest manufacturing exporter in the world, behind China and\nGermany. The United States remains a manufacturing powerhouse. But the graph in the Economist has a second line, which is manufacturing employment. The\nnumber of manufacturing jobs in the United States has fallen steadily; roughly six million\nmanufacturing jobs were lost in the last decade. Together, these two stories---rising\nmanufacturing output and falling employment---tell the complete story. Manufacturing in the\nUnited States has grown steadily more productive, meaning that factories are producing more\noutput with fewer workers. This is good from a global competitiveness standpoint, for it makes\nAmerican products more competitive with manufactured goods from low-wage countries. (One\nway to compete with a firm that can pay workers $2 an hour is to create a manufacturing\nprocess so efficient that one worker earning $40 can do twenty times as much.) But there are a\nlot fewer manufacturing jobs, which is terrible news for the displaced workers who depended\non those wages. Since this is a book about statistics and not manufacturing, let’s go back to the main point,\nwhich is that the “health” of U.S. manufacturing---something seemingly easy to quantify---\ndepends on how one chooses to define health: output or employment? In this case (and many\nothers), the most complete story comes from including both figures, as the Economist wisely\nchose to do in its graph. Even when we agree on a single measure of success, say, student test scores, there is plenty\nof statistical wiggle room. See if you can reconcile the following hypothetical statements, both\nof which could be true:\n\nPolitician A (the challenger): “Our schools are getting worse!", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nprofitable and globally competitive, or is it shrinking in the face of intense foreign competition? Both. The British news magazine the Economist reconciled the two seemingly contradictory\n\nviews of American manufacturing with the following graph. “The Rustbelt Recovery,” March 10, 2011\n\nThe seeming contradiction lies in how one defines the “health” of U.S. manufacturing. In\nterms of output---the total value of goods produced and sold---the U.S. manufacturing sector\ngrew steadily in the 2000s, took a big hit during the Great Recession, and has since bounced\nback robustly. This is consistent with data from the CIA’s World Factbook showing that the\nUnited States is the third-largest manufacturing exporter in the world, behind China and\nGermany. The United States remains a manufacturing powerhouse. But the graph in the Economist has a second line, which is manufacturing employment. The\nnumber of manufacturing jobs in the United States has fallen steadily; roughly six million\nmanufacturing jobs were lost in the last decade. Together, these two stories---rising\nmanufacturing output and falling employment---tell the complete story. Manufacturing in the\nUnited States has grown steadily more productive, meaning that factories are producing more\noutput with fewer workers. This is good from a global competitiveness standpoint, for it makes\nAmerican products more competitive with manufactured goods from low-wage countries. (One\nway to compete with a firm that can pay workers $2 an hour is to create a manufacturing\nprocess so efficient that one worker earning $40 can do twenty times as much.) But there are a\nlot fewer manufacturing jobs, which is terrible news for the displaced workers who depended\non those wages. Since this is a book about statistics and not manufacturing, let’s go back to the main point,\nwhich is that the “health” of U.S. manufacturing---something seemingly easy to quantify---\ndepends on how one chooses to define health: output or employment? In this case (and many\nothers), the most complete story comes from including both figures, as the Economist wisely\nchose to do in its graph. Even when we agree on a single measure of success, say, student test scores, there is plenty\nof statistical wiggle room. See if you can reconcile the following hypothetical statements, both\nof which could be true:\n\nPolitician A (the challenger): “Our schools are getting worse!", "tokens": 487, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 37, "segment_id": "00037", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000057"}
{"type": "chunk", "text": "manufacturing---something seemingly easy to quantify---\ndepends on how one chooses to define health: output or employment? In this case (and many\nothers), the most complete story comes from including both figures, as the Economist wisely\nchose to do in its graph. Even when we agree on a single measure of success, say, student test scores, there is plenty\nof statistical wiggle room. See if you can reconcile the following hypothetical statements, both\nof which could be true:\n\nPolitician A (the challenger): “Our schools are getting worse! Sixty percent of our schools\n\nhad lower test scores this year than last year.”\n\nPolitician B (the incumbent): “Our schools are getting better! Eighty percent of our students", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nmanufacturing---something seemingly easy to quantify---\ndepends on how one chooses to define health: output or employment? In this case (and many\nothers), the most complete story comes from including both figures, as the Economist wisely\nchose to do in its graph. Even when we agree on a single measure of success, say, student test scores, there is plenty\nof statistical wiggle room. See if you can reconcile the following hypothetical statements, both\nof which could be true:\n\nPolitician A (the challenger): “Our schools are getting worse! Sixty percent of our schools\n\nhad lower test scores this year than last year.”\n\nPolitician B (the incumbent): “Our schools are getting better! Eighty percent of our students", "tokens": 150, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 37, "segment_id": "00037", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000058"}
{"type": "chunk", "text": "had higher test scores this year than last year.”\n\nHere’s a hint: The schools do not all necessarily have the same number of students. If you\ntake another look at the seemingly contradictory statements, what you’ll see is that one\npolitician is using schools as his unit of analysis (“Sixty percent of our schools . . .”), and the\nother is using students as the unit of analysis (“Eighty percent of our students . . .”). The unit\nof analysis is the entity being compared or described by the statistics---school performance by\none of them and student performance by the other. It’s entirely possible for most of the\nstudents to be improving and most of the schools to be getting worse---if the students showing\nimprovement happen to be in very big schools. To make this example more intuitive, let’s do\nthe same exercise by using American states:\n\nPolitician A (a populist): “Our economy is in the crapper! Thirty states had falling incomes\n\nlast year.”\n\nPolitician B (more of an elitist): “Our economy is showing appreciable gains: Seventy\n\npercent of Americans had rising incomes last year.”\n\nWhat I would infer from those statements is that the biggest states have the healthiest\neconomies: New York, California, Texas, Illinois, and so on. The thirty states with falling\naverage incomes are likely to be much smaller: Vermont, North Dakota, Rhode Island, and so\non. Given the disparity in the size of the states, it’s entirely possible that the majority of states\nare doing worse while the majority of Americans are doing better. The key lesson is to pay\nattention to the unit of analysis. Who or what is being described, and is that different from the\n“who” or “what” being described by someone else? Although the examples above are hypothetical, here is a crucial statistical question that is\nnot: Is globalization making income inequality around the planet better or worse? By one\ninterpretation, globalization has merely exacerbated existing income inequalities; richer\ncountries in 1980 (as measured by GDP per capita) tended to grow faster between 1980 and\n2000 than poorer countries.2 The rich countries just got richer, suggesting that trade,\noutsourcing, foreign investment, and the other components of “globalization” are merely tools\nfor the developed world to extend its economic hegemony. Down with globalization! Down\nwith globalization! But hold on a moment.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nhad higher test scores this year than last year.”\n\nHere’s a hint: The schools do not all necessarily have the same number of students. If you\ntake another look at the seemingly contradictory statements, what you’ll see is that one\npolitician is using schools as his unit of analysis (“Sixty percent of our schools . . .”), and the\nother is using students as the unit of analysis (“Eighty percent of our students . . .”). The unit\nof analysis is the entity being compared or described by the statistics---school performance by\none of them and student performance by the other. It’s entirely possible for most of the\nstudents to be improving and most of the schools to be getting worse---if the students showing\nimprovement happen to be in very big schools. To make this example more intuitive, let’s do\nthe same exercise by using American states:\n\nPolitician A (a populist): “Our economy is in the crapper! Thirty states had falling incomes\n\nlast year.”\n\nPolitician B (more of an elitist): “Our economy is showing appreciable gains: Seventy\n\npercent of Americans had rising incomes last year.”\n\nWhat I would infer from those statements is that the biggest states have the healthiest\neconomies: New York, California, Texas, Illinois, and so on. The thirty states with falling\naverage incomes are likely to be much smaller: Vermont, North Dakota, Rhode Island, and so\non. Given the disparity in the size of the states, it’s entirely possible that the majority of states\nare doing worse while the majority of Americans are doing better. The key lesson is to pay\nattention to the unit of analysis. Who or what is being described, and is that different from the\n“who” or “what” being described by someone else? Although the examples above are hypothetical, here is a crucial statistical question that is\nnot: Is globalization making income inequality around the planet better or worse? By one\ninterpretation, globalization has merely exacerbated existing income inequalities; richer\ncountries in 1980 (as measured by GDP per capita) tended to grow faster between 1980 and\n2000 than poorer countries.2 The rich countries just got richer, suggesting that trade,\noutsourcing, foreign investment, and the other components of “globalization” are merely tools\nfor the developed world to extend its economic hegemony. Down with globalization! Down\nwith globalization! But hold on a moment.", "tokens": 503, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 38, "segment_id": "00038", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000059"}
{"type": "chunk", "text": "By one\ninterpretation, globalization has merely exacerbated existing income inequalities; richer\ncountries in 1980 (as measured by GDP per capita) tended to grow faster between 1980 and\n2000 than poorer countries.2 The rich countries just got richer, suggesting that trade,\noutsourcing, foreign investment, and the other components of “globalization” are merely tools\nfor the developed world to extend its economic hegemony. Down with globalization! Down\nwith globalization! But hold on a moment. The same data can (and should) be interpreted entirely differently if\none changes the unit of analysis. We don’t care about poor countries; we care about poor\npeople. And a high proportion of the world’s poor people happen to live in China and India. Both countries are huge (with a population over a billion); each was relatively poor in 1980. Not only have China and India grown rapidly over the past several decades, but they have done\nso in large part because of their increased economic integration with the rest of the world. They\nare “rapid globalizers,” as the Economist has described them. Given that our goal is to\nameliorate human misery, it makes no sense to give China (population 1.3 billion) the same\nweight as Mauritius (population 1.3 million) when examining the effects of globalization on the\npoor. The unit of analysis should be people, not countries. What really happened between 1980\nand 2000 is a lot like my fake school example above. The bulk of the world’s poor happened", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nBy one\ninterpretation, globalization has merely exacerbated existing income inequalities; richer\ncountries in 1980 (as measured by GDP per capita) tended to grow faster between 1980 and\n2000 than poorer countries.2 The rich countries just got richer, suggesting that trade,\noutsourcing, foreign investment, and the other components of “globalization” are merely tools\nfor the developed world to extend its economic hegemony. Down with globalization! Down\nwith globalization! But hold on a moment. The same data can (and should) be interpreted entirely differently if\none changes the unit of analysis. We don’t care about poor countries; we care about poor\npeople. And a high proportion of the world’s poor people happen to live in China and India. Both countries are huge (with a population over a billion); each was relatively poor in 1980. Not only have China and India grown rapidly over the past several decades, but they have done\nso in large part because of their increased economic integration with the rest of the world. They\nare “rapid globalizers,” as the Economist has described them. Given that our goal is to\nameliorate human misery, it makes no sense to give China (population 1.3 billion) the same\nweight as Mauritius (population 1.3 million) when examining the effects of globalization on the\npoor. The unit of analysis should be people, not countries. What really happened between 1980\nand 2000 is a lot like my fake school example above. The bulk of the world’s poor happened", "tokens": 324, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 38, "segment_id": "00038", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000060"}
{"type": "chunk", "text": "to live in two giant countries that grew extremely fast as they became more integrated into the\nglobal economy. The proper analysis yields an entirely different conclusion about the benefits\nof globalization for the world’s poor. As the Economist points out, “If you consider people, not\ncountries, global inequality is falling rapidly.”\n\nThe telecommunications companies AT&T and Verizon have recently engaged in an\nadvertising battle that exploits this kind of ambiguity about what is being described. Both\ncompanies provide cellular phone service. One of the primary concerns of most cell phone\nusers is the quality of the service in places where they are likely to make or receive phone\ncalls. Thus, a logical point of comparison between the two firms is the size and quality of their\nnetworks. While consumers just want decent cell phone service in lots of places, both AT&T\nand Verizon have come up with different metrics for measuring the somewhat amorphous\ndemand for “decent cell phone service in lots of places.” Verizon launched an aggressive\nadvertising campaign touting the geographic coverage of its network; you may remember the\nmaps of the United States that showed the large percentage of the country covered by the\nVerizon network compared with the relatively paltry geographic coverage of the AT&T\nnetwork. The unit of analysis chosen by Verizon is geographic area covered---because the\ncompany has more of it. AT&T countered by launching a campaign that changed the unit of analysis. Its billboards\nadvertised that “AT&T covers 97 percent of Americans.” Note the use of the word\n“Americans” rather than “America.” AT&T focused on the fact that most people don’t live in\nrural Montana or the Arizona desert. Since the population is not evenly distributed across the\nphysical geography of the United States, the key to good cell service (the campaign argued\nimplicitly) is having a network in place where callers actually live and work, not necessarily\nwhere they go camping. As someone who spends a fair bit of time in rural New Hampshire,\nhowever, my sympathies are with Verizon on this one. Our old friends the mean and the median can also be used for nefarious ends. As you should\nrecall from the last chapter, both the median and the mean are measures of the “middle” of a\ndistribution, or its “central tendency.” The mean is a simple average: the sum of the\nobservations divided by the number of observations.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nto live in two giant countries that grew extremely fast as they became more integrated into the\nglobal economy. The proper analysis yields an entirely different conclusion about the benefits\nof globalization for the world’s poor. As the Economist points out, “If you consider people, not\ncountries, global inequality is falling rapidly.”\n\nThe telecommunications companies AT&T and Verizon have recently engaged in an\nadvertising battle that exploits this kind of ambiguity about what is being described. Both\ncompanies provide cellular phone service. One of the primary concerns of most cell phone\nusers is the quality of the service in places where they are likely to make or receive phone\ncalls. Thus, a logical point of comparison between the two firms is the size and quality of their\nnetworks. While consumers just want decent cell phone service in lots of places, both AT&T\nand Verizon have come up with different metrics for measuring the somewhat amorphous\ndemand for “decent cell phone service in lots of places.” Verizon launched an aggressive\nadvertising campaign touting the geographic coverage of its network; you may remember the\nmaps of the United States that showed the large percentage of the country covered by the\nVerizon network compared with the relatively paltry geographic coverage of the AT&T\nnetwork. The unit of analysis chosen by Verizon is geographic area covered---because the\ncompany has more of it. AT&T countered by launching a campaign that changed the unit of analysis. Its billboards\nadvertised that “AT&T covers 97 percent of Americans.” Note the use of the word\n“Americans” rather than “America.” AT&T focused on the fact that most people don’t live in\nrural Montana or the Arizona desert. Since the population is not evenly distributed across the\nphysical geography of the United States, the key to good cell service (the campaign argued\nimplicitly) is having a network in place where callers actually live and work, not necessarily\nwhere they go camping. As someone who spends a fair bit of time in rural New Hampshire,\nhowever, my sympathies are with Verizon on this one. Our old friends the mean and the median can also be used for nefarious ends. As you should\nrecall from the last chapter, both the median and the mean are measures of the “middle” of a\ndistribution, or its “central tendency.” The mean is a simple average: the sum of the\nobservations divided by the number of observations.", "tokens": 492, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 39, "segment_id": "00039", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000061"}
{"type": "chunk", "text": "As someone who spends a fair bit of time in rural New Hampshire,\nhowever, my sympathies are with Verizon on this one. Our old friends the mean and the median can also be used for nefarious ends. As you should\nrecall from the last chapter, both the median and the mean are measures of the “middle” of a\ndistribution, or its “central tendency.” The mean is a simple average: the sum of the\nobservations divided by the number of observations. (The mean of 3, 4, 5, 6, and 102 is 24.)\nThe median is the midpoint of the distribution; half of the observations lie above the median\nand half lie below. (The median of 3, 4, 5, 6, and 102 is 5.) Now, the clever reader will see that\nthere is a sizable difference between 24 and 5. If, for some reason, I would like to describe this\ngroup of numbers in a way that makes it look big, I will focus on the mean. If I want to make it\nlook smaller, I will cite the median. Now let’s look at how this plays out in real life. Consider the George W. Bush tax cuts,\nwhich were touted by the Bush administration as something good for most American families. While pushing the plan, the administration pointed out that 92 million Americans would receive\nan average tax reduction of over $1,000 ($1,083 to be precise). But was that summary of the\ntax cut accurate? According to the New York Times , “The data don’t lie, but some of them are\nmum.”\n\nWould 92 million Americans be getting a tax cut? Yes.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nAs someone who spends a fair bit of time in rural New Hampshire,\nhowever, my sympathies are with Verizon on this one. Our old friends the mean and the median can also be used for nefarious ends. As you should\nrecall from the last chapter, both the median and the mean are measures of the “middle” of a\ndistribution, or its “central tendency.” The mean is a simple average: the sum of the\nobservations divided by the number of observations. (The mean of 3, 4, 5, 6, and 102 is 24.)\nThe median is the midpoint of the distribution; half of the observations lie above the median\nand half lie below. (The median of 3, 4, 5, 6, and 102 is 5.) Now, the clever reader will see that\nthere is a sizable difference between 24 and 5. If, for some reason, I would like to describe this\ngroup of numbers in a way that makes it look big, I will focus on the mean. If I want to make it\nlook smaller, I will cite the median. Now let’s look at how this plays out in real life. Consider the George W. Bush tax cuts,\nwhich were touted by the Bush administration as something good for most American families. While pushing the plan, the administration pointed out that 92 million Americans would receive\nan average tax reduction of over $1,000 ($1,083 to be precise). But was that summary of the\ntax cut accurate? According to the New York Times , “The data don’t lie, but some of them are\nmum.”\n\nWould 92 million Americans be getting a tax cut? Yes.", "tokens": 354, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 39, "segment_id": "00039", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000062"}
{"type": "chunk", "text": "Would most of those people be getting a tax cut of around $1,000? No. The median tax cut\n\nwas less than $100. A relatively small number of extremely wealthy individuals were eligible for very large tax\ncuts; these big numbers skew the mean, making the average tax cut look bigger than what most\nAmericans would likely receive. The median is not sensitive to outliers, and, in this case, is\nprobably a more accurate description of how the tax cuts affected the typical household. Of course, the median can also do its share of dissembling because it is not sensitive to\noutliers. Suppose that you have a potentially fatal illness. The good news is that a new drug\nhas been developed that might be effective. The drawback is that it’s extremely expensive and\nhas many unpleasant side effects. “But does it work?” you ask. The doctor informs you that\nthe new drug increases the median life expectancy among patients with your disease by two\nweeks. That is hardly encouraging news; the drug may not be worth the cost and\nunpleasantness. Your insurance company refuses to pay for the treatment; it has a pretty good\ncase on the basis of the median life expectancy figures. Yet the median may be a horribly misleading statistic in this case. Suppose that many\npatients do not respond to the new treatment but that some large number of patients, say 30 or\n40 percent, are cured entirely. This success would not show up in the median (though the mean\nlife expectancy of those taking the drug would look very impressive). In this case, the outliers\n---those who take the drug and live for a long time---would be highly relevant to your decision. And it is not merely a hypothetical case. Evolutionary biologist Stephen Jay Gould was\ndiagnosed with a form of cancer that had a median survival time of eight months; he died of a\ndifferent and unrelated kind of cancer twenty years later.3 Gould subsequently wrote a famous\narticle called “The Median Isn’t the Message,” in which he argued that his scientific knowledge\nof statistics saved him from the erroneous conclusion that he would necessarily be dead in\neight months. The definition of the median tells us that half the patients will live at least eight\nmonths---and possibly much, much longer than that.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nWould most of those people be getting a tax cut of around $1,000? No. The median tax cut\n\nwas less than $100. A relatively small number of extremely wealthy individuals were eligible for very large tax\ncuts; these big numbers skew the mean, making the average tax cut look bigger than what most\nAmericans would likely receive. The median is not sensitive to outliers, and, in this case, is\nprobably a more accurate description of how the tax cuts affected the typical household. Of course, the median can also do its share of dissembling because it is not sensitive to\noutliers. Suppose that you have a potentially fatal illness. The good news is that a new drug\nhas been developed that might be effective. The drawback is that it’s extremely expensive and\nhas many unpleasant side effects. “But does it work?” you ask. The doctor informs you that\nthe new drug increases the median life expectancy among patients with your disease by two\nweeks. That is hardly encouraging news; the drug may not be worth the cost and\nunpleasantness. Your insurance company refuses to pay for the treatment; it has a pretty good\ncase on the basis of the median life expectancy figures. Yet the median may be a horribly misleading statistic in this case. Suppose that many\npatients do not respond to the new treatment but that some large number of patients, say 30 or\n40 percent, are cured entirely. This success would not show up in the median (though the mean\nlife expectancy of those taking the drug would look very impressive). In this case, the outliers\n---those who take the drug and live for a long time---would be highly relevant to your decision. And it is not merely a hypothetical case. Evolutionary biologist Stephen Jay Gould was\ndiagnosed with a form of cancer that had a median survival time of eight months; he died of a\ndifferent and unrelated kind of cancer twenty years later.3 Gould subsequently wrote a famous\narticle called “The Median Isn’t the Message,” in which he argued that his scientific knowledge\nof statistics saved him from the erroneous conclusion that he would necessarily be dead in\neight months. The definition of the median tells us that half the patients will live at least eight\nmonths---and possibly much, much longer than that.", "tokens": 468, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 40, "segment_id": "00040", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000063"}
{"type": "chunk", "text": "And it is not merely a hypothetical case. Evolutionary biologist Stephen Jay Gould was\ndiagnosed with a form of cancer that had a median survival time of eight months; he died of a\ndifferent and unrelated kind of cancer twenty years later.3 Gould subsequently wrote a famous\narticle called “The Median Isn’t the Message,” in which he argued that his scientific knowledge\nof statistics saved him from the erroneous conclusion that he would necessarily be dead in\neight months. The definition of the median tells us that half the patients will live at least eight\nmonths---and possibly much, much longer than that. The mortality distribution is “rightskewed,” which is more than a technicality if you happen to have the disease.4\n\nIn this example, the defining characteristic of the median---that it does not weight\nobservations on the basis of how far they lie from the midpoint, only on whether they lie above\nor below---turns out to be its weakness. In contrast, the mean is affected by dispersion. From\nthe standpoint of accuracy, the median versus mean question revolves around whether the\noutliers in a distribution distort what is being described or are instead an important part of the\nmessage. (Once again, judgment trumps math.) Of course, nothing says that you must choose\nthe median or the mean. Any comprehensive statistical analysis would likely present both. When just the median or the mean appears, it may be for the sake of brevity---or it may be\nbecause someone is seeking to “persuade” with statistics. Those of a certain age may remember the following exchange (as I recollect it) between the\ncharacters played by Chevy Chase and Ted Knight in the movie Caddyshack. The two men\nmeet in the locker room after both have just come off the golf course:\n\nTED KNIGHT: What did you shoot?", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nAnd it is not merely a hypothetical case. Evolutionary biologist Stephen Jay Gould was\ndiagnosed with a form of cancer that had a median survival time of eight months; he died of a\ndifferent and unrelated kind of cancer twenty years later.3 Gould subsequently wrote a famous\narticle called “The Median Isn’t the Message,” in which he argued that his scientific knowledge\nof statistics saved him from the erroneous conclusion that he would necessarily be dead in\neight months. The definition of the median tells us that half the patients will live at least eight\nmonths---and possibly much, much longer than that. The mortality distribution is “rightskewed,” which is more than a technicality if you happen to have the disease.4\n\nIn this example, the defining characteristic of the median---that it does not weight\nobservations on the basis of how far they lie from the midpoint, only on whether they lie above\nor below---turns out to be its weakness. In contrast, the mean is affected by dispersion. From\nthe standpoint of accuracy, the median versus mean question revolves around whether the\noutliers in a distribution distort what is being described or are instead an important part of the\nmessage. (Once again, judgment trumps math.) Of course, nothing says that you must choose\nthe median or the mean. Any comprehensive statistical analysis would likely present both. When just the median or the mean appears, it may be for the sake of brevity---or it may be\nbecause someone is seeking to “persuade” with statistics. Those of a certain age may remember the following exchange (as I recollect it) between the\ncharacters played by Chevy Chase and Ted Knight in the movie Caddyshack. The two men\nmeet in the locker room after both have just come off the golf course:\n\nTED KNIGHT: What did you shoot?", "tokens": 381, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 40, "segment_id": "00040", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000064"}
{"type": "chunk", "text": "CHEVY CHASE: Oh, I don’t keep score. TED KNIGHT: Then how do you compare yourself to other golfers? CHEVY CHASE: By height. I’m not going to try to explain why this is funny. I will say that a great many statistical\nshenanigans arise from “apples and oranges” comparisons. Suppose you are trying to compare\nthe price of a hotel room in London with the price of a hotel room in Paris. You send your sixyear-old to the computer to do some Internet research, since she is much faster and better at it\nthan you are. Your child reports back that hotel rooms in Paris are more expensive, around 180\na night; a comparable room in London is 150 a night. You would likely explain to your child the difference between pounds and euros, and then\nsend her back to the computer to find the exchange rate between the two currencies so that you\ncould make a meaningful comparison. (This example is loosely rooted in truth; after I paid 100\nrupees for a pot of tea in India, my daughter wanted to know why everything in India was so\nexpensive.) Obviously the numbers on currency from different countries mean nothing until we\nconvert them into comparable units. What is the exchange rate between the pound and the euro,\nor, in the case of India, between the dollar and the rupee? This seems like a painfully obvious lesson---yet one that is routinely ignored, particularly by\npoliticians and Hollywood studios. These folks clearly recognize the difference between euros\nand pounds; instead, they overlook a more subtle example of apples and oranges: inflation. A\ndollar today is not the same as a dollar sixty years ago; it buys much less. Because of inflation,\nsomething that cost $1 in 1950 would cost $9.37 in 2011. As a result, any monetary\ncomparison between 1950 and 2011 without adjusting for changes in the value of the dollar\nwould be less accurate than comparing figures in euros and pounds---since the euro and the\npound are closer to each other in value than a 1950 dollar is to a 2011 dollar. This is such an important phenomenon that economists have terms to denote whether figures\nhave been adjusted for inflation or not. Nominal figures are not adjusted for inflation.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nCHEVY CHASE: Oh, I don’t keep score. TED KNIGHT: Then how do you compare yourself to other golfers? CHEVY CHASE: By height. I’m not going to try to explain why this is funny. I will say that a great many statistical\nshenanigans arise from “apples and oranges” comparisons. Suppose you are trying to compare\nthe price of a hotel room in London with the price of a hotel room in Paris. You send your sixyear-old to the computer to do some Internet research, since she is much faster and better at it\nthan you are. Your child reports back that hotel rooms in Paris are more expensive, around 180\na night; a comparable room in London is 150 a night. You would likely explain to your child the difference between pounds and euros, and then\nsend her back to the computer to find the exchange rate between the two currencies so that you\ncould make a meaningful comparison. (This example is loosely rooted in truth; after I paid 100\nrupees for a pot of tea in India, my daughter wanted to know why everything in India was so\nexpensive.) Obviously the numbers on currency from different countries mean nothing until we\nconvert them into comparable units. What is the exchange rate between the pound and the euro,\nor, in the case of India, between the dollar and the rupee? This seems like a painfully obvious lesson---yet one that is routinely ignored, particularly by\npoliticians and Hollywood studios. These folks clearly recognize the difference between euros\nand pounds; instead, they overlook a more subtle example of apples and oranges: inflation. A\ndollar today is not the same as a dollar sixty years ago; it buys much less. Because of inflation,\nsomething that cost $1 in 1950 would cost $9.37 in 2011. As a result, any monetary\ncomparison between 1950 and 2011 without adjusting for changes in the value of the dollar\nwould be less accurate than comparing figures in euros and pounds---since the euro and the\npound are closer to each other in value than a 1950 dollar is to a 2011 dollar. This is such an important phenomenon that economists have terms to denote whether figures\nhave been adjusted for inflation or not. Nominal figures are not adjusted for inflation.", "tokens": 483, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 41, "segment_id": "00041", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000065"}
{"type": "chunk", "text": "Because of inflation,\nsomething that cost $1 in 1950 would cost $9.37 in 2011. As a result, any monetary\ncomparison between 1950 and 2011 without adjusting for changes in the value of the dollar\nwould be less accurate than comparing figures in euros and pounds---since the euro and the\npound are closer to each other in value than a 1950 dollar is to a 2011 dollar. This is such an important phenomenon that economists have terms to denote whether figures\nhave been adjusted for inflation or not. Nominal figures are not adjusted for inflation. A\ncomparison of the nominal cost of a government program in 1970 to the nominal cost of the\nsame program in 2011 merely compares the size of the checks that the Treasury wrote in those\ntwo years---without any recognition that a dollar in 1970 bought more stuff than a dollar in\n2011. If we spent $10 million on a program in 1970 to provide war veterans with housing\nassistance and $40 million on the same program in 2011, the federal commitment to that\nprogram has actually gone down. Yes, spending has gone up in nominal terms, but that does\nnot reflect the changing value of the dollars being spent. One 1970 dollar is equal to $5.83 in\n2011; the government would need to spend $58.3 million on veterans’ housing benefits in 2011\nto provide support comparable to the $10 million it was spending in 1970. Real figures, on the other hand, are adjusted for inflation. The most commonly accepted\nmethodology is to convert all of the figures into a single unit, such as 2011 dollars, to make an\n“apples and apples” comparison. Many websites, including that of the U.S. Bureau of Labor\nStatistics, have simple inflation calculators that will compare the value of a dollar at different\npoints in time.* For a real (yes, a pun) example of how statistics can look different when\nadjusted for inflation, check out the following graph of the U.S. federal minimum wage, which", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nBecause of inflation,\nsomething that cost $1 in 1950 would cost $9.37 in 2011. As a result, any monetary\ncomparison between 1950 and 2011 without adjusting for changes in the value of the dollar\nwould be less accurate than comparing figures in euros and pounds---since the euro and the\npound are closer to each other in value than a 1950 dollar is to a 2011 dollar. This is such an important phenomenon that economists have terms to denote whether figures\nhave been adjusted for inflation or not. Nominal figures are not adjusted for inflation. A\ncomparison of the nominal cost of a government program in 1970 to the nominal cost of the\nsame program in 2011 merely compares the size of the checks that the Treasury wrote in those\ntwo years---without any recognition that a dollar in 1970 bought more stuff than a dollar in\n2011. If we spent $10 million on a program in 1970 to provide war veterans with housing\nassistance and $40 million on the same program in 2011, the federal commitment to that\nprogram has actually gone down. Yes, spending has gone up in nominal terms, but that does\nnot reflect the changing value of the dollars being spent. One 1970 dollar is equal to $5.83 in\n2011; the government would need to spend $58.3 million on veterans’ housing benefits in 2011\nto provide support comparable to the $10 million it was spending in 1970. Real figures, on the other hand, are adjusted for inflation. The most commonly accepted\nmethodology is to convert all of the figures into a single unit, such as 2011 dollars, to make an\n“apples and apples” comparison. Many websites, including that of the U.S. Bureau of Labor\nStatistics, have simple inflation calculators that will compare the value of a dollar at different\npoints in time.* For a real (yes, a pun) example of how statistics can look different when\nadjusted for inflation, check out the following graph of the U.S. federal minimum wage, which", "tokens": 441, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 41, "segment_id": "00041", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000066"}
{"type": "chunk", "text": "plots both the nominal value of the minimum wage and its real purchasing power in 2010\ndollars. Source: http://oregonstate.edu/instruct/anth484/minwage.html. The federal minimum wage---the number posted on the bulletin board in some remote corner\nof your office---is set by Congress. This wage, currently $7.25, is a nominal figure. Your boss\ndoes not have to ensure that $7.25 buys as much as it did two years ago; he just has to make\nsure that you get a minimum of $7.25 for every hour of work that you do. It’s all about the\nnumber on the check, not what that number can buy. Yet inflation erodes the purchasing power of the minimum wage over time (and every other\nnominal wage, which is why unions typically negotiate “cost of living adjustments”). If prices\nrise faster than Congress raises the minimum wage, the real value of that minimum hourly\npayment will fall. Supporters of a minimum wage should care about the real value of that\nwage, since the whole point of the law is to guarantee low-wage workers some minimum level\nof consumption for an hour of work, not to give them a check with a big number on it that buys\nless than it used to. (If that were the case, then we could just pay low-wage workers in rupees.)\nHollywood studios may be the most egregiously oblivious to the distortions caused by\ninflation when comparing figures at different points in time---and deliberately so. What were\nthe top five highest-grossing films (domestic) of all time as of 2011?5\n\n1. Avatar (2009)\n2. Titanic (1997)\n3. The Dark Knight (2008)\n4. Star Wars Episode IV (1977)\n5. Shrek 2 (2004)\n\nNow you may feel that list looks a little suspect. These were successful films---but Shrek\n2? Was that really a greater commercial success than Gone with the Wind? The Godfather? Jaws? No, no, and no. Hollywood likes to make each blockbuster look bigger and more\nsuccessful than the last. One way to do that would be to quote box office receipts in Indian", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nplots both the nominal value of the minimum wage and its real purchasing power in 2010\ndollars. Source: http://oregonstate.edu/instruct/anth484/minwage.html. The federal minimum wage---the number posted on the bulletin board in some remote corner\nof your office---is set by Congress. This wage, currently $7.25, is a nominal figure. Your boss\ndoes not have to ensure that $7.25 buys as much as it did two years ago; he just has to make\nsure that you get a minimum of $7.25 for every hour of work that you do. It’s all about the\nnumber on the check, not what that number can buy. Yet inflation erodes the purchasing power of the minimum wage over time (and every other\nnominal wage, which is why unions typically negotiate “cost of living adjustments”). If prices\nrise faster than Congress raises the minimum wage, the real value of that minimum hourly\npayment will fall. Supporters of a minimum wage should care about the real value of that\nwage, since the whole point of the law is to guarantee low-wage workers some minimum level\nof consumption for an hour of work, not to give them a check with a big number on it that buys\nless than it used to. (If that were the case, then we could just pay low-wage workers in rupees.)\nHollywood studios may be the most egregiously oblivious to the distortions caused by\ninflation when comparing figures at different points in time---and deliberately so. What were\nthe top five highest-grossing films (domestic) of all time as of 2011?5\n\n1. Avatar (2009)\n2. Titanic (1997)\n3. The Dark Knight (2008)\n4. Star Wars Episode IV (1977)\n5. Shrek 2 (2004)\n\nNow you may feel that list looks a little suspect. These were successful films---but Shrek\n2? Was that really a greater commercial success than Gone with the Wind? The Godfather? Jaws? No, no, and no. Hollywood likes to make each blockbuster look bigger and more\nsuccessful than the last. One way to do that would be to quote box office receipts in Indian", "tokens": 467, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 42, "segment_id": "00042", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000067"}
{"type": "chunk", "text": "rupees, which would inspire headlines such as the following: “Harry Potter Breaks Box Office\nRecord with Weekend Receipts of 1.3 Trillion!” But even the most dim-witted moviegoers\nwould be suspicious of figures that are large only because they are quoted in a currency with\nrelatively little purchasing power. Instead, Hollywood studios (and the journalists who report\non them) merely use nominal figures, which makes recent movies look successful largely\nbecause ticket prices are higher now than they were ten, twenty, or fifty years ago. (When\nGone with the Wind came out in 1939, a ticket cost somewhere in the range of $.50.) The most\naccurate way to compare commercial success over time would be to adjust ticket receipts for\ninflation. Earning $100 million in 1939 is a lot more impressive than earning $500 million in\n2011. So what are the top grossing films in the U.S. of all time, adjusted for inflation?6\n\n1. Gone with the Wind (1939)\n2. Star Wars Episode IV (1977)\n3. The Sound of Music (1965)\n4. E.T. (1982)\n5. The Ten Commandments (1956)\n\nIn real terms, Avatar falls to number 14; Shrek 2 falls all the way to 31st. Even comparing apples and apples leaves plenty of room for shenanigans. As discussed in\nthe last chapter, one important role of statistics is to describe changes in quantities over time. Are taxes going up? How many cheeseburgers are we selling compared with last year? By how\nmuch have we reduced the arsenic in our drinking water? We often use percentages to express\nthese changes because they give us a sense of scale and context. We understand what it means\nto reduce the amount of arsenic in the drinking water by 22 percent, whereas few of us would\nknow whether reducing arsenic by one microgram (the absolute reduction) would be a\nsignificant change or not. Percentages don’t lie---but they can exaggerate. One way to make\ngrowth look explosive is to use percentage change to describe some change relative to a very\nlow starting point. I live in Cook County, Illinois. I was shocked one day to learn that the\nportion of my taxes supporting the Suburban Cook County Tuberculosis Sanitarium District\nwas slated to rise by 527 percent!", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nrupees, which would inspire headlines such as the following: “Harry Potter Breaks Box Office\nRecord with Weekend Receipts of 1.3 Trillion!” But even the most dim-witted moviegoers\nwould be suspicious of figures that are large only because they are quoted in a currency with\nrelatively little purchasing power. Instead, Hollywood studios (and the journalists who report\non them) merely use nominal figures, which makes recent movies look successful largely\nbecause ticket prices are higher now than they were ten, twenty, or fifty years ago. (When\nGone with the Wind came out in 1939, a ticket cost somewhere in the range of $.50.) The most\naccurate way to compare commercial success over time would be to adjust ticket receipts for\ninflation. Earning $100 million in 1939 is a lot more impressive than earning $500 million in\n2011. So what are the top grossing films in the U.S. of all time, adjusted for inflation?6\n\n1. Gone with the Wind (1939)\n2. Star Wars Episode IV (1977)\n3. The Sound of Music (1965)\n4. E.T. (1982)\n5. The Ten Commandments (1956)\n\nIn real terms, Avatar falls to number 14; Shrek 2 falls all the way to 31st. Even comparing apples and apples leaves plenty of room for shenanigans. As discussed in\nthe last chapter, one important role of statistics is to describe changes in quantities over time. Are taxes going up? How many cheeseburgers are we selling compared with last year? By how\nmuch have we reduced the arsenic in our drinking water? We often use percentages to express\nthese changes because they give us a sense of scale and context. We understand what it means\nto reduce the amount of arsenic in the drinking water by 22 percent, whereas few of us would\nknow whether reducing arsenic by one microgram (the absolute reduction) would be a\nsignificant change or not. Percentages don’t lie---but they can exaggerate. One way to make\ngrowth look explosive is to use percentage change to describe some change relative to a very\nlow starting point. I live in Cook County, Illinois. I was shocked one day to learn that the\nportion of my taxes supporting the Suburban Cook County Tuberculosis Sanitarium District\nwas slated to rise by 527 percent!", "tokens": 504, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 43, "segment_id": "00043", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000068"}
{"type": "chunk", "text": "Percentages don’t lie---but they can exaggerate. One way to make\ngrowth look explosive is to use percentage change to describe some change relative to a very\nlow starting point. I live in Cook County, Illinois. I was shocked one day to learn that the\nportion of my taxes supporting the Suburban Cook County Tuberculosis Sanitarium District\nwas slated to rise by 527 percent! However, I called off my massive antitax rally (which was\nreally still in the planning phase) when I learned that this change would cost me less than a\ngood turkey sandwich. The Tuberculosis Sanitarium District deals with roughly a hundred\ncases a year; it is not a large or expensive organization. The Chicago Sun-Times pointed out\nthat for the typical homeowner, the tax bill would go from $1.15 to $6.7 Researchers will\nsometimes qualify a growth figure by pointing out that it is “from a low base,” meaning that\nany increase is going to look large by comparison. Obviously the flip side is true. A small percentage of an enormous sum can be a big\nnumber. Suppose the secretary of defense reports that defense spending will grow only 4\npercent this year. Great news! Not really, given that the Defense Department budget is nearly\n$700 billion. Four percent of $700 billion is $28 billion, which can buy a lot of turkey\nsandwiches. In fact, that seemingly paltry 4 percent increase in the defense budget is more than\nthe entire NASA budget and about the same as the budgets of the Labor and Treasury", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nPercentages don’t lie---but they can exaggerate. One way to make\ngrowth look explosive is to use percentage change to describe some change relative to a very\nlow starting point. I live in Cook County, Illinois. I was shocked one day to learn that the\nportion of my taxes supporting the Suburban Cook County Tuberculosis Sanitarium District\nwas slated to rise by 527 percent! However, I called off my massive antitax rally (which was\nreally still in the planning phase) when I learned that this change would cost me less than a\ngood turkey sandwich. The Tuberculosis Sanitarium District deals with roughly a hundred\ncases a year; it is not a large or expensive organization. The Chicago Sun-Times pointed out\nthat for the typical homeowner, the tax bill would go from $1.15 to $6.7 Researchers will\nsometimes qualify a growth figure by pointing out that it is “from a low base,” meaning that\nany increase is going to look large by comparison. Obviously the flip side is true. A small percentage of an enormous sum can be a big\nnumber. Suppose the secretary of defense reports that defense spending will grow only 4\npercent this year. Great news! Not really, given that the Defense Department budget is nearly\n$700 billion. Four percent of $700 billion is $28 billion, which can buy a lot of turkey\nsandwiches. In fact, that seemingly paltry 4 percent increase in the defense budget is more than\nthe entire NASA budget and about the same as the budgets of the Labor and Treasury", "tokens": 326, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 43, "segment_id": "00043", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000069"}
{"type": "chunk", "text": "Departments combined. In a similar vein, your kindhearted boss might point out that as a matter of fairness, every\nemployee will be getting the same raise this year, 10 percent. What a magnanimous gesture---\nexcept that if your boss makes $1 million and you make $50,000, his raise will be $100,000\nand yours will be $5,000. The statement “everyone will get the same 10 percent raise this\nyear” just sounds so much better than “my raise will be twenty times bigger than yours.” Both\nare true in this case. Any comparison of a quantity changing over time must have a start point and an end point. One can sometimes manipulate those points in ways that affect the message. I once had a\nprofessor who liked to speak about his “Republican slides” and his “Democratic slides.” He\nwas referring to data on defense spending, and what he meant was that he could organize the\nsame data in different ways in order to please either Democratic or Republican audiences. For\nhis Republican audiences, he would offer the following slide with data on increases in defense\nspending under Ronald Reagan. Clearly Reagan helped restore our commitment to defense and\nsecurity, which in turn helped to win the Cold War. No one can look at these numbers and not\nappreciate the steely determination of Ronald Reagan to face down the Soviets. Defense Spending in Billions, 1981--1988\n\nFor the Democrats, my former professor merely used the same (nominal) data, but a longer\ntime frame. For this group, he pointed out that Jimmy Carter deserves credit for beginning the\ndefense buildup. As the following “Democratic” slide shows, the defense spending increases\nfrom 1977 to 1980 show the same basic trend as the increases during the Reagan presidency. Thank goodness that Jimmy Carter---a graduate of Annapolis and a former naval officer---\nbegan the process of making America strong again! Defense Spending in Billions, 1977--1988", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nDepartments combined. In a similar vein, your kindhearted boss might point out that as a matter of fairness, every\nemployee will be getting the same raise this year, 10 percent. What a magnanimous gesture---\nexcept that if your boss makes $1 million and you make $50,000, his raise will be $100,000\nand yours will be $5,000. The statement “everyone will get the same 10 percent raise this\nyear” just sounds so much better than “my raise will be twenty times bigger than yours.” Both\nare true in this case. Any comparison of a quantity changing over time must have a start point and an end point. One can sometimes manipulate those points in ways that affect the message. I once had a\nprofessor who liked to speak about his “Republican slides” and his “Democratic slides.” He\nwas referring to data on defense spending, and what he meant was that he could organize the\nsame data in different ways in order to please either Democratic or Republican audiences. For\nhis Republican audiences, he would offer the following slide with data on increases in defense\nspending under Ronald Reagan. Clearly Reagan helped restore our commitment to defense and\nsecurity, which in turn helped to win the Cold War. No one can look at these numbers and not\nappreciate the steely determination of Ronald Reagan to face down the Soviets. Defense Spending in Billions, 1981--1988\n\nFor the Democrats, my former professor merely used the same (nominal) data, but a longer\ntime frame. For this group, he pointed out that Jimmy Carter deserves credit for beginning the\ndefense buildup. As the following “Democratic” slide shows, the defense spending increases\nfrom 1977 to 1980 show the same basic trend as the increases during the Reagan presidency. Thank goodness that Jimmy Carter---a graduate of Annapolis and a former naval officer---\nbegan the process of making America strong again! Defense Spending in Billions, 1977--1988", "tokens": 417, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 44, "segment_id": "00044", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000070"}
{"type": "chunk", "text": "Source:\nspan=usgs302&year=1988&view=1&expand=30&expandC=&units=b&fy=fy12&local=s&state=US&pie=#usgs302. http:///spend.php? While the main point of statistics is to present a meaningful picture of things we care about,\nin many cases we also hope to act on these numbers. NFL teams want a simple measure of\nquarterback quality so that they can find and draft talented players out of college. Firms\nmeasure the performance of their employees so that they can promote those who are valuable\nand fire those who are not. There is a common business aphorism: “You can’t manage what\nyou can’t measure.” True. But you had better be darn sure that what you are measuring is\nreally what you are trying to manage. Consider school quality. This is a crucial thing to measure, since we would like to reward\nand emulate “good” schools while sanctioning or fixing “bad” schools. (And within each\nschool, we have the similar challenge of measuring teacher quality, for the same basic reason.)\nThe most common measure of quality for both schools and teachers is test scores. If students\nare achieving impressive scores on a well-conceived standardized test, then presumably the\nteacher and school are doing a fine job. Conversely, bad test scores are a clear signal that lots\nof people should be fired, sooner rather than later. These statistics can take us a long way\ntoward fixing our public education system, right? Wrong. Any evaluation of teachers or schools that is based solely on test scores will present\na dangerously inaccurate picture. Students who walk through the front door of different schools\nhave vastly different backgrounds and abilities. We know, for example, that the education and\nincome of a student’s parents have a significant impact on achievement, regardless of what\nschool he or she attends. The statistic that we’re missing in this case happens to be the only\none that matters for our purposes: How much of a student’s performance, good or bad, can be\nattributed to what happens inside the school (or inside a particular classroom)? Students who live in affluent, highly educated communities are going to test well from the\nmoment their parents drop them off at school on the first day of kindergarten. The flip side is\nalso true.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nSource:\nspan=usgs302&year=1988&view=1&expand=30&expandC=&units=b&fy=fy12&local=s&state=US&pie=#usgs302. http:///spend.php? While the main point of statistics is to present a meaningful picture of things we care about,\nin many cases we also hope to act on these numbers. NFL teams want a simple measure of\nquarterback quality so that they can find and draft talented players out of college. Firms\nmeasure the performance of their employees so that they can promote those who are valuable\nand fire those who are not. There is a common business aphorism: “You can’t manage what\nyou can’t measure.” True. But you had better be darn sure that what you are measuring is\nreally what you are trying to manage. Consider school quality. This is a crucial thing to measure, since we would like to reward\nand emulate “good” schools while sanctioning or fixing “bad” schools. (And within each\nschool, we have the similar challenge of measuring teacher quality, for the same basic reason.)\nThe most common measure of quality for both schools and teachers is test scores. If students\nare achieving impressive scores on a well-conceived standardized test, then presumably the\nteacher and school are doing a fine job. Conversely, bad test scores are a clear signal that lots\nof people should be fired, sooner rather than later. These statistics can take us a long way\ntoward fixing our public education system, right? Wrong. Any evaluation of teachers or schools that is based solely on test scores will present\na dangerously inaccurate picture. Students who walk through the front door of different schools\nhave vastly different backgrounds and abilities. We know, for example, that the education and\nincome of a student’s parents have a significant impact on achievement, regardless of what\nschool he or she attends. The statistic that we’re missing in this case happens to be the only\none that matters for our purposes: How much of a student’s performance, good or bad, can be\nattributed to what happens inside the school (or inside a particular classroom)? Students who live in affluent, highly educated communities are going to test well from the\nmoment their parents drop them off at school on the first day of kindergarten. The flip side is\nalso true.", "tokens": 485, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 45, "segment_id": "00045", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000071"}
{"type": "chunk", "text": "We know, for example, that the education and\nincome of a student’s parents have a significant impact on achievement, regardless of what\nschool he or she attends. The statistic that we’re missing in this case happens to be the only\none that matters for our purposes: How much of a student’s performance, good or bad, can be\nattributed to what happens inside the school (or inside a particular classroom)? Students who live in affluent, highly educated communities are going to test well from the\nmoment their parents drop them off at school on the first day of kindergarten. The flip side is\nalso true. There are schools with extremely disadvantaged populations in which teachers may\nbe doing a remarkable job but the student test scores will still be low---albeit not nearly as low\nas they would have been if the teachers had not been doing a good job. What we need is some\nmeasure of “value-added” at the school level, or even at the classroom level. We don’t want to\nknow the absolute level of student achievement; we want to know how much that student", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nWe know, for example, that the education and\nincome of a student’s parents have a significant impact on achievement, regardless of what\nschool he or she attends. The statistic that we’re missing in this case happens to be the only\none that matters for our purposes: How much of a student’s performance, good or bad, can be\nattributed to what happens inside the school (or inside a particular classroom)? Students who live in affluent, highly educated communities are going to test well from the\nmoment their parents drop them off at school on the first day of kindergarten. The flip side is\nalso true. There are schools with extremely disadvantaged populations in which teachers may\nbe doing a remarkable job but the student test scores will still be low---albeit not nearly as low\nas they would have been if the teachers had not been doing a good job. What we need is some\nmeasure of “value-added” at the school level, or even at the classroom level. We don’t want to\nknow the absolute level of student achievement; we want to know how much that student", "tokens": 222, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 45, "segment_id": "00045", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000072"}
{"type": "chunk", "text": "achievement has been affected by the educational factors we are trying to evaluate. At first glance, this seems an easy task, as we can simply give students a pretest and a\nposttest. If we know student test scores when they enter a particular school or classroom, then\nwe can measure their performance at the end and attribute the difference to whatever happened\nin that school or classroom. Alas, wrong again. Students with different abilities or backgrounds may also learn at\ndifferent rates. Some students will grasp the material faster than others for reasons that have\nnothing to do with the quality of the teaching. So if students in Affluent School A and Poor\nSchool B both start algebra at the same time and level, the explanation for the fact that students\nat Affluent School A test better in algebra a year later may be that the teachers are better, or it\nmay be that the students were capable of learning faster---or both. Researchers are working to\ndevelop statistical techniques that measure instructional quality in ways that account\nappropriately for different student backgrounds and abilities. In the meantime, our attempts to\nidentify the “best” schools can be ridiculously misleading. Every fall, several Chicago newspapers and magazines publish a ranking of the “best” high\nschools in the region, usually on the basis of state test score data. Here is the part that is laughout-loud funny from a statistical standpoint: Several of the high schools consistently at the top\nof the rankings are selective enrollment schools, meaning that students must apply to get in,\nand only a small proportion of those students are accepted. One of the most important\nadmissions criteria is standardized test scores. So let’s summarize: (1) these schools are being\nrecognized as “excellent” for having students with high test scores; (2) to get into such a\nschool, one must have high test scores. This is the logical equivalent of giving an award to the\nbasketball team for doing such an excellent job of producing tall students. Even if you have a solid indicator of what you are trying to measure and manage, the\nchallenges are not over. The good news is that “managing by statistics” can change the\nunderlying behavior of the person or institution being managed for the better.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nachievement has been affected by the educational factors we are trying to evaluate. At first glance, this seems an easy task, as we can simply give students a pretest and a\nposttest. If we know student test scores when they enter a particular school or classroom, then\nwe can measure their performance at the end and attribute the difference to whatever happened\nin that school or classroom. Alas, wrong again. Students with different abilities or backgrounds may also learn at\ndifferent rates. Some students will grasp the material faster than others for reasons that have\nnothing to do with the quality of the teaching. So if students in Affluent School A and Poor\nSchool B both start algebra at the same time and level, the explanation for the fact that students\nat Affluent School A test better in algebra a year later may be that the teachers are better, or it\nmay be that the students were capable of learning faster---or both. Researchers are working to\ndevelop statistical techniques that measure instructional quality in ways that account\nappropriately for different student backgrounds and abilities. In the meantime, our attempts to\nidentify the “best” schools can be ridiculously misleading. Every fall, several Chicago newspapers and magazines publish a ranking of the “best” high\nschools in the region, usually on the basis of state test score data. Here is the part that is laughout-loud funny from a statistical standpoint: Several of the high schools consistently at the top\nof the rankings are selective enrollment schools, meaning that students must apply to get in,\nand only a small proportion of those students are accepted. One of the most important\nadmissions criteria is standardized test scores. So let’s summarize: (1) these schools are being\nrecognized as “excellent” for having students with high test scores; (2) to get into such a\nschool, one must have high test scores. This is the logical equivalent of giving an award to the\nbasketball team for doing such an excellent job of producing tall students. Even if you have a solid indicator of what you are trying to measure and manage, the\nchallenges are not over. The good news is that “managing by statistics” can change the\nunderlying behavior of the person or institution being managed for the better.", "tokens": 461, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 46, "segment_id": "00046", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000073"}
{"type": "chunk", "text": "So let’s summarize: (1) these schools are being\nrecognized as “excellent” for having students with high test scores; (2) to get into such a\nschool, one must have high test scores. This is the logical equivalent of giving an award to the\nbasketball team for doing such an excellent job of producing tall students. Even if you have a solid indicator of what you are trying to measure and manage, the\nchallenges are not over. The good news is that “managing by statistics” can change the\nunderlying behavior of the person or institution being managed for the better. If you can\nmeasure the proportion of defective products coming off an assembly line, and if those defects\nare a function of things happening at the plant, then some kind of bonus for workers that is tied\nto a reduction in defective products would presumably change behavior in the right kinds of\nways. Each of us responds to incentives (even if it is just praise or a better parking spot). Statistics measure the outcomes that matter; incentives give us a reason to improve those\noutcomes. Or, in some cases, just to make the statistics look better. That’s the bad news. If school administrators are evaluated---and perhaps even compensated---on the basis of the\nhigh school graduation rate for students in a particular school district, they will focus their\nefforts on boosting the number of students who graduate. Of course, they may also devote\nsome effort to improving the graduation rate, which is not necessarily the same thing. For\nexample, students who leave school before graduation can be classified as “moving away”\nrather than dropping out. This is not merely a hypothetical example; it is a charge that was\nleveled against former secretary of education Rod Paige during his tenure as the Houston\nschool superintendent. Paige was hired by President George W. Bush to be U.S. secretary of", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nSo let’s summarize: (1) these schools are being\nrecognized as “excellent” for having students with high test scores; (2) to get into such a\nschool, one must have high test scores. This is the logical equivalent of giving an award to the\nbasketball team for doing such an excellent job of producing tall students. Even if you have a solid indicator of what you are trying to measure and manage, the\nchallenges are not over. The good news is that “managing by statistics” can change the\nunderlying behavior of the person or institution being managed for the better. If you can\nmeasure the proportion of defective products coming off an assembly line, and if those defects\nare a function of things happening at the plant, then some kind of bonus for workers that is tied\nto a reduction in defective products would presumably change behavior in the right kinds of\nways. Each of us responds to incentives (even if it is just praise or a better parking spot). Statistics measure the outcomes that matter; incentives give us a reason to improve those\noutcomes. Or, in some cases, just to make the statistics look better. That’s the bad news. If school administrators are evaluated---and perhaps even compensated---on the basis of the\nhigh school graduation rate for students in a particular school district, they will focus their\nefforts on boosting the number of students who graduate. Of course, they may also devote\nsome effort to improving the graduation rate, which is not necessarily the same thing. For\nexample, students who leave school before graduation can be classified as “moving away”\nrather than dropping out. This is not merely a hypothetical example; it is a charge that was\nleveled against former secretary of education Rod Paige during his tenure as the Houston\nschool superintendent. Paige was hired by President George W. Bush to be U.S. secretary of", "tokens": 385, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 46, "segment_id": "00046", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000074"}
{"type": "chunk", "text": "education because of his remarkable success in Houston in reducing the dropout rate and\nboosting test scores. If you’re keeping track of the little business aphorisms I keep tossing your way, here is\nanother one: “It’s never a good day when 60 Minutes shows up at your door.” Dan Rather and\nthe 60 Minutes II crew made a trip to Houston and found that the manipulation of statistics was\nfar more impressive than the educational improvement.8 High schools routinely classified\nstudents who quit high school as transferring to another school, returning to their native\ncountry, or leaving to pursue a General Equivalency Diploma (GED)---none of which count as\ndropping out in the official statistics. Houston reported a citywide dropout rate of 1.5 percent\nin the year that was examined; 60 Minutes calculated that the true dropout rate was between 25\nand 50 percent. The statistical chicanery with test scores was every bit as impressive. One way to improve\ntest scores (in Houston or anywhere else) is to improve the quality of education so that\nstudents learn more and test better. This is a good thing. Another (less virtuous) way to\nimprove test scores is to prevent the worst students from taking the test. If the scores of the\nlowest-performing students are eliminated, the average test score for the school or district will\ngo up, even if all the rest of the students show no improvement at all. In Texas, the statewide\nachievement test is given in tenth grade. There was evidence that Houston schools were trying\nto keep the weakest students from reaching tenth grade. In one particularly egregious example,\na student spent three years in ninth grade and then was promoted straight to eleventh grade---a\ndeviously clever way of keeping a weak student from taking a tenth-grade benchmark exam\nwithout forcing him to drop out (which would have showed up on a different statistic). It’s not clear that Rod Paige was complicit in this statistical trickery during his tenure as\nHouston superintendent; however, he did implement a rigorous accountability program that\ngave cash bonuses to principals who met their dropout and test score goals and that fired or\ndemoted principals who failed to meet their targets. Principals definitely responded to the\nincentives; that’s the larger lesson. But you had better be darn certain that the folks being\nevaluated can’t make themselves look better (statistically) in ways that are not consistent with\nthe goal at hand.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\neducation because of his remarkable success in Houston in reducing the dropout rate and\nboosting test scores. If you’re keeping track of the little business aphorisms I keep tossing your way, here is\nanother one: “It’s never a good day when 60 Minutes shows up at your door.” Dan Rather and\nthe 60 Minutes II crew made a trip to Houston and found that the manipulation of statistics was\nfar more impressive than the educational improvement.8 High schools routinely classified\nstudents who quit high school as transferring to another school, returning to their native\ncountry, or leaving to pursue a General Equivalency Diploma (GED)---none of which count as\ndropping out in the official statistics. Houston reported a citywide dropout rate of 1.5 percent\nin the year that was examined; 60 Minutes calculated that the true dropout rate was between 25\nand 50 percent. The statistical chicanery with test scores was every bit as impressive. One way to improve\ntest scores (in Houston or anywhere else) is to improve the quality of education so that\nstudents learn more and test better. This is a good thing. Another (less virtuous) way to\nimprove test scores is to prevent the worst students from taking the test. If the scores of the\nlowest-performing students are eliminated, the average test score for the school or district will\ngo up, even if all the rest of the students show no improvement at all. In Texas, the statewide\nachievement test is given in tenth grade. There was evidence that Houston schools were trying\nto keep the weakest students from reaching tenth grade. In one particularly egregious example,\na student spent three years in ninth grade and then was promoted straight to eleventh grade---a\ndeviously clever way of keeping a weak student from taking a tenth-grade benchmark exam\nwithout forcing him to drop out (which would have showed up on a different statistic). It’s not clear that Rod Paige was complicit in this statistical trickery during his tenure as\nHouston superintendent; however, he did implement a rigorous accountability program that\ngave cash bonuses to principals who met their dropout and test score goals and that fired or\ndemoted principals who failed to meet their targets. Principals definitely responded to the\nincentives; that’s the larger lesson. But you had better be darn certain that the folks being\nevaluated can’t make themselves look better (statistically) in ways that are not consistent with\nthe goal at hand.", "tokens": 511, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 47, "segment_id": "00047", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000075"}
{"type": "chunk", "text": "It’s not clear that Rod Paige was complicit in this statistical trickery during his tenure as\nHouston superintendent; however, he did implement a rigorous accountability program that\ngave cash bonuses to principals who met their dropout and test score goals and that fired or\ndemoted principals who failed to meet their targets. Principals definitely responded to the\nincentives; that’s the larger lesson. But you had better be darn certain that the folks being\nevaluated can’t make themselves look better (statistically) in ways that are not consistent with\nthe goal at hand. The state of New York learned this the hard way. The state introduced “scorecards” that\nevaluate the mortality rates for the patients of cardiologists performing coronary angioplasty, a\ncommon treatment for heart disease.9 This seems like a perfectly reasonable and helpful use of\ndescriptive statistics. The proportion of a cardiologist’s patients who die in surgery is an\nimportant thing to know, and it makes sense for the government to collect and promulgate such\ndata since individual consumers would not otherwise have access to it. So is this a good\npolicy? Yes, other than the fact that it probably ended up killing people. Cardiologists obviously care about their “scorecard.” However, the easiest way for a\nsurgeon to improve his mortality rate is not by killing fewer people; presumably most doctors\nare already trying very hard to keep their patients alive. The easiest way for a doctor to\nimprove his mortality rate is by refusing to operate on the sickest patients. According to a\nsurvey conducted by the School of Medicine and Dentistry at the University of Rochester, the", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nIt’s not clear that Rod Paige was complicit in this statistical trickery during his tenure as\nHouston superintendent; however, he did implement a rigorous accountability program that\ngave cash bonuses to principals who met their dropout and test score goals and that fired or\ndemoted principals who failed to meet their targets. Principals definitely responded to the\nincentives; that’s the larger lesson. But you had better be darn certain that the folks being\nevaluated can’t make themselves look better (statistically) in ways that are not consistent with\nthe goal at hand. The state of New York learned this the hard way. The state introduced “scorecards” that\nevaluate the mortality rates for the patients of cardiologists performing coronary angioplasty, a\ncommon treatment for heart disease.9 This seems like a perfectly reasonable and helpful use of\ndescriptive statistics. The proportion of a cardiologist’s patients who die in surgery is an\nimportant thing to know, and it makes sense for the government to collect and promulgate such\ndata since individual consumers would not otherwise have access to it. So is this a good\npolicy? Yes, other than the fact that it probably ended up killing people. Cardiologists obviously care about their “scorecard.” However, the easiest way for a\nsurgeon to improve his mortality rate is not by killing fewer people; presumably most doctors\nare already trying very hard to keep their patients alive. The easiest way for a doctor to\nimprove his mortality rate is by refusing to operate on the sickest patients. According to a\nsurvey conducted by the School of Medicine and Dentistry at the University of Rochester, the", "tokens": 340, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 47, "segment_id": "00047", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000076"}
{"type": "chunk", "text": "scorecard, which ostensibly serves patients, can also work to their detriment: 83 percent of the\ncardiologists surveyed said that, because of the public mortality statistics, some patients who\nmight benefit from angioplasty might not receive the procedure; 79 percent of the doctors said\nthat some of their personal medical decisions had been influenced by the knowledge that\nmortality data are collected and made public. The sad paradox of this seemingly helpful\ndescriptive statistic is that cardiologists responded rationally by withholding care from the\npatients who needed it most. A statistical index has all the potential pitfalls of any descriptive statistic---plus the\ndistortions introduced by combining multiple indicators into a single number. By definition, any\nindex is going to be sensitive to how it is constructed; it will be affected both by what\nmeasures go into the index and by how each of those measures is weighted. For example, why\ndoes the NFL passer rating not include any measure of third down completions? And for the\nHuman Development Index, how should a country’s literacy rate be weighted in the index\nrelative to per capita income? In the end, the important question is whether the simplicity and\nease of use introduced by collapsing many indicators into a single number outweighs the\ninherent inaccuracy of the process. Sometimes that answer may be no, which brings us back (as\npromised) to the U.S. News & World Report (USNWR) college rankings. T h e USNWR rankings use sixteen indicators to score and rank America’s colleges,\nuniversities, and professional schools. In 2010, for example, the ranking of national universities\nand liberal arts colleges used “student selectivity” as 15 percent of the index; student\nselectivity is in turn calculated on the basis of a school’s acceptance rate, the proportion of the\nentering students who were in the top 10 percent of their high school class, and the average\nSAT and ACT scores of entering students. The benefit of the USNWR rankings is that they\nprovide lots of information about thousands of schools in a simple and accessible way. Even\nthe critics concede that much of the information collected on America’s colleges and\nuniversities is valuable. Prospective students should know an institution’s graduation rate and\nthe average class size. Of course, providing meaningful information is an enterprise entirely different from that of\ncollapsing all of that information into a single ranking that purports to be authoritative.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nscorecard, which ostensibly serves patients, can also work to their detriment: 83 percent of the\ncardiologists surveyed said that, because of the public mortality statistics, some patients who\nmight benefit from angioplasty might not receive the procedure; 79 percent of the doctors said\nthat some of their personal medical decisions had been influenced by the knowledge that\nmortality data are collected and made public. The sad paradox of this seemingly helpful\ndescriptive statistic is that cardiologists responded rationally by withholding care from the\npatients who needed it most. A statistical index has all the potential pitfalls of any descriptive statistic---plus the\ndistortions introduced by combining multiple indicators into a single number. By definition, any\nindex is going to be sensitive to how it is constructed; it will be affected both by what\nmeasures go into the index and by how each of those measures is weighted. For example, why\ndoes the NFL passer rating not include any measure of third down completions? And for the\nHuman Development Index, how should a country’s literacy rate be weighted in the index\nrelative to per capita income? In the end, the important question is whether the simplicity and\nease of use introduced by collapsing many indicators into a single number outweighs the\ninherent inaccuracy of the process. Sometimes that answer may be no, which brings us back (as\npromised) to the U.S. News & World Report (USNWR) college rankings. T h e USNWR rankings use sixteen indicators to score and rank America’s colleges,\nuniversities, and professional schools. In 2010, for example, the ranking of national universities\nand liberal arts colleges used “student selectivity” as 15 percent of the index; student\nselectivity is in turn calculated on the basis of a school’s acceptance rate, the proportion of the\nentering students who were in the top 10 percent of their high school class, and the average\nSAT and ACT scores of entering students. The benefit of the USNWR rankings is that they\nprovide lots of information about thousands of schools in a simple and accessible way. Even\nthe critics concede that much of the information collected on America’s colleges and\nuniversities is valuable. Prospective students should know an institution’s graduation rate and\nthe average class size. Of course, providing meaningful information is an enterprise entirely different from that of\ncollapsing all of that information into a single ranking that purports to be authoritative.", "tokens": 511, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 48, "segment_id": "00048", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000077"}
{"type": "chunk", "text": "The benefit of the USNWR rankings is that they\nprovide lots of information about thousands of schools in a simple and accessible way. Even\nthe critics concede that much of the information collected on America’s colleges and\nuniversities is valuable. Prospective students should know an institution’s graduation rate and\nthe average class size. Of course, providing meaningful information is an enterprise entirely different from that of\ncollapsing all of that information into a single ranking that purports to be authoritative. To\ncritics, the rankings are sloppily constructed, misleading, and detrimental to the long-term\ninterests of students. “One concern is simply about its being a list that claims to rank\ninstitutions in numerical order, which is a level of precision that those data just don’t support,”\nsays Michael McPherson, the former president of Macalester College in Minnesota.10 Why\nshould alumni giving count for 5 percent of a school’s score? And if it’s important, why does it\nnot count for ten percent? According to U.S. News & World Report, “Each indicator is assigned a weight (expressed\nas a percentage) based on our judgments about which measures of quality matter most.”11\nJudgment is one thing; arbitrariness is another. The most heavily weighted variable in the\nranking of national universities and colleges is “academic reputation.” This reputation is\ndetermined on the basis of a “peer assessment survey” filled out by administrators at other\ncolleges and universities and from a survey of high school guidance counselors. In his general", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThe benefit of the USNWR rankings is that they\nprovide lots of information about thousands of schools in a simple and accessible way. Even\nthe critics concede that much of the information collected on America’s colleges and\nuniversities is valuable. Prospective students should know an institution’s graduation rate and\nthe average class size. Of course, providing meaningful information is an enterprise entirely different from that of\ncollapsing all of that information into a single ranking that purports to be authoritative. To\ncritics, the rankings are sloppily constructed, misleading, and detrimental to the long-term\ninterests of students. “One concern is simply about its being a list that claims to rank\ninstitutions in numerical order, which is a level of precision that those data just don’t support,”\nsays Michael McPherson, the former president of Macalester College in Minnesota.10 Why\nshould alumni giving count for 5 percent of a school’s score? And if it’s important, why does it\nnot count for ten percent? According to U.S. News & World Report, “Each indicator is assigned a weight (expressed\nas a percentage) based on our judgments about which measures of quality matter most.”11\nJudgment is one thing; arbitrariness is another. The most heavily weighted variable in the\nranking of national universities and colleges is “academic reputation.” This reputation is\ndetermined on the basis of a “peer assessment survey” filled out by administrators at other\ncolleges and universities and from a survey of high school guidance counselors. In his general", "tokens": 324, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 48, "segment_id": "00048", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000078"}
{"type": "chunk", "text": "critique of rankings, Malcolm Gladwell offers a scathing (though humorous) indictment of the\npeer assessment methodology. He cites a questionnaire sent out by a former chief justice of the\nMichigan Supreme Court to roughly one hundred lawyers asking them to rank ten law schools\nin order of quality. Penn State’s was one of the law schools on the list; the lawyers ranked it\nnear the middle. At the time, Penn State did not have a law school.12\n\nFor all the data collected by USNWR, it’s not obvious that the rankings measure what\nprospective students ought to care about: How much learning is going on at any given\ninstitution? Football fans may quibble about the composition of the passer index, but no one\ncan deny that its component parts---completions, yardage, touchdowns, and interceptions---are\nan important part of a quarterback’s overall performance. That is not necessarily the case with\nthe USNWR criteria, most of which focus on inputs (e.g., what kind of students are admitted,\nhow much faculty are paid, the percentage of faculty who are full-time) rather than educational\noutputs. Two notable exceptions are the freshman retention rate and the graduation rate, but\neven those indicators do not measure learning. As Michael McPherson points out, “We don’t\nreally learn anything from U.S. News about whether the education they got during those four\nyears actually improved their talents or enriched their knowledge.”\n\nAll of this would still be a harmless exercise, but for the fact that it appears to encourage\nbehavior that is not necessarily good for students or higher education. For example, one\nstatistic used to calculate the rankings is financial resources per student; the problem is that\nthere is no corresponding measure of how well that money is being spent. An institution that\nspends less money to better effect (and therefore can charge lower tuition) is punished in the\nranking process. Colleges and universities also have an incentive to encourage large numbers\nof students to apply, including those with no realistic hope of getting in, because it makes the\nschool appear more selective. This is a waste of resources for the schools soliciting bogus\napplications and for students who end up applying with no meaningful chance of being\naccepted. Since we are about to move on to a chapter on probability, I will bet that the U.S. News &\nWorld Report rankings are not going away anytime soon.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\ncritique of rankings, Malcolm Gladwell offers a scathing (though humorous) indictment of the\npeer assessment methodology. He cites a questionnaire sent out by a former chief justice of the\nMichigan Supreme Court to roughly one hundred lawyers asking them to rank ten law schools\nin order of quality. Penn State’s was one of the law schools on the list; the lawyers ranked it\nnear the middle. At the time, Penn State did not have a law school.12\n\nFor all the data collected by USNWR, it’s not obvious that the rankings measure what\nprospective students ought to care about: How much learning is going on at any given\ninstitution? Football fans may quibble about the composition of the passer index, but no one\ncan deny that its component parts---completions, yardage, touchdowns, and interceptions---are\nan important part of a quarterback’s overall performance. That is not necessarily the case with\nthe USNWR criteria, most of which focus on inputs (e.g., what kind of students are admitted,\nhow much faculty are paid, the percentage of faculty who are full-time) rather than educational\noutputs. Two notable exceptions are the freshman retention rate and the graduation rate, but\neven those indicators do not measure learning. As Michael McPherson points out, “We don’t\nreally learn anything from U.S. News about whether the education they got during those four\nyears actually improved their talents or enriched their knowledge.”\n\nAll of this would still be a harmless exercise, but for the fact that it appears to encourage\nbehavior that is not necessarily good for students or higher education. For example, one\nstatistic used to calculate the rankings is financial resources per student; the problem is that\nthere is no corresponding measure of how well that money is being spent. An institution that\nspends less money to better effect (and therefore can charge lower tuition) is punished in the\nranking process. Colleges and universities also have an incentive to encourage large numbers\nof students to apply, including those with no realistic hope of getting in, because it makes the\nschool appear more selective. This is a waste of resources for the schools soliciting bogus\napplications and for students who end up applying with no meaningful chance of being\naccepted. Since we are about to move on to a chapter on probability, I will bet that the U.S. News &\nWorld Report rankings are not going away anytime soon.", "tokens": 497, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 49, "segment_id": "00049", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000079"}
{"type": "chunk", "text": "Colleges and universities also have an incentive to encourage large numbers\nof students to apply, including those with no realistic hope of getting in, because it makes the\nschool appear more selective. This is a waste of resources for the schools soliciting bogus\napplications and for students who end up applying with no meaningful chance of being\naccepted. Since we are about to move on to a chapter on probability, I will bet that the U.S. News &\nWorld Report rankings are not going away anytime soon. As Leon Botstein, president of Bard\nCollege, has pointed out, “People love easy answers. What is the best place? Number 1.”13\n\nThe overall lesson of this chapter is that statistical malfeasance has very little to do with bad\nmath. If anything, impressive calculations can obscure nefarious motives. The fact that you’ve\ncalculated the mean correctly will not alter the fact that the median is a more accurate\nindicator. Judgment and integrity turn out to be surprisingly important. A detailed knowledge\nof statistics does not deter wrongdoing any more than a detailed knowledge of the law averts\ncriminal behavior. With both statistics and crime, the bad guys often know exactly what they’re\ndoing! * Twain attributed this phrase to British prime minister Benjamin Disraeli, but there is no record of Disraeli’s ever saying\nor writing it. * Available at http://www.bls.gov/data/inflation_calculator.htm.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nColleges and universities also have an incentive to encourage large numbers\nof students to apply, including those with no realistic hope of getting in, because it makes the\nschool appear more selective. This is a waste of resources for the schools soliciting bogus\napplications and for students who end up applying with no meaningful chance of being\naccepted. Since we are about to move on to a chapter on probability, I will bet that the U.S. News &\nWorld Report rankings are not going away anytime soon. As Leon Botstein, president of Bard\nCollege, has pointed out, “People love easy answers. What is the best place? Number 1.”13\n\nThe overall lesson of this chapter is that statistical malfeasance has very little to do with bad\nmath. If anything, impressive calculations can obscure nefarious motives. The fact that you’ve\ncalculated the mean correctly will not alter the fact that the median is a more accurate\nindicator. Judgment and integrity turn out to be surprisingly important. A detailed knowledge\nof statistics does not deter wrongdoing any more than a detailed knowledge of the law averts\ncriminal behavior. With both statistics and crime, the bad guys often know exactly what they’re\ndoing! * Twain attributed this phrase to British prime minister Benjamin Disraeli, but there is no record of Disraeli’s ever saying\nor writing it. * Available at http://www.bls.gov/data/inflation_calculator.htm.", "tokens": 295, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 49, "segment_id": "00049", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000080"}
{"type": "chunk", "text": "CHAPTER 4\nCorrelation\nHow does Netflix know what movies I like? Netflix insists that I’ll like the film Bhutto, a documentary that offers an “in-depth and at\n\ntimes incendiary look at the life and tragic death of former Pakistani prime minister Benazir\nBhutto.” I probably will like the film Bhutto. (I’ve added it to my queue.) The Netflix\nrecommendations that I’ve watched in the past have been terrific. And when a film is\nrecommended that I’ve already seen, it’s typically one I’ve really enjoyed. How does Netflix do that? Is there some massive team of interns at corporate headquarters\nwho have used a combination of Google and interviews with my family and friends to\ndetermine that I might like a documentary about a former Pakistani prime minister? Of course\nnot. Netflix has merely mastered some very sophisticated statistics. Netflix doesn’t know me. But it does know what films I’ve liked in the past (because I’ve rated them). Using that\ninformation, along with ratings from other customers and a powerful computer, Netflix can\nmake shockingly accurate predictions about my tastes. I’ll come back to the specific Netflix algorithm for making these picks; for now, the\nimportant point is that it’s all based on correlation. Netflix recommends movies that are similar\nto other films that I’ve liked; it also recommends films that have been highly rated by other\ncustomers whose ratings are similar to mine. Bhutto was recommended because of my five-star\nratings for two other documentaries, Enron: The Smartest Guys in the Room and Fog of War. Correlation measures the degree to which two phenomena are related to one another. For\nexample, there is a correlation between summer temperatures and ice cream sales. When one\ngoes up, so does the other. Two variables are positively correlated if a change in one is\nassociated with a change in the other in the same direction, such as the relationship between\nheight and weight. Taller people weigh more (on average); shorter people weigh less. A\ncorrelation is negative if a positive change in one variable is associated with a negative change\nin the other, such as the relationship between exercise and weight. The tricky thing about these kinds of associations is that not every observation fits the\npattern. Sometimes short people weigh more than tall people. Sometimes people who don’t\nexercise are skinnier than people who exercise all the time.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nCHAPTER 4\nCorrelation\nHow does Netflix know what movies I like? Netflix insists that I’ll like the film Bhutto, a documentary that offers an “in-depth and at\n\ntimes incendiary look at the life and tragic death of former Pakistani prime minister Benazir\nBhutto.” I probably will like the film Bhutto. (I’ve added it to my queue.) The Netflix\nrecommendations that I’ve watched in the past have been terrific. And when a film is\nrecommended that I’ve already seen, it’s typically one I’ve really enjoyed. How does Netflix do that? Is there some massive team of interns at corporate headquarters\nwho have used a combination of Google and interviews with my family and friends to\ndetermine that I might like a documentary about a former Pakistani prime minister? Of course\nnot. Netflix has merely mastered some very sophisticated statistics. Netflix doesn’t know me. But it does know what films I’ve liked in the past (because I’ve rated them). Using that\ninformation, along with ratings from other customers and a powerful computer, Netflix can\nmake shockingly accurate predictions about my tastes. I’ll come back to the specific Netflix algorithm for making these picks; for now, the\nimportant point is that it’s all based on correlation. Netflix recommends movies that are similar\nto other films that I’ve liked; it also recommends films that have been highly rated by other\ncustomers whose ratings are similar to mine. Bhutto was recommended because of my five-star\nratings for two other documentaries, Enron: The Smartest Guys in the Room and Fog of War. Correlation measures the degree to which two phenomena are related to one another. For\nexample, there is a correlation between summer temperatures and ice cream sales. When one\ngoes up, so does the other. Two variables are positively correlated if a change in one is\nassociated with a change in the other in the same direction, such as the relationship between\nheight and weight. Taller people weigh more (on average); shorter people weigh less. A\ncorrelation is negative if a positive change in one variable is associated with a negative change\nin the other, such as the relationship between exercise and weight. The tricky thing about these kinds of associations is that not every observation fits the\npattern. Sometimes short people weigh more than tall people. Sometimes people who don’t\nexercise are skinnier than people who exercise all the time.", "tokens": 499, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 50, "segment_id": "00050", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000081"}
{"type": "chunk", "text": "Two variables are positively correlated if a change in one is\nassociated with a change in the other in the same direction, such as the relationship between\nheight and weight. Taller people weigh more (on average); shorter people weigh less. A\ncorrelation is negative if a positive change in one variable is associated with a negative change\nin the other, such as the relationship between exercise and weight. The tricky thing about these kinds of associations is that not every observation fits the\npattern. Sometimes short people weigh more than tall people. Sometimes people who don’t\nexercise are skinnier than people who exercise all the time. Still, there is a meaningful\nrelationship between height and weight, and between exercise and weight. If we were to do a scatter plot of the heights and weights of a random sample of American\n\nadults, we would expect to see something like the following:\n\nScatter Plot for Height and Weight", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nTwo variables are positively correlated if a change in one is\nassociated with a change in the other in the same direction, such as the relationship between\nheight and weight. Taller people weigh more (on average); shorter people weigh less. A\ncorrelation is negative if a positive change in one variable is associated with a negative change\nin the other, such as the relationship between exercise and weight. The tricky thing about these kinds of associations is that not every observation fits the\npattern. Sometimes short people weigh more than tall people. Sometimes people who don’t\nexercise are skinnier than people who exercise all the time. Still, there is a meaningful\nrelationship between height and weight, and between exercise and weight. If we were to do a scatter plot of the heights and weights of a random sample of American\n\nadults, we would expect to see something like the following:\n\nScatter Plot for Height and Weight", "tokens": 186, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 50, "segment_id": "00050", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000082"}
{"type": "chunk", "text": "If we were to create a scatter plot of the association between exercise (as measured by\nminutes of intensive exercise per week) and weight, we would expect a negative correlation,\nwith those who exercise more tending to weigh less. But a pattern consisting of dots scattered\nacross the page is a somewhat unwieldy tool. (If Netflix tried to make film recommendations\nfor me by plotting the ratings for thousands of films by millions of customers, the results would\nbury the headquarters in scatter plots.) Instead, the power of correlation as a statistical tool is\nthat we can encapsulate an association between two variables in a single descriptive statistic:\nthe correlation coefficient. The correlation coefficient has two fabulously attractive characteristics. First, for math\nreasons that have been relegated to the appendix, it is a single number ranging from --1 to 1. A\ncorrelation of 1, often described as perfect correlation, means that every change in one variable\nis associated with an equivalent change in the other variable in the same direction. A correlation of --1, or perfect negative correlation, means that every change in one variable\n\nis associated with an equivalent change in the other variable in the opposite direction. The closer the correlation is to 1 or --1, the stronger the association. A correlation of 0 (or\nclose to it) means that the variables have no meaningful association with one another, such as\nthe relationship between shoe size and SAT scores. The second attractive feature of the correlation coefficient is that it has no units attached to\nit. We can calculate the correlation between height and weight---even though height is\nmeasured in inches and weight is measured in pounds. We can even calculate the correlation\nbetween the number of televisions high school students have in their homes and their SAT\nscores, which I assure you will be positive. (More on that relationship in a moment.) The\ncorrelation coefficient does a seemingly miraculous thing: It collapses a complex mess of data\nmeasured in different units (like our scatter plots of height and weight) into a single, elegant\ndescriptive statistic. How? As usual, I’ve put the most common formula for calculating the correlation coefficient in the\nappendix at the end of the chapter. This is not a statistic that you are going to be calculating by\nhand. (After you’ve entered the data, a basic software package like Microsoft Excel will\ncalculate the correlation between two variables.) Still, the intuition is not that difficult.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nIf we were to create a scatter plot of the association between exercise (as measured by\nminutes of intensive exercise per week) and weight, we would expect a negative correlation,\nwith those who exercise more tending to weigh less. But a pattern consisting of dots scattered\nacross the page is a somewhat unwieldy tool. (If Netflix tried to make film recommendations\nfor me by plotting the ratings for thousands of films by millions of customers, the results would\nbury the headquarters in scatter plots.) Instead, the power of correlation as a statistical tool is\nthat we can encapsulate an association between two variables in a single descriptive statistic:\nthe correlation coefficient. The correlation coefficient has two fabulously attractive characteristics. First, for math\nreasons that have been relegated to the appendix, it is a single number ranging from --1 to 1. A\ncorrelation of 1, often described as perfect correlation, means that every change in one variable\nis associated with an equivalent change in the other variable in the same direction. A correlation of --1, or perfect negative correlation, means that every change in one variable\n\nis associated with an equivalent change in the other variable in the opposite direction. The closer the correlation is to 1 or --1, the stronger the association. A correlation of 0 (or\nclose to it) means that the variables have no meaningful association with one another, such as\nthe relationship between shoe size and SAT scores. The second attractive feature of the correlation coefficient is that it has no units attached to\nit. We can calculate the correlation between height and weight---even though height is\nmeasured in inches and weight is measured in pounds. We can even calculate the correlation\nbetween the number of televisions high school students have in their homes and their SAT\nscores, which I assure you will be positive. (More on that relationship in a moment.) The\ncorrelation coefficient does a seemingly miraculous thing: It collapses a complex mess of data\nmeasured in different units (like our scatter plots of height and weight) into a single, elegant\ndescriptive statistic. How? As usual, I’ve put the most common formula for calculating the correlation coefficient in the\nappendix at the end of the chapter. This is not a statistic that you are going to be calculating by\nhand. (After you’ve entered the data, a basic software package like Microsoft Excel will\ncalculate the correlation between two variables.) Still, the intuition is not that difficult.", "tokens": 505, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 51, "segment_id": "00051", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000083"}
{"type": "chunk", "text": "1. Calculates the mean and standard deviation for both variables. If we stick with the height\nand weight example, we would then know the mean height for people in the sample, the\nmean weight for people in the sample, and the standard deviation for both height and\nweight. 2. Converts all the data so that each observation is represented by its distance (in standard\ndeviations) from the mean. Stick with me; it’s not that complicated. Suppose that the\nmean height in the sample is 66 inches (with a standard deviation of 5 inches) and that\nthe mean weight is 177 pounds (with a standard deviation of 10 pounds). Now suppose\nthat you are 72 inches tall and weigh 168 pounds. We can also say that you your height\nis 1.2 standard deviations above the mean in height [(72 -- 66)/5)] and .9 standard\ndeviations below the mean in weight, or --0.9 for purposes of the formula [(168 --\n177)/10]. Yes, it’s unusual for someone to be above the mean in height and below the\nmean in weight, but since you’ve paid good money for this book, I figured I should at\nleast make you tall and thin. Notice that your height and weight, formerly in inches and\npounds, have been reduced to 1.2 and --0.9. This is what makes the units go away. 3. Here I’ll wave my hands and let the computer do the work. The formula then calculates\nthe relationship between height and weight across all the individuals in the sample as\nmeasured by standard units. When individuals in the sample are tall, say, 1.5 or 2\nstandard deviations above the mean, what do their weights tend to be as measured in\nstandard deviations from the mean for weight? And when individuals are near to the\nmean in terms of height, what are their weights as measured in standard units? If the distance from the mean for one variable tends to be broadly consistent with distance\nfrom the mean for the other variable (e.g., people who are far from the mean for height in either\ndirection tend also to be far from the mean in the same direction for weight), then we would\nexpect a strong positive correlation.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\n1. Calculates the mean and standard deviation for both variables. If we stick with the height\nand weight example, we would then know the mean height for people in the sample, the\nmean weight for people in the sample, and the standard deviation for both height and\nweight. 2. Converts all the data so that each observation is represented by its distance (in standard\ndeviations) from the mean. Stick with me; it’s not that complicated. Suppose that the\nmean height in the sample is 66 inches (with a standard deviation of 5 inches) and that\nthe mean weight is 177 pounds (with a standard deviation of 10 pounds). Now suppose\nthat you are 72 inches tall and weigh 168 pounds. We can also say that you your height\nis 1.2 standard deviations above the mean in height [(72 -- 66)/5)] and .9 standard\ndeviations below the mean in weight, or --0.9 for purposes of the formula [(168 --\n177)/10]. Yes, it’s unusual for someone to be above the mean in height and below the\nmean in weight, but since you’ve paid good money for this book, I figured I should at\nleast make you tall and thin. Notice that your height and weight, formerly in inches and\npounds, have been reduced to 1.2 and --0.9. This is what makes the units go away. 3. Here I’ll wave my hands and let the computer do the work. The formula then calculates\nthe relationship between height and weight across all the individuals in the sample as\nmeasured by standard units. When individuals in the sample are tall, say, 1.5 or 2\nstandard deviations above the mean, what do their weights tend to be as measured in\nstandard deviations from the mean for weight? And when individuals are near to the\nmean in terms of height, what are their weights as measured in standard units? If the distance from the mean for one variable tends to be broadly consistent with distance\nfrom the mean for the other variable (e.g., people who are far from the mean for height in either\ndirection tend also to be far from the mean in the same direction for weight), then we would\nexpect a strong positive correlation.", "tokens": 472, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 52, "segment_id": "00052", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000084"}
{"type": "chunk", "text": "And when individuals are near to the\nmean in terms of height, what are their weights as measured in standard units? If the distance from the mean for one variable tends to be broadly consistent with distance\nfrom the mean for the other variable (e.g., people who are far from the mean for height in either\ndirection tend also to be far from the mean in the same direction for weight), then we would\nexpect a strong positive correlation. If distance from the mean for one variable tends to correspond to a similar distance from the\nmean for the second variable in the other direction (e.g., people who are far above the mean in\nterms of exercise tend to be far below the mean in terms of weight), then we would expect a\nstrong negative correlation. If two variables do not tend to deviate from the mean in any meaningful pattern (e.g., shoe\n\nsize and exercise) then we would expect little or no correlation. You suffered mightily in that section; we’ll get back to film rentals soon. Before we return to\nNetflix, however, let’s reflect on another aspect of life where correlation matters: the SAT. Yes, that SAT. The SAT Reasoning Test, formerly known as the Scholastic Aptitude Test, is a\nstandardized exam made up of three sections: math, reading, and writing. You probably took\nthe SAT, or will soon. You probably did not reflect deeply on why you had to take the SAT. The purpose of the test is to measure academic ability and predict college performance. Of\ncourse, one might reasonably ask (particularly those who don’t like standardized tests): Isn’t\nthat what high school is for? Why is a four-hour test so important when college admissions\nofficers have access to four years of high school grades? The answer to those questions is lurking back in Chapters 1 and 2. High school grades are", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nAnd when individuals are near to the\nmean in terms of height, what are their weights as measured in standard units? If the distance from the mean for one variable tends to be broadly consistent with distance\nfrom the mean for the other variable (e.g., people who are far from the mean for height in either\ndirection tend also to be far from the mean in the same direction for weight), then we would\nexpect a strong positive correlation. If distance from the mean for one variable tends to correspond to a similar distance from the\nmean for the second variable in the other direction (e.g., people who are far above the mean in\nterms of exercise tend to be far below the mean in terms of weight), then we would expect a\nstrong negative correlation. If two variables do not tend to deviate from the mean in any meaningful pattern (e.g., shoe\n\nsize and exercise) then we would expect little or no correlation. You suffered mightily in that section; we’ll get back to film rentals soon. Before we return to\nNetflix, however, let’s reflect on another aspect of life where correlation matters: the SAT. Yes, that SAT. The SAT Reasoning Test, formerly known as the Scholastic Aptitude Test, is a\nstandardized exam made up of three sections: math, reading, and writing. You probably took\nthe SAT, or will soon. You probably did not reflect deeply on why you had to take the SAT. The purpose of the test is to measure academic ability and predict college performance. Of\ncourse, one might reasonably ask (particularly those who don’t like standardized tests): Isn’t\nthat what high school is for? Why is a four-hour test so important when college admissions\nofficers have access to four years of high school grades? The answer to those questions is lurking back in Chapters 1 and 2. High school grades are", "tokens": 385, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 52, "segment_id": "00052", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000085"}
{"type": "chunk", "text": "an imperfect descriptive statistic. A student who gets mediocre grades while taking a tough\nschedule of math and science classes may have more academic ability and potential than a\nstudent at the same school with better grades in less challenging classes. Obviously there are\neven larger potential discrepancies across schools. According to the College Board, which\nproduces and administers the SAT, the test was created to “democratize access to college for\nall students.” Fair enough. The SAT offers a standardized measure of ability that can be\ncompared easily across all students applying to college. But is it a good measure of ability? If\nwe want a metric that can be compared easily across students, we could also have all high\nschool seniors run the 100 yard dash, which is cheaper and easier than administering the SAT. The problem, of course, is that performance in the 100 yard dash is uncorrelated with college\nperformance. It’s easy to get the data; they just won’t tell us anything meaningful. So how well does the SAT fare in this regard? Sadly for future generations of high school\nstudents, the SAT does a reasonably good job of predicting first-year college grades. The\nCollege Board publishes the relevant correlations. On a scale of 0 (no correlation at all) to 1\n(perfect correlation), the correlation between high school grade point average and first-year\ncollege grade point average is .56. (To put that in perspective, the correlation between height\nand weight for adult men in the United States is about .4.) The correlation between the SAT\ncomposite score (critical reading, math, and writing) and first-year college GPA is also .56. 1\nThat would seem to argue for ditching the SAT, as the test does not seem to do any better at\npredicting college performance than high school grades. In fact, the best predictor of all is a\ncombination of SAT scores and high school GPA, which has a correlation of .64 with first-year\ncollege grades. Sorry about that. One crucial point in this general discussion is that correlation does not imply causation; a\npositive or negative association between two variables does not necessarily mean that a change\nin one of the variables is causing the change in the other. For example, I alluded earlier to a\nlikely positive correlation between a student’s SAT scores and the number of televisions that\nhis family owns.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nan imperfect descriptive statistic. A student who gets mediocre grades while taking a tough\nschedule of math and science classes may have more academic ability and potential than a\nstudent at the same school with better grades in less challenging classes. Obviously there are\neven larger potential discrepancies across schools. According to the College Board, which\nproduces and administers the SAT, the test was created to “democratize access to college for\nall students.” Fair enough. The SAT offers a standardized measure of ability that can be\ncompared easily across all students applying to college. But is it a good measure of ability? If\nwe want a metric that can be compared easily across students, we could also have all high\nschool seniors run the 100 yard dash, which is cheaper and easier than administering the SAT. The problem, of course, is that performance in the 100 yard dash is uncorrelated with college\nperformance. It’s easy to get the data; they just won’t tell us anything meaningful. So how well does the SAT fare in this regard? Sadly for future generations of high school\nstudents, the SAT does a reasonably good job of predicting first-year college grades. The\nCollege Board publishes the relevant correlations. On a scale of 0 (no correlation at all) to 1\n(perfect correlation), the correlation between high school grade point average and first-year\ncollege grade point average is .56. (To put that in perspective, the correlation between height\nand weight for adult men in the United States is about .4.) The correlation between the SAT\ncomposite score (critical reading, math, and writing) and first-year college GPA is also .56. 1\nThat would seem to argue for ditching the SAT, as the test does not seem to do any better at\npredicting college performance than high school grades. In fact, the best predictor of all is a\ncombination of SAT scores and high school GPA, which has a correlation of .64 with first-year\ncollege grades. Sorry about that. One crucial point in this general discussion is that correlation does not imply causation; a\npositive or negative association between two variables does not necessarily mean that a change\nin one of the variables is causing the change in the other. For example, I alluded earlier to a\nlikely positive correlation between a student’s SAT scores and the number of televisions that\nhis family owns.", "tokens": 491, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 53, "segment_id": "00053", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000086"}
{"type": "chunk", "text": "In fact, the best predictor of all is a\ncombination of SAT scores and high school GPA, which has a correlation of .64 with first-year\ncollege grades. Sorry about that. One crucial point in this general discussion is that correlation does not imply causation; a\npositive or negative association between two variables does not necessarily mean that a change\nin one of the variables is causing the change in the other. For example, I alluded earlier to a\nlikely positive correlation between a student’s SAT scores and the number of televisions that\nhis family owns. This does not mean that overeager parents can boost their children’s test\nscores by buying an extra five televisions for the house. Nor does it likely mean that watching\nlots of television is good for academic achievement. The most logical explanation for such a correlation would be that highly educated parents\ncan afford a lot of televisions and tend to have children who test better than average. Both the\ntelevisions and the test scores are likely caused by a third variable, which is parental\neducation. I can’t prove the correlation between TVs in the home and SAT scores. (The\nCollege Board does not provide such data.) However, I can prove that students in wealthy\nfamilies have higher mean SAT scores than students in less wealthy families. According to the\nCollege Board, students with a family income over $200,000 have a mean SAT math score of\n586, compared with a mean SAT math score of 460 for students with a family income of\n$20,000 or less.2 Meanwhile, it’s also likely that families with incomes over $200,000 have\nmore televisions in their (multiple) homes than families with incomes of $20,000 or less. I began writing this chapter many days ago. Since then, I’ve had a chance to watch the", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nIn fact, the best predictor of all is a\ncombination of SAT scores and high school GPA, which has a correlation of .64 with first-year\ncollege grades. Sorry about that. One crucial point in this general discussion is that correlation does not imply causation; a\npositive or negative association between two variables does not necessarily mean that a change\nin one of the variables is causing the change in the other. For example, I alluded earlier to a\nlikely positive correlation between a student’s SAT scores and the number of televisions that\nhis family owns. This does not mean that overeager parents can boost their children’s test\nscores by buying an extra five televisions for the house. Nor does it likely mean that watching\nlots of television is good for academic achievement. The most logical explanation for such a correlation would be that highly educated parents\ncan afford a lot of televisions and tend to have children who test better than average. Both the\ntelevisions and the test scores are likely caused by a third variable, which is parental\neducation. I can’t prove the correlation between TVs in the home and SAT scores. (The\nCollege Board does not provide such data.) However, I can prove that students in wealthy\nfamilies have higher mean SAT scores than students in less wealthy families. According to the\nCollege Board, students with a family income over $200,000 have a mean SAT math score of\n586, compared with a mean SAT math score of 460 for students with a family income of\n$20,000 or less.2 Meanwhile, it’s also likely that families with incomes over $200,000 have\nmore televisions in their (multiple) homes than families with incomes of $20,000 or less. I began writing this chapter many days ago. Since then, I’ve had a chance to watch the", "tokens": 377, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 53, "segment_id": "00053", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000087"}
{"type": "chunk", "text": "documentary film Bhutto. Wow! This is a remarkable film about a remarkable family. The\noriginal footage, stretching all the way from the partition of India and Pakistan in 1947 to the\nassassination of Benazir Bhutto in 2007, is extraordinary. Bhutto’s voice is woven effectively\nthroughout the film in the form of speeches and interviews. Anyway, I gave the film five stars,\nwhich is pretty much what Netflix predicted. At the most basic level, Netflix is exploiting the concept of correlation. First, I rate a set of\nfilms. Netflix compares my ratings with those of other customers to identify those whose\nratings are highly correlated with mine. Those customers tend to like the films that I like. Once\nthat is established, Netflix can recommend films that like-minded customers have rated highly\nbut that I have not yet seen. That’s the “big picture.” The actual methodology is much more complex. In fact, Netflix\nlaunched a contest in 2006 in which members of the public were invited to design a mechanism\nthat improved on existing Netflix recommendations by at least 10 percent (meaning that the\nsystem was 10 percent more accurate in predicting how a customer would rate a film after\nseeing it). The winner would get $1,000,000. Every individual or team that registered for the contest was given “training data” consisting\nof more than 100 million ratings of 18,000 films by 480,000 Netflix customers. A separate set\nof 2.8 million ratings was “withheld,” meaning that Netflix knew how the customers rated these\nfilms but the contest participants did not. The competitors were judged on how well their\nalgorithms predicted the actual customer reviews for these withheld films. Over three years,\nthousands of teams from over 180 countries submitted proposals. There were two requirements\nfor entry. First, the winner had to license the algorithm to Netflix. And second, the winner had\nto “describe to the world how you did it and why it works.”3\n\nIn 2009 Netflix announced a winner: a seven-person team made up of statisticians and\ncomputer scientists from the United States, Austria, Canada, and Israel. Alas, I cannot describe\nthe winning system, even in an appendix. The paper explaining the system is ninety-two pages\nlong.* I’m impressed by the quality of the Netflix recommendations.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\ndocumentary film Bhutto. Wow! This is a remarkable film about a remarkable family. The\noriginal footage, stretching all the way from the partition of India and Pakistan in 1947 to the\nassassination of Benazir Bhutto in 2007, is extraordinary. Bhutto’s voice is woven effectively\nthroughout the film in the form of speeches and interviews. Anyway, I gave the film five stars,\nwhich is pretty much what Netflix predicted. At the most basic level, Netflix is exploiting the concept of correlation. First, I rate a set of\nfilms. Netflix compares my ratings with those of other customers to identify those whose\nratings are highly correlated with mine. Those customers tend to like the films that I like. Once\nthat is established, Netflix can recommend films that like-minded customers have rated highly\nbut that I have not yet seen. That’s the “big picture.” The actual methodology is much more complex. In fact, Netflix\nlaunched a contest in 2006 in which members of the public were invited to design a mechanism\nthat improved on existing Netflix recommendations by at least 10 percent (meaning that the\nsystem was 10 percent more accurate in predicting how a customer would rate a film after\nseeing it). The winner would get $1,000,000. Every individual or team that registered for the contest was given “training data” consisting\nof more than 100 million ratings of 18,000 films by 480,000 Netflix customers. A separate set\nof 2.8 million ratings was “withheld,” meaning that Netflix knew how the customers rated these\nfilms but the contest participants did not. The competitors were judged on how well their\nalgorithms predicted the actual customer reviews for these withheld films. Over three years,\nthousands of teams from over 180 countries submitted proposals. There were two requirements\nfor entry. First, the winner had to license the algorithm to Netflix. And second, the winner had\nto “describe to the world how you did it and why it works.”3\n\nIn 2009 Netflix announced a winner: a seven-person team made up of statisticians and\ncomputer scientists from the United States, Austria, Canada, and Israel. Alas, I cannot describe\nthe winning system, even in an appendix. The paper explaining the system is ninety-two pages\nlong.* I’m impressed by the quality of the Netflix recommendations.", "tokens": 493, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 54, "segment_id": "00054", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000088"}
{"type": "chunk", "text": "There were two requirements\nfor entry. First, the winner had to license the algorithm to Netflix. And second, the winner had\nto “describe to the world how you did it and why it works.”3\n\nIn 2009 Netflix announced a winner: a seven-person team made up of statisticians and\ncomputer scientists from the United States, Austria, Canada, and Israel. Alas, I cannot describe\nthe winning system, even in an appendix. The paper explaining the system is ninety-two pages\nlong.* I’m impressed by the quality of the Netflix recommendations. Still, the system is just a\nsuper fancy variation on what people have been doing since the dawn of film: find someone\nwith similar tastes and ask for a recommendation. You tend to like what I like, and to dislike\nwhat I dislike, so what did you think of the new George Clooney film? That is the essence of correlation. APPENDIX TO CHAPTER 4\nTo calculate the correlation coefficient between two sets of numbers, you would perform the\nfollowing steps, each of which is illustrated by use of the data on heights and weights for 15\nhypothetical students in the table below. 1. Convert the height of each student to standard units: (height -- mean)/standard deviation. 2. Convert the weight of each student to standard units: (weight -- mean)/standard deviation. 3. Calculate the product for each student of (weight in standard units) × (height in\nstandard units). You should see that this number will be largest in absolute value when a", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThere were two requirements\nfor entry. First, the winner had to license the algorithm to Netflix. And second, the winner had\nto “describe to the world how you did it and why it works.”3\n\nIn 2009 Netflix announced a winner: a seven-person team made up of statisticians and\ncomputer scientists from the United States, Austria, Canada, and Israel. Alas, I cannot describe\nthe winning system, even in an appendix. The paper explaining the system is ninety-two pages\nlong.* I’m impressed by the quality of the Netflix recommendations. Still, the system is just a\nsuper fancy variation on what people have been doing since the dawn of film: find someone\nwith similar tastes and ask for a recommendation. You tend to like what I like, and to dislike\nwhat I dislike, so what did you think of the new George Clooney film? That is the essence of correlation. APPENDIX TO CHAPTER 4\nTo calculate the correlation coefficient between two sets of numbers, you would perform the\nfollowing steps, each of which is illustrated by use of the data on heights and weights for 15\nhypothetical students in the table below. 1. Convert the height of each student to standard units: (height -- mean)/standard deviation. 2. Convert the weight of each student to standard units: (weight -- mean)/standard deviation. 3. Calculate the product for each student of (weight in standard units) × (height in\nstandard units). You should see that this number will be largest in absolute value when a", "tokens": 322, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 54, "segment_id": "00054", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000089"}
{"type": "chunk", "text": "student’s height and weight are both relatively far from the mean. 4. The correlation coefficient is the sum of the products calculated above divided by the\nnumber of observations (15 in this case). The correlation between height and weight for\nthis group of students is .83. Given that the correlation coefficient can range from --1 to\n1, this is a relatively high degree of positive correlation, as we would expect with height\nand weight. The formula for calculating the correlation coefficient requires a little detour with regard to\nnotation. The figure ∑, known as the summation sign, is a handy character in statistics. It\nrepresents the summation of the quantity that comes after it. For example, if there is a set of\nobservations x1, x2, x3, and x4, then ∑ (xi) tells us that we should sum the four observations: x1\n+ x2 + x3 + x4. Thus, ∑ (xi) = x1 + x2 + x3 + x4. Our formula for the mean of a set of i\nobservations could be represented as the following: mean = ∑ (xi)/n. We can make the formula even more adaptable by writing\n\n, which sums the quantity x1 +\nx2 + x3 + . . . xn, or, in other words, all the terms beginning with x1 (because i = 1) up to xn\n(because i = n). Our formula for the mean of a set of n observations could be represented as\nthe following:\n\nGiven that general notation, the formula for calculating the correlation coefficient, r, for two\n\nvariables x and y is the following:", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nstudent’s height and weight are both relatively far from the mean. 4. The correlation coefficient is the sum of the products calculated above divided by the\nnumber of observations (15 in this case). The correlation between height and weight for\nthis group of students is .83. Given that the correlation coefficient can range from --1 to\n1, this is a relatively high degree of positive correlation, as we would expect with height\nand weight. The formula for calculating the correlation coefficient requires a little detour with regard to\nnotation. The figure ∑, known as the summation sign, is a handy character in statistics. It\nrepresents the summation of the quantity that comes after it. For example, if there is a set of\nobservations x1, x2, x3, and x4, then ∑ (xi) tells us that we should sum the four observations: x1\n+ x2 + x3 + x4. Thus, ∑ (xi) = x1 + x2 + x3 + x4. Our formula for the mean of a set of i\nobservations could be represented as the following: mean = ∑ (xi)/n. We can make the formula even more adaptable by writing\n\n, which sums the quantity x1 +\nx2 + x3 + . . . xn, or, in other words, all the terms beginning with x1 (because i = 1) up to xn\n(because i = n). Our formula for the mean of a set of n observations could be represented as\nthe following:\n\nGiven that general notation, the formula for calculating the correlation coefficient, r, for two\n\nvariables x and y is the following:", "tokens": 348, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 55, "segment_id": "00055", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000090"}
{"type": "chunk", "text": "where\n\nn = the number of observations;\n is the mean for variable x;\n is the mean for variable y;\nσx is the standard deviation for variable x;\nσy is the standard deviation for variable y. Any statistical software program with statistical tools can also calculate the correlation\ncoefficient between two variables. In the student height and weight example, using Microsoft\nExcel yields the same correlation between height and weight for the fifteen students as the hand\ncalculation in the chart above: 0.83. * You can read it at http:///assets/GrandPrize2009_BPC_PragmaticTheory.pdf.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nwhere\n\nn = the number of observations;\n is the mean for variable x;\n is the mean for variable y;\nσx is the standard deviation for variable x;\nσy is the standard deviation for variable y. Any statistical software program with statistical tools can also calculate the correlation\ncoefficient between two variables. In the student height and weight example, using Microsoft\nExcel yields the same correlation between height and weight for the fifteen students as the hand\ncalculation in the chart above: 0.83. * You can read it at http:///assets/GrandPrize2009_BPC_PragmaticTheory.pdf.", "tokens": 125, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 56, "segment_id": "00056", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000091"}
{"type": "chunk", "text": "CHAPTER 5\nBasic Probability\nDon’t buy the extended warranty on your $99 printer\n\nIn 1981, the Joseph Schlitz Brewing Company spent $1.7 million for what appeared to be a\n\nshockingly bold and risky marketing campaign for its flagging brand, Schlitz. At halftime of the\nSuper Bowl, in front of 100 million people around the world, the company broadcast a live\ntaste test pitting Schlitz Beer against a key competitor, Michelob.1 Bolder yet, the company did\nnot pick random beer drinkers to evaluate the two beers; it picked 100 Michelob drinkers. This\nwas the culmination of a campaign that had run throughout the NFL playoffs.2 There were five\nlive television taste tests in all, each of which had 100 consumers of a competing brand\n(Budweiser, Miller, or Michelob) conduct a blind taste test between their supposed favorite\nbeer and Schlitz. Each of the beer taste-offs was promoted aggressively, just like the playoff\ngame during which it would be held (e.g., “Watch Schlitz v. Bud, Live during the AFC\nPlayoffs”). The marketing message was clear: Even beer drinkers who think they like another brand will\nprefer Schlitz in a blind taste test. For the Super Bowl spot, Schlitz even hired a former NFL\nreferee to oversee the test. Given the risky nature of conducting blind taste tests in front of\nhuge audiences on live TV, one can assume that Schlitz produced a spectacularly delicious\nbeer, right? Not necessarily. Schlitz needed only a mediocre beer and a solid grasp of statistics to know\nthat this ploy---a term I do not use lightly, even when it comes to beer advertising---would\nalmost certainly work out in its favor. Most beers in the Schlitz category taste about the same;\nironically, that is exactly the fact that this advertising campaign exploited. Assume that the\ntypical beer drinker off the street cannot tell Schlitz from Budweiser from Michelob from\nMiller. In that case, a blind taste test between any two of the beers is essentially a coin flip. On\naverage, half the taste testers will pick Schlitz, and half will pick the beer it is “challenging.”\nThis fact alone would probably not make a particularly effective advertising campaign.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nCHAPTER 5\nBasic Probability\nDon’t buy the extended warranty on your $99 printer\n\nIn 1981, the Joseph Schlitz Brewing Company spent $1.7 million for what appeared to be a\n\nshockingly bold and risky marketing campaign for its flagging brand, Schlitz. At halftime of the\nSuper Bowl, in front of 100 million people around the world, the company broadcast a live\ntaste test pitting Schlitz Beer against a key competitor, Michelob.1 Bolder yet, the company did\nnot pick random beer drinkers to evaluate the two beers; it picked 100 Michelob drinkers. This\nwas the culmination of a campaign that had run throughout the NFL playoffs.2 There were five\nlive television taste tests in all, each of which had 100 consumers of a competing brand\n(Budweiser, Miller, or Michelob) conduct a blind taste test between their supposed favorite\nbeer and Schlitz. Each of the beer taste-offs was promoted aggressively, just like the playoff\ngame during which it would be held (e.g., “Watch Schlitz v. Bud, Live during the AFC\nPlayoffs”). The marketing message was clear: Even beer drinkers who think they like another brand will\nprefer Schlitz in a blind taste test. For the Super Bowl spot, Schlitz even hired a former NFL\nreferee to oversee the test. Given the risky nature of conducting blind taste tests in front of\nhuge audiences on live TV, one can assume that Schlitz produced a spectacularly delicious\nbeer, right? Not necessarily. Schlitz needed only a mediocre beer and a solid grasp of statistics to know\nthat this ploy---a term I do not use lightly, even when it comes to beer advertising---would\nalmost certainly work out in its favor. Most beers in the Schlitz category taste about the same;\nironically, that is exactly the fact that this advertising campaign exploited. Assume that the\ntypical beer drinker off the street cannot tell Schlitz from Budweiser from Michelob from\nMiller. In that case, a blind taste test between any two of the beers is essentially a coin flip. On\naverage, half the taste testers will pick Schlitz, and half will pick the beer it is “challenging.”\nThis fact alone would probably not make a particularly effective advertising campaign.", "tokens": 481, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 57, "segment_id": "00057", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000092"}
{"type": "chunk", "text": "Most beers in the Schlitz category taste about the same;\nironically, that is exactly the fact that this advertising campaign exploited. Assume that the\ntypical beer drinker off the street cannot tell Schlitz from Budweiser from Michelob from\nMiller. In that case, a blind taste test between any two of the beers is essentially a coin flip. On\naverage, half the taste testers will pick Schlitz, and half will pick the beer it is “challenging.”\nThis fact alone would probably not make a particularly effective advertising campaign. (“You\ncan’t tell the difference, so you might as well drink Schlitz.”) And Schlitz absolutely,\npositively would not want to do this test among its own loyal customers; roughly half of these\nSchlitz drinkers would pick the competing beer. It looks bad when the beer drinkers\nsupposedly most committed to your brand choose a competitor in a blind taste test---which is\nexactly what Schlitz was trying to do to its competitors. Schlitz did something cleverer. The genius of the campaign was conducting the taste test\nexclusively among beer drinkers who stated that they preferred a competing beer. If the blind\ntaste test is really just a coin flip, then roughly half of the Budweiser or Miller or Michelob\ndrinkers will end up picking Schlitz. That makes Schlitz look really good. Half of all Bud\ndrinkers like Schlitz better!", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nMost beers in the Schlitz category taste about the same;\nironically, that is exactly the fact that this advertising campaign exploited. Assume that the\ntypical beer drinker off the street cannot tell Schlitz from Budweiser from Michelob from\nMiller. In that case, a blind taste test between any two of the beers is essentially a coin flip. On\naverage, half the taste testers will pick Schlitz, and half will pick the beer it is “challenging.”\nThis fact alone would probably not make a particularly effective advertising campaign. (“You\ncan’t tell the difference, so you might as well drink Schlitz.”) And Schlitz absolutely,\npositively would not want to do this test among its own loyal customers; roughly half of these\nSchlitz drinkers would pick the competing beer. It looks bad when the beer drinkers\nsupposedly most committed to your brand choose a competitor in a blind taste test---which is\nexactly what Schlitz was trying to do to its competitors. Schlitz did something cleverer. The genius of the campaign was conducting the taste test\nexclusively among beer drinkers who stated that they preferred a competing beer. If the blind\ntaste test is really just a coin flip, then roughly half of the Budweiser or Miller or Michelob\ndrinkers will end up picking Schlitz. That makes Schlitz look really good. Half of all Bud\ndrinkers like Schlitz better!", "tokens": 297, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 57, "segment_id": "00057", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000093"}
{"type": "chunk", "text": "And it looks particularly good at halftime of the Super Bowl with a former NFL referee (in\nuniform) conducting the taste test. Still, it’s live television. Even if the statisticians at Schlitz\nhad determined with loads of previous private trials that the typical Michelob drinker will pick\nSchlitz 50 percent of the time, what if the 100 Michelob drinkers taking the test at halftime of\nthe Super Bowl turn out to be quirky? Yes, the blind taste test is the equivalent of a coin toss,\nbut what if most of the tasters chose Michelob just by chance? After all, if we lined up the\nsame 100 guys and asked them to flip a coin, it’s entirely possible that they would flip 85 or\n90 tails. That kind of bad luck in the taste test would be a disaster for the Schlitz brand (not to\nmention a waste of the $1.7 million for the live television coverage). Statistics to the rescue! If there were some kind of statistics superhero,* this is when he or\nshe would have swooped into the Schlitz corporate headquarters and unveiled the details of\nwhat statisticians call a binomial experiment (also called a Bernoulli trial). The key\ncharacteristics of a binomial experiment are that we have a fixed number of trials (e.g., 100\ntaste testers), each with two possible outcomes (Schlitz or Michelob), and the probability of\n“success” is the same in each trial. (I am assuming the probability of picking one beer or the\nother is 50 percent, and I am defining “success” as a tester picking Schlitz.) We also assume\nthat all the “trials” are independent, meaning that one blind taste tester’s decision has no impact\non any other tester’s decision. With only this information, a statistical superhero can calculate the probability of all the\ndifferent outcomes for the 100 trials, such as 52 Schlitz and 48 Michelob or 31 Schlitz and 69\nMichelob. Those of us who are not statistical superheroes can use a computer to do the same\nthing. The chances of all 100\nin\n1,267,650,600,228,229,401,496,703,205,376. There was probably a bigger chance that all of\nthe testers would be killed at halftime by an asteroid.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nAnd it looks particularly good at halftime of the Super Bowl with a former NFL referee (in\nuniform) conducting the taste test. Still, it’s live television. Even if the statisticians at Schlitz\nhad determined with loads of previous private trials that the typical Michelob drinker will pick\nSchlitz 50 percent of the time, what if the 100 Michelob drinkers taking the test at halftime of\nthe Super Bowl turn out to be quirky? Yes, the blind taste test is the equivalent of a coin toss,\nbut what if most of the tasters chose Michelob just by chance? After all, if we lined up the\nsame 100 guys and asked them to flip a coin, it’s entirely possible that they would flip 85 or\n90 tails. That kind of bad luck in the taste test would be a disaster for the Schlitz brand (not to\nmention a waste of the $1.7 million for the live television coverage). Statistics to the rescue! If there were some kind of statistics superhero,* this is when he or\nshe would have swooped into the Schlitz corporate headquarters and unveiled the details of\nwhat statisticians call a binomial experiment (also called a Bernoulli trial). The key\ncharacteristics of a binomial experiment are that we have a fixed number of trials (e.g., 100\ntaste testers), each with two possible outcomes (Schlitz or Michelob), and the probability of\n“success” is the same in each trial. (I am assuming the probability of picking one beer or the\nother is 50 percent, and I am defining “success” as a tester picking Schlitz.) We also assume\nthat all the “trials” are independent, meaning that one blind taste tester’s decision has no impact\non any other tester’s decision. With only this information, a statistical superhero can calculate the probability of all the\ndifferent outcomes for the 100 trials, such as 52 Schlitz and 48 Michelob or 31 Schlitz and 69\nMichelob. Those of us who are not statistical superheroes can use a computer to do the same\nthing. The chances of all 100\nin\n1,267,650,600,228,229,401,496,703,205,376. There was probably a bigger chance that all of\nthe testers would be killed at halftime by an asteroid.", "tokens": 494, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 58, "segment_id": "00058", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000094"}
{"type": "chunk", "text": "With only this information, a statistical superhero can calculate the probability of all the\ndifferent outcomes for the 100 trials, such as 52 Schlitz and 48 Michelob or 31 Schlitz and 69\nMichelob. Those of us who are not statistical superheroes can use a computer to do the same\nthing. The chances of all 100\nin\n1,267,650,600,228,229,401,496,703,205,376. There was probably a bigger chance that all of\nthe testers would be killed at halftime by an asteroid. More important, the same basic\ncalculations can give us the cumulative probability for a range of outcomes, such as the\nchances that 40 or fewer testers pick Schlitz. These numbers would clearly have assuaged the\nfears of the Schlitz marketing folks. testers picking Michelob were 1\n\ntaste\n\nLet’s assume that Schlitz would have been pleased if at least 40 of the 100 tasters picked\nSchlitz---an impressive number given that all of the men taking the live blind taste test had\nprofessed to be Michelob drinkers. An outcome at least that good was highly likely. If the\ntaste test is really like a flip of the coin, then basic probability tells us that there was a 98\npercent chance that at least 40 of the tasters would pick Schlitz, and an 86 percent chance that\nat least 45 of the tasters would.† In theory, this wasn’t a very risky gambit at all. So what happened to Schlitz? At halftime of the 1981 Super Bowl, exactly 50 percent of the\n\nMichelob drinkers chose Schlitz in the blind taste test. There are two important lessons here: probability is a remarkably powerful tool, and many\nleading beers in the 1980s were indistinguishable from one another. This chapter will focus\nprimarily on the first lesson. Probability is the study of events and outcomes involving an element of uncertainty. Investing\nin the stock market involves uncertainty. So does flipping a coin, which may come up heads or", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nWith only this information, a statistical superhero can calculate the probability of all the\ndifferent outcomes for the 100 trials, such as 52 Schlitz and 48 Michelob or 31 Schlitz and 69\nMichelob. Those of us who are not statistical superheroes can use a computer to do the same\nthing. The chances of all 100\nin\n1,267,650,600,228,229,401,496,703,205,376. There was probably a bigger chance that all of\nthe testers would be killed at halftime by an asteroid. More important, the same basic\ncalculations can give us the cumulative probability for a range of outcomes, such as the\nchances that 40 or fewer testers pick Schlitz. These numbers would clearly have assuaged the\nfears of the Schlitz marketing folks. testers picking Michelob were 1\n\ntaste\n\nLet’s assume that Schlitz would have been pleased if at least 40 of the 100 tasters picked\nSchlitz---an impressive number given that all of the men taking the live blind taste test had\nprofessed to be Michelob drinkers. An outcome at least that good was highly likely. If the\ntaste test is really like a flip of the coin, then basic probability tells us that there was a 98\npercent chance that at least 40 of the tasters would pick Schlitz, and an 86 percent chance that\nat least 45 of the tasters would.† In theory, this wasn’t a very risky gambit at all. So what happened to Schlitz? At halftime of the 1981 Super Bowl, exactly 50 percent of the\n\nMichelob drinkers chose Schlitz in the blind taste test. There are two important lessons here: probability is a remarkably powerful tool, and many\nleading beers in the 1980s were indistinguishable from one another. This chapter will focus\nprimarily on the first lesson. Probability is the study of events and outcomes involving an element of uncertainty. Investing\nin the stock market involves uncertainty. So does flipping a coin, which may come up heads or", "tokens": 441, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 58, "segment_id": "00058", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000095"}
{"type": "chunk", "text": "tails. Flipping a coin four times in a row involves additional layers of uncertainty, because\neach of the four flips can result in a head or a tail. If you flip a coin four times in a row, I\ncannot know the outcome in advance with certainty (nor can you). Yet I can determine in\nadvance that some outcomes (two heads, two tails) are more likely than others (four heads). As\nthe folks at Schlitz reckoned, those kinds of probability-based insights can be extremely\nhelpful. In fact, if you can understand why the probability of flipping four heads in a row with a\nfair coin is 1 in 16, you can (with some work) understand everything from how the insurance\nindustry works to whether a pro football team should kick the extra point after a touchdown or\ngo for a two-point conversion. Let’s start with the easy part: Many events have known probabilities. The probability of\nflipping heads with a fair coin is 1⁄2. The probability of rolling a one with a single die is\n. Other events have probabilities that can be inferred on the basis of past data. The probability\nof successfully kicking the extra point after touchdown in professional football is .94, meaning\nthat kickers make, on average, 94 out of every 100 extra-point attempts. (Obviously this figure\nmight vary slightly for different kickers, under different weather circumstances, and so on, but\nit’s not going to change radically.) Simply having and appreciating this kind of information can\noften clarify decision making and render risks explicit. For example, the Australian Transport\nSafety Board published a report quantifying the fatality risks for different modes of transport. Despite widespread fear of flying, the risks associated with commercial air travel are tiny. Australia hasn’t had a commercial air fatality since the 1960s, so the fatality rate per 100\nmillion kilometers traveled is essentially zero. The rate for drivers is .5 fatalities per 100\nmillion kilometers traveled. The really impressive number is for motorcycles---if you aspire to\nbe an organ donor. The fatality rate is thirty-five times higher for motorcycles than for cars.3\n\nIn September of 2011, a 6.5-ton NASA satellite was plummeting to earth and was expected\nto break apart once it hit the earth’s atmosphere. What were the chances of being struck by the\ndebris? Should I have kept the kids home from school?", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\ntails. Flipping a coin four times in a row involves additional layers of uncertainty, because\neach of the four flips can result in a head or a tail. If you flip a coin four times in a row, I\ncannot know the outcome in advance with certainty (nor can you). Yet I can determine in\nadvance that some outcomes (two heads, two tails) are more likely than others (four heads). As\nthe folks at Schlitz reckoned, those kinds of probability-based insights can be extremely\nhelpful. In fact, if you can understand why the probability of flipping four heads in a row with a\nfair coin is 1 in 16, you can (with some work) understand everything from how the insurance\nindustry works to whether a pro football team should kick the extra point after a touchdown or\ngo for a two-point conversion. Let’s start with the easy part: Many events have known probabilities. The probability of\nflipping heads with a fair coin is 1⁄2. The probability of rolling a one with a single die is\n. Other events have probabilities that can be inferred on the basis of past data. The probability\nof successfully kicking the extra point after touchdown in professional football is .94, meaning\nthat kickers make, on average, 94 out of every 100 extra-point attempts. (Obviously this figure\nmight vary slightly for different kickers, under different weather circumstances, and so on, but\nit’s not going to change radically.) Simply having and appreciating this kind of information can\noften clarify decision making and render risks explicit. For example, the Australian Transport\nSafety Board published a report quantifying the fatality risks for different modes of transport. Despite widespread fear of flying, the risks associated with commercial air travel are tiny. Australia hasn’t had a commercial air fatality since the 1960s, so the fatality rate per 100\nmillion kilometers traveled is essentially zero. The rate for drivers is .5 fatalities per 100\nmillion kilometers traveled. The really impressive number is for motorcycles---if you aspire to\nbe an organ donor. The fatality rate is thirty-five times higher for motorcycles than for cars.3\n\nIn September of 2011, a 6.5-ton NASA satellite was plummeting to earth and was expected\nto break apart once it hit the earth’s atmosphere. What were the chances of being struck by the\ndebris? Should I have kept the kids home from school?", "tokens": 508, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 59, "segment_id": "00059", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000096"}
{"type": "chunk", "text": "The rate for drivers is .5 fatalities per 100\nmillion kilometers traveled. The really impressive number is for motorcycles---if you aspire to\nbe an organ donor. The fatality rate is thirty-five times higher for motorcycles than for cars.3\n\nIn September of 2011, a 6.5-ton NASA satellite was plummeting to earth and was expected\nto break apart once it hit the earth’s atmosphere. What were the chances of being struck by the\ndebris? Should I have kept the kids home from school? The rocket scientists at NASA\nestimated that the probability of any individual person’s being hit by a part of the falling\nsatellite was 1 in 21 trillion. Yet the chances that anyone anywhere on earth might get hit were\na more sobering 1 in 3,200.* In the end, the satellite did break apart on reentry, but scientists\naren’t entirely certain where all the pieces ended up.4 No one reported being hurt. Probabilities\ndo not tell us what will happen for sure; they tell us what is likely to happen and what is less\nlikely to happen. Sensible people can make use of these kinds of numbers in business and life. For example, when you hear on the radio that a satellite is plummeting to earth, you should not\nrace home on your motorcycle to warn the family. When it comes to risk, our fears do not always track with what the numbers tell us we\nshould be afraid of. One of the striking findings from Freakonomics, by Steve Levitt and\nStephen Dubner, was that swimming pools in the backyard are far more dangerous than guns in\nthe closet.5 Levitt and Dubner calculate that a child under ten is one hundred times more likely\nto die in a swimming pool than from a gun accident.† An intriguing paper by three Cornell\nresearchers, Garrick Blalock, Vrinda Kadiyali, and Daniel Simon, found that thousands of\nAmericans may have died since the September 11 attacks because they were afraid to fly.6", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThe rate for drivers is .5 fatalities per 100\nmillion kilometers traveled. The really impressive number is for motorcycles---if you aspire to\nbe an organ donor. The fatality rate is thirty-five times higher for motorcycles than for cars.3\n\nIn September of 2011, a 6.5-ton NASA satellite was plummeting to earth and was expected\nto break apart once it hit the earth’s atmosphere. What were the chances of being struck by the\ndebris? Should I have kept the kids home from school? The rocket scientists at NASA\nestimated that the probability of any individual person’s being hit by a part of the falling\nsatellite was 1 in 21 trillion. Yet the chances that anyone anywhere on earth might get hit were\na more sobering 1 in 3,200.* In the end, the satellite did break apart on reentry, but scientists\naren’t entirely certain where all the pieces ended up.4 No one reported being hurt. Probabilities\ndo not tell us what will happen for sure; they tell us what is likely to happen and what is less\nlikely to happen. Sensible people can make use of these kinds of numbers in business and life. For example, when you hear on the radio that a satellite is plummeting to earth, you should not\nrace home on your motorcycle to warn the family. When it comes to risk, our fears do not always track with what the numbers tell us we\nshould be afraid of. One of the striking findings from Freakonomics, by Steve Levitt and\nStephen Dubner, was that swimming pools in the backyard are far more dangerous than guns in\nthe closet.5 Levitt and Dubner calculate that a child under ten is one hundred times more likely\nto die in a swimming pool than from a gun accident.† An intriguing paper by three Cornell\nresearchers, Garrick Blalock, Vrinda Kadiyali, and Daniel Simon, found that thousands of\nAmericans may have died since the September 11 attacks because they were afraid to fly.6", "tokens": 426, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 59, "segment_id": "00059", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000097"}
{"type": "chunk", "text": "We will never know the true risks associated with terrorism; we do know that driving is\ndangerous. When more Americans opted to drive rather than to fly after 9/11, there were an\nestimated 344 additional traffic deaths per month in October, November, and December of\n2001 (taking into account the average number of fatalities and other factors that typically\ncontribute to road accidents, such as weather). This effect dissipated over time, presumably as\nthe fear of terrorism diminished, but the authors of the study estimate that the September 11\nattacks may have caused more than 2,000 driving deaths. Probability can also sometimes tell us after the fact what likely happened and what likely did\nnot happen---as in the case of DNA analysis. When the technicians on CSI: Miami find a trace\nof saliva on an apple core near a murder victim, that saliva does not have the murderer’s name\non it, even when viewed under a powerful microscope by a very attractive technician. Instead,\nthe saliva (or hair, or skin, or bone fragment) will contain a DNA segment. Each DNA segment\nin turn has regions, or loci, that can vary from individual to individual (except for identical\ntwins, who share the same DNA). When the medical examiner reports that a DNA sample is a\n“match,” that’s only part of what the prosecution has to prove. Yes, the loci tested on the DNA\nsample from the crime scene must match the loci on the DNA sample taken from the suspect. However, the prosecutors must also prove that the match between the two DNA samples is not\nmerely a coincidence. Humans share similarities in their DNA, just as we share other similarities: shoe size, height,\neye color. (More than 99 percent of all DNA is identical among all humans.) If researchers\nhave access to only a small sample of DNA on which only a few loci can be tested, it’s\npossible that thousands or even millions of individuals may share that genetic fragment. Therefore, the more loci that can be tested, and the more natural genetic variation there is in\neach of those loci, the more certain the match becomes. Or, to put it a bit differently, the less\nlikely it becomes that the DNA sample will match more than one person.7\n\nTo get your mind around this, imagine that your “DNA number” consists of your phone\nnumber attached to your Social Security number.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nWe will never know the true risks associated with terrorism; we do know that driving is\ndangerous. When more Americans opted to drive rather than to fly after 9/11, there were an\nestimated 344 additional traffic deaths per month in October, November, and December of\n2001 (taking into account the average number of fatalities and other factors that typically\ncontribute to road accidents, such as weather). This effect dissipated over time, presumably as\nthe fear of terrorism diminished, but the authors of the study estimate that the September 11\nattacks may have caused more than 2,000 driving deaths. Probability can also sometimes tell us after the fact what likely happened and what likely did\nnot happen---as in the case of DNA analysis. When the technicians on CSI: Miami find a trace\nof saliva on an apple core near a murder victim, that saliva does not have the murderer’s name\non it, even when viewed under a powerful microscope by a very attractive technician. Instead,\nthe saliva (or hair, or skin, or bone fragment) will contain a DNA segment. Each DNA segment\nin turn has regions, or loci, that can vary from individual to individual (except for identical\ntwins, who share the same DNA). When the medical examiner reports that a DNA sample is a\n“match,” that’s only part of what the prosecution has to prove. Yes, the loci tested on the DNA\nsample from the crime scene must match the loci on the DNA sample taken from the suspect. However, the prosecutors must also prove that the match between the two DNA samples is not\nmerely a coincidence. Humans share similarities in their DNA, just as we share other similarities: shoe size, height,\neye color. (More than 99 percent of all DNA is identical among all humans.) If researchers\nhave access to only a small sample of DNA on which only a few loci can be tested, it’s\npossible that thousands or even millions of individuals may share that genetic fragment. Therefore, the more loci that can be tested, and the more natural genetic variation there is in\neach of those loci, the more certain the match becomes. Or, to put it a bit differently, the less\nlikely it becomes that the DNA sample will match more than one person.7\n\nTo get your mind around this, imagine that your “DNA number” consists of your phone\nnumber attached to your Social Security number.", "tokens": 503, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 60, "segment_id": "00060", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000098"}
{"type": "chunk", "text": "Therefore, the more loci that can be tested, and the more natural genetic variation there is in\neach of those loci, the more certain the match becomes. Or, to put it a bit differently, the less\nlikely it becomes that the DNA sample will match more than one person.7\n\nTo get your mind around this, imagine that your “DNA number” consists of your phone\nnumber attached to your Social Security number. This nineteen-digit sequence uniquely\nidentifies you. Consider each digit a “locus” with ten possibilities: 0, 1, 2, 3, and so on. Now\nsuppose that crime scene investigators find the remnant of a “DNA number” at a crime scene:\n_ _ 4 5 9 _ _ _ 4 _ 0 _ 9 8 1 7 _ _ _. This happens to match exactly with your “DNA number.”\nAre you guilty? You should see three things. First, anything less than a full match of the entire genome\nleaves some room for uncertainty. Second, the more “loci” that can be tested, the less\nuncertainty remains. And third, context matters. This match would be extremely compelling if\nyou also happened to be caught speeding away from the crime scene with the victim’s credit\ncards in your pocket. When researchers have unlimited time and resources, the typical process involves testing\nthirteen different loci. The chances that two people share the same DNA profile across all\nthirteen loci are extremely low. When DNA was used to identify the remains found in the\nWorld Trade Center after September 11, samples found at the scene were matched to samples\nprovided by family members of the victims. The probability required to establish positive", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nTherefore, the more loci that can be tested, and the more natural genetic variation there is in\neach of those loci, the more certain the match becomes. Or, to put it a bit differently, the less\nlikely it becomes that the DNA sample will match more than one person.7\n\nTo get your mind around this, imagine that your “DNA number” consists of your phone\nnumber attached to your Social Security number. This nineteen-digit sequence uniquely\nidentifies you. Consider each digit a “locus” with ten possibilities: 0, 1, 2, 3, and so on. Now\nsuppose that crime scene investigators find the remnant of a “DNA number” at a crime scene:\n_ _ 4 5 9 _ _ _ 4 _ 0 _ 9 8 1 7 _ _ _. This happens to match exactly with your “DNA number.”\nAre you guilty? You should see three things. First, anything less than a full match of the entire genome\nleaves some room for uncertainty. Second, the more “loci” that can be tested, the less\nuncertainty remains. And third, context matters. This match would be extremely compelling if\nyou also happened to be caught speeding away from the crime scene with the victim’s credit\ncards in your pocket. When researchers have unlimited time and resources, the typical process involves testing\nthirteen different loci. The chances that two people share the same DNA profile across all\nthirteen loci are extremely low. When DNA was used to identify the remains found in the\nWorld Trade Center after September 11, samples found at the scene were matched to samples\nprovided by family members of the victims. The probability required to establish positive", "tokens": 361, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 60, "segment_id": "00060", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000099"}
{"type": "chunk", "text": "identification was one in a billion, meaning that the probability that the discovered remains\nbelonged to someone other than the identified victim had to be judged as one in one billion or\nless. Later in the search, this standard was relaxed, as there were fewer unidentified victims\nwith whom the remains could be confused. When resources are limited, or the available DNA sample is too small or too contaminated\nfor thirteen loci to be tested, things get more interesting and controversial. The Los Angeles\nTimes ran a series in 2008 examining the use of DNA as criminal evidence.8 In particular, the\nTimes questioned whether the probabilities typically used by law enforcement understate the\nlikelihood of coincidental matches. (Since no one knows the DNA profile of the entire\npopulation, the probabilities presented in court by the FBI and other law enforcement entities\nare estimates.) The intellectual pushback was instigated when a crime lab analyst in Arizona\nrunning tests with the state’s DNA database discovered two unrelated felons whose DNA\nmatched at nine loci; according to the FBI, the chances of a nine-loci match between two\nunrelated persons are 1 in 113 billion. Subsequent searches of other DNA databases turned up\nmore than a thousand human pairs with genetic matches at nine loci or more. I’ll leave this\nissue for law enforcement and defense lawyers to work out. For now, the lesson is that the\ndazzling science of DNA analysis is only as good as the probabilities used to support it. Often it is extremely valuable to know the likelihood of multiple events’ happening. What is\nthe probability that the electricity goes out and the generator doesn’t work? The probability of\ntwo independent events’ both happening is the product of their respective probabilities. In other\nwords, the probability of Event A happening and Event B happening is the probability of Event\nA multiplied by the probability of Event B. An example makes it much more intuitive. If the\nprobability of flipping heads with a fair coin is 1⁄2, then the probability of flipping heads twice\nin a row is 1⁄2 × 1⁄2, or 1⁄4. The probability of flipping three heads in a row is 1⁄8, the probability\nof four heads in a row is 1/16, and so on.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nidentification was one in a billion, meaning that the probability that the discovered remains\nbelonged to someone other than the identified victim had to be judged as one in one billion or\nless. Later in the search, this standard was relaxed, as there were fewer unidentified victims\nwith whom the remains could be confused. When resources are limited, or the available DNA sample is too small or too contaminated\nfor thirteen loci to be tested, things get more interesting and controversial. The Los Angeles\nTimes ran a series in 2008 examining the use of DNA as criminal evidence.8 In particular, the\nTimes questioned whether the probabilities typically used by law enforcement understate the\nlikelihood of coincidental matches. (Since no one knows the DNA profile of the entire\npopulation, the probabilities presented in court by the FBI and other law enforcement entities\nare estimates.) The intellectual pushback was instigated when a crime lab analyst in Arizona\nrunning tests with the state’s DNA database discovered two unrelated felons whose DNA\nmatched at nine loci; according to the FBI, the chances of a nine-loci match between two\nunrelated persons are 1 in 113 billion. Subsequent searches of other DNA databases turned up\nmore than a thousand human pairs with genetic matches at nine loci or more. I’ll leave this\nissue for law enforcement and defense lawyers to work out. For now, the lesson is that the\ndazzling science of DNA analysis is only as good as the probabilities used to support it. Often it is extremely valuable to know the likelihood of multiple events’ happening. What is\nthe probability that the electricity goes out and the generator doesn’t work? The probability of\ntwo independent events’ both happening is the product of their respective probabilities. In other\nwords, the probability of Event A happening and Event B happening is the probability of Event\nA multiplied by the probability of Event B. An example makes it much more intuitive. If the\nprobability of flipping heads with a fair coin is 1⁄2, then the probability of flipping heads twice\nin a row is 1⁄2 × 1⁄2, or 1⁄4. The probability of flipping three heads in a row is 1⁄8, the probability\nof four heads in a row is 1/16, and so on.", "tokens": 481, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 61, "segment_id": "00061", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000100"}
{"type": "chunk", "text": "In other\nwords, the probability of Event A happening and Event B happening is the probability of Event\nA multiplied by the probability of Event B. An example makes it much more intuitive. If the\nprobability of flipping heads with a fair coin is 1⁄2, then the probability of flipping heads twice\nin a row is 1⁄2 × 1⁄2, or 1⁄4. The probability of flipping three heads in a row is 1⁄8, the probability\nof four heads in a row is 1/16, and so on. (You should see that the probability of throwing four\ntails in a row is also 1/16.) This explains why the system administrator at your school or office\nis constantly on your case to improve the “quality” of your password. If you have a six-digit\npassword using only numerical digits, we can calculate the number of possible passwords: 10\n× 10 × 10 × 10 × 10 × 10, which equals 106, or 1,000,000. That sounds like a lot of\npossibilities, but a computer could blow through all 1,000,000 possible combinations in a\nfraction of a second. So let’s suppose that your system administrator harangues you long enough that you include\nletters in your password. At that point, each of the 6 digits now has 36 combinations: 26 letters\nand 10 digits. The number of possible passwords grows to 36 × 36 × 36 × 36 × 36 × 36,\nor 366, which is over two billion. If your administrator demands eight digits and urges you to\nuse symbols like #, @, % and !, as the University of Chicago does, the number of potential\npasswords climbs to 468, or just over 20 trillion. There is one crucial distinction here. This formula is applicable only if the events are\nindependent, meaning that the outcome of one has no effect on the outcome of another. For\nexample, the probability that you throw heads on the first flip does not change the likelihood of", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nIn other\nwords, the probability of Event A happening and Event B happening is the probability of Event\nA multiplied by the probability of Event B. An example makes it much more intuitive. If the\nprobability of flipping heads with a fair coin is 1⁄2, then the probability of flipping heads twice\nin a row is 1⁄2 × 1⁄2, or 1⁄4. The probability of flipping three heads in a row is 1⁄8, the probability\nof four heads in a row is 1/16, and so on. (You should see that the probability of throwing four\ntails in a row is also 1/16.) This explains why the system administrator at your school or office\nis constantly on your case to improve the “quality” of your password. If you have a six-digit\npassword using only numerical digits, we can calculate the number of possible passwords: 10\n× 10 × 10 × 10 × 10 × 10, which equals 106, or 1,000,000. That sounds like a lot of\npossibilities, but a computer could blow through all 1,000,000 possible combinations in a\nfraction of a second. So let’s suppose that your system administrator harangues you long enough that you include\nletters in your password. At that point, each of the 6 digits now has 36 combinations: 26 letters\nand 10 digits. The number of possible passwords grows to 36 × 36 × 36 × 36 × 36 × 36,\nor 366, which is over two billion. If your administrator demands eight digits and urges you to\nuse symbols like #, @, % and !, as the University of Chicago does, the number of potential\npasswords climbs to 468, or just over 20 trillion. There is one crucial distinction here. This formula is applicable only if the events are\nindependent, meaning that the outcome of one has no effect on the outcome of another. For\nexample, the probability that you throw heads on the first flip does not change the likelihood of", "tokens": 444, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 61, "segment_id": "00061", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000101"}
{"type": "chunk", "text": "your throwing heads on the second flip. On the other hand, the probability that it rains today is\nnot independent of whether it rained yesterday, since storm fronts can last for days. Similarly,\nthe probability of crashing your car today and crashing your car next year are not independent. Whatever caused you to crash this year might also cause you to crash next year; you might be\nprone to drunk driving, drag racing, texting while driving, or just driving badly. (This is why\nyour auto insurance rates go up after an accident; it is not simply that the company wants to\nrecover the money that it has paid out for the claim; rather, it now has new information about\nyour probability of crashing in the future, which---after you’ve driven the car through your\ngarage door---has gone up.)\n\nSuppose you are interested in the probability that one event happens or another event\nhappens: outcome A or outcome B (again assuming that they are independent). In this case, the\nprobability of getting A or B consists of the sum of their individual probabilities: the\nprobability of A plus the probability of B. For example, the likelihood of throwing a 1, 2, or 3,\nwith a single die is the sum of their individual probabilities:\n = 1⁄2. This should\nmake intuitive sense. There are six possible outcomes for the roll of a die. The numbers 1, 2,\nand 3 collectively make up half of those possible outcomes. Therefore you have a 50 percent\nchance of rolling a 1, 2, or 3. If you are playing craps in Las Vegas, the chance of rolling a 7 or\n11 in a single throw is the number of combinations that sum to 7 or 11 divided by the total\nnumber of combinations that can be thrown with two dice, or\n\n +\n\n +\n\n =\n\n.*\n\nProbability also enables us to calculate what might be the most useful tool in all of\nmanagerial decision making, particularly finance: expected value. The expected value takes\nbasic probability one step further. The expected value or payoff from some event, say\npurchasing a lottery ticket, is the sum of all the different outcomes, each weighted by its\nprobability and payoff. As usual, an example makes this clearer. Suppose you are invited to\nplay a game in which you roll a single die.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nyour throwing heads on the second flip. On the other hand, the probability that it rains today is\nnot independent of whether it rained yesterday, since storm fronts can last for days. Similarly,\nthe probability of crashing your car today and crashing your car next year are not independent. Whatever caused you to crash this year might also cause you to crash next year; you might be\nprone to drunk driving, drag racing, texting while driving, or just driving badly. (This is why\nyour auto insurance rates go up after an accident; it is not simply that the company wants to\nrecover the money that it has paid out for the claim; rather, it now has new information about\nyour probability of crashing in the future, which---after you’ve driven the car through your\ngarage door---has gone up.)\n\nSuppose you are interested in the probability that one event happens or another event\nhappens: outcome A or outcome B (again assuming that they are independent). In this case, the\nprobability of getting A or B consists of the sum of their individual probabilities: the\nprobability of A plus the probability of B. For example, the likelihood of throwing a 1, 2, or 3,\nwith a single die is the sum of their individual probabilities:\n = 1⁄2. This should\nmake intuitive sense. There are six possible outcomes for the roll of a die. The numbers 1, 2,\nand 3 collectively make up half of those possible outcomes. Therefore you have a 50 percent\nchance of rolling a 1, 2, or 3. If you are playing craps in Las Vegas, the chance of rolling a 7 or\n11 in a single throw is the number of combinations that sum to 7 or 11 divided by the total\nnumber of combinations that can be thrown with two dice, or\n\n +\n\n +\n\n =\n\n.*\n\nProbability also enables us to calculate what might be the most useful tool in all of\nmanagerial decision making, particularly finance: expected value. The expected value takes\nbasic probability one step further. The expected value or payoff from some event, say\npurchasing a lottery ticket, is the sum of all the different outcomes, each weighted by its\nprobability and payoff. As usual, an example makes this clearer. Suppose you are invited to\nplay a game in which you roll a single die.", "tokens": 492, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 62, "segment_id": "00062", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000102"}
{"type": "chunk", "text": "The expected value takes\nbasic probability one step further. The expected value or payoff from some event, say\npurchasing a lottery ticket, is the sum of all the different outcomes, each weighted by its\nprobability and payoff. As usual, an example makes this clearer. Suppose you are invited to\nplay a game in which you roll a single die. The payoff to this game is $1 if you roll a 1; $2 if\nyou roll a 2; $3 if you roll a 3; and so on. What is the expected value for a single roll of the\ndie? Each possible outcome has a\n ($2) +\n\n probability, so the expected value is:\n\n, or $3.50. ($6) =\n\n ($3) +\n\n ($4) +\n\n ($5) +\n\n ($1) +\n\nAt first glance, the expected value of $3.50 might appear to be a relatively useless figure. After all, you can’t actually earn $3.50 with a single roll of the die (since your payoff has to be\na whole number). In fact, the expected value turns out to be extremely powerful because it can\ntell you whether a particular event is “fair,” given its price and expected outcome. Suppose\nyou have the chance to play the above game for $3 a throw. Does it make sense to play? Yes,\nbecause the expected value of the outcome ($3.50) is higher than the cost of playing ($3.00). This does not guarantee that you will make money by playing once, but it does help clarify\nwhich risks are worth taking and which are not. We can take this hypothetical example and apply it to professional football. As noted\nearlier, after a touchdown, teams have a choice between kicking an extra point and attempting a\ntwo-point conversion. The former involves kicking the ball through the goalposts from the three\nyard line; the latter involves running or passing it into the end zone from the three yard line,\nwhich is significantly more difficult. Teams can choose the easy option and get one point, or", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThe expected value takes\nbasic probability one step further. The expected value or payoff from some event, say\npurchasing a lottery ticket, is the sum of all the different outcomes, each weighted by its\nprobability and payoff. As usual, an example makes this clearer. Suppose you are invited to\nplay a game in which you roll a single die. The payoff to this game is $1 if you roll a 1; $2 if\nyou roll a 2; $3 if you roll a 3; and so on. What is the expected value for a single roll of the\ndie? Each possible outcome has a\n ($2) +\n\n probability, so the expected value is:\n\n, or $3.50. ($6) =\n\n ($3) +\n\n ($4) +\n\n ($5) +\n\n ($1) +\n\nAt first glance, the expected value of $3.50 might appear to be a relatively useless figure. After all, you can’t actually earn $3.50 with a single roll of the die (since your payoff has to be\na whole number). In fact, the expected value turns out to be extremely powerful because it can\ntell you whether a particular event is “fair,” given its price and expected outcome. Suppose\nyou have the chance to play the above game for $3 a throw. Does it make sense to play? Yes,\nbecause the expected value of the outcome ($3.50) is higher than the cost of playing ($3.00). This does not guarantee that you will make money by playing once, but it does help clarify\nwhich risks are worth taking and which are not. We can take this hypothetical example and apply it to professional football. As noted\nearlier, after a touchdown, teams have a choice between kicking an extra point and attempting a\ntwo-point conversion. The former involves kicking the ball through the goalposts from the three\nyard line; the latter involves running or passing it into the end zone from the three yard line,\nwhich is significantly more difficult. Teams can choose the easy option and get one point, or", "tokens": 426, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 62, "segment_id": "00062", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000103"}
{"type": "chunk", "text": "they can choose the harder option and get two points. What to do? Statisticians may not play football or date cheerleaders, but they can provide statistical\nguidance for football coaches.9 As pointed out earlier, the probability of making the kick after\na touchdown is .94. This means that the expected value of a point-after attempt is also .94,\nsince it equals the payoff (1 point) multiplied by the probability of success (.94). No team ever\nscores .94 points, but this figure is helpful in quantifying the value of attempting this option\nafter a touchdown relative to the alternative, which is the two-point conversion. The expected\nvalue of “going for two” is much lower: .74. Yes, the payoff is higher (2 points), but the\nsuccess rate is dramatically lower (.37). Obviously if there is one second left in the game and\na team is behind by two points after scoring a touchdown, it has no choice but to go for a twopoint conversion. But if a team’s goal is to maximize points scored over time, then kicking the\nextra point is the strategy that will do that. The same basic analysis can illustrate why you should never buy a lottery ticket. In Illinois,\nthe probabilities associated with the various possible payoffs for the game are printed on the\nback of each ticket. I purchased a $1 instant ticket. (Note to self: Is this tax deductible?) On\nthe back---in tiny, tiny print---are the chances of winning different cash prizes, or a free new\nticket: 1 in 10 (free ticket); 1 in 15 ($2); 1 in 42.86 ($4); 1 in 75 ($5); and so on up to the 1 in\n40,000 chance of winning $1,000. I calculated the expected payout for my instant ticket by\nadding up each possible cash prize weighted by its probability.* It turns out that my $1 lottery\nticket has an expected payout of roughly $.56, making it an absolutely miserable way to spend\n$1. As luck would have it, I won $2. My $2 prize notwithstanding, buying the ticket was a stupid thing to do. This is one of the\ncrucial lessons of probability. Good decisions---as measured by the underlying probabilities---\ncan turn out badly. And bad decisions---like spending $1 on the Illinois lottery---can still turn\nout well, at least in the short run.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nthey can choose the harder option and get two points. What to do? Statisticians may not play football or date cheerleaders, but they can provide statistical\nguidance for football coaches.9 As pointed out earlier, the probability of making the kick after\na touchdown is .94. This means that the expected value of a point-after attempt is also .94,\nsince it equals the payoff (1 point) multiplied by the probability of success (.94). No team ever\nscores .94 points, but this figure is helpful in quantifying the value of attempting this option\nafter a touchdown relative to the alternative, which is the two-point conversion. The expected\nvalue of “going for two” is much lower: .74. Yes, the payoff is higher (2 points), but the\nsuccess rate is dramatically lower (.37). Obviously if there is one second left in the game and\na team is behind by two points after scoring a touchdown, it has no choice but to go for a twopoint conversion. But if a team’s goal is to maximize points scored over time, then kicking the\nextra point is the strategy that will do that. The same basic analysis can illustrate why you should never buy a lottery ticket. In Illinois,\nthe probabilities associated with the various possible payoffs for the game are printed on the\nback of each ticket. I purchased a $1 instant ticket. (Note to self: Is this tax deductible?) On\nthe back---in tiny, tiny print---are the chances of winning different cash prizes, or a free new\nticket: 1 in 10 (free ticket); 1 in 15 ($2); 1 in 42.86 ($4); 1 in 75 ($5); and so on up to the 1 in\n40,000 chance of winning $1,000. I calculated the expected payout for my instant ticket by\nadding up each possible cash prize weighted by its probability.* It turns out that my $1 lottery\nticket has an expected payout of roughly $.56, making it an absolutely miserable way to spend\n$1. As luck would have it, I won $2. My $2 prize notwithstanding, buying the ticket was a stupid thing to do. This is one of the\ncrucial lessons of probability. Good decisions---as measured by the underlying probabilities---\ncan turn out badly. And bad decisions---like spending $1 on the Illinois lottery---can still turn\nout well, at least in the short run.", "tokens": 512, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 63, "segment_id": "00063", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000104"}
{"type": "chunk", "text": "As luck would have it, I won $2. My $2 prize notwithstanding, buying the ticket was a stupid thing to do. This is one of the\ncrucial lessons of probability. Good decisions---as measured by the underlying probabilities---\ncan turn out badly. And bad decisions---like spending $1 on the Illinois lottery---can still turn\nout well, at least in the short run. But probability triumphs in the end. An important theorem\nknown as the law of large numbers tells us that as the number of trials increases, the average of\nthe outcomes will get closer and closer to its expected value. Yes, I won $2 playing the lotto\ntoday. And I might win $2 again tomorrow. But if I buy thousands of $1 lottery tickets, each\nwith an expected payout of $.56, then it becomes a near mathematical certainty that I will lose\nmoney. By the time I’ve spent $1 million on tickets, I’m going to end up with something\nstrikingly close to $560,000. The law of large numbers explains why casinos always make money in the long run. The\nprobabilities associated with all casino games favor the house (assuming that the casino can\nsuccessfully prevent blackjack players from counting cards). If enough bets are wagered over a\nlong enough time, the casino will be certain to win more than it loses. The law of large numbers\nalso demonstrates why Schlitz was much better off doing 100 blind taste tests at halftime of the\nSuper Bowl rather than just 10. Check out the “probability density functions” for a Schlitz type\nof test with 10, 100, and 1,000 trials. (Although it sounds fancy, a probability density function\nmerely plots the assorted outcomes along the x-axis and the expected probability of each\noutcome on the y-axis; the weighted probabilities---each outcome multiplied by its expected\nfrequency---will add up to 1.) Again I’m assuming that the taste test is just like a coin flip and", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nAs luck would have it, I won $2. My $2 prize notwithstanding, buying the ticket was a stupid thing to do. This is one of the\ncrucial lessons of probability. Good decisions---as measured by the underlying probabilities---\ncan turn out badly. And bad decisions---like spending $1 on the Illinois lottery---can still turn\nout well, at least in the short run. But probability triumphs in the end. An important theorem\nknown as the law of large numbers tells us that as the number of trials increases, the average of\nthe outcomes will get closer and closer to its expected value. Yes, I won $2 playing the lotto\ntoday. And I might win $2 again tomorrow. But if I buy thousands of $1 lottery tickets, each\nwith an expected payout of $.56, then it becomes a near mathematical certainty that I will lose\nmoney. By the time I’ve spent $1 million on tickets, I’m going to end up with something\nstrikingly close to $560,000. The law of large numbers explains why casinos always make money in the long run. The\nprobabilities associated with all casino games favor the house (assuming that the casino can\nsuccessfully prevent blackjack players from counting cards). If enough bets are wagered over a\nlong enough time, the casino will be certain to win more than it loses. The law of large numbers\nalso demonstrates why Schlitz was much better off doing 100 blind taste tests at halftime of the\nSuper Bowl rather than just 10. Check out the “probability density functions” for a Schlitz type\nof test with 10, 100, and 1,000 trials. (Although it sounds fancy, a probability density function\nmerely plots the assorted outcomes along the x-axis and the expected probability of each\noutcome on the y-axis; the weighted probabilities---each outcome multiplied by its expected\nfrequency---will add up to 1.) Again I’m assuming that the taste test is just like a coin flip and", "tokens": 416, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 63, "segment_id": "00063", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000105"}
{"type": "chunk", "text": "each tester has a .5 probability of choosing Schlitz. As you can see below, the expected\noutcome converges around 50 percent of tasters’ choosing Schlitz as the number of tasters gets\nlarger. At the same time, the probability of getting an outcome that deviates sharply from 50\npercent falls sharply as the number of trials gets large. 10 Trials\n\n100 Trials\n\n1,000 Trials\n\nI stipulated earlier that Schlitz executives would be happy if 40 percent or more of the\nMichelob drinkers chose Schlitz in the blind taste test. The figures below reflect the probability\nof getting that outcome as the number of tasters gets larger:\n\n10 blind taste testers: .83\n100 blind taste testers: .98", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\neach tester has a .5 probability of choosing Schlitz. As you can see below, the expected\noutcome converges around 50 percent of tasters’ choosing Schlitz as the number of tasters gets\nlarger. At the same time, the probability of getting an outcome that deviates sharply from 50\npercent falls sharply as the number of trials gets large. 10 Trials\n\n100 Trials\n\n1,000 Trials\n\nI stipulated earlier that Schlitz executives would be happy if 40 percent or more of the\nMichelob drinkers chose Schlitz in the blind taste test. The figures below reflect the probability\nof getting that outcome as the number of tasters gets larger:\n\n10 blind taste testers: .83\n100 blind taste testers: .98", "tokens": 156, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 64, "segment_id": "00064", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000106"}
{"type": "chunk", "text": "1,000 blind taste testers: .9999999999\n1,000,000 blind taste testers: 1\n\nBy now the intuition is obvious behind the chapter subtitle, “Don’t buy the extended\nwarranty on your $99 printer.” Okay, maybe that’s not so obvious. Let me back up. The entire\ninsurance industry is built on probability. (A warranty is just a form of insurance.) When you\ninsure anything, you are contracting to receive some specified payoff in the event of a clearly\ndefined contingency. For example, your auto insurance will replace your car in the event that it\ngets stolen or crushed by a tree. In exchange for this guarantee, you agree to pay some fixed\namount of money for the period in which you are insured. The key idea is that in exchange for\na regular and predictable payment, you have transferred to the insurance company the risk of\nhaving your car stolen, crushed, or even totaled by your own bad driving. Why are these companies willing to assume such risks? Because they will earn large profits\nin the long run if they price their premiums correctly. Obviously some cars insured by Allstate\nwill get stolen. Others will get totaled when their owners drive over a fire hydrant, as happened\nto my high school girlfriend. (She also had to replace the fire hydrant, which is far more\nexpensive than you might think.) But most cars insured by Allstate or any other company will\nbe just fine. To make money, the insurance company need only collect more in premiums than\nit pays out in claims. And to do that, the firm must have a solid grasp of what is known in\nindustry jargon as the “expected loss” on every policy. This is exactly the same concept as\nexpected value, only with an insurance twist. If your car is insured for $40,000, and the\nchances of its getting stolen in any given year are 1 in 1,000, then the annual expected loss on\nyour car is $40. The annual premium for the theft portion of the coverage needs to be more\nthan $40. At that point, the insurance company becomes just like the casino or the Illinois\nlottery. Yes, there will be payouts, but over the long run what comes in will be more than what\ngoes out. As a consumer, you should recognize that insurance will not save you money in the long run.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\n1,000 blind taste testers: .9999999999\n1,000,000 blind taste testers: 1\n\nBy now the intuition is obvious behind the chapter subtitle, “Don’t buy the extended\nwarranty on your $99 printer.” Okay, maybe that’s not so obvious. Let me back up. The entire\ninsurance industry is built on probability. (A warranty is just a form of insurance.) When you\ninsure anything, you are contracting to receive some specified payoff in the event of a clearly\ndefined contingency. For example, your auto insurance will replace your car in the event that it\ngets stolen or crushed by a tree. In exchange for this guarantee, you agree to pay some fixed\namount of money for the period in which you are insured. The key idea is that in exchange for\na regular and predictable payment, you have transferred to the insurance company the risk of\nhaving your car stolen, crushed, or even totaled by your own bad driving. Why are these companies willing to assume such risks? Because they will earn large profits\nin the long run if they price their premiums correctly. Obviously some cars insured by Allstate\nwill get stolen. Others will get totaled when their owners drive over a fire hydrant, as happened\nto my high school girlfriend. (She also had to replace the fire hydrant, which is far more\nexpensive than you might think.) But most cars insured by Allstate or any other company will\nbe just fine. To make money, the insurance company need only collect more in premiums than\nit pays out in claims. And to do that, the firm must have a solid grasp of what is known in\nindustry jargon as the “expected loss” on every policy. This is exactly the same concept as\nexpected value, only with an insurance twist. If your car is insured for $40,000, and the\nchances of its getting stolen in any given year are 1 in 1,000, then the annual expected loss on\nyour car is $40. The annual premium for the theft portion of the coverage needs to be more\nthan $40. At that point, the insurance company becomes just like the casino or the Illinois\nlottery. Yes, there will be payouts, but over the long run what comes in will be more than what\ngoes out. As a consumer, you should recognize that insurance will not save you money in the long run.", "tokens": 501, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 65, "segment_id": "00065", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000107"}
{"type": "chunk", "text": "If your car is insured for $40,000, and the\nchances of its getting stolen in any given year are 1 in 1,000, then the annual expected loss on\nyour car is $40. The annual premium for the theft portion of the coverage needs to be more\nthan $40. At that point, the insurance company becomes just like the casino or the Illinois\nlottery. Yes, there will be payouts, but over the long run what comes in will be more than what\ngoes out. As a consumer, you should recognize that insurance will not save you money in the long run. What it will do is prevent some unacceptably high loss, such as replacing a $40,000 car that\nwas stolen or a $350,000 house that burned down. Buying insurance is a “bad bet” from a\nstatistical standpoint since you will pay the insurance company, on average, more than you get\nback. Yet it can still be a sensible tool for protecting against outcomes that would otherwise\nwreck your life. Ironically, someone as rich as Warren Buffett can save money by not\npurchasing car insurance, homeowner’s insurance, or even health insurance because he can\nafford whatever bad things might happen to him. Which finally brings us back to your $99 printer! We’ll assume that you’ve just picked out\nthe perfect new laser printer at Best Buy or some other retailer.* When you reach the checkout\ncounter, the sales assistant will offer you a series of extended warranty options. For another\n$25 or $50, Best Buy will fix or replace the printer should it break in the next year or two. On\nthe basis of your understanding of probability, insurance, and basic economics, you should\nimmediately be able to surmise all of the following: (1) Best Buy is a for-profit business that\nseeks to maximize profits. (2) The sales assistant is eager for you to buy the extended\nwarranty. (3) From numbers 1 and 2, we can infer that the cost of the warranty to you is greater", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nIf your car is insured for $40,000, and the\nchances of its getting stolen in any given year are 1 in 1,000, then the annual expected loss on\nyour car is $40. The annual premium for the theft portion of the coverage needs to be more\nthan $40. At that point, the insurance company becomes just like the casino or the Illinois\nlottery. Yes, there will be payouts, but over the long run what comes in will be more than what\ngoes out. As a consumer, you should recognize that insurance will not save you money in the long run. What it will do is prevent some unacceptably high loss, such as replacing a $40,000 car that\nwas stolen or a $350,000 house that burned down. Buying insurance is a “bad bet” from a\nstatistical standpoint since you will pay the insurance company, on average, more than you get\nback. Yet it can still be a sensible tool for protecting against outcomes that would otherwise\nwreck your life. Ironically, someone as rich as Warren Buffett can save money by not\npurchasing car insurance, homeowner’s insurance, or even health insurance because he can\nafford whatever bad things might happen to him. Which finally brings us back to your $99 printer! We’ll assume that you’ve just picked out\nthe perfect new laser printer at Best Buy or some other retailer.* When you reach the checkout\ncounter, the sales assistant will offer you a series of extended warranty options. For another\n$25 or $50, Best Buy will fix or replace the printer should it break in the next year or two. On\nthe basis of your understanding of probability, insurance, and basic economics, you should\nimmediately be able to surmise all of the following: (1) Best Buy is a for-profit business that\nseeks to maximize profits. (2) The sales assistant is eager for you to buy the extended\nwarranty. (3) From numbers 1 and 2, we can infer that the cost of the warranty to you is greater", "tokens": 433, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 65, "segment_id": "00065", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000108"}
{"type": "chunk", "text": "than the expected cost of fixing or repairing the printer for Best Buy. If this were not the case,\nBest Buy would not be so aggressive in trying to sell it to you. (4) If your $99 printer breaks\nand you have to pay out of pocket to fix or replace it, this will not meaningfully change your\nlife. On average, you’ll pay more for the extended warranty than you would to repair the printer. The broader lesson---and one of the core lessons of personal finance---is that you should\nalways insure yourself against any adverse contingency that you cannot comfortably afford to\nwithstand. You should skip buying insurance on everything else. Expected value can also help us untangle complex decisions that involve many contingencies\nat different points in time. Suppose a friend of yours has asked you to invest $1 million in a\nresearch venture examining a new cure for male pattern baldness. You would probably ask\nwhat the likelihood of success will be; you’ll get a complicated answer. This is a research\nproject, so there is only a 30 percent chance that the team will discover a cure that works. If\nthe team does not find a cure, you will get $250,000 of your investment back, as those funds\nwill have been reserved for taking the drug to market (testing, marketing, etc.) Even if the\nresearchers are successful, there is only a 60 percent chance that the U.S. Food and Drug\nAdministration will approve the new miracle baldness cure as safe for use on humans. Even\nthen, if the drug is safe and effective, there is a 10 percent chance that a competitor will come\nto market with a better drug at about the same time, wiping out any potential profits. If\neverything goes well---the drug is safe, effective, and unchallenged by competitors---then the\nbest estimate on the return on your investment is $25 million. Should you make the investment? This seems like a muddle of information. The potential payday is huge---25 times your\ninitial investment---but there are so many potential pitfalls. A decision tree can help organize\nthis kind of information and---if the probabilities associated with each outcome are correct---\ngive you a probabilistic assessment of what you ought to do. The decision tree maps out each\nsource of uncertainty and the probabilities associated with all possible outcomes. The end of\nthe tree gives us all the possible payoffs and the probability of each.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nthan the expected cost of fixing or repairing the printer for Best Buy. If this were not the case,\nBest Buy would not be so aggressive in trying to sell it to you. (4) If your $99 printer breaks\nand you have to pay out of pocket to fix or replace it, this will not meaningfully change your\nlife. On average, you’ll pay more for the extended warranty than you would to repair the printer. The broader lesson---and one of the core lessons of personal finance---is that you should\nalways insure yourself against any adverse contingency that you cannot comfortably afford to\nwithstand. You should skip buying insurance on everything else. Expected value can also help us untangle complex decisions that involve many contingencies\nat different points in time. Suppose a friend of yours has asked you to invest $1 million in a\nresearch venture examining a new cure for male pattern baldness. You would probably ask\nwhat the likelihood of success will be; you’ll get a complicated answer. This is a research\nproject, so there is only a 30 percent chance that the team will discover a cure that works. If\nthe team does not find a cure, you will get $250,000 of your investment back, as those funds\nwill have been reserved for taking the drug to market (testing, marketing, etc.) Even if the\nresearchers are successful, there is only a 60 percent chance that the U.S. Food and Drug\nAdministration will approve the new miracle baldness cure as safe for use on humans. Even\nthen, if the drug is safe and effective, there is a 10 percent chance that a competitor will come\nto market with a better drug at about the same time, wiping out any potential profits. If\neverything goes well---the drug is safe, effective, and unchallenged by competitors---then the\nbest estimate on the return on your investment is $25 million. Should you make the investment? This seems like a muddle of information. The potential payday is huge---25 times your\ninitial investment---but there are so many potential pitfalls. A decision tree can help organize\nthis kind of information and---if the probabilities associated with each outcome are correct---\ngive you a probabilistic assessment of what you ought to do. The decision tree maps out each\nsource of uncertainty and the probabilities associated with all possible outcomes. The end of\nthe tree gives us all the possible payoffs and the probability of each.", "tokens": 505, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 66, "segment_id": "00066", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000109"}
{"type": "chunk", "text": "Should you make the investment? This seems like a muddle of information. The potential payday is huge---25 times your\ninitial investment---but there are so many potential pitfalls. A decision tree can help organize\nthis kind of information and---if the probabilities associated with each outcome are correct---\ngive you a probabilistic assessment of what you ought to do. The decision tree maps out each\nsource of uncertainty and the probabilities associated with all possible outcomes. The end of\nthe tree gives us all the possible payoffs and the probability of each. If we weight each payoff\nby its likelihood, and sum all the possibilities, we will get the expected value of this\ninvestment opportunity. As usual, the best way to understand this is to take a look. The Investment Decision", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nShould you make the investment? This seems like a muddle of information. The potential payday is huge---25 times your\ninitial investment---but there are so many potential pitfalls. A decision tree can help organize\nthis kind of information and---if the probabilities associated with each outcome are correct---\ngive you a probabilistic assessment of what you ought to do. The decision tree maps out each\nsource of uncertainty and the probabilities associated with all possible outcomes. The end of\nthe tree gives us all the possible payoffs and the probability of each. If we weight each payoff\nby its likelihood, and sum all the possibilities, we will get the expected value of this\ninvestment opportunity. As usual, the best way to understand this is to take a look. The Investment Decision", "tokens": 156, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 66, "segment_id": "00066", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000110"}
{"type": "chunk", "text": "This particular opportunity has an attractive expected value. The weighted payoff is $4.225\nmillion. Still, this investment may not be the wisest thing to do with the college tuition money\nthat you’ve squirreled away for your children. The decision tree lets you know that your\nexpected payoff is far higher than what you are being asked to invest. On the other hand, the\nmost likely outcome, meaning the one that will happen most often, is that the company will not\ndiscover a cure for baldness and you will get only $250,000 back. Your appetite for this\ninvestment might depend on your risk profile. The law of large numbers suggests that an\ninvestment firm, or a rich individual like Warren Buffet, should seek out hundreds of\nopportunities like this with uncertain outcomes but attractive expected returns. Some will\nwork; many won’t. On average, these investors will make a lot of money, just like an insurance\ncompany or a casino. If the expected payoff is in your favor, more trials are always better. The same basic process can be used to explain a seemingly counterintuitive phenomenon. Sometimes it does not make sense to screen the entire population for a rare but serious\ndisease, such as HIV/AIDS. Suppose we can test for some rare disease with a high degree of\naccuracy. For the sake of example, let’s assume the disease affects 1 of every 100,000 adults\nand the test is 99.9999 percent accurate. The test never generates a false negative (meaning\nthat it never misses someone who has the disease); however, roughly 1 in 10,000 tests\nconducted on a healthy person will generate a false positive, meaning that the person tests\npositive but does not actually have the disease. The striking outcome here is that despite the\nimpressive accuracy of the test, most of the people who test positive will not have the disease. This will generate enormous anxiety among those who falsely test positive; it can also waste\nfinite health care resources on follow-up tests and treatment. If we test the entire American adult population, or roughly 175 million people, the decision\n\ntree looks like the following:\n\nWidespread Screening for a Rare Disease", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThis particular opportunity has an attractive expected value. The weighted payoff is $4.225\nmillion. Still, this investment may not be the wisest thing to do with the college tuition money\nthat you’ve squirreled away for your children. The decision tree lets you know that your\nexpected payoff is far higher than what you are being asked to invest. On the other hand, the\nmost likely outcome, meaning the one that will happen most often, is that the company will not\ndiscover a cure for baldness and you will get only $250,000 back. Your appetite for this\ninvestment might depend on your risk profile. The law of large numbers suggests that an\ninvestment firm, or a rich individual like Warren Buffet, should seek out hundreds of\nopportunities like this with uncertain outcomes but attractive expected returns. Some will\nwork; many won’t. On average, these investors will make a lot of money, just like an insurance\ncompany or a casino. If the expected payoff is in your favor, more trials are always better. The same basic process can be used to explain a seemingly counterintuitive phenomenon. Sometimes it does not make sense to screen the entire population for a rare but serious\ndisease, such as HIV/AIDS. Suppose we can test for some rare disease with a high degree of\naccuracy. For the sake of example, let’s assume the disease affects 1 of every 100,000 adults\nand the test is 99.9999 percent accurate. The test never generates a false negative (meaning\nthat it never misses someone who has the disease); however, roughly 1 in 10,000 tests\nconducted on a healthy person will generate a false positive, meaning that the person tests\npositive but does not actually have the disease. The striking outcome here is that despite the\nimpressive accuracy of the test, most of the people who test positive will not have the disease. This will generate enormous anxiety among those who falsely test positive; it can also waste\nfinite health care resources on follow-up tests and treatment. If we test the entire American adult population, or roughly 175 million people, the decision\n\ntree looks like the following:\n\nWidespread Screening for a Rare Disease", "tokens": 456, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 67, "segment_id": "00067", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000111"}
{"type": "chunk", "text": "Only 1,750 adults have the disease. They all test positive. Over 174 million adults do not\nhave the disease. Of this healthy group who are tested, 99.9999 get the correct result that they\ndo not have the disease. Only .0001 get a false positive. But .0001 of 174 million is still a big\nnumber. In fact, 17,500 people will, on average, get false positives. Let’s look at what that means. A total of 19,250 people are notified that they have the\ndisease; only 9 percent of them are actually sick! And that’s with a test that has a very low rate\nof false positives. Without going too far off topic, this should give you some insight into why\ncost containment in health care sometimes involves less screening of healthy people for\ndiseases, not more. In the case of a disease like HIV/AIDS, public health officials will often\nrecommend that the resources available be used to screen the populations at highest risk, such\nas gay men or intravenous drug users. Sometimes probability helps us by flagging suspicious patterns. Chapter 1 introduced the\nproblem of institutionalized cheating on standardized tests and one of the firms that roots it out,\nCaveon Test Security. The Securities and Exchange Commission (SEC), the government\nagency responsible for enforcing federal laws related to securities trading, uses a similar\nmethodology for catching inside traders. (Inside trading involves illegally using private\ninformation, such as a law firm’s knowledge of an impending corporate acquisition, to trade\nstock or other securities in the affected companies.) The SEC uses powerful computers to\nscrutinize hundreds of millions of stock trades and look for suspicious activity, such as a big\npurchase of shares in a company just before a takeover is announced, or the dumping of shares\njust before a company announces disappointing earnings.10 The SEC will also investigate\ninvestment managers with unusually high returns over long periods of time. (Both economic", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nOnly 1,750 adults have the disease. They all test positive. Over 174 million adults do not\nhave the disease. Of this healthy group who are tested, 99.9999 get the correct result that they\ndo not have the disease. Only .0001 get a false positive. But .0001 of 174 million is still a big\nnumber. In fact, 17,500 people will, on average, get false positives. Let’s look at what that means. A total of 19,250 people are notified that they have the\ndisease; only 9 percent of them are actually sick! And that’s with a test that has a very low rate\nof false positives. Without going too far off topic, this should give you some insight into why\ncost containment in health care sometimes involves less screening of healthy people for\ndiseases, not more. In the case of a disease like HIV/AIDS, public health officials will often\nrecommend that the resources available be used to screen the populations at highest risk, such\nas gay men or intravenous drug users. Sometimes probability helps us by flagging suspicious patterns. Chapter 1 introduced the\nproblem of institutionalized cheating on standardized tests and one of the firms that roots it out,\nCaveon Test Security. The Securities and Exchange Commission (SEC), the government\nagency responsible for enforcing federal laws related to securities trading, uses a similar\nmethodology for catching inside traders. (Inside trading involves illegally using private\ninformation, such as a law firm’s knowledge of an impending corporate acquisition, to trade\nstock or other securities in the affected companies.) The SEC uses powerful computers to\nscrutinize hundreds of millions of stock trades and look for suspicious activity, such as a big\npurchase of shares in a company just before a takeover is announced, or the dumping of shares\njust before a company announces disappointing earnings.10 The SEC will also investigate\ninvestment managers with unusually high returns over long periods of time. (Both economic", "tokens": 411, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 68, "segment_id": "00068", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000112"}
{"type": "chunk", "text": "theory and historical data suggest that it is extremely difficult for a single investor to get\nabove-average returns year after year.) Of course, smart investors are always trying to\nanticipate good and bad news and to devise perfectly legal strategies that consistently beat the\nmarket. Being a good investor does not necessarily make one a criminal. How does a computer\ntell the difference? I called the enforcement division of the SEC several times to ask what\nparticular patterns are most likely to signal criminal activity. They still have not called me\nback. In the 2002 film Minority Report, Tom Cruise plays a “pre-crime” detective who is part of a\nbureau that uses technology to predict crimes before they’re committed. Well folks, that’s not science fiction anymore. In 2011, the New York Times ran the\nfollowing headline: “Sending the Police before There’s a Crime.”11 The story described how\ndetectives were dispatched to a parking garage in downtown Santa Cruz by a computer\nprogram that predicted that there was a high likelihood of burglaries from cars at that location\non that day. Police subsequently arrested two women peering into car windows. One had\noutstanding arrest warrants; the other was carrying illegal drugs. The Santa Cruz system was designed by two mathematicians, an anthropologist, and a\ncriminologist. The Chicago Police Department has created an entire predictive analytics unit,\nin part because gang activity, the source of much of the city’s violence, follows certain\npatterns. The book Data Mining and Predictive Analysis: Intelligence Gathering and Crime\nAnalysis, a guide to statistics for law enforcement, begins enthusiastically, “It is now possible\nto predict the future when it comes to crime, such as identifying crime trends, anticipating\nhotspots in the community, refining resource deployment decisions, and ensuring the greatest\nprotection for citizens in the most efficient manner.” (Look, I read these kinds of things so that\nyou don’t have to.)\n\n“Predictive policing” is part of a broader movement called predictive analytics. Crime will\nalways involve an element of uncertainty, as will determining who is going to crash his car or\ndefault on her mortgage. Probability helps us navigate those risks. And information refines our\nunderstanding of the relevant probabilities. Businesses facing uncertainty have always sought\nto quantify their risks. Lenders request things like income verification and a credit score. Yet\nthese blunt credit instruments are starting to feel like the prediction equivalent of a caveman’s\nstone tools.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\ntheory and historical data suggest that it is extremely difficult for a single investor to get\nabove-average returns year after year.) Of course, smart investors are always trying to\nanticipate good and bad news and to devise perfectly legal strategies that consistently beat the\nmarket. Being a good investor does not necessarily make one a criminal. How does a computer\ntell the difference? I called the enforcement division of the SEC several times to ask what\nparticular patterns are most likely to signal criminal activity. They still have not called me\nback. In the 2002 film Minority Report, Tom Cruise plays a “pre-crime” detective who is part of a\nbureau that uses technology to predict crimes before they’re committed. Well folks, that’s not science fiction anymore. In 2011, the New York Times ran the\nfollowing headline: “Sending the Police before There’s a Crime.”11 The story described how\ndetectives were dispatched to a parking garage in downtown Santa Cruz by a computer\nprogram that predicted that there was a high likelihood of burglaries from cars at that location\non that day. Police subsequently arrested two women peering into car windows. One had\noutstanding arrest warrants; the other was carrying illegal drugs. The Santa Cruz system was designed by two mathematicians, an anthropologist, and a\ncriminologist. The Chicago Police Department has created an entire predictive analytics unit,\nin part because gang activity, the source of much of the city’s violence, follows certain\npatterns. The book Data Mining and Predictive Analysis: Intelligence Gathering and Crime\nAnalysis, a guide to statistics for law enforcement, begins enthusiastically, “It is now possible\nto predict the future when it comes to crime, such as identifying crime trends, anticipating\nhotspots in the community, refining resource deployment decisions, and ensuring the greatest\nprotection for citizens in the most efficient manner.” (Look, I read these kinds of things so that\nyou don’t have to.)\n\n“Predictive policing” is part of a broader movement called predictive analytics. Crime will\nalways involve an element of uncertainty, as will determining who is going to crash his car or\ndefault on her mortgage. Probability helps us navigate those risks. And information refines our\nunderstanding of the relevant probabilities. Businesses facing uncertainty have always sought\nto quantify their risks. Lenders request things like income verification and a credit score. Yet\nthese blunt credit instruments are starting to feel like the prediction equivalent of a caveman’s\nstone tools.", "tokens": 511, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 69, "segment_id": "00069", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000113"}
{"type": "chunk", "text": "Crime will\nalways involve an element of uncertainty, as will determining who is going to crash his car or\ndefault on her mortgage. Probability helps us navigate those risks. And information refines our\nunderstanding of the relevant probabilities. Businesses facing uncertainty have always sought\nto quantify their risks. Lenders request things like income verification and a credit score. Yet\nthese blunt credit instruments are starting to feel like the prediction equivalent of a caveman’s\nstone tools. The confluence of huge amounts of digital data and cheap computing power has\ngenerated fascinating insights into human behavior. Insurance officials correctly describe their\nbusiness as the “transfer of risk”---and so they had better understand the risks being transferred\nto them. Companies like Allstate are in the business of knowing things that might otherwise\nseem like random trivia:12\n\n• Twenty to twenty-four-year-old drivers are the most likely to be involved in a fatal crash. • The most commonly stolen car in Illinois is the Honda Civic (as opposed to full-size\n\nChevrolet pickups in Alabama).*\n\n• Texting while driving causes crashes, but state laws banning the practice do not seem to\nstop drivers from doing it. In fact, such laws might even make things worse by prompting", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nCrime will\nalways involve an element of uncertainty, as will determining who is going to crash his car or\ndefault on her mortgage. Probability helps us navigate those risks. And information refines our\nunderstanding of the relevant probabilities. Businesses facing uncertainty have always sought\nto quantify their risks. Lenders request things like income verification and a credit score. Yet\nthese blunt credit instruments are starting to feel like the prediction equivalent of a caveman’s\nstone tools. The confluence of huge amounts of digital data and cheap computing power has\ngenerated fascinating insights into human behavior. Insurance officials correctly describe their\nbusiness as the “transfer of risk”---and so they had better understand the risks being transferred\nto them. Companies like Allstate are in the business of knowing things that might otherwise\nseem like random trivia:12\n\n• Twenty to twenty-four-year-old drivers are the most likely to be involved in a fatal crash. • The most commonly stolen car in Illinois is the Honda Civic (as opposed to full-size\n\nChevrolet pickups in Alabama).*\n\n• Texting while driving causes crashes, but state laws banning the practice do not seem to\nstop drivers from doing it. In fact, such laws might even make things worse by prompting", "tokens": 251, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 69, "segment_id": "00069", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000114"}
{"type": "chunk", "text": "drivers to hide their phones and therefore take their eyes off the road while texting. The credit card companies are at the forefront of this kind of analysis, both because they are\nprivy to so much data on our spending habits and because their business model depends so\nheavily on finding customers who are just barely a good credit risk. (The customers who are\nthe best credit risks tend to be money losers because they pay their bills in full each month; the\ncustomers who carry large balances at high interest rates are the ones who generate fat profits\n---as long as they don’t default.) One of the most intriguing studies of who is likely to pay a\nbill and who is likely to walk away wasJ. P. Martin, “a math-loving executive” at\nCanadian Tire, a large retailer that sells a wide range of automotive products and other retail\ngoods.13 When Martin analyzed the data---every transaction using a Canadian Tire credit card\nfrom the prior year---he discovered that what customers purchased was a remarkably precise\npredictor of their subsequent payment behavior when used in conjunction with traditional tools\nlike income and credit history. A New York Times Magazine article entitled “What Does Your Credit Card Company\nKnow about You?” described some of Martin’s most intriguing findings: “People who bought\ncheap, generic automotive oil were much more likely to miss a credit-card payment than\nsomeone who got the expensive, name-brand stuff. People who bought carbon-monoxide\nmonitors for their homes or those little felt pads that stop chair legs from scratching the floor\nalmost never missed payments. Anyone who purchased a chrome-skull car accessory or a\n‘Mega Thruster Exhaust System’ was pretty likely to miss paying his bill eventually.”\n\nProbability gives us tools for dealing with life’s uncertainties. You shouldn’t play the lottery. You should invest in the stock market if you have a long investment horizon (because stocks\ntypically have the best long-term returns). You should buy insurance for some things, but not\nothers. Probability can even help you maximize your winnings on game shows (as the next\nchapter will show.)\n\nThat said (or written), probability is not deterministic. No, you shouldn’t buy a lottery ticket\n---but you still might win money if you do. And yes, probability can help us catch cheaters and\ncriminals---but when used inappropriately it can also send innocent people to jail. That’s why\nwe have Chapter 6.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\ndrivers to hide their phones and therefore take their eyes off the road while texting. The credit card companies are at the forefront of this kind of analysis, both because they are\nprivy to so much data on our spending habits and because their business model depends so\nheavily on finding customers who are just barely a good credit risk. (The customers who are\nthe best credit risks tend to be money losers because they pay their bills in full each month; the\ncustomers who carry large balances at high interest rates are the ones who generate fat profits\n---as long as they don’t default.) One of the most intriguing studies of who is likely to pay a\nbill and who is likely to walk away wasJ. P. Martin, “a math-loving executive” at\nCanadian Tire, a large retailer that sells a wide range of automotive products and other retail\ngoods.13 When Martin analyzed the data---every transaction using a Canadian Tire credit card\nfrom the prior year---he discovered that what customers purchased was a remarkably precise\npredictor of their subsequent payment behavior when used in conjunction with traditional tools\nlike income and credit history. A New York Times Magazine article entitled “What Does Your Credit Card Company\nKnow about You?” described some of Martin’s most intriguing findings: “People who bought\ncheap, generic automotive oil were much more likely to miss a credit-card payment than\nsomeone who got the expensive, name-brand stuff. People who bought carbon-monoxide\nmonitors for their homes or those little felt pads that stop chair legs from scratching the floor\nalmost never missed payments. Anyone who purchased a chrome-skull car accessory or a\n‘Mega Thruster Exhaust System’ was pretty likely to miss paying his bill eventually.”\n\nProbability gives us tools for dealing with life’s uncertainties. You shouldn’t play the lottery. You should invest in the stock market if you have a long investment horizon (because stocks\ntypically have the best long-term returns). You should buy insurance for some things, but not\nothers. Probability can even help you maximize your winnings on game shows (as the next\nchapter will show.)\n\nThat said (or written), probability is not deterministic. No, you shouldn’t buy a lottery ticket\n---but you still might win money if you do. And yes, probability can help us catch cheaters and\ncriminals---but when used inappropriately it can also send innocent people to jail. That’s why\nwe have Chapter 6.", "tokens": 501, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 70, "segment_id": "00070", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000115"}
{"type": "chunk", "text": "You should buy insurance for some things, but not\nothers. Probability can even help you maximize your winnings on game shows (as the next\nchapter will show.)\n\nThat said (or written), probability is not deterministic. No, you shouldn’t buy a lottery ticket\n---but you still might win money if you do. And yes, probability can help us catch cheaters and\ncriminals---but when used inappropriately it can also send innocent people to jail. That’s why\nwe have Chapter 6. * I have in mind “Six Sigma Man.” The lowercase Greek letter sigma, σ, represents the standard deviation. Six Sigma Man is\nsix standard deviations above the norm in terms of statistical ability, strength, and intelligence. † For all of these calculations, I’ve used a handy online binomial calculator, at http://stattrek.com/Tables/Binomial.aspx. * NASA also pointed out that even falling space debris is government property. Apparently it is illegal to keep a satellite\nsouvenir, even if it lands in your backyard. † The Levitt and Dubner calculations are as follows. Each year roughly 550 children under ten drown and 175 children\nunder ten die from gun accidents. The rates they compare are 1 drowning for every 11,000 residential pools compared with\n1 gun death per “million-plus” guns. For adolescents, I suspect the numbers may change sharply, both because they are\nbetter able to swim and because they are more likely to cause a tragedy if they stumble upon a loaded gun. However, I have\nnot checked the data on this point. * There are 6 ways to throw a 7 with two dice: (1,6); (2,5); (3,4); (6,1); (5,2); and (4,3). There are only 2 ways to throw an\n11: (5,6) and (6,5). Meanwhile, there are 36 total possible throws with two dice: (1,1); (1,2); (1,3); (1,4); (1,5); (1,6). And (2,1); (2,2); (2,3);", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nYou should buy insurance for some things, but not\nothers. Probability can even help you maximize your winnings on game shows (as the next\nchapter will show.)\n\nThat said (or written), probability is not deterministic. No, you shouldn’t buy a lottery ticket\n---but you still might win money if you do. And yes, probability can help us catch cheaters and\ncriminals---but when used inappropriately it can also send innocent people to jail. That’s why\nwe have Chapter 6. * I have in mind “Six Sigma Man.” The lowercase Greek letter sigma, σ, represents the standard deviation. Six Sigma Man is\nsix standard deviations above the norm in terms of statistical ability, strength, and intelligence. † For all of these calculations, I’ve used a handy online binomial calculator, at http://stattrek.com/Tables/Binomial.aspx. * NASA also pointed out that even falling space debris is government property. Apparently it is illegal to keep a satellite\nsouvenir, even if it lands in your backyard. † The Levitt and Dubner calculations are as follows. Each year roughly 550 children under ten drown and 175 children\nunder ten die from gun accidents. The rates they compare are 1 drowning for every 11,000 residential pools compared with\n1 gun death per “million-plus” guns. For adolescents, I suspect the numbers may change sharply, both because they are\nbetter able to swim and because they are more likely to cause a tragedy if they stumble upon a loaded gun. However, I have\nnot checked the data on this point. * There are 6 ways to throw a 7 with two dice: (1,6); (2,5); (3,4); (6,1); (5,2); and (4,3). There are only 2 ways to throw an\n11: (5,6) and (6,5). Meanwhile, there are 36 total possible throws with two dice: (1,1); (1,2); (1,3); (1,4); (1,5); (1,6). And (2,1); (2,2); (2,3);", "tokens": 458, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 70, "segment_id": "00070", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000116"}
{"type": "chunk", "text": "(2,4); (2,5); (2,6). And (3,1); (3,2); (3,3); (3,4); (3,5); (3,6). And (4,1); (4,2); (4,3); (4,4); (4,5); (4,6). And (5,1); (5,2);\n(5,3); (5,4); (5,5); (5,6). And, finally, (6,1); (6,2); (6,3); (6,4); (6,5); and (6,6). Thus, the chance of throwing a 7 or 11 is the number of possible ways of throwing either of those two numbers divided by\nthe total number of possible throws with two dice, which is 8/36. Incidentally, much of the earlier research on probability\nwas done by gamblers to determine exactly this kind of thing. * The full expected value for the Illinois Dugout Doubler $1 ticket (rounded to the nearest cent) is as follows: 1/15 ($2) +\n1/42.86 ($4) + 1/75 ($5) + 1/200 ($10) + 1/300 ($25) + 1/1,589.40 ($50) + 1/8000 ($100) + 1/16,000 ($200) +\n1/48,000 ($500) + 1/40,000 ($1,000) = $.13 + $.09 + $.07 + $.05 + $.08 + $.03 + $.01 + $.01 + $.01 + $.03 = $.51. However, there is also a 1/10 chance of getting a free ticket, which has an expected payout of $.51, so the overall expected\npayout is $.51 + .1 ($.51) = $.51 + $.05 = $.56. * Earlier in the book I used an example that involved drunken employees producing defective laser printers. You will need\nto forget that example here and assume that the company has fixed its quality problems. * Since I’ve admonished you to be a stickler about descriptive statistics, I feel compelled to point out that the most\ncommonly stolen car is not necessarily the kind of car that is most likely to be stolen.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\n(2,4); (2,5); (2,6). And (3,1); (3,2); (3,3); (3,4); (3,5); (3,6). And (4,1); (4,2); (4,3); (4,4); (4,5); (4,6). And (5,1); (5,2);\n(5,3); (5,4); (5,5); (5,6). And, finally, (6,1); (6,2); (6,3); (6,4); (6,5); and (6,6). Thus, the chance of throwing a 7 or 11 is the number of possible ways of throwing either of those two numbers divided by\nthe total number of possible throws with two dice, which is 8/36. Incidentally, much of the earlier research on probability\nwas done by gamblers to determine exactly this kind of thing. * The full expected value for the Illinois Dugout Doubler $1 ticket (rounded to the nearest cent) is as follows: 1/15 ($2) +\n1/42.86 ($4) + 1/75 ($5) + 1/200 ($10) + 1/300 ($25) + 1/1,589.40 ($50) + 1/8000 ($100) + 1/16,000 ($200) +\n1/48,000 ($500) + 1/40,000 ($1,000) = $.13 + $.09 + $.07 + $.05 + $.08 + $.03 + $.01 + $.01 + $.01 + $.03 = $.51. However, there is also a 1/10 chance of getting a free ticket, which has an expected payout of $.51, so the overall expected\npayout is $.51 + .1 ($.51) = $.51 + $.05 = $.56. * Earlier in the book I used an example that involved drunken employees producing defective laser printers. You will need\nto forget that example here and assume that the company has fixed its quality problems. * Since I’ve admonished you to be a stickler about descriptive statistics, I feel compelled to point out that the most\ncommonly stolen car is not necessarily the kind of car that is most likely to be stolen.", "tokens": 500, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 71, "segment_id": "00071", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000117"}
{"type": "chunk", "text": "* Earlier in the book I used an example that involved drunken employees producing defective laser printers. You will need\nto forget that example here and assume that the company has fixed its quality problems. * Since I’ve admonished you to be a stickler about descriptive statistics, I feel compelled to point out that the most\ncommonly stolen car is not necessarily the kind of car that is most likely to be stolen. A high number of Honda Civics are\nreported stolen because there are a lot of them on the road; the chances that any individual Honda Civic is stolen (which is\nwhat car insurance companies care about) might be quite low. In contrast, even if 99 percent of all Ferraris are stolen,\nFerrari would not make the “most commonly stolen” list, because there are not that many of them to steal.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\n* Earlier in the book I used an example that involved drunken employees producing defective laser printers. You will need\nto forget that example here and assume that the company has fixed its quality problems. * Since I’ve admonished you to be a stickler about descriptive statistics, I feel compelled to point out that the most\ncommonly stolen car is not necessarily the kind of car that is most likely to be stolen. A high number of Honda Civics are\nreported stolen because there are a lot of them on the road; the chances that any individual Honda Civic is stolen (which is\nwhat car insurance companies care about) might be quite low. In contrast, even if 99 percent of all Ferraris are stolen,\nFerrari would not make the “most commonly stolen” list, because there are not that many of them to steal.", "tokens": 170, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 71, "segment_id": "00071", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000118"}
{"type": "chunk", "text": "CHAPTER 51⁄2\nThe Monty Hall Problem\n\nThe “Monty Hall problem” is a famous probability-related conundrum faced by participants\n\non the game show Let’s Make a Deal , which premiered in the United States in 1963 and is still\nrunning in some markets around the world. (I remember watching the show whenever I was\nhome sick from elementary school.) The program’s gift to statisticians was described in the\nintroduction. At the end of each day’s show a contestant was invited to stand with host Monty\nHall facing three big doors: Door no. 1, Door no. 2, and Door no. 3. Monty explained to the\ncontestant that there was a highly desirable prize behind one of the doors and a goat behind the\nother two doors. The player chose one of the three doors and would get as a prize whatever\nwas behind it. (I don’t know if the participants actually got to keep the goat; for our purposes,\nassume that most players preferred the new car.)\n\nThe initial probability of winning was straightforward. There were two goats and one car. As\nthe participant stood facing the doors with Monty, he or she had a 1 in 3 chance of choosing the\ndoor that would be opened to reveal the car. But as noted earlier, Let’s Make a Deal had a\ntwist, which is why the show and its host have been immortalized in the probability literature. After the contestant chose a door, Monty would open one of the two doors that the contestant\nhad not picked, always revealing a goat. At that point, Monty would ask the contestant if he\nwould like to change his pick---to switch from the closed door that he had picked originally to\nthe other remaining closed door. For the sake of example, assume that the contestant has originally chosen Door no. 1. Monty\nwould then open Door no. 3; a live goat would be standing there on a stage. Two doors would\nstill be closed, nos. 1 and 2. If the valuable prize was behind no. 1, the contestant would win; if\nit was behind no. 2, he would lose. That’s when Monty would turn to the player and ask\nwhether he would like to change his mind and switch doors, from no. 1 to no. 2 in this case. Remember, both doors are still closed.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nCHAPTER 51⁄2\nThe Monty Hall Problem\n\nThe “Monty Hall problem” is a famous probability-related conundrum faced by participants\n\non the game show Let’s Make a Deal , which premiered in the United States in 1963 and is still\nrunning in some markets around the world. (I remember watching the show whenever I was\nhome sick from elementary school.) The program’s gift to statisticians was described in the\nintroduction. At the end of each day’s show a contestant was invited to stand with host Monty\nHall facing three big doors: Door no. 1, Door no. 2, and Door no. 3. Monty explained to the\ncontestant that there was a highly desirable prize behind one of the doors and a goat behind the\nother two doors. The player chose one of the three doors and would get as a prize whatever\nwas behind it. (I don’t know if the participants actually got to keep the goat; for our purposes,\nassume that most players preferred the new car.)\n\nThe initial probability of winning was straightforward. There were two goats and one car. As\nthe participant stood facing the doors with Monty, he or she had a 1 in 3 chance of choosing the\ndoor that would be opened to reveal the car. But as noted earlier, Let’s Make a Deal had a\ntwist, which is why the show and its host have been immortalized in the probability literature. After the contestant chose a door, Monty would open one of the two doors that the contestant\nhad not picked, always revealing a goat. At that point, Monty would ask the contestant if he\nwould like to change his pick---to switch from the closed door that he had picked originally to\nthe other remaining closed door. For the sake of example, assume that the contestant has originally chosen Door no. 1. Monty\nwould then open Door no. 3; a live goat would be standing there on a stage. Two doors would\nstill be closed, nos. 1 and 2. If the valuable prize was behind no. 1, the contestant would win; if\nit was behind no. 2, he would lose. That’s when Monty would turn to the player and ask\nwhether he would like to change his mind and switch doors, from no. 1 to no. 2 in this case. Remember, both doors are still closed.", "tokens": 506, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 72, "segment_id": "00072", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000119"}
{"type": "chunk", "text": "For the sake of example, assume that the contestant has originally chosen Door no. 1. Monty\nwould then open Door no. 3; a live goat would be standing there on a stage. Two doors would\nstill be closed, nos. 1 and 2. If the valuable prize was behind no. 1, the contestant would win; if\nit was behind no. 2, he would lose. That’s when Monty would turn to the player and ask\nwhether he would like to change his mind and switch doors, from no. 1 to no. 2 in this case. Remember, both doors are still closed. The only new information the contestant has received is\nthat a goat showed up behind one of the doors that he did not pick. Should he switch? Yes. The contestant has a 1/3 chance of winning if he sticks with his initial choice and a 2/3\n\nchance of winning if he switches. If you don’t believe me, read on. I’ll concede that this answer seems entirely unintuitive at first. It would appear that the\ncontestant has a one-third chance of winning no matter what he does. There are three closed\ndoors. At the beginning, each door has a one in three chance of holding the valuable prize. How\ncould it matter whether he switches from one closed door to another? The answer lies in the fact that Monty Hall knows what is behind each door. If the contestant\npicks Door no. 1 and there is a car behind it, then Monty can open either no. 2 or no. 3 to\ndisplay a goat. If the contestant picks Door no. 1 and the car is behind no. 2, then Monty opens no. 3.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nFor the sake of example, assume that the contestant has originally chosen Door no. 1. Monty\nwould then open Door no. 3; a live goat would be standing there on a stage. Two doors would\nstill be closed, nos. 1 and 2. If the valuable prize was behind no. 1, the contestant would win; if\nit was behind no. 2, he would lose. That’s when Monty would turn to the player and ask\nwhether he would like to change his mind and switch doors, from no. 1 to no. 2 in this case. Remember, both doors are still closed. The only new information the contestant has received is\nthat a goat showed up behind one of the doors that he did not pick. Should he switch? Yes. The contestant has a 1/3 chance of winning if he sticks with his initial choice and a 2/3\n\nchance of winning if he switches. If you don’t believe me, read on. I’ll concede that this answer seems entirely unintuitive at first. It would appear that the\ncontestant has a one-third chance of winning no matter what he does. There are three closed\ndoors. At the beginning, each door has a one in three chance of holding the valuable prize. How\ncould it matter whether he switches from one closed door to another? The answer lies in the fact that Monty Hall knows what is behind each door. If the contestant\npicks Door no. 1 and there is a car behind it, then Monty can open either no. 2 or no. 3 to\ndisplay a goat. If the contestant picks Door no. 1 and the car is behind no. 2, then Monty opens no. 3.", "tokens": 369, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 72, "segment_id": "00072", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000120"}
{"type": "chunk", "text": "If the contestant picks Door no. 1 and the car is behind no. 3, then Monty opens no. 2. By switching after a door is opened, the contestant gets the benefit of choosing two doors\n\nrather than one. I will try to persuade you in three different ways that this analysis is correct. The first is empirical. In 2008, New York Times columnist John Tierney wrote about the\nMonty Hall phenomenon.1 The Times then constructed an interactive feature that allows you to\nplay the game yourself, including the decision to switch or not. (There are even little goats and\ncars that pop out from behind the doors.) The game keeps track of your success when you\nswitch doors after making your initial decision compared with when you do not. Try it\nyourself.* I paid one of my children to play the game 100 times, switching each time. I paid her\nbrother to play the game 100 times without switching. The switcher won 72 times; the\nnonswitcher won 33 times. Both received two dollars for their efforts. The data from episodes of Let’s Make a Deal suggest the same thing. According to Leonard\nMlodinow, author of The Drunkard’s Walk , those contestants who switched their choice won\nabout twice as often as those who did not.2\n\nMy second explanation gets at the intuition. Let’s suppose the rules were modified slightly. Assume that the contestant begins by picking one of the three doors: no. 1, no. 2, or no. 3, just\nas the game is ordinarily played. But then, before any door is opened to reveal a goat, Monty\nsays, “Would you like to give up your choice in exchange for both of the other doors that you\ndid not choose?” So if you picked Door no. 1, you could ditch that door in exchange for what\nis behind no. 2 and no. 3. If you picked no. 3, you could switch to no. 1 and no. 2. And so on. That would not be a particularly hard decision. Obviously you should give up one door in\nexchange for two, as it increases your chances of winning from 1/3 to 2/3. Here is the intriguing\npart: That is exactly what Monty Hall allows you to do in the real game after he reveals the\ngoat.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nIf the contestant picks Door no. 1 and the car is behind no. 3, then Monty opens no. 2. By switching after a door is opened, the contestant gets the benefit of choosing two doors\n\nrather than one. I will try to persuade you in three different ways that this analysis is correct. The first is empirical. In 2008, New York Times columnist John Tierney wrote about the\nMonty Hall phenomenon.1 The Times then constructed an interactive feature that allows you to\nplay the game yourself, including the decision to switch or not. (There are even little goats and\ncars that pop out from behind the doors.) The game keeps track of your success when you\nswitch doors after making your initial decision compared with when you do not. Try it\nyourself.* I paid one of my children to play the game 100 times, switching each time. I paid her\nbrother to play the game 100 times without switching. The switcher won 72 times; the\nnonswitcher won 33 times. Both received two dollars for their efforts. The data from episodes of Let’s Make a Deal suggest the same thing. According to Leonard\nMlodinow, author of The Drunkard’s Walk , those contestants who switched their choice won\nabout twice as often as those who did not.2\n\nMy second explanation gets at the intuition. Let’s suppose the rules were modified slightly. Assume that the contestant begins by picking one of the three doors: no. 1, no. 2, or no. 3, just\nas the game is ordinarily played. But then, before any door is opened to reveal a goat, Monty\nsays, “Would you like to give up your choice in exchange for both of the other doors that you\ndid not choose?” So if you picked Door no. 1, you could ditch that door in exchange for what\nis behind no. 2 and no. 3. If you picked no. 3, you could switch to no. 1 and no. 2. And so on. That would not be a particularly hard decision. Obviously you should give up one door in\nexchange for two, as it increases your chances of winning from 1/3 to 2/3. Here is the intriguing\npart: That is exactly what Monty Hall allows you to do in the real game after he reveals the\ngoat.", "tokens": 503, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 73, "segment_id": "00073", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000121"}
{"type": "chunk", "text": "1, you could ditch that door in exchange for what\nis behind no. 2 and no. 3. If you picked no. 3, you could switch to no. 1 and no. 2. And so on. That would not be a particularly hard decision. Obviously you should give up one door in\nexchange for two, as it increases your chances of winning from 1/3 to 2/3. Here is the intriguing\npart: That is exactly what Monty Hall allows you to do in the real game after he reveals the\ngoat. The fundamental insight is that if you were to choose two doors, one of them would\nalways have a goat behind it anyway. When he opens a door to reveal a goat before asking if\nyou’d like to switch, he’s doing you a huge favor! He’s saying (in effect), “There is a twothirds chance that the car is behind one of the doors you didn’t choose, and look, it’s not that\none!”\n\nThink of it this way. Suppose you picked Door no. 1. Monty then offers you the option to\ntake Doors 2 and 3 instead. You take the offer, giving up one door and getting two, meaning\nthat you can reasonably expect to win the car 2/3 of the time. At that point, what if Monty were\nto open Door no. 3---one of your doors---to reveal a goat? Should you feel less certain about\nyour decision? Of course not. If the car were behind no. 3, he would have opened no. 2! He’s\nshown you nothing. When the game is played normally, Monty is really giving you a choice between the door\nyou originally picked and the other two doors, only one of which could possibly have a car\nbehind it. When he opens a door to reveal a goat, he’s merely doing you the courtesy of\nshowing you which of the other two doors does not have the car. You have the same\nprobability of winning in both of the following scenarios:\n\n1. Choosing Door no. 1, then agreeing to switch to Door no. 2 and Door no. 3 before any", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\n1, you could ditch that door in exchange for what\nis behind no. 2 and no. 3. If you picked no. 3, you could switch to no. 1 and no. 2. And so on. That would not be a particularly hard decision. Obviously you should give up one door in\nexchange for two, as it increases your chances of winning from 1/3 to 2/3. Here is the intriguing\npart: That is exactly what Monty Hall allows you to do in the real game after he reveals the\ngoat. The fundamental insight is that if you were to choose two doors, one of them would\nalways have a goat behind it anyway. When he opens a door to reveal a goat before asking if\nyou’d like to switch, he’s doing you a huge favor! He’s saying (in effect), “There is a twothirds chance that the car is behind one of the doors you didn’t choose, and look, it’s not that\none!”\n\nThink of it this way. Suppose you picked Door no. 1. Monty then offers you the option to\ntake Doors 2 and 3 instead. You take the offer, giving up one door and getting two, meaning\nthat you can reasonably expect to win the car 2/3 of the time. At that point, what if Monty were\nto open Door no. 3---one of your doors---to reveal a goat? Should you feel less certain about\nyour decision? Of course not. If the car were behind no. 3, he would have opened no. 2! He’s\nshown you nothing. When the game is played normally, Monty is really giving you a choice between the door\nyou originally picked and the other two doors, only one of which could possibly have a car\nbehind it. When he opens a door to reveal a goat, he’s merely doing you the courtesy of\nshowing you which of the other two doors does not have the car. You have the same\nprobability of winning in both of the following scenarios:\n\n1. Choosing Door no. 1, then agreeing to switch to Door no. 2 and Door no. 3 before any", "tokens": 463, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 73, "segment_id": "00073", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000122"}
{"type": "chunk", "text": "door is opened. 2. Choosing Door no. 1, then agreeing to switch to Door no. 2 after Monty reveals a goat\n\nbehind Door no. 3 (or choosing no. 3 after he reveals a goat behind no. 2). In both cases, switching gives you the benefit of two doors instead of one, and you can\ntherefore double your chances of winning, from 1/3 to 2/3. My third explanation is a more extreme version of the same basic intuition. Assume that Monty\nHall offers you a choice from among 100 doors rather than just three. After you pick your\ndoor, say, no. 47, he opens 98 other doors with goats behind them. Now there are only two\ndoors that remain closed, no. 47 (your original choice) and one other, say, no. 61. Should you\nswitch? Of course you should. There is a 99 percent chance that the car was behind one of the doors\nthat you did not originally choose. Monty did you the favor of opening 98 of those doors that\nyou did not choose, all of which he knew did not have the car behind them. There is only a 1 in\n100 chance that your original pick was correct (no. 47). There is a 99 in 100 chance that your\noriginal pick was not correct. And if your original pick was not correct, then the car is sitting\nbehind the other door, no. 61. If you want to win 99 times out of 100, you should switch to no. 61. In short, if you ever find yourself as a contestant on Let’s Make a Deal , you should definitely\nswitch doors when Monty Hall (or his replacement) gives you the option. The more broadly\napplicable lesson is that your gut instinct on probability can sometimes steer you astray. * You can play the game at http:///2008/04/08/science/08monty.html?_r=2&oref=slogin&oref=slogin.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\ndoor is opened. 2. Choosing Door no. 1, then agreeing to switch to Door no. 2 after Monty reveals a goat\n\nbehind Door no. 3 (or choosing no. 3 after he reveals a goat behind no. 2). In both cases, switching gives you the benefit of two doors instead of one, and you can\ntherefore double your chances of winning, from 1/3 to 2/3. My third explanation is a more extreme version of the same basic intuition. Assume that Monty\nHall offers you a choice from among 100 doors rather than just three. After you pick your\ndoor, say, no. 47, he opens 98 other doors with goats behind them. Now there are only two\ndoors that remain closed, no. 47 (your original choice) and one other, say, no. 61. Should you\nswitch? Of course you should. There is a 99 percent chance that the car was behind one of the doors\nthat you did not originally choose. Monty did you the favor of opening 98 of those doors that\nyou did not choose, all of which he knew did not have the car behind them. There is only a 1 in\n100 chance that your original pick was correct (no. 47). There is a 99 in 100 chance that your\noriginal pick was not correct. And if your original pick was not correct, then the car is sitting\nbehind the other door, no. 61. If you want to win 99 times out of 100, you should switch to no. 61. In short, if you ever find yourself as a contestant on Let’s Make a Deal , you should definitely\nswitch doors when Monty Hall (or his replacement) gives you the option. The more broadly\napplicable lesson is that your gut instinct on probability can sometimes steer you astray. * You can play the game at http:///2008/04/08/science/08monty.html?_r=2&oref=slogin&oref=slogin.", "tokens": 435, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 74, "segment_id": "00074", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000123"}
{"type": "chunk", "text": "CHAPTER 6\nProblems with Probability\nHow overconfident math geeks nearly\ndestroyed the global financial system\n\nStatistics cannot be any smarter than the people who use them. And in some cases, they can\n\nmake smart people do dumb things. One of the most irresponsible uses of statistics in recent\nmemory involved the mechanism for gauging risk on Wall Street prior to the 2008 financial\ncrisis. At that time, firms throughout the financial industry used a common barometer of risk,\nthe Value at Risk model, or VaR. In theory, VaR combined the elegance of an indicator\n(collapsing lots of information into a single number) with the power of probability (attaching an\nexpected gain or loss to each of the firm’s assets or trading positions). The model assumed that\nthere is a range of possible outcomes for every one of the firm’s investments. For example, if\nthe firm owns General Electric stock, the value of those shares can go up or down. When the\nVaR is being calculated for some short period of time, say, one week, the most likely outcome\nis that the shares will have roughly the same value at the end of that stretch as they had at the\nbeginning. There is a smaller chance that the shares may rise or fall by 10 percent. And an\neven smaller chance that they may rise or fall 25 percent, and so on. On the basis of past data for market movements, the firm’s quantitative experts (often called\n“quants” in the industry and “rich nerds” everywhere else) could assign a dollar figure, say $13\nmillion, that represented the maximum that the firm could lose on that position over the time\nperiod being examined, with 99 percent probability. In other words, 99 times out of 100 the\nfirm would not lose more than $13 million on a particular trading position; 1 time out of 100, it\nwould. Remember that last part, because it will soon become important. Prior to the financial crisis of 2008, firms trusted the VaR model to quantify their overall\nrisk. If a single trader had 923 different open positions (investments that could move up or\ndown in value), each of those investments could be evaluated as described above for the\nGeneral Electric stock; from there, the trader’s total portfolio risk could be calculated. The\nformula even took into account the correlations among different positions.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nCHAPTER 6\nProblems with Probability\nHow overconfident math geeks nearly\ndestroyed the global financial system\n\nStatistics cannot be any smarter than the people who use them. And in some cases, they can\n\nmake smart people do dumb things. One of the most irresponsible uses of statistics in recent\nmemory involved the mechanism for gauging risk on Wall Street prior to the 2008 financial\ncrisis. At that time, firms throughout the financial industry used a common barometer of risk,\nthe Value at Risk model, or VaR. In theory, VaR combined the elegance of an indicator\n(collapsing lots of information into a single number) with the power of probability (attaching an\nexpected gain or loss to each of the firm’s assets or trading positions). The model assumed that\nthere is a range of possible outcomes for every one of the firm’s investments. For example, if\nthe firm owns General Electric stock, the value of those shares can go up or down. When the\nVaR is being calculated for some short period of time, say, one week, the most likely outcome\nis that the shares will have roughly the same value at the end of that stretch as they had at the\nbeginning. There is a smaller chance that the shares may rise or fall by 10 percent. And an\neven smaller chance that they may rise or fall 25 percent, and so on. On the basis of past data for market movements, the firm’s quantitative experts (often called\n“quants” in the industry and “rich nerds” everywhere else) could assign a dollar figure, say $13\nmillion, that represented the maximum that the firm could lose on that position over the time\nperiod being examined, with 99 percent probability. In other words, 99 times out of 100 the\nfirm would not lose more than $13 million on a particular trading position; 1 time out of 100, it\nwould. Remember that last part, because it will soon become important. Prior to the financial crisis of 2008, firms trusted the VaR model to quantify their overall\nrisk. If a single trader had 923 different open positions (investments that could move up or\ndown in value), each of those investments could be evaluated as described above for the\nGeneral Electric stock; from there, the trader’s total portfolio risk could be calculated. The\nformula even took into account the correlations among different positions.", "tokens": 506, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 75, "segment_id": "00075", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000124"}
{"type": "chunk", "text": "Remember that last part, because it will soon become important. Prior to the financial crisis of 2008, firms trusted the VaR model to quantify their overall\nrisk. If a single trader had 923 different open positions (investments that could move up or\ndown in value), each of those investments could be evaluated as described above for the\nGeneral Electric stock; from there, the trader’s total portfolio risk could be calculated. The\nformula even took into account the correlations among different positions. For example, if two\ninvestments had expected returns that were negatively correlated, a loss in one would likely\nhave been offset by a gain in the other, making the two investments together less risky than\neither one separately. Overall, the head of the trading desk would know that bond trader Bob\nSmith has a 24-hour VaR (the value at risk over the next 24 hours) of $19 million, again with\n99 percent probability. The most that Bob Smith could lose over the next 24 hours would be\n$19 million, 99 times out of 100. Then, even better, the aggregate risk for the firm could be calculated at any point in time by", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nRemember that last part, because it will soon become important. Prior to the financial crisis of 2008, firms trusted the VaR model to quantify their overall\nrisk. If a single trader had 923 different open positions (investments that could move up or\ndown in value), each of those investments could be evaluated as described above for the\nGeneral Electric stock; from there, the trader’s total portfolio risk could be calculated. The\nformula even took into account the correlations among different positions. For example, if two\ninvestments had expected returns that were negatively correlated, a loss in one would likely\nhave been offset by a gain in the other, making the two investments together less risky than\neither one separately. Overall, the head of the trading desk would know that bond trader Bob\nSmith has a 24-hour VaR (the value at risk over the next 24 hours) of $19 million, again with\n99 percent probability. The most that Bob Smith could lose over the next 24 hours would be\n$19 million, 99 times out of 100. Then, even better, the aggregate risk for the firm could be calculated at any point in time by", "tokens": 242, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 75, "segment_id": "00075", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000125"}
{"type": "chunk", "text": "taking the same basic process one step further. The underlying mathematical mechanics are\nobviously fabulously complicated, as firms had a dizzying array of investments in different\ncurrencies, with different amounts of leverage (the amount of money that was borrowed to\nmake the investment), trading in markets with different degrees of liquidity, and so on. Despite\nall that, the firm’s managers ostensibly had a precise measure of the magnitude of the risk that\nthe firm had taken on at any moment in time. As former New York Times business writer Joe\nNocera has explained, “VaR’s great appeal, and its great selling point to people who do not\nhappen to be quants, is that it expresses risk as a single number, a dollar figure, no less.”1 At\nJ. P. Morgan, where the VaR model was developed and refined, the daily VaR calculation was\nknown as the “4:15 report” because it would be on the desks of top executives every afternoon\nat 4:15, just after the American financial markets had closed for the day. Presumably this was a good thing, as more information is generally better, particularly when\nit comes to risk. After all, probability is a powerful tool. Isn’t this just the same kind of\ncalculation that the Schlitz executives did before spending a lot of money on blind taste tests at\nhalftime of the Super Bowl? Not necessarily. VaR has been called “potentially catastrophic,” “a fraud,” and many other\nthings not fit for a family book about statistics like this one. In particular, the model has been\nblamed for the onset and severity of the financial crisis. The primary critique of VaR is that the\nunderlying risks associated with financial markets are not as predictable as a coin flip or even\na blind taste test between two beers. The false precision embedded in the models created a\nfalse sense of security. The VaR was like a faulty speedometer, which is arguably worse than\nno speedometer at all. If you place too much faith in the broken speedometer, you will be\noblivious to other signs that your speed is unsafe. In contrast, if there is no speedometer at all,\nyou have no choice but to look around for clues as to how fast you are really going. By around 2005, with the VaR dropping on desks at 4:15 every weekday, Wall Street was\ndriving pretty darn fast.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\ntaking the same basic process one step further. The underlying mathematical mechanics are\nobviously fabulously complicated, as firms had a dizzying array of investments in different\ncurrencies, with different amounts of leverage (the amount of money that was borrowed to\nmake the investment), trading in markets with different degrees of liquidity, and so on. Despite\nall that, the firm’s managers ostensibly had a precise measure of the magnitude of the risk that\nthe firm had taken on at any moment in time. As former New York Times business writer Joe\nNocera has explained, “VaR’s great appeal, and its great selling point to people who do not\nhappen to be quants, is that it expresses risk as a single number, a dollar figure, no less.”1 At\nJ. P. Morgan, where the VaR model was developed and refined, the daily VaR calculation was\nknown as the “4:15 report” because it would be on the desks of top executives every afternoon\nat 4:15, just after the American financial markets had closed for the day. Presumably this was a good thing, as more information is generally better, particularly when\nit comes to risk. After all, probability is a powerful tool. Isn’t this just the same kind of\ncalculation that the Schlitz executives did before spending a lot of money on blind taste tests at\nhalftime of the Super Bowl? Not necessarily. VaR has been called “potentially catastrophic,” “a fraud,” and many other\nthings not fit for a family book about statistics like this one. In particular, the model has been\nblamed for the onset and severity of the financial crisis. The primary critique of VaR is that the\nunderlying risks associated with financial markets are not as predictable as a coin flip or even\na blind taste test between two beers. The false precision embedded in the models created a\nfalse sense of security. The VaR was like a faulty speedometer, which is arguably worse than\nno speedometer at all. If you place too much faith in the broken speedometer, you will be\noblivious to other signs that your speed is unsafe. In contrast, if there is no speedometer at all,\nyou have no choice but to look around for clues as to how fast you are really going. By around 2005, with the VaR dropping on desks at 4:15 every weekday, Wall Street was\ndriving pretty darn fast.", "tokens": 509, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 76, "segment_id": "00076", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000126"}
{"type": "chunk", "text": "The false precision embedded in the models created a\nfalse sense of security. The VaR was like a faulty speedometer, which is arguably worse than\nno speedometer at all. If you place too much faith in the broken speedometer, you will be\noblivious to other signs that your speed is unsafe. In contrast, if there is no speedometer at all,\nyou have no choice but to look around for clues as to how fast you are really going. By around 2005, with the VaR dropping on desks at 4:15 every weekday, Wall Street was\ndriving pretty darn fast. Unfortunately, there were two huge problems with the risk profiles\nencapsulated by the VaR models. First, the underlying probabilities on which the models were\nbuilt were based on past market movements; however, in financial markets (unlike beer\ntasting), the future does not necessarily look like the past. There was no intellectual\njustification for assuming that the market movements from 1980 to 2005 were the best\npredictor of market movements after 2005. In some ways, this failure of imagination resembles\nthe military’s periodic mistaken assumption that the next war will look like the last one. In the\n1990s and early 2000s, commercial banks were using lending models for home mortgages that\nassigned zero probability to large declines in housing prices.2 Housing prices had never before\nfallen as far and as fast as they did beginning in 2007. But that’s what happened. Former\nFederal Reserve chairman Alan Greenspan explained to a congressional committee after the\nfact, “The whole intellectual edifice, however, collapsed in the summer of [2007] because the\ndata input into the risk management models generally covered only the past two decades, a\nperiod of euphoria. Had instead the models been fitted more appropriately to historic periods\nof stress, capital requirements would have been much higher and the financial world would be\nin far better shape, in my judgment.”3", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThe false precision embedded in the models created a\nfalse sense of security. The VaR was like a faulty speedometer, which is arguably worse than\nno speedometer at all. If you place too much faith in the broken speedometer, you will be\noblivious to other signs that your speed is unsafe. In contrast, if there is no speedometer at all,\nyou have no choice but to look around for clues as to how fast you are really going. By around 2005, with the VaR dropping on desks at 4:15 every weekday, Wall Street was\ndriving pretty darn fast. Unfortunately, there were two huge problems with the risk profiles\nencapsulated by the VaR models. First, the underlying probabilities on which the models were\nbuilt were based on past market movements; however, in financial markets (unlike beer\ntasting), the future does not necessarily look like the past. There was no intellectual\njustification for assuming that the market movements from 1980 to 2005 were the best\npredictor of market movements after 2005. In some ways, this failure of imagination resembles\nthe military’s periodic mistaken assumption that the next war will look like the last one. In the\n1990s and early 2000s, commercial banks were using lending models for home mortgages that\nassigned zero probability to large declines in housing prices.2 Housing prices had never before\nfallen as far and as fast as they did beginning in 2007. But that’s what happened. Former\nFederal Reserve chairman Alan Greenspan explained to a congressional committee after the\nfact, “The whole intellectual edifice, however, collapsed in the summer of [2007] because the\ndata input into the risk management models generally covered only the past two decades, a\nperiod of euphoria. Had instead the models been fitted more appropriately to historic periods\nof stress, capital requirements would have been much higher and the financial world would be\nin far better shape, in my judgment.”3", "tokens": 415, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 76, "segment_id": "00076", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000127"}
{"type": "chunk", "text": "Second, even if the underlying data could accurately predict future risk, the 99 percent\nassurance offered by the VaR model was dangerously useless, because it’s the 1 percent that\nis going to really mess you up. Hedge fund manager David Einhorn explained, “This is like an\nair bag that works all the time, except when you have a car accident.” If a firm has a Value at\nRisk of $500 million, that can be interpreted to mean that the firm has a 99 percent chance of\nlosing no more than $500 million over the time period specified. Well, hello, that also means\nthat the firm has a 1 percent chance of losing more than $500 million---much, much more under\nsome circumstances. In fact, the models had nothing to say about how bad that 1 percent\nscenario might turn out to be. Very little attention was devoted to the “tail risk,” the small risk\n(named for the tail of the distribution) of some catastrophic outcome. (If you drive home from\na bar with a blood alcohol level of .15, there is probably less than a 1 percent chance that you\nwill crash and die; that does not make it a sensible thing to do.) Many firms compounded this\nerror by making unrealistic assumptions about their preparedness for rare events. Former\ntreasury secretary Hank Paulson has explained that many firms assumed they could raise cash\nin a pinch by selling assets.4 But during a crisis, every other firm needs cash, too, so all are\ntrying to sell the same kinds of assets. It’s the risk management equivalent of saying, “I don’t\nneed to stock up on water because if there is a natural disaster, I’ll just go to the supermarket\nand buy some.” Of course, after an asteroid hits your town, fifty thousand other people are\nalso trying to buy water; by the time you get to the supermarket, the windows are broken and\nthe shelves are empty. The fact that you’ve never contemplated that your town might be flattened by a massive\nasteroid was exactly the problem with VaR. Here is New York Times columnist Joe Nocera\nagain, summarizing thoughts of Nicholas Taleb, author of The Black Swan: The Impact of the\nHighly Improbable and a scathing critic of VaR: “The greatest risks are never the ones you can\nsee and measure, but the ones you can’t see and therefore can never measure.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nSecond, even if the underlying data could accurately predict future risk, the 99 percent\nassurance offered by the VaR model was dangerously useless, because it’s the 1 percent that\nis going to really mess you up. Hedge fund manager David Einhorn explained, “This is like an\nair bag that works all the time, except when you have a car accident.” If a firm has a Value at\nRisk of $500 million, that can be interpreted to mean that the firm has a 99 percent chance of\nlosing no more than $500 million over the time period specified. Well, hello, that also means\nthat the firm has a 1 percent chance of losing more than $500 million---much, much more under\nsome circumstances. In fact, the models had nothing to say about how bad that 1 percent\nscenario might turn out to be. Very little attention was devoted to the “tail risk,” the small risk\n(named for the tail of the distribution) of some catastrophic outcome. (If you drive home from\na bar with a blood alcohol level of .15, there is probably less than a 1 percent chance that you\nwill crash and die; that does not make it a sensible thing to do.) Many firms compounded this\nerror by making unrealistic assumptions about their preparedness for rare events. Former\ntreasury secretary Hank Paulson has explained that many firms assumed they could raise cash\nin a pinch by selling assets.4 But during a crisis, every other firm needs cash, too, so all are\ntrying to sell the same kinds of assets. It’s the risk management equivalent of saying, “I don’t\nneed to stock up on water because if there is a natural disaster, I’ll just go to the supermarket\nand buy some.” Of course, after an asteroid hits your town, fifty thousand other people are\nalso trying to buy water; by the time you get to the supermarket, the windows are broken and\nthe shelves are empty. The fact that you’ve never contemplated that your town might be flattened by a massive\nasteroid was exactly the problem with VaR. Here is New York Times columnist Joe Nocera\nagain, summarizing thoughts of Nicholas Taleb, author of The Black Swan: The Impact of the\nHighly Improbable and a scathing critic of VaR: “The greatest risks are never the ones you can\nsee and measure, but the ones you can’t see and therefore can never measure.", "tokens": 508, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 77, "segment_id": "00077", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000128"}
{"type": "chunk", "text": "The fact that you’ve never contemplated that your town might be flattened by a massive\nasteroid was exactly the problem with VaR. Here is New York Times columnist Joe Nocera\nagain, summarizing thoughts of Nicholas Taleb, author of The Black Swan: The Impact of the\nHighly Improbable and a scathing critic of VaR: “The greatest risks are never the ones you can\nsee and measure, but the ones you can’t see and therefore can never measure. The ones that\nseem so far outside the boundary of normal probability that you can’t imagine they could\nhappen in your lifetime---even though, of course, they do happen, more often than you care to\nrealize.”\n\nIn some ways, the VaR debacle is the opposite of the Schlitz example in Chapter 5. Schlitz\nwas operating with a known probability distribution. Whatever data the company had on the\nlikelihood of blind taste testers’ choosing Schlitz was a good estimate of how similar testers\nwould behave live at halftime. Schlitz even managed its downside by performing the whole test\non men who said they liked the other beers better. Even if no more than twenty-five Michelob\ndrinkers chose Schlitz (an almost impossibly low outcome), Schlitz could still claim that one in\nfour beer drinkers ought to consider switching. Perhaps most important, this was all just beer,\nnot the global financial system. The Wall Street quants made three fundamental errors. First,\nthey confused precision with accuracy. The VaR models were just like my golf range finder\nwhen it was set to meters instead of yards: exact and wrong. The false precision led Wall\nStreet executives to believe that they had risk on a leash when in fact they did not. Second, the\nestimates of the underlying probabilities were wrong. As Alan Greenspan pointed out in\ntestimony quoted earlier in the chapter, the relatively tranquil and prosperous decades before", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThe fact that you’ve never contemplated that your town might be flattened by a massive\nasteroid was exactly the problem with VaR. Here is New York Times columnist Joe Nocera\nagain, summarizing thoughts of Nicholas Taleb, author of The Black Swan: The Impact of the\nHighly Improbable and a scathing critic of VaR: “The greatest risks are never the ones you can\nsee and measure, but the ones you can’t see and therefore can never measure. The ones that\nseem so far outside the boundary of normal probability that you can’t imagine they could\nhappen in your lifetime---even though, of course, they do happen, more often than you care to\nrealize.”\n\nIn some ways, the VaR debacle is the opposite of the Schlitz example in Chapter 5. Schlitz\nwas operating with a known probability distribution. Whatever data the company had on the\nlikelihood of blind taste testers’ choosing Schlitz was a good estimate of how similar testers\nwould behave live at halftime. Schlitz even managed its downside by performing the whole test\non men who said they liked the other beers better. Even if no more than twenty-five Michelob\ndrinkers chose Schlitz (an almost impossibly low outcome), Schlitz could still claim that one in\nfour beer drinkers ought to consider switching. Perhaps most important, this was all just beer,\nnot the global financial system. The Wall Street quants made three fundamental errors. First,\nthey confused precision with accuracy. The VaR models were just like my golf range finder\nwhen it was set to meters instead of yards: exact and wrong. The false precision led Wall\nStreet executives to believe that they had risk on a leash when in fact they did not. Second, the\nestimates of the underlying probabilities were wrong. As Alan Greenspan pointed out in\ntestimony quoted earlier in the chapter, the relatively tranquil and prosperous decades before", "tokens": 396, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 77, "segment_id": "00077", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000129"}
{"type": "chunk", "text": "2005 should not have been used to create probability distributions for what might happen in the\nmarkets in the ensuing decades. This is the equivalent of walking into a casino and thinking\nthat you will win at roulette 62 percent of the time because that’s what happened last time you\nwent gambling. It would be a long, expensive evening. Third, firms neglected their “tail risk.”\nThe VaR models predicted what would happen 99 times out of 100. That’s the way probability\nworks (as the second half of the book will emphasize repeatedly). Unlikely things happen. In\nfact, over a long enough period of time, they are not even that unlikely. People get hit by\nlightning all the time. My mother has had three holes in one. The statistical hubris at commercial banks and on Wall Street ultimately contributed to the\nmost severe global financial contraction since the Great Depression. The crisis that began in\n2008 destroyed trillions of dollars in wealth in the United States, drove unemployment over 10\npercent, created waves of home foreclosures and business failures, and saddled governments\naround the world with huge debts as they struggled to contain the economic damage. This is a\nsadly ironic outcome, given that sophisticated tools like VaR were designed to mitigate risk. Probability offers a powerful and useful set of tools---many of which can be employed\ncorrectly to understand the world or incorrectly to wreak havoc on it. In sticking with the\n“statistics as a powerful weapon” metaphor that I’ve used throughout the book, I will\nparaphrase the gun rights lobby: Probability doesn’t make mistakes; people using probability\nmake mistakes. The balance of this chapter will catalog some of the most common probabilityrelated errors, misunderstandings, and ethical dilemmas. Assuming events are independent when they are not. The probability of flipping heads with a\nfair coin is 1⁄2. The probability of flipping two heads in a row is (1⁄2)2, or 1⁄4, since the\nlikelihood of two independent events’ both happening is the product of their individual\nprobabilities. Now that you are armed with this powerful knowledge, let’s assume that you\nhave been promoted to head of risk management at a major airline. Your assistant informs you\nthat the probability of a jet engine’s failing for any reason during a transatlantic flight is 1 in\n100,000.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\n2005 should not have been used to create probability distributions for what might happen in the\nmarkets in the ensuing decades. This is the equivalent of walking into a casino and thinking\nthat you will win at roulette 62 percent of the time because that’s what happened last time you\nwent gambling. It would be a long, expensive evening. Third, firms neglected their “tail risk.”\nThe VaR models predicted what would happen 99 times out of 100. That’s the way probability\nworks (as the second half of the book will emphasize repeatedly). Unlikely things happen. In\nfact, over a long enough period of time, they are not even that unlikely. People get hit by\nlightning all the time. My mother has had three holes in one. The statistical hubris at commercial banks and on Wall Street ultimately contributed to the\nmost severe global financial contraction since the Great Depression. The crisis that began in\n2008 destroyed trillions of dollars in wealth in the United States, drove unemployment over 10\npercent, created waves of home foreclosures and business failures, and saddled governments\naround the world with huge debts as they struggled to contain the economic damage. This is a\nsadly ironic outcome, given that sophisticated tools like VaR were designed to mitigate risk. Probability offers a powerful and useful set of tools---many of which can be employed\ncorrectly to understand the world or incorrectly to wreak havoc on it. In sticking with the\n“statistics as a powerful weapon” metaphor that I’ve used throughout the book, I will\nparaphrase the gun rights lobby: Probability doesn’t make mistakes; people using probability\nmake mistakes. The balance of this chapter will catalog some of the most common probabilityrelated errors, misunderstandings, and ethical dilemmas. Assuming events are independent when they are not. The probability of flipping heads with a\nfair coin is 1⁄2. The probability of flipping two heads in a row is (1⁄2)2, or 1⁄4, since the\nlikelihood of two independent events’ both happening is the product of their individual\nprobabilities. Now that you are armed with this powerful knowledge, let’s assume that you\nhave been promoted to head of risk management at a major airline. Your assistant informs you\nthat the probability of a jet engine’s failing for any reason during a transatlantic flight is 1 in\n100,000.", "tokens": 497, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 78, "segment_id": "00078", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000130"}
{"type": "chunk", "text": "The probability of flipping heads with a\nfair coin is 1⁄2. The probability of flipping two heads in a row is (1⁄2)2, or 1⁄4, since the\nlikelihood of two independent events’ both happening is the product of their individual\nprobabilities. Now that you are armed with this powerful knowledge, let’s assume that you\nhave been promoted to head of risk management at a major airline. Your assistant informs you\nthat the probability of a jet engine’s failing for any reason during a transatlantic flight is 1 in\n100,000. Given the number of transatlantic flights, this is not an acceptable risk. Fortunately\neach jet making such a trip has at least two engines. Your assistant has calculated that the risk\nof both engines’ shutting down over the Atlantic must be (1/100,000) 2, or 1 in 10 billion, which\nis a reasonable safety risk. This would be a good time to tell your assistant to use up his\nvacation days before he is fired. The two engine failures are not independent events. If a plane\nflies through a flock of geese while taking off, both engines are likely to be compromised in a\nsimilar way. The same would be true of many other factors that affect the performance of a jet\nengine, from weather to improper maintenance. If one engine fails, the probability that the\nsecond engine fails is going to be significantly higher than 1 in 100,000. Does this seem obvious? It was not obvious throughout the 1990s as British prosecutors\ncommitted a grave miscarriage of justice because of an improper use of probability. As with\nthe hypothetical jet engine example, the statistical mistake was in assuming that several events\nwere independent (as in flipping a coin) rather than dependent (when a certain outcome makes\na similar outcome more likely in the future). This mistake was real, however, and innocent", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThe probability of flipping heads with a\nfair coin is 1⁄2. The probability of flipping two heads in a row is (1⁄2)2, or 1⁄4, since the\nlikelihood of two independent events’ both happening is the product of their individual\nprobabilities. Now that you are armed with this powerful knowledge, let’s assume that you\nhave been promoted to head of risk management at a major airline. Your assistant informs you\nthat the probability of a jet engine’s failing for any reason during a transatlantic flight is 1 in\n100,000. Given the number of transatlantic flights, this is not an acceptable risk. Fortunately\neach jet making such a trip has at least two engines. Your assistant has calculated that the risk\nof both engines’ shutting down over the Atlantic must be (1/100,000) 2, or 1 in 10 billion, which\nis a reasonable safety risk. This would be a good time to tell your assistant to use up his\nvacation days before he is fired. The two engine failures are not independent events. If a plane\nflies through a flock of geese while taking off, both engines are likely to be compromised in a\nsimilar way. The same would be true of many other factors that affect the performance of a jet\nengine, from weather to improper maintenance. If one engine fails, the probability that the\nsecond engine fails is going to be significantly higher than 1 in 100,000. Does this seem obvious? It was not obvious throughout the 1990s as British prosecutors\ncommitted a grave miscarriage of justice because of an improper use of probability. As with\nthe hypothetical jet engine example, the statistical mistake was in assuming that several events\nwere independent (as in flipping a coin) rather than dependent (when a certain outcome makes\na similar outcome more likely in the future). This mistake was real, however, and innocent", "tokens": 402, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 78, "segment_id": "00078", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000131"}
{"type": "chunk", "text": "people were sent to jail as a result. The mistake arose in the context of sudden infant death syndrome (SIDS), a phenomenon in\nwhich a perfectly healthy infant dies in his or her crib. (The Brits refer to SIDS as a “cot\ndeath.”) SIDS was a medical mystery that attracted more attention as infant deaths from other\ncauses became less common.* Because these infant deaths were so mysterious and poorly\nunderstood, they bred suspicion. Sometimes that suspicion was warranted. SIDS was used on\noccasion to cover up parental negligence or abuse; a postmortem exam cannot necessarily\ndistinguish natural deaths from those in which foul play is involved. British prosecutors and\ncourts became convinced that one way to separate foul play from natural deaths would be to\nfocus on families in which there were multiple cot deaths. Sir Roy Meadow, a prominent\nBritish pediatrician, was a frequent expert witness on this point. As the British news magazine\nthe Economist explains, “What became known as Meadow’s Law---the idea that one infant\ndeath is a tragedy, two are suspicious and three are murder---is based on the notion that if an\nevent is rare, two or more instances of it in the same family are so improbable that they are\nunlikely to be the result of chance.”5 Sir Meadow explained to juries that the chance that a\nfamily could have two infants die suddenly of natural causes was an extraordinary 1 in 73\nmillion. He explained the calculation: Since the incidence of a cot death is rare, 1 in 8,500, the\nchance of having two cot deaths in the same family would be (1/8,500)2 which is roughly 1 in\n73 million. This reeks of foul play. That’s what juries decided, sending many parents to prison\non the basis of this testimony on the statistics of cot deaths (often without any corroborating\nmedical evidence of abuse or neglect). In some cases, infants were taken away from their\nparents at birth because of the unexplained death of a sibling. The Economist explained how a misunderstanding of statistical independence became a flaw\n\nin the Meadow testimony:\n\nThere is an obvious flaw in this reasoning, as the Royal Statistical Society, protective of\nits derided subject, has pointed out. The probability calculation works fine, so long as it is\ncertain that cot deaths are entirely random and not linked by some unknown factor.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\npeople were sent to jail as a result. The mistake arose in the context of sudden infant death syndrome (SIDS), a phenomenon in\nwhich a perfectly healthy infant dies in his or her crib. (The Brits refer to SIDS as a “cot\ndeath.”) SIDS was a medical mystery that attracted more attention as infant deaths from other\ncauses became less common.* Because these infant deaths were so mysterious and poorly\nunderstood, they bred suspicion. Sometimes that suspicion was warranted. SIDS was used on\noccasion to cover up parental negligence or abuse; a postmortem exam cannot necessarily\ndistinguish natural deaths from those in which foul play is involved. British prosecutors and\ncourts became convinced that one way to separate foul play from natural deaths would be to\nfocus on families in which there were multiple cot deaths. Sir Roy Meadow, a prominent\nBritish pediatrician, was a frequent expert witness on this point. As the British news magazine\nthe Economist explains, “What became known as Meadow’s Law---the idea that one infant\ndeath is a tragedy, two are suspicious and three are murder---is based on the notion that if an\nevent is rare, two or more instances of it in the same family are so improbable that they are\nunlikely to be the result of chance.”5 Sir Meadow explained to juries that the chance that a\nfamily could have two infants die suddenly of natural causes was an extraordinary 1 in 73\nmillion. He explained the calculation: Since the incidence of a cot death is rare, 1 in 8,500, the\nchance of having two cot deaths in the same family would be (1/8,500)2 which is roughly 1 in\n73 million. This reeks of foul play. That’s what juries decided, sending many parents to prison\non the basis of this testimony on the statistics of cot deaths (often without any corroborating\nmedical evidence of abuse or neglect). In some cases, infants were taken away from their\nparents at birth because of the unexplained death of a sibling. The Economist explained how a misunderstanding of statistical independence became a flaw\n\nin the Meadow testimony:\n\nThere is an obvious flaw in this reasoning, as the Royal Statistical Society, protective of\nits derided subject, has pointed out. The probability calculation works fine, so long as it is\ncertain that cot deaths are entirely random and not linked by some unknown factor.", "tokens": 499, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 79, "segment_id": "00079", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000132"}
{"type": "chunk", "text": "In some cases, infants were taken away from their\nparents at birth because of the unexplained death of a sibling. The Economist explained how a misunderstanding of statistical independence became a flaw\n\nin the Meadow testimony:\n\nThere is an obvious flaw in this reasoning, as the Royal Statistical Society, protective of\nits derided subject, has pointed out. The probability calculation works fine, so long as it is\ncertain that cot deaths are entirely random and not linked by some unknown factor. But\nwith something as mysterious as cot deaths, it is quite possible that there is a link---\nsomething genetic, for instance, which would make a family that had suffered one cot\ndeath more, not less, likely to suffer another. And since those women were convicted,\nscientists have been suggesting that there may be just such a link. In 2004, the British government announced that it would review 258 trials in which parents had\nbeen convicted of murdering their infant children. Not understanding when events ARE independent. A different kind of mistake occurs when\nevents that are independent are not treated as such. If you find yourself in a casino (a place,\nstatistically speaking, that you should not go to), you will see people looking longingly at the\ndice or cards and declaring that they are “due.” If the roulette ball has landed on black five\ntimes in a row, then clearly now it must turn up red. No, no, no! The probability of the ball’s\nlanding on a red number remains unchanged: 16/38. The belief otherwise is sometimes called", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nIn some cases, infants were taken away from their\nparents at birth because of the unexplained death of a sibling. The Economist explained how a misunderstanding of statistical independence became a flaw\n\nin the Meadow testimony:\n\nThere is an obvious flaw in this reasoning, as the Royal Statistical Society, protective of\nits derided subject, has pointed out. The probability calculation works fine, so long as it is\ncertain that cot deaths are entirely random and not linked by some unknown factor. But\nwith something as mysterious as cot deaths, it is quite possible that there is a link---\nsomething genetic, for instance, which would make a family that had suffered one cot\ndeath more, not less, likely to suffer another. And since those women were convicted,\nscientists have been suggesting that there may be just such a link. In 2004, the British government announced that it would review 258 trials in which parents had\nbeen convicted of murdering their infant children. Not understanding when events ARE independent. A different kind of mistake occurs when\nevents that are independent are not treated as such. If you find yourself in a casino (a place,\nstatistically speaking, that you should not go to), you will see people looking longingly at the\ndice or cards and declaring that they are “due.” If the roulette ball has landed on black five\ntimes in a row, then clearly now it must turn up red. No, no, no! The probability of the ball’s\nlanding on a red number remains unchanged: 16/38. The belief otherwise is sometimes called", "tokens": 319, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 79, "segment_id": "00079", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000133"}
{"type": "chunk", "text": "“the gambler’s fallacy.” In fact, if you flip a fair coin 1,000,000 times and get 1,000,000 heads\nin a row, the probability of getting tails on the next flip is still 1⁄2. The very definition of\nstatistical independence between two events is that the outcome of one has no effect on the\noutcome of the other. Even if you don’t find the statistics persuasive, you might ask yourself\nabout the physics: How can flipping a series of tails in a row make it more likely that the coin\nwill turn up heads on the next flip? Even in sports, the notion of streaks may be illusory. One of the most famous and interesting\nprobability-related academic papers refutes the common notion that basketball players\nperiodically develop a streak of good shooting during a game, or “a hot hand.” Certainly most\nsports fans would tell you that a player who makes a shot is more likely to hit the next shot\nthan a player who has just missed. Not according to research by Thomas Gilovich, Robert\nVallone, and Amos Tversky, who tested the hot hand in three different ways. 6 First, they\nanalyzed shooting data for the Philadelphia 76ers home games during the 1980--81 season. (At\nthe time, similar data were not available for other teams in the NBA.) They found “no evidence\nfor a positive correlation between the outcomes of successive shots.” Second, they did the\nsame thing for free throw data for the Boston Celtics, which produced the same result. And\nlast, they did a controlled experiment with members of the Cornell men’s and women’s\nbasketball teams. The players hit an average of 48 percent of their field goals after hitting their\nlast shot and 47 percent after missing. For fourteen of twenty-six players, the correlation\nbetween making one shot and then making the next was negative. Only one player showed a\nsignificant positive correlation between one shot and the next. That’s not what most basketball fans will tell you. For example, 91 percent of basketball\nfans surveyed at Stanford and Cornell by the authors of the paper agreed with the statement\nthat a player has a better chance of making his next shot after making his last two or three shots\nthan he does after missing his last two or three shots. The significance of the “hot hand” paper\nlies in the difference between the perception and the empirical reality.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\n“the gambler’s fallacy.” In fact, if you flip a fair coin 1,000,000 times and get 1,000,000 heads\nin a row, the probability of getting tails on the next flip is still 1⁄2. The very definition of\nstatistical independence between two events is that the outcome of one has no effect on the\noutcome of the other. Even if you don’t find the statistics persuasive, you might ask yourself\nabout the physics: How can flipping a series of tails in a row make it more likely that the coin\nwill turn up heads on the next flip? Even in sports, the notion of streaks may be illusory. One of the most famous and interesting\nprobability-related academic papers refutes the common notion that basketball players\nperiodically develop a streak of good shooting during a game, or “a hot hand.” Certainly most\nsports fans would tell you that a player who makes a shot is more likely to hit the next shot\nthan a player who has just missed. Not according to research by Thomas Gilovich, Robert\nVallone, and Amos Tversky, who tested the hot hand in three different ways. 6 First, they\nanalyzed shooting data for the Philadelphia 76ers home games during the 1980--81 season. (At\nthe time, similar data were not available for other teams in the NBA.) They found “no evidence\nfor a positive correlation between the outcomes of successive shots.” Second, they did the\nsame thing for free throw data for the Boston Celtics, which produced the same result. And\nlast, they did a controlled experiment with members of the Cornell men’s and women’s\nbasketball teams. The players hit an average of 48 percent of their field goals after hitting their\nlast shot and 47 percent after missing. For fourteen of twenty-six players, the correlation\nbetween making one shot and then making the next was negative. Only one player showed a\nsignificant positive correlation between one shot and the next. That’s not what most basketball fans will tell you. For example, 91 percent of basketball\nfans surveyed at Stanford and Cornell by the authors of the paper agreed with the statement\nthat a player has a better chance of making his next shot after making his last two or three shots\nthan he does after missing his last two or three shots. The significance of the “hot hand” paper\nlies in the difference between the perception and the empirical reality.", "tokens": 512, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 80, "segment_id": "00080", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000134"}
{"type": "chunk", "text": "Only one player showed a\nsignificant positive correlation between one shot and the next. That’s not what most basketball fans will tell you. For example, 91 percent of basketball\nfans surveyed at Stanford and Cornell by the authors of the paper agreed with the statement\nthat a player has a better chance of making his next shot after making his last two or three shots\nthan he does after missing his last two or three shots. The significance of the “hot hand” paper\nlies in the difference between the perception and the empirical reality. The authors note,\n“People’s intuitive conceptions of randomness depart systematically from the laws of chance.”\nWe see patterns where none may really exist. Like cancer clusters. Clusters happen. You’ve probably read the story in the newspaper, or perhaps seen the news\nexposé: Some statistically unlikely number of people in a particular area have contracted a\nrare form of cancer. It must be the water, or the local power plant, or the cell phone tower. Of\ncourse, any one of those things might really be causing adverse health outcomes. (Later\nchapters will explore how statistics can identify such causal relationships.) But this cluster of\ncases may also be the product of pure chance, even when the number of cases appears highly\nimprobable. Yes, the probability that five people in the same school or church or workplace\nwill contract the same rare form of leukemia may be one in a million, but there are millions of\nschools and churches and workplaces. It’s not highly improbable that five people might get the\nsame rare form of leukemia in one of those places. We just aren’t thinking about all the\nschools and churches and workplaces where this hasn’t happened. To use a different variation\non the same basic example, the chance of winning the lotto may be 1 in 20 million, but none of", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nOnly one player showed a\nsignificant positive correlation between one shot and the next. That’s not what most basketball fans will tell you. For example, 91 percent of basketball\nfans surveyed at Stanford and Cornell by the authors of the paper agreed with the statement\nthat a player has a better chance of making his next shot after making his last two or three shots\nthan he does after missing his last two or three shots. The significance of the “hot hand” paper\nlies in the difference between the perception and the empirical reality. The authors note,\n“People’s intuitive conceptions of randomness depart systematically from the laws of chance.”\nWe see patterns where none may really exist. Like cancer clusters. Clusters happen. You’ve probably read the story in the newspaper, or perhaps seen the news\nexposé: Some statistically unlikely number of people in a particular area have contracted a\nrare form of cancer. It must be the water, or the local power plant, or the cell phone tower. Of\ncourse, any one of those things might really be causing adverse health outcomes. (Later\nchapters will explore how statistics can identify such causal relationships.) But this cluster of\ncases may also be the product of pure chance, even when the number of cases appears highly\nimprobable. Yes, the probability that five people in the same school or church or workplace\nwill contract the same rare form of leukemia may be one in a million, but there are millions of\nschools and churches and workplaces. It’s not highly improbable that five people might get the\nsame rare form of leukemia in one of those places. We just aren’t thinking about all the\nschools and churches and workplaces where this hasn’t happened. To use a different variation\non the same basic example, the chance of winning the lotto may be 1 in 20 million, but none of", "tokens": 380, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 80, "segment_id": "00080", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000135"}
{"type": "chunk", "text": "us is surprised when someone wins, because millions of tickets have been sold. (Despite my\ngeneral aversion to lotteries, I do admire the Illinois slogan: “Someone’s gonna Lotto, might as\nwell be you.”)\n\nHere is an exercise that I do with my students to make the same basic point. The larger the\nclass, the better it works. I ask everyone in the class to take out a coin and stand up. We all\nflip the coin; anyone who flips heads must sit down. Assuming we start with 100 students,\nroughly 50 will sit down after the first flip. Then we do it again, after which 25 or so are still\nstanding. And so on. More often than not, there will be a student standing at the end who has\nflipped five or six tails in a row. At that point, I ask the student questions like “How did you\ndo it?” and “What are the best training exercises for flipping so many tails in a row?” or “Is\nthere a special diet that helped you pull off this impressive accomplishment?” These questions\nelicit laughter because the class has just watched the whole process unfold; they know that the\nstudent who flipped six tails in a row has no special coin-flipping talent. He or she just\nhappened to be the one who ended up with a lot of tails. When we see an anomalous event like\nthat out of context, however, we assume that something besides randomness must be\nresponsible. The prosecutor’s fallacy. Suppose you hear testimony in court to the following effect: (1) a\nDNA sample found at the scene of a crime matches a sample taken from the defendant; and (2)\nthere is only one chance in a million that the sample recovered at the scene of the crime would\nmatch anyone’s besides the defendant. (For the sake of this example, you can assume that the\nprosecution’s probabilities are correct.) On the basis of that evidence, would you vote to\nconvict? I sure hope not. The prosecutor’s fallacy occurs when the context surrounding statistical evidence is\nneglected. Here are two scenarios, each of which could explain the DNA evidence being used\nto prosecute the defendant. Defendant 1: This defendant, a spurned lover of the victim, was arrested three blocks from\nthe crime scene carrying the murder weapon.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nus is surprised when someone wins, because millions of tickets have been sold. (Despite my\ngeneral aversion to lotteries, I do admire the Illinois slogan: “Someone’s gonna Lotto, might as\nwell be you.”)\n\nHere is an exercise that I do with my students to make the same basic point. The larger the\nclass, the better it works. I ask everyone in the class to take out a coin and stand up. We all\nflip the coin; anyone who flips heads must sit down. Assuming we start with 100 students,\nroughly 50 will sit down after the first flip. Then we do it again, after which 25 or so are still\nstanding. And so on. More often than not, there will be a student standing at the end who has\nflipped five or six tails in a row. At that point, I ask the student questions like “How did you\ndo it?” and “What are the best training exercises for flipping so many tails in a row?” or “Is\nthere a special diet that helped you pull off this impressive accomplishment?” These questions\nelicit laughter because the class has just watched the whole process unfold; they know that the\nstudent who flipped six tails in a row has no special coin-flipping talent. He or she just\nhappened to be the one who ended up with a lot of tails. When we see an anomalous event like\nthat out of context, however, we assume that something besides randomness must be\nresponsible. The prosecutor’s fallacy. Suppose you hear testimony in court to the following effect: (1) a\nDNA sample found at the scene of a crime matches a sample taken from the defendant; and (2)\nthere is only one chance in a million that the sample recovered at the scene of the crime would\nmatch anyone’s besides the defendant. (For the sake of this example, you can assume that the\nprosecution’s probabilities are correct.) On the basis of that evidence, would you vote to\nconvict? I sure hope not. The prosecutor’s fallacy occurs when the context surrounding statistical evidence is\nneglected. Here are two scenarios, each of which could explain the DNA evidence being used\nto prosecute the defendant. Defendant 1: This defendant, a spurned lover of the victim, was arrested three blocks from\nthe crime scene carrying the murder weapon.", "tokens": 493, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 81, "segment_id": "00081", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000136"}
{"type": "chunk", "text": "(For the sake of this example, you can assume that the\nprosecution’s probabilities are correct.) On the basis of that evidence, would you vote to\nconvict? I sure hope not. The prosecutor’s fallacy occurs when the context surrounding statistical evidence is\nneglected. Here are two scenarios, each of which could explain the DNA evidence being used\nto prosecute the defendant. Defendant 1: This defendant, a spurned lover of the victim, was arrested three blocks from\nthe crime scene carrying the murder weapon. After he was arrested, the court compelled him to\noffer a DNA sample, which matched a sample taken from a hair found at the scene of the\ncrime. Defendant 2: This defendant was convicted of a similar crime in a different state several\nyears ago. As a result of that conviction, his DNA was included in a national DNA database of\nover a million violent felons. The DNA sample taken from the hair found at the scene of the\ncrime was run through that database and matched to this individual, who has no known\nassociation with the victim. As noted above, in both cases the prosecutor can rightfully say that the DNA sample taken\nfrom the crime scene matches the defendant’s and that there is only a one in a million chance\nthat it would match with anyone else’s. But in the case of Defendant 2, there is a darn good\nchance that he could be that random someone else, the one in a million guy whose DNA just\nhappens to be similar to the real killer’s by chance. Because the chances of finding a\ncoincidental one in a million match are relatively high if you run the sample through a", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\n(For the sake of this example, you can assume that the\nprosecution’s probabilities are correct.) On the basis of that evidence, would you vote to\nconvict? I sure hope not. The prosecutor’s fallacy occurs when the context surrounding statistical evidence is\nneglected. Here are two scenarios, each of which could explain the DNA evidence being used\nto prosecute the defendant. Defendant 1: This defendant, a spurned lover of the victim, was arrested three blocks from\nthe crime scene carrying the murder weapon. After he was arrested, the court compelled him to\noffer a DNA sample, which matched a sample taken from a hair found at the scene of the\ncrime. Defendant 2: This defendant was convicted of a similar crime in a different state several\nyears ago. As a result of that conviction, his DNA was included in a national DNA database of\nover a million violent felons. The DNA sample taken from the hair found at the scene of the\ncrime was run through that database and matched to this individual, who has no known\nassociation with the victim. As noted above, in both cases the prosecutor can rightfully say that the DNA sample taken\nfrom the crime scene matches the defendant’s and that there is only a one in a million chance\nthat it would match with anyone else’s. But in the case of Defendant 2, there is a darn good\nchance that he could be that random someone else, the one in a million guy whose DNA just\nhappens to be similar to the real killer’s by chance. Because the chances of finding a\ncoincidental one in a million match are relatively high if you run the sample through a", "tokens": 347, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 81, "segment_id": "00081", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000137"}
{"type": "chunk", "text": "database with samples from a million people. Reversion to the mean (or regression to the mean). Perhaps you’ve heard of the Sports\nIllustrated jinx, whereby individual athletes or teams featured on the cover of Sports\nIllustrated subsequently see their performance fall off. One explanation is that being on the\ncover of the magazine has some adverse effect on subsequent performance. The more\nstatistically sound explanation is that teams and athletes appear on its cover after some\nanomalously good stretch (such as a twenty-game winning streak) and that their subsequent\nperformance merely reverts back to what is normal, or the mean. This is the phenomenon\nknown as reversion to the mean. Probability tells us that any outlier---an observation that is\nparticularly far from the mean in one direction or the other---is likely to be followed by\noutcomes that are more consistent with the long-term average. Reversion to the mean can explain why the Chicago Cubs always seem to pay huge salaries\nfor free agents who subsequently disappoint fans like me. Players are able to negotiate huge\nsalaries with the Cubs after an exceptional season or two. Putting on a Cubs uniform does not\nnecessarily make these players worse (though I would not necessarily rule that out); rather, the\nCubs pay big bucks for these superstars at the end of some exceptional stretch---an outlier\nyear or two---after which their performance for the Cubs reverts to something closer to normal. The same phenomenon can explain why students who do much better than they normally do\non some kind of test will, on average, do slightly worse on a retest, and students who have\ndone worse than usual will tend to do slightly better when retested. One way to think about this\nmean reversion is that performance---both mental and physical---consists of some underlying\ntalent-related effort plus an element of luck, good or bad. (Statisticians would call this random\nerror.) In any case, those individuals who perform far above the mean for some stretch are\nlikely to have had luck on their side; those who perform far below the mean are likely to have\nhad bad luck. (In the case of an exam, think about students guessing right or wrong; in the case\nof a baseball player, think about a hit that can either go foul or land one foot fair for a triple.)\nWhen a spell of very good luck or very bad luck ends---as it inevitably will---the resulting\nperformance will be closer to the mean.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\ndatabase with samples from a million people. Reversion to the mean (or regression to the mean). Perhaps you’ve heard of the Sports\nIllustrated jinx, whereby individual athletes or teams featured on the cover of Sports\nIllustrated subsequently see their performance fall off. One explanation is that being on the\ncover of the magazine has some adverse effect on subsequent performance. The more\nstatistically sound explanation is that teams and athletes appear on its cover after some\nanomalously good stretch (such as a twenty-game winning streak) and that their subsequent\nperformance merely reverts back to what is normal, or the mean. This is the phenomenon\nknown as reversion to the mean. Probability tells us that any outlier---an observation that is\nparticularly far from the mean in one direction or the other---is likely to be followed by\noutcomes that are more consistent with the long-term average. Reversion to the mean can explain why the Chicago Cubs always seem to pay huge salaries\nfor free agents who subsequently disappoint fans like me. Players are able to negotiate huge\nsalaries with the Cubs after an exceptional season or two. Putting on a Cubs uniform does not\nnecessarily make these players worse (though I would not necessarily rule that out); rather, the\nCubs pay big bucks for these superstars at the end of some exceptional stretch---an outlier\nyear or two---after which their performance for the Cubs reverts to something closer to normal. The same phenomenon can explain why students who do much better than they normally do\non some kind of test will, on average, do slightly worse on a retest, and students who have\ndone worse than usual will tend to do slightly better when retested. One way to think about this\nmean reversion is that performance---both mental and physical---consists of some underlying\ntalent-related effort plus an element of luck, good or bad. (Statisticians would call this random\nerror.) In any case, those individuals who perform far above the mean for some stretch are\nlikely to have had luck on their side; those who perform far below the mean are likely to have\nhad bad luck. (In the case of an exam, think about students guessing right or wrong; in the case\nof a baseball player, think about a hit that can either go foul or land one foot fair for a triple.)\nWhen a spell of very good luck or very bad luck ends---as it inevitably will---the resulting\nperformance will be closer to the mean.", "tokens": 513, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 82, "segment_id": "00082", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000138"}
{"type": "chunk", "text": "(Statisticians would call this random\nerror.) In any case, those individuals who perform far above the mean for some stretch are\nlikely to have had luck on their side; those who perform far below the mean are likely to have\nhad bad luck. (In the case of an exam, think about students guessing right or wrong; in the case\nof a baseball player, think about a hit that can either go foul or land one foot fair for a triple.)\nWhen a spell of very good luck or very bad luck ends---as it inevitably will---the resulting\nperformance will be closer to the mean. Imagine that I am trying to assemble a superstar coin-flipping team (under the erroneous\nimpression that talent matters when it comes to coin flipping). After I observe a student\nflipping six tails in a row, I offer him a ten-year, $50 million contract. Needless to say, I’m\ngoing to be disappointed when this student flips only 50 percent tails over those ten years. At first glance, reversion to the mean may appear to be at odds with the “gambler’s fallacy.”\nAfter the student throws six tails in a row, is he “due” to throw heads or not? The probability\nthat he throws heads on the next flip is the same as it always is: 1⁄2. The fact that he has thrown\nlots of tails in a row does not make heads more likely on the next flip. Each flip is an\nindependent event. However, we can expect the results of the ensuing flips to be consistent\nwith what probability predicts, which is half heads and half tails, rather than what it has been in\nthe past, which is all tails. It’s a virtual certainty that someone who has flipped all tails will\nbegin throwing more heads in the ensuing 10, 20, or 100 flips. And the more flips, the more\nclosely the outcome will resemble the 50-50 mean outcome that the law of large numbers", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\n(Statisticians would call this random\nerror.) In any case, those individuals who perform far above the mean for some stretch are\nlikely to have had luck on their side; those who perform far below the mean are likely to have\nhad bad luck. (In the case of an exam, think about students guessing right or wrong; in the case\nof a baseball player, think about a hit that can either go foul or land one foot fair for a triple.)\nWhen a spell of very good luck or very bad luck ends---as it inevitably will---the resulting\nperformance will be closer to the mean. Imagine that I am trying to assemble a superstar coin-flipping team (under the erroneous\nimpression that talent matters when it comes to coin flipping). After I observe a student\nflipping six tails in a row, I offer him a ten-year, $50 million contract. Needless to say, I’m\ngoing to be disappointed when this student flips only 50 percent tails over those ten years. At first glance, reversion to the mean may appear to be at odds with the “gambler’s fallacy.”\nAfter the student throws six tails in a row, is he “due” to throw heads or not? The probability\nthat he throws heads on the next flip is the same as it always is: 1⁄2. The fact that he has thrown\nlots of tails in a row does not make heads more likely on the next flip. Each flip is an\nindependent event. However, we can expect the results of the ensuing flips to be consistent\nwith what probability predicts, which is half heads and half tails, rather than what it has been in\nthe past, which is all tails. It’s a virtual certainty that someone who has flipped all tails will\nbegin throwing more heads in the ensuing 10, 20, or 100 flips. And the more flips, the more\nclosely the outcome will resemble the 50-50 mean outcome that the law of large numbers", "tokens": 414, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 82, "segment_id": "00082", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000139"}
{"type": "chunk", "text": "predicts. (Or, alternatively, we should start looking for evidence of fraud.)\n\nAs a curious side note, researchers have also documented a Businessweek phenomenon. When CEOs receive high-profile awards, including being named one of Businessweek’s “Best\nManagers,” their companies subsequently underperform over the next three years as measured\nby both accounting profits and stock price. However, unlike the Sports Illustrated effect, this\neffect appears to be more than reversion to the mean. According to Ulrike Malmendier and\nGeoffrey Tate, economists at the University of California at Berkeley and UCLA, respectively,\nwhen CEOs achieve “superstar” status, they get distracted by their new prominence.7 They\nwrite their memoirs. They are invited to sit on outside boards. They begin searching for trophy\nspouses. (The authors propose only the first two explanations, but I find the last one plausible\nas well.) Malmendier and Tate write, “Our results suggest that media-induced superstar culture\nleads to behavioral distortions beyond mere mean reversion.” In other words, when a CEO\nappears on the cover of Businessweek, sell the stock. Statistical discrimination. When is it okay to act on the basis of what probability tells us is\nlikely to happen, and when is it not okay? In 2003, Anna Diamantopoulou, the European\ncommissioner for employment and social affairs, proposed a directive declaring that insurance\ncompanies may not charge different rates to men and women, because it violates the European\nUnion’s principle of equal treatment.8 To insurers, however, gender-based premiums aren’t\ndiscrimination; they’re just statistics. Men typically pay more for auto insurance because they\ncrash more. Women pay more for annuities (a financial product that pays a fixed monthly or\nyearly sum until death) because they live longer. Obviously many women crash more than\nmany men, and many men live longer than many women. But, as explained in the last chapter,\ninsurance companies don’t care about that. They care only about what happens on average,\nbecause if they get that right, the firm will make money.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\npredicts. (Or, alternatively, we should start looking for evidence of fraud.)\n\nAs a curious side note, researchers have also documented a Businessweek phenomenon. When CEOs receive high-profile awards, including being named one of Businessweek’s “Best\nManagers,” their companies subsequently underperform over the next three years as measured\nby both accounting profits and stock price. However, unlike the Sports Illustrated effect, this\neffect appears to be more than reversion to the mean. According to Ulrike Malmendier and\nGeoffrey Tate, economists at the University of California at Berkeley and UCLA, respectively,\nwhen CEOs achieve “superstar” status, they get distracted by their new prominence.7 They\nwrite their memoirs. They are invited to sit on outside boards. They begin searching for trophy\nspouses. (The authors propose only the first two explanations, but I find the last one plausible\nas well.) Malmendier and Tate write, “Our results suggest that media-induced superstar culture\nleads to behavioral distortions beyond mere mean reversion.” In other words, when a CEO\nappears on the cover of Businessweek, sell the stock. Statistical discrimination. When is it okay to act on the basis of what probability tells us is\nlikely to happen, and when is it not okay? In 2003, Anna Diamantopoulou, the European\ncommissioner for employment and social affairs, proposed a directive declaring that insurance\ncompanies may not charge different rates to men and women, because it violates the European\nUnion’s principle of equal treatment.8 To insurers, however, gender-based premiums aren’t\ndiscrimination; they’re just statistics. Men typically pay more for auto insurance because they\ncrash more. Women pay more for annuities (a financial product that pays a fixed monthly or\nyearly sum until death) because they live longer. Obviously many women crash more than\nmany men, and many men live longer than many women. But, as explained in the last chapter,\ninsurance companies don’t care about that. They care only about what happens on average,\nbecause if they get that right, the firm will make money.", "tokens": 445, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 83, "segment_id": "00083", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000140"}
{"type": "chunk", "text": "Men typically pay more for auto insurance because they\ncrash more. Women pay more for annuities (a financial product that pays a fixed monthly or\nyearly sum until death) because they live longer. Obviously many women crash more than\nmany men, and many men live longer than many women. But, as explained in the last chapter,\ninsurance companies don’t care about that. They care only about what happens on average,\nbecause if they get that right, the firm will make money. The interesting thing about the\nEuropean Commission policy banning gender-based insurance premiums, which is being\nimplemented in 2012, is that the authorities are not pretending that gender is unrelated to the\nrisks being insured; they are simply declaring that disparate rates based on sex are\nunacceptable.*\n\nAt first, that feels like an annoying nod to political correctness. Upon reflection, I’m not so\nsure. Remember all that impressive stuff about preventing crimes before they happen? Probability can lead us to some intriguing but distressing places in this regard. How should we\nreact when our probability-based models tell us that methamphetamine smugglers from Mexico\nare most likely to be Hispanic men aged between eighteen and thirty and driving red pickup\ntrucks between 9:00 p.m. and midnight when we also know that the vast majority of Hispanic\nmen who fit that profile are not smuggling methamphetamine? Yep, I used the profiling word,\nbecause that’s the less glamorous description of the predictive analytics that I described so\nglowingly in the last chapter, or at least one potential aspect of it. Probability tells us what is more likely and what is less likely. Yes, that is just basic\nstatistics---the tools described over the last few chapters. But it is also statistics with social\nimplications. If we want to catch violent criminals and terrorists and drug smugglers and other", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nMen typically pay more for auto insurance because they\ncrash more. Women pay more for annuities (a financial product that pays a fixed monthly or\nyearly sum until death) because they live longer. Obviously many women crash more than\nmany men, and many men live longer than many women. But, as explained in the last chapter,\ninsurance companies don’t care about that. They care only about what happens on average,\nbecause if they get that right, the firm will make money. The interesting thing about the\nEuropean Commission policy banning gender-based insurance premiums, which is being\nimplemented in 2012, is that the authorities are not pretending that gender is unrelated to the\nrisks being insured; they are simply declaring that disparate rates based on sex are\nunacceptable.*\n\nAt first, that feels like an annoying nod to political correctness. Upon reflection, I’m not so\nsure. Remember all that impressive stuff about preventing crimes before they happen? Probability can lead us to some intriguing but distressing places in this regard. How should we\nreact when our probability-based models tell us that methamphetamine smugglers from Mexico\nare most likely to be Hispanic men aged between eighteen and thirty and driving red pickup\ntrucks between 9:00 p.m. and midnight when we also know that the vast majority of Hispanic\nmen who fit that profile are not smuggling methamphetamine? Yep, I used the profiling word,\nbecause that’s the less glamorous description of the predictive analytics that I described so\nglowingly in the last chapter, or at least one potential aspect of it. Probability tells us what is more likely and what is less likely. Yes, that is just basic\nstatistics---the tools described over the last few chapters. But it is also statistics with social\nimplications. If we want to catch violent criminals and terrorists and drug smugglers and other", "tokens": 379, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 83, "segment_id": "00083", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000141"}
{"type": "chunk", "text": "individuals with the potential to do enormous harm, then we ought to use every tool at our\ndisposal. Probability can be one of those tools. It would be naïve to think that gender, age,\nrace, ethnicity, religion, and country of origin collectively tell us nothing about anything related\nto law enforcement. But what we can or should do with that kind of information (assuming it has some predictive\nvalue) is a philosophical and legal question, not a statistical one. We’re getting more and more\ninformation every day about more and more things. Is it okay to discriminate if the data tell us\nthat we’ll be right far more often than we’re wrong? (This is the origin of the term “statistical\ndiscrimination,” or “rational discrimination.”) The same kind of analysis that can be used to\ndetermine that people who buy birdseed are less likely to default on their credit cards (yes,\nthat’s really true) can be applied everywhere else in life. How much of that is acceptable? If\nwe can build a model that identifies drug smugglers correctly 80 out of 100 times, what\nhappens to the poor souls in the 20 percent---because our model is going to harass them over\nand over and over again. The broader point here is that our ability to analyze data has grown far more sophisticated\nthan our thinking about what we ought to do with the results. You can agree or disagree with\nthe European Commission decision to ban gender-based insurance premiums, but I promise\nyou it will not be the last tricky decision of that sort. We like to think of numbers as “cold,\nhard facts.” If we do the calculations right, then we must have the right answer. The more\ninteresting and dangerous reality is that we can sometimes do the calculations correctly and\nend up blundering in a dangerous direction. We can blow up the financial system or harass a\ntwenty-two-year-old white guy standing on a particular street corner at a particular time of day,\nbecause, according to our statistical model, he is almost certainly there to buy drugs. For all\nthe elegance and precision of probability, there is no substitute for thinking about what\ncalculations we are doing and why we are doing them. * SIDS is still a medical mystery, though many of the risk factors have been identified. For example, infant deaths can be\nreduced sharply by putting babies to sleep on their backs.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nindividuals with the potential to do enormous harm, then we ought to use every tool at our\ndisposal. Probability can be one of those tools. It would be naïve to think that gender, age,\nrace, ethnicity, religion, and country of origin collectively tell us nothing about anything related\nto law enforcement. But what we can or should do with that kind of information (assuming it has some predictive\nvalue) is a philosophical and legal question, not a statistical one. We’re getting more and more\ninformation every day about more and more things. Is it okay to discriminate if the data tell us\nthat we’ll be right far more often than we’re wrong? (This is the origin of the term “statistical\ndiscrimination,” or “rational discrimination.”) The same kind of analysis that can be used to\ndetermine that people who buy birdseed are less likely to default on their credit cards (yes,\nthat’s really true) can be applied everywhere else in life. How much of that is acceptable? If\nwe can build a model that identifies drug smugglers correctly 80 out of 100 times, what\nhappens to the poor souls in the 20 percent---because our model is going to harass them over\nand over and over again. The broader point here is that our ability to analyze data has grown far more sophisticated\nthan our thinking about what we ought to do with the results. You can agree or disagree with\nthe European Commission decision to ban gender-based insurance premiums, but I promise\nyou it will not be the last tricky decision of that sort. We like to think of numbers as “cold,\nhard facts.” If we do the calculations right, then we must have the right answer. The more\ninteresting and dangerous reality is that we can sometimes do the calculations correctly and\nend up blundering in a dangerous direction. We can blow up the financial system or harass a\ntwenty-two-year-old white guy standing on a particular street corner at a particular time of day,\nbecause, according to our statistical model, he is almost certainly there to buy drugs. For all\nthe elegance and precision of probability, there is no substitute for thinking about what\ncalculations we are doing and why we are doing them. * SIDS is still a medical mystery, though many of the risk factors have been identified. For example, infant deaths can be\nreduced sharply by putting babies to sleep on their backs.", "tokens": 502, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 84, "segment_id": "00084", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000142"}
{"type": "chunk", "text": "We can blow up the financial system or harass a\ntwenty-two-year-old white guy standing on a particular street corner at a particular time of day,\nbecause, according to our statistical model, he is almost certainly there to buy drugs. For all\nthe elegance and precision of probability, there is no substitute for thinking about what\ncalculations we are doing and why we are doing them. * SIDS is still a medical mystery, though many of the risk factors have been identified. For example, infant deaths can be\nreduced sharply by putting babies to sleep on their backs. * The policy change was ultimately precipitated by a 2011 ruling by the Court of Justice of the European Union that\ndifferent premiums for men and women constitute sex discrimination.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nWe can blow up the financial system or harass a\ntwenty-two-year-old white guy standing on a particular street corner at a particular time of day,\nbecause, according to our statistical model, he is almost certainly there to buy drugs. For all\nthe elegance and precision of probability, there is no substitute for thinking about what\ncalculations we are doing and why we are doing them. * SIDS is still a medical mystery, though many of the risk factors have been identified. For example, infant deaths can be\nreduced sharply by putting babies to sleep on their backs. * The policy change was ultimately precipitated by a 2011 ruling by the Court of Justice of the European Union that\ndifferent premiums for men and women constitute sex discrimination.", "tokens": 152, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 84, "segment_id": "00084", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000143"}
{"type": "chunk", "text": "CHAPTER 7\nThe Importance of Data\n“Garbage in, garbage out”\n\nIn the spring of 2012, researchers published a striking finding in the esteemed journal Science. According to this cutting-edge research, when male fruit flies are spurned repeatedly by female\nfruit flies, they drown their sorrows in alcohol. The New York Times described the study in a\nfront page article: “They were young males on the make, and they struck out not once, not\ntwice, but a dozen times with a group of attractive females hovering nearby. So they did what\nso many men do after being repeatedly rejected: they got drunk, using alcohol as a balm for\nunfulfilled desire.”1\n\nThis research advances our understanding of the brain’s reward system, which in turn can\nhelp us find new strategies for dealing with drug and alcohol dependence. A substance abuse\nexpert described reading the study as “looking back in time, to see the very origins of the\nreward circuit that drives fundamental behaviors like sex, eating and sleeping.”\n\nSince I am not an expert in this field, I had two slightly different reactions upon reading\nabout spurned fruit flies. First, it made me nostalgic for college. Second, my inner researcher\ngot to wondering how fruit flies get drunk. Is there a miniature fruit fly bar, with assorted fruit\nbased liquors and an empathetic fruit fly bartender? Is country western music playing in the\nbackground? Do fruit flies even like country western music? It turns out that the design of the experiment was devilishly simple. One group of male fruit\nflies was allowed to mate freely with virgin females. Another group of males was released\namong female fruit flies that had already mated and were therefore indifferent to the males’\namorous overtures. Both sets of male fruit flies were then offered feeding straws that offered a\nchoice between standard fruit fly fare, yeast and sugar, and the “hard stuff”: yeast, sugar, and\n15 percent alcohol. The males who had spent days trying to mate with indifferent females were\nsignificantly more likely to hit the booze. The levity notwithstanding, these results have important implications for humans. They\nsuggest a connection between stress, chemical responses in the brain, and an appetite for\nalcohol. However, the results are not a triumph of statistics. They are a triumph of data, which\nmade relatively basic statistical analysis possible.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nCHAPTER 7\nThe Importance of Data\n“Garbage in, garbage out”\n\nIn the spring of 2012, researchers published a striking finding in the esteemed journal Science. According to this cutting-edge research, when male fruit flies are spurned repeatedly by female\nfruit flies, they drown their sorrows in alcohol. The New York Times described the study in a\nfront page article: “They were young males on the make, and they struck out not once, not\ntwice, but a dozen times with a group of attractive females hovering nearby. So they did what\nso many men do after being repeatedly rejected: they got drunk, using alcohol as a balm for\nunfulfilled desire.”1\n\nThis research advances our understanding of the brain’s reward system, which in turn can\nhelp us find new strategies for dealing with drug and alcohol dependence. A substance abuse\nexpert described reading the study as “looking back in time, to see the very origins of the\nreward circuit that drives fundamental behaviors like sex, eating and sleeping.”\n\nSince I am not an expert in this field, I had two slightly different reactions upon reading\nabout spurned fruit flies. First, it made me nostalgic for college. Second, my inner researcher\ngot to wondering how fruit flies get drunk. Is there a miniature fruit fly bar, with assorted fruit\nbased liquors and an empathetic fruit fly bartender? Is country western music playing in the\nbackground? Do fruit flies even like country western music? It turns out that the design of the experiment was devilishly simple. One group of male fruit\nflies was allowed to mate freely with virgin females. Another group of males was released\namong female fruit flies that had already mated and were therefore indifferent to the males’\namorous overtures. Both sets of male fruit flies were then offered feeding straws that offered a\nchoice between standard fruit fly fare, yeast and sugar, and the “hard stuff”: yeast, sugar, and\n15 percent alcohol. The males who had spent days trying to mate with indifferent females were\nsignificantly more likely to hit the booze. The levity notwithstanding, these results have important implications for humans. They\nsuggest a connection between stress, chemical responses in the brain, and an appetite for\nalcohol. However, the results are not a triumph of statistics. They are a triumph of data, which\nmade relatively basic statistical analysis possible.", "tokens": 494, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 85, "segment_id": "00085", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000144"}
{"type": "chunk", "text": "Both sets of male fruit flies were then offered feeding straws that offered a\nchoice between standard fruit fly fare, yeast and sugar, and the “hard stuff”: yeast, sugar, and\n15 percent alcohol. The males who had spent days trying to mate with indifferent females were\nsignificantly more likely to hit the booze. The levity notwithstanding, these results have important implications for humans. They\nsuggest a connection between stress, chemical responses in the brain, and an appetite for\nalcohol. However, the results are not a triumph of statistics. They are a triumph of data, which\nmade relatively basic statistical analysis possible. The genius of this study was figuring out a\nway to create a group of sexually satiated male fruit flies and a group of sexually frustrated\nmale fruit flies---and then to find a way to compare their drinking habits. Once the researchers\ndid that, the number crunching wasn’t any more complicated than that of a typical high school\nscience fair project. Data are to statistics what a good offensive line is to a star quarterback. In front of every\nstar quarterback is a good group of blockers. They usually don’t get much credit. But without\nthem, you won’t ever see a star quarterback. Most statistics books assume that you are using", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nBoth sets of male fruit flies were then offered feeding straws that offered a\nchoice between standard fruit fly fare, yeast and sugar, and the “hard stuff”: yeast, sugar, and\n15 percent alcohol. The males who had spent days trying to mate with indifferent females were\nsignificantly more likely to hit the booze. The levity notwithstanding, these results have important implications for humans. They\nsuggest a connection between stress, chemical responses in the brain, and an appetite for\nalcohol. However, the results are not a triumph of statistics. They are a triumph of data, which\nmade relatively basic statistical analysis possible. The genius of this study was figuring out a\nway to create a group of sexually satiated male fruit flies and a group of sexually frustrated\nmale fruit flies---and then to find a way to compare their drinking habits. Once the researchers\ndid that, the number crunching wasn’t any more complicated than that of a typical high school\nscience fair project. Data are to statistics what a good offensive line is to a star quarterback. In front of every\nstar quarterback is a good group of blockers. They usually don’t get much credit. But without\nthem, you won’t ever see a star quarterback. Most statistics books assume that you are using", "tokens": 260, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 85, "segment_id": "00085", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000145"}
{"type": "chunk", "text": "good data, just as a cookbook assumes that you are not buying rancid meat and rotten\nvegetables. But even the finest recipe isn’t going to salvage a meal that begins with spoiled\ningredients. So it is with statistics; no amount of fancy analysis can make up for fundamentally\nflawed data. Hence the expression “garbage in, garbage out.” Data deserve respect, just like\noffensive linemen. We generally ask our data to do one of three things. First, we may demand a data sample that\nis representative of some larger group or population. If we are trying to gauge voters’ attitudes\ntoward a particular political candidate, we will need to interview a sample of prospective\nvoters who are representative of all voters in the relevant political jurisdiction. (And\nremember, we don’t want a sample that is representative of everyone living in that jurisdiction;\nwe want a sample of those who are likely to vote.) One of the most powerful findings in\nstatistics, which will be explained in greater depth over the next two chapters, is that inferences\nmade from reasonably large, properly drawn samples can be every bit as accurate as\nattempting to elicit the same information from the entire population. The easiest way to gather a representative sample of a larger population is to select some\nsubset of that population randomly. (Shockingly, this is known as a simple random sample.)\nThe key to this methodology is that each observation in the relevant population must have an\nequal chance of being included in the sample. If you plan to survey a random sample of 100\nadults in a neighborhood with 4,328 adult residents, your methodology has to ensure that each\nof those 4,328 residents has the same probability of ending up as one of the 100 adults who are\nsurveyed. Statistics books almost always illustrate this point by drawing colored marbles out\nof an urn. (In fact, it’s about the only place where one sees the word “urn” used with any\nregularity.) If there are 60,000 blue marbles and 40,000 red marbles in a giant urn, then the\nmost likely composition of a sample of 100 marbles drawn randomly from the urn would be 60\nblue marbles and 40 red marbles.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\ngood data, just as a cookbook assumes that you are not buying rancid meat and rotten\nvegetables. But even the finest recipe isn’t going to salvage a meal that begins with spoiled\ningredients. So it is with statistics; no amount of fancy analysis can make up for fundamentally\nflawed data. Hence the expression “garbage in, garbage out.” Data deserve respect, just like\noffensive linemen. We generally ask our data to do one of three things. First, we may demand a data sample that\nis representative of some larger group or population. If we are trying to gauge voters’ attitudes\ntoward a particular political candidate, we will need to interview a sample of prospective\nvoters who are representative of all voters in the relevant political jurisdiction. (And\nremember, we don’t want a sample that is representative of everyone living in that jurisdiction;\nwe want a sample of those who are likely to vote.) One of the most powerful findings in\nstatistics, which will be explained in greater depth over the next two chapters, is that inferences\nmade from reasonably large, properly drawn samples can be every bit as accurate as\nattempting to elicit the same information from the entire population. The easiest way to gather a representative sample of a larger population is to select some\nsubset of that population randomly. (Shockingly, this is known as a simple random sample.)\nThe key to this methodology is that each observation in the relevant population must have an\nequal chance of being included in the sample. If you plan to survey a random sample of 100\nadults in a neighborhood with 4,328 adult residents, your methodology has to ensure that each\nof those 4,328 residents has the same probability of ending up as one of the 100 adults who are\nsurveyed. Statistics books almost always illustrate this point by drawing colored marbles out\nof an urn. (In fact, it’s about the only place where one sees the word “urn” used with any\nregularity.) If there are 60,000 blue marbles and 40,000 red marbles in a giant urn, then the\nmost likely composition of a sample of 100 marbles drawn randomly from the urn would be 60\nblue marbles and 40 red marbles.", "tokens": 471, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 86, "segment_id": "00086", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000146"}
{"type": "chunk", "text": "Statistics books almost always illustrate this point by drawing colored marbles out\nof an urn. (In fact, it’s about the only place where one sees the word “urn” used with any\nregularity.) If there are 60,000 blue marbles and 40,000 red marbles in a giant urn, then the\nmost likely composition of a sample of 100 marbles drawn randomly from the urn would be 60\nblue marbles and 40 red marbles. If we did this more than once, there would obviously be\ndeviations from sample to sample---some might have 62 blue marbles and 38 red marbles, or\n58 blue and 42 red. But the chances of drawing any random sample that deviates hugely from\nthe composition of marbles in the urn are very, very low. Now, admittedly, there are some practical challenges here. Most populations we care about\ntend to be more complicated than an urn full of marbles. How, exactly, would one select a\nrandom sample of the American adult population to be included in a telephone poll? Even a\nseemingly elegant solution like a telephone random dialer has potential flaws. Some\nindividuals (particularly low-income persons) may not have a telephone. Others (particularly\nhigh-income persons) may be more prone to screen calls and choose not to answer. Chapter 10\nwill outline some of the strategies that polling firms use to surmount these kinds of sampling\nchallenges (most of which got even more complicated with the advent of cell phones). The key\nidea is that a properly drawn sample will look like the population from which it is drawn. In\nterms of intuition, you can envision sampling a pot of soup with a single spoonful. If you’ve\nstirred your soup adequately, a single spoonful can tell you how the whole pot tastes. A statistics text will include far more detail on sampling methods. Polling firms and market\nresearch companies spend their days figuring out how to get good representative data from", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nStatistics books almost always illustrate this point by drawing colored marbles out\nof an urn. (In fact, it’s about the only place where one sees the word “urn” used with any\nregularity.) If there are 60,000 blue marbles and 40,000 red marbles in a giant urn, then the\nmost likely composition of a sample of 100 marbles drawn randomly from the urn would be 60\nblue marbles and 40 red marbles. If we did this more than once, there would obviously be\ndeviations from sample to sample---some might have 62 blue marbles and 38 red marbles, or\n58 blue and 42 red. But the chances of drawing any random sample that deviates hugely from\nthe composition of marbles in the urn are very, very low. Now, admittedly, there are some practical challenges here. Most populations we care about\ntend to be more complicated than an urn full of marbles. How, exactly, would one select a\nrandom sample of the American adult population to be included in a telephone poll? Even a\nseemingly elegant solution like a telephone random dialer has potential flaws. Some\nindividuals (particularly low-income persons) may not have a telephone. Others (particularly\nhigh-income persons) may be more prone to screen calls and choose not to answer. Chapter 10\nwill outline some of the strategies that polling firms use to surmount these kinds of sampling\nchallenges (most of which got even more complicated with the advent of cell phones). The key\nidea is that a properly drawn sample will look like the population from which it is drawn. In\nterms of intuition, you can envision sampling a pot of soup with a single spoonful. If you’ve\nstirred your soup adequately, a single spoonful can tell you how the whole pot tastes. A statistics text will include far more detail on sampling methods. Polling firms and market\nresearch companies spend their days figuring out how to get good representative data from", "tokens": 418, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 86, "segment_id": "00086", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000147"}
{"type": "chunk", "text": "various populations in the most cost-effective way. For now, you should appreciate several\nimportant things: (1) A representative sample is a fabulously important thing, for it opens the\ndoor to some of the most powerful tools that statistics has to offer. (2) Getting a good sample\nis harder than it looks. (3) Many of the most egregious statistical assertions are caused by\ngood statistical methods applied to bad samples, not the opposite. (4) Size matters, and bigger\nis better. The details will be explained in the coming chapters, but it should be intuitive that a\nlarger sample will help to smooth away any freak variation. (A bowl of soup will be an even\nbetter test than a spoonful.) One crucial caveat is that a bigger sample will not make up for\nerrors in its composition, or “bias.” A bad sample is a bad sample. No supercomputer or fancy\nformula is going to rescue the validity of your national presidential poll if the respondents are\ndrawn only from a telephone survey of Washington, D.C., residents. The residents of\nWashington, D.C., don’t vote like the rest of America; calling 100,000 D.C. residents rather\nthan 1,000 is not going to fix that fundamental problem with your poll. In fact, a large, biased\nsample is arguably worse than a small, biased sample because it will give a false sense of\nconfidence regarding the results. The second thing we often ask of data is that they provide some source of comparison. Is a\nnew medicine more effective than the current treatment? Are ex-convicts who receive job\ntraining less likely to return to prison than ex-convicts who do not receive such training? Do\nstudents who attend charter schools perform better than similar students who attend regular\npublic schools? In these cases, the goal is to find two groups of subjects who are broadly similar except for\nthe application of whatever “treatment” we care about. In a social science context, the word\n“treatment” is broad enough to encompass anything from being a sexually frustrated fruit fly to\nreceiving an income tax rebate. As with any other application of the scientific method, we are\ntrying to isolate the impact of one specific intervention or attribute. This was the genius of the\nfruit fly experiment.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nvarious populations in the most cost-effective way. For now, you should appreciate several\nimportant things: (1) A representative sample is a fabulously important thing, for it opens the\ndoor to some of the most powerful tools that statistics has to offer. (2) Getting a good sample\nis harder than it looks. (3) Many of the most egregious statistical assertions are caused by\ngood statistical methods applied to bad samples, not the opposite. (4) Size matters, and bigger\nis better. The details will be explained in the coming chapters, but it should be intuitive that a\nlarger sample will help to smooth away any freak variation. (A bowl of soup will be an even\nbetter test than a spoonful.) One crucial caveat is that a bigger sample will not make up for\nerrors in its composition, or “bias.” A bad sample is a bad sample. No supercomputer or fancy\nformula is going to rescue the validity of your national presidential poll if the respondents are\ndrawn only from a telephone survey of Washington, D.C., residents. The residents of\nWashington, D.C., don’t vote like the rest of America; calling 100,000 D.C. residents rather\nthan 1,000 is not going to fix that fundamental problem with your poll. In fact, a large, biased\nsample is arguably worse than a small, biased sample because it will give a false sense of\nconfidence regarding the results. The second thing we often ask of data is that they provide some source of comparison. Is a\nnew medicine more effective than the current treatment? Are ex-convicts who receive job\ntraining less likely to return to prison than ex-convicts who do not receive such training? Do\nstudents who attend charter schools perform better than similar students who attend regular\npublic schools? In these cases, the goal is to find two groups of subjects who are broadly similar except for\nthe application of whatever “treatment” we care about. In a social science context, the word\n“treatment” is broad enough to encompass anything from being a sexually frustrated fruit fly to\nreceiving an income tax rebate. As with any other application of the scientific method, we are\ntrying to isolate the impact of one specific intervention or attribute. This was the genius of the\nfruit fly experiment.", "tokens": 478, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 87, "segment_id": "00087", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000148"}
{"type": "chunk", "text": "Do\nstudents who attend charter schools perform better than similar students who attend regular\npublic schools? In these cases, the goal is to find two groups of subjects who are broadly similar except for\nthe application of whatever “treatment” we care about. In a social science context, the word\n“treatment” is broad enough to encompass anything from being a sexually frustrated fruit fly to\nreceiving an income tax rebate. As with any other application of the scientific method, we are\ntrying to isolate the impact of one specific intervention or attribute. This was the genius of the\nfruit fly experiment. The researchers figured out a way to create a control group (the males\nwho mated) and a “treatment” group (the males who were shot down); the subsequent\ndifference in their drinking behaviors can then be attributed to whether they were sexually\nspurned or not. In the physical and biological sciences, creating treatment and control groups is relatively\nstraightforward. Chemists can make small variations from test tube to test tube and then study\nthe difference in outcomes. Biologists can do the same thing with their petri dishes. Even most\nanimal testing is simpler than trying to get fruit flies to drink alcohol. We can have one group\nof rats exercise regularly on a treadmill and then compare their mental acuity in a maze with\nthe performance of another group of rats that didn’t exercise. But when humans become\ninvolved, things grow more complicated. Sound statistical analysis often requires a treatment\nand a control group, yet we cannot force people to do the things that we make laboratory rats\ndo. (And many people do not like making even the lab rats do these things.) Do repeated\nconcussions cause serious neurological problems later in life? This is a really important\nquestion. The future of football (and perhaps other sports) hangs on the answer. Yet it is a\nquestion that cannot be answered with experiments on humans. So unless and until we can", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nDo\nstudents who attend charter schools perform better than similar students who attend regular\npublic schools? In these cases, the goal is to find two groups of subjects who are broadly similar except for\nthe application of whatever “treatment” we care about. In a social science context, the word\n“treatment” is broad enough to encompass anything from being a sexually frustrated fruit fly to\nreceiving an income tax rebate. As with any other application of the scientific method, we are\ntrying to isolate the impact of one specific intervention or attribute. This was the genius of the\nfruit fly experiment. The researchers figured out a way to create a control group (the males\nwho mated) and a “treatment” group (the males who were shot down); the subsequent\ndifference in their drinking behaviors can then be attributed to whether they were sexually\nspurned or not. In the physical and biological sciences, creating treatment and control groups is relatively\nstraightforward. Chemists can make small variations from test tube to test tube and then study\nthe difference in outcomes. Biologists can do the same thing with their petri dishes. Even most\nanimal testing is simpler than trying to get fruit flies to drink alcohol. We can have one group\nof rats exercise regularly on a treadmill and then compare their mental acuity in a maze with\nthe performance of another group of rats that didn’t exercise. But when humans become\ninvolved, things grow more complicated. Sound statistical analysis often requires a treatment\nand a control group, yet we cannot force people to do the things that we make laboratory rats\ndo. (And many people do not like making even the lab rats do these things.) Do repeated\nconcussions cause serious neurological problems later in life? This is a really important\nquestion. The future of football (and perhaps other sports) hangs on the answer. Yet it is a\nquestion that cannot be answered with experiments on humans. So unless and until we can", "tokens": 402, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 87, "segment_id": "00087", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000149"}
{"type": "chunk", "text": "teach fruit flies to wear helmets and run the spread offense, we have to find other ways to\nstudy the long-term impact of head trauma. One recurring research challenge with human subjects is creating treatment and control\ngroups that differ only in that one group is getting the treatment and the other is not. For this\nreason, the “gold standard” of research is randomization, a process by which human subjects\n(or schools, or hospitals, or whatever we’re studying) are randomly assigned to either the\ntreatment or the control group. We do not assume that all the experimental subjects are\nidentical. Instead, probability becomes our friend (once again), and we assume that\nrandomization will evenly divide all relevant characteristics between the two groups---both the\ncharacteristics we can observe, like race or income, but also confounding characteristics that\nwe cannot measure or had not considered, such as perseverance or faith. The third reason we collect data is, to quote my teenage daughter, “Just because.” We\nsometimes have no specific idea what we will do with the information---but we suspect it will\ncome in handy at some point. This is similar to a crime scene detective who demands that all\npossible evidence be captured so that it can be sorted later for clues. Some of this evidence\nwill prove useful, some will not. If we knew exactly what would be useful, we probably would\nnot need to be doing the investigation in the first place. You probably know that smoking and obesity are risk factors for heart disease. You\nprobably don’t know that a long-running study of the residents of Framingham, Massachusetts,\nhelped to clarify those relationships. Framingham is a suburban town of some 67,000 people\nabout twenty miles west of Boston. To nonresearchers, it is best known as a suburb of Boston\nwith reasonably priced housing and convenient access to the impressive and upscale Natick\nMall. To researchers, Framingham is best known as the home of the Framingham Heart Study,\none of the most successful and influential longitudinal studies in the history of modern science. A longitudinal study collects information on a large group of subjects at many different\npoints in time, such as once every two years. The same participants may be interviewed\nperiodically for ten, twenty, or even fifty years after they enter the study, creating a remarkably\nrich trove of information.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nteach fruit flies to wear helmets and run the spread offense, we have to find other ways to\nstudy the long-term impact of head trauma. One recurring research challenge with human subjects is creating treatment and control\ngroups that differ only in that one group is getting the treatment and the other is not. For this\nreason, the “gold standard” of research is randomization, a process by which human subjects\n(or schools, or hospitals, or whatever we’re studying) are randomly assigned to either the\ntreatment or the control group. We do not assume that all the experimental subjects are\nidentical. Instead, probability becomes our friend (once again), and we assume that\nrandomization will evenly divide all relevant characteristics between the two groups---both the\ncharacteristics we can observe, like race or income, but also confounding characteristics that\nwe cannot measure or had not considered, such as perseverance or faith. The third reason we collect data is, to quote my teenage daughter, “Just because.” We\nsometimes have no specific idea what we will do with the information---but we suspect it will\ncome in handy at some point. This is similar to a crime scene detective who demands that all\npossible evidence be captured so that it can be sorted later for clues. Some of this evidence\nwill prove useful, some will not. If we knew exactly what would be useful, we probably would\nnot need to be doing the investigation in the first place. You probably know that smoking and obesity are risk factors for heart disease. You\nprobably don’t know that a long-running study of the residents of Framingham, Massachusetts,\nhelped to clarify those relationships. Framingham is a suburban town of some 67,000 people\nabout twenty miles west of Boston. To nonresearchers, it is best known as a suburb of Boston\nwith reasonably priced housing and convenient access to the impressive and upscale Natick\nMall. To researchers, Framingham is best known as the home of the Framingham Heart Study,\none of the most successful and influential longitudinal studies in the history of modern science. A longitudinal study collects information on a large group of subjects at many different\npoints in time, such as once every two years. The same participants may be interviewed\nperiodically for ten, twenty, or even fifty years after they enter the study, creating a remarkably\nrich trove of information.", "tokens": 488, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 88, "segment_id": "00088", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000150"}
{"type": "chunk", "text": "To nonresearchers, it is best known as a suburb of Boston\nwith reasonably priced housing and convenient access to the impressive and upscale Natick\nMall. To researchers, Framingham is best known as the home of the Framingham Heart Study,\none of the most successful and influential longitudinal studies in the history of modern science. A longitudinal study collects information on a large group of subjects at many different\npoints in time, such as once every two years. The same participants may be interviewed\nperiodically for ten, twenty, or even fifty years after they enter the study, creating a remarkably\nrich trove of information. In the case of the Framingham study, researchers gathered\ninformation on 5,209 adult residents of Framingham in 1948: height, weight, blood pressure,\neducational background, family structure, diet, smoking behavior, drug use, and so on. Most\nimportant, researchers have gathered follow-up data from the same participants ever since\n(and also data on their offspring, to examine genetic factors related to heart disease). The\nFramingham data have been used to produce over two thousand academic articles since 1950,\nincluding nearly a thousand between 2000 and 2009. These studies have produced findings crucial to our understanding of cardiovascular\ndisease, many of which we now take for granted: cigarette smoking increases the risk of heart\ndisease (1960); physical activity reduces the risk of heart disease and obesity increases it\n(1967); high blood pressure increases the risk of stroke (1970); high levels of HDL cholesterol\n(henceforth known as the “good cholesterol”) reduce the risk of death (1988); individuals with\nparents and siblings who have cardiovascular disease are at significantly higher risk of the\nsame (2004 and 2005).", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nTo nonresearchers, it is best known as a suburb of Boston\nwith reasonably priced housing and convenient access to the impressive and upscale Natick\nMall. To researchers, Framingham is best known as the home of the Framingham Heart Study,\none of the most successful and influential longitudinal studies in the history of modern science. A longitudinal study collects information on a large group of subjects at many different\npoints in time, such as once every two years. The same participants may be interviewed\nperiodically for ten, twenty, or even fifty years after they enter the study, creating a remarkably\nrich trove of information. In the case of the Framingham study, researchers gathered\ninformation on 5,209 adult residents of Framingham in 1948: height, weight, blood pressure,\neducational background, family structure, diet, smoking behavior, drug use, and so on. Most\nimportant, researchers have gathered follow-up data from the same participants ever since\n(and also data on their offspring, to examine genetic factors related to heart disease). The\nFramingham data have been used to produce over two thousand academic articles since 1950,\nincluding nearly a thousand between 2000 and 2009. These studies have produced findings crucial to our understanding of cardiovascular\ndisease, many of which we now take for granted: cigarette smoking increases the risk of heart\ndisease (1960); physical activity reduces the risk of heart disease and obesity increases it\n(1967); high blood pressure increases the risk of stroke (1970); high levels of HDL cholesterol\n(henceforth known as the “good cholesterol”) reduce the risk of death (1988); individuals with\nparents and siblings who have cardiovascular disease are at significantly higher risk of the\nsame (2004 and 2005).", "tokens": 369, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 88, "segment_id": "00088", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000151"}
{"type": "chunk", "text": "Longitudinal data sets are the research equivalent of a Ferrari. The data are particularly\nvaluable when it comes to exploring causal relationships that may take years or decades to\nunfold. For example, the Perry Preschool Study began in the late 1960s with a group of 123\nAfrican American three- and four-year-olds from poor families. The participating children were\nrandomly assigned into a group that received an intensive preschool program and a comparison\ngroup that did not. Researchers then measured various outcomes for both groups for the next\nforty years. The results make a compelling case for the benefits of early childhood education. The students who received the intensive preschool experience had higher IQs at age five. They\nwere more likely to graduate from high school. They had higher earnings at age forty. In\ncontrast, the participants who did not receive the preschool program were significantly more\nlikely to have been arrested five or more times by age forty. Not surprisingly, we can’t always have the Ferrari. The research equivalent of a Toyota is a\ncross-sectional data set, which is a collection of data gathered at a single point in time. For\nexample, if epidemiologists are searching for the cause of a new disease (or an outbreak of an\nold one), they may gather data from all those afflicted in hopes of finding a pattern that leads to\nthe source. What have they eaten? Where have they traveled? What else do they have in\ncommon? Researchers may also gather data from individuals who are not afflicted by the\ndisease to highlight contrasts between the two groups. In fact, all of this exciting cross-sectional data talk reminds me of the week before my\nwedding, when I became part of a data set. I was working in Kathmandu, Nepal, when I tested\npositive for a poorly understood stomach illness called “blue-green algae,” which had been\nfound in only two places in the world. Researchers had isolated the pathogen that caused the\ndisease, but they were not yet sure what kind of organism it was, as it had never been\nidentified before. When I called home to inform my fiancée about my diagnosis, I\nacknowledged that there was some bad news. The disease had no known means of\ntransmission, no known cure, and could cause extreme fatigue and other unpleasant side effects\nfor anywhere from a few days to many months.* With the wedding only one week away, yes,\nthis could be a problem.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nLongitudinal data sets are the research equivalent of a Ferrari. The data are particularly\nvaluable when it comes to exploring causal relationships that may take years or decades to\nunfold. For example, the Perry Preschool Study began in the late 1960s with a group of 123\nAfrican American three- and four-year-olds from poor families. The participating children were\nrandomly assigned into a group that received an intensive preschool program and a comparison\ngroup that did not. Researchers then measured various outcomes for both groups for the next\nforty years. The results make a compelling case for the benefits of early childhood education. The students who received the intensive preschool experience had higher IQs at age five. They\nwere more likely to graduate from high school. They had higher earnings at age forty. In\ncontrast, the participants who did not receive the preschool program were significantly more\nlikely to have been arrested five or more times by age forty. Not surprisingly, we can’t always have the Ferrari. The research equivalent of a Toyota is a\ncross-sectional data set, which is a collection of data gathered at a single point in time. For\nexample, if epidemiologists are searching for the cause of a new disease (or an outbreak of an\nold one), they may gather data from all those afflicted in hopes of finding a pattern that leads to\nthe source. What have they eaten? Where have they traveled? What else do they have in\ncommon? Researchers may also gather data from individuals who are not afflicted by the\ndisease to highlight contrasts between the two groups. In fact, all of this exciting cross-sectional data talk reminds me of the week before my\nwedding, when I became part of a data set. I was working in Kathmandu, Nepal, when I tested\npositive for a poorly understood stomach illness called “blue-green algae,” which had been\nfound in only two places in the world. Researchers had isolated the pathogen that caused the\ndisease, but they were not yet sure what kind of organism it was, as it had never been\nidentified before. When I called home to inform my fiancée about my diagnosis, I\nacknowledged that there was some bad news. The disease had no known means of\ntransmission, no known cure, and could cause extreme fatigue and other unpleasant side effects\nfor anywhere from a few days to many months.* With the wedding only one week away, yes,\nthis could be a problem.", "tokens": 507, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 89, "segment_id": "00089", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000152"}
{"type": "chunk", "text": "Researchers had isolated the pathogen that caused the\ndisease, but they were not yet sure what kind of organism it was, as it had never been\nidentified before. When I called home to inform my fiancée about my diagnosis, I\nacknowledged that there was some bad news. The disease had no known means of\ntransmission, no known cure, and could cause extreme fatigue and other unpleasant side effects\nfor anywhere from a few days to many months.* With the wedding only one week away, yes,\nthis could be a problem. Would I have complete control of my digestive system as I walked\ndown the aisle? Maybe. But then I really tried to focus on the good news. First, “blue-green algae” was thought to be\nnonfatal. And second, experts in tropical diseases from as far away as Bangkok had taken a\npersonal interest in my case. How cool is that? (Also, I did a terrific job of repeatedly steering\nthe discussion back to the wedding planning: “Enough about my incurable disease. Tell me\nmore about the flowers.”)\n\nI spent my final hours in Kathmandu filling out a thirty-page survey describing every aspect\nof my life: Where did I eat? What did I eat? How did I cook? Did I go swimming? Where and\nhow often? Everyone else who had been diagnosed with the disease was doing the same thing. Eventually the pathogen was identified as a water-borne form of cyanobacteria. (These\nbacteria are blue, and they are the only kind of bacteria that get their energy from\nphotosynthesis; hence the original description of the disease as “blue-green algae.”) The illness\nwas found to respond to treatment with traditional antibiotics, but, curiously, not to some of the", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nResearchers had isolated the pathogen that caused the\ndisease, but they were not yet sure what kind of organism it was, as it had never been\nidentified before. When I called home to inform my fiancée about my diagnosis, I\nacknowledged that there was some bad news. The disease had no known means of\ntransmission, no known cure, and could cause extreme fatigue and other unpleasant side effects\nfor anywhere from a few days to many months.* With the wedding only one week away, yes,\nthis could be a problem. Would I have complete control of my digestive system as I walked\ndown the aisle? Maybe. But then I really tried to focus on the good news. First, “blue-green algae” was thought to be\nnonfatal. And second, experts in tropical diseases from as far away as Bangkok had taken a\npersonal interest in my case. How cool is that? (Also, I did a terrific job of repeatedly steering\nthe discussion back to the wedding planning: “Enough about my incurable disease. Tell me\nmore about the flowers.”)\n\nI spent my final hours in Kathmandu filling out a thirty-page survey describing every aspect\nof my life: Where did I eat? What did I eat? How did I cook? Did I go swimming? Where and\nhow often? Everyone else who had been diagnosed with the disease was doing the same thing. Eventually the pathogen was identified as a water-borne form of cyanobacteria. (These\nbacteria are blue, and they are the only kind of bacteria that get their energy from\nphotosynthesis; hence the original description of the disease as “blue-green algae.”) The illness\nwas found to respond to treatment with traditional antibiotics, but, curiously, not to some of the", "tokens": 367, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 89, "segment_id": "00089", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000153"}
{"type": "chunk", "text": "newer ones. All of these discoveries were too late to help me, but I was lucky enough to\nrecover quickly anyway. I had near-perfect control of my digestive system by wedding day. Behind every important study there are good data that made the analysis possible. And behind\nevery bad study . . . well, read on. People often speak about “lying with statistics.” I would\nargue that some of the most egregious statistical mistakes involve lying with data; the\nstatistical analysis is fine, but the data on which the calculations are performed are bogus or\ninappropriate. Here are some common examples of “garbage in, garbage out.”\n\nSelection bias. Pauline Kael, the longtime film critic for The New Yorker, is alleged to have\nsaid after Richard Nixon’s election as president, “Nixon couldn’t have won. I don’t know\nanyone who voted for him.” The quotation is most likely apocryphal, but it’s a lovely example\nof how a lousy sample (one’s group of liberal friends) can offer a misleading snapshot of a\nlarger population (voters from across America). And it introduces the question one should\nalways ask: How have we chosen the sample or samples that we are evaluating? If each\nmember of the relevant population does not have an equal chance of ending up in the sample,\nwe are going to have a problem with whatever results emerge from that sample. One ritual of\npresidential politics is the Iowa straw poll, in which Republican candidates descend on Ames,\nIowa, in August of the year before a presidential election to woo participants, each of whom\npays $30 to cast a vote in the poll. The Iowa straw poll does not tell us that much about the\nfuture of Republican candidates. (The poll has predicted only three of the last five Republican\nnominees.) Why? Because Iowans who pay $30 to vote in the straw poll are different from\nother Iowa Republicans; and Iowa Republicans are different from Republican voters in the rest\nof the country. Selection bias can be introduced in many other ways. A survey of consumers in an airport is\ngoing to be biased by the fact that people who fly are likely to be wealthier than the general\npublic; a survey at a rest stop on Interstate 90 may have the opposite problem.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nnewer ones. All of these discoveries were too late to help me, but I was lucky enough to\nrecover quickly anyway. I had near-perfect control of my digestive system by wedding day. Behind every important study there are good data that made the analysis possible. And behind\nevery bad study . . . well, read on. People often speak about “lying with statistics.” I would\nargue that some of the most egregious statistical mistakes involve lying with data; the\nstatistical analysis is fine, but the data on which the calculations are performed are bogus or\ninappropriate. Here are some common examples of “garbage in, garbage out.”\n\nSelection bias. Pauline Kael, the longtime film critic for The New Yorker, is alleged to have\nsaid after Richard Nixon’s election as president, “Nixon couldn’t have won. I don’t know\nanyone who voted for him.” The quotation is most likely apocryphal, but it’s a lovely example\nof how a lousy sample (one’s group of liberal friends) can offer a misleading snapshot of a\nlarger population (voters from across America). And it introduces the question one should\nalways ask: How have we chosen the sample or samples that we are evaluating? If each\nmember of the relevant population does not have an equal chance of ending up in the sample,\nwe are going to have a problem with whatever results emerge from that sample. One ritual of\npresidential politics is the Iowa straw poll, in which Republican candidates descend on Ames,\nIowa, in August of the year before a presidential election to woo participants, each of whom\npays $30 to cast a vote in the poll. The Iowa straw poll does not tell us that much about the\nfuture of Republican candidates. (The poll has predicted only three of the last five Republican\nnominees.) Why? Because Iowans who pay $30 to vote in the straw poll are different from\nother Iowa Republicans; and Iowa Republicans are different from Republican voters in the rest\nof the country. Selection bias can be introduced in many other ways. A survey of consumers in an airport is\ngoing to be biased by the fact that people who fly are likely to be wealthier than the general\npublic; a survey at a rest stop on Interstate 90 may have the opposite problem.", "tokens": 480, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 90, "segment_id": "00090", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000154"}
{"type": "chunk", "text": "(The poll has predicted only three of the last five Republican\nnominees.) Why? Because Iowans who pay $30 to vote in the straw poll are different from\nother Iowa Republicans; and Iowa Republicans are different from Republican voters in the rest\nof the country. Selection bias can be introduced in many other ways. A survey of consumers in an airport is\ngoing to be biased by the fact that people who fly are likely to be wealthier than the general\npublic; a survey at a rest stop on Interstate 90 may have the opposite problem. Both surveys\nare likely to be biased by the fact that people who are willing to answer a survey in a public\nplace are different from people who would prefer not to be bothered. If you ask 100 people in\na public place to complete a short survey, and 60 are willing to answer your questions, those\n60 are likely to be different in significant ways from the 40 who walked by without making\neye contact. One of the most famous statistical blunders of all time, the notorious Literary Digest poll of\n1936, was caused by a biased sample. In that year, Kansas governor Alf Landon, a Republican,\nwas running for president against incumbent Franklin Roosevelt, a Democrat. Literary Digest,\nan influential weekly news magazine at the time, mailed a poll to its subscribers and to\nautomobile and telephone owners whose addresses could be culled from public records. All\ntold,\nthe Literary Digest poll included 10 million prospective voters, which is an\nastronomically large sample. As polls with good samples get larger, they get better, since the\nmargin of error shrinks. As polls with bad samples get larger, the pile of garbage just gets\nbigger and smellier. Literary Digest predicted that Landon would beat Roosevelt with 57\npercent of the popular vote. In fact, Roosevelt won in a landslide, with 60 percent of the", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\n(The poll has predicted only three of the last five Republican\nnominees.) Why? Because Iowans who pay $30 to vote in the straw poll are different from\nother Iowa Republicans; and Iowa Republicans are different from Republican voters in the rest\nof the country. Selection bias can be introduced in many other ways. A survey of consumers in an airport is\ngoing to be biased by the fact that people who fly are likely to be wealthier than the general\npublic; a survey at a rest stop on Interstate 90 may have the opposite problem. Both surveys\nare likely to be biased by the fact that people who are willing to answer a survey in a public\nplace are different from people who would prefer not to be bothered. If you ask 100 people in\na public place to complete a short survey, and 60 are willing to answer your questions, those\n60 are likely to be different in significant ways from the 40 who walked by without making\neye contact. One of the most famous statistical blunders of all time, the notorious Literary Digest poll of\n1936, was caused by a biased sample. In that year, Kansas governor Alf Landon, a Republican,\nwas running for president against incumbent Franklin Roosevelt, a Democrat. Literary Digest,\nan influential weekly news magazine at the time, mailed a poll to its subscribers and to\nautomobile and telephone owners whose addresses could be culled from public records. All\ntold,\nthe Literary Digest poll included 10 million prospective voters, which is an\nastronomically large sample. As polls with good samples get larger, they get better, since the\nmargin of error shrinks. As polls with bad samples get larger, the pile of garbage just gets\nbigger and smellier. Literary Digest predicted that Landon would beat Roosevelt with 57\npercent of the popular vote. In fact, Roosevelt won in a landslide, with 60 percent of the", "tokens": 394, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 90, "segment_id": "00090", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000155"}
{"type": "chunk", "text": "popular vote and forty-six of forty-eight states in the electoral college. The Literary Digest\nsample was “garbage in”: the magazine’s subscribers were wealthier than average Americans,\nand therefore more likely to vote Republican, as were households with telephones and cars in\n1936.2\n\nWe can end up with the same basic problem when we compare outcomes between a\ntreatment and a control group if the mechanism for sorting individuals into one group or the\nother is not random. Consider a recent finding in the medical literature on the side effects of\ntreatment for prostate cancer. There are three common treatments for prostate cancer: surgical\nremoval of the prostate; radiation therapy; or brachytherapy (which involves implanting\nradioactive “seeds” near the cancer).3 Impotence is a common side effect of prostate cancer\ntreatment, so researchers have documented the sexual function of men who receive each of the\nthree treatments. A study of 1,000 men found that two years after treatment, 35 percent of the\nmen in the surgery group were able to have sexual intercourse, compared with 37 percent in the\nradiation group and 43 percent in the brachytherapy group. Can one look at these data and assume that brachytherapy is least likely to damage a man’s\nsexual function? No, no, no. The authors of the study explicitly warn that we cannot conclude\nthat brachytherapy is better at preserving sexual function, since the men who receive this\ntreatment are generally younger and fitter than men who receive the other treatment. The\npurpose of the study was merely to document the degree of sexual side effects across all types\nof treatment. A related source of bias, known as self-selection bias, will arise whenever individuals\nvolunteer to be in a treatment group. For example, prisoners who volunteer for a drug treatment\ngroup are different from other prisoners because they have volunteered to be in a drug\ntreatment program. If the participants in this program are more likely to stay out of prison\nafter release than other prisoners, that’s great---but it tells us absolutely nothing about the\nvalue of the drug treatment program. These former inmates may have changed their lives\nbecause the program helped them kick drugs. Or they may have changed their lives because of\nother factors that also happened to make them more likely to volunteer for a drug treatment\nprogram (such as having a really strong desire not to go back to prison).", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\npopular vote and forty-six of forty-eight states in the electoral college. The Literary Digest\nsample was “garbage in”: the magazine’s subscribers were wealthier than average Americans,\nand therefore more likely to vote Republican, as were households with telephones and cars in\n1936.2\n\nWe can end up with the same basic problem when we compare outcomes between a\ntreatment and a control group if the mechanism for sorting individuals into one group or the\nother is not random. Consider a recent finding in the medical literature on the side effects of\ntreatment for prostate cancer. There are three common treatments for prostate cancer: surgical\nremoval of the prostate; radiation therapy; or brachytherapy (which involves implanting\nradioactive “seeds” near the cancer).3 Impotence is a common side effect of prostate cancer\ntreatment, so researchers have documented the sexual function of men who receive each of the\nthree treatments. A study of 1,000 men found that two years after treatment, 35 percent of the\nmen in the surgery group were able to have sexual intercourse, compared with 37 percent in the\nradiation group and 43 percent in the brachytherapy group. Can one look at these data and assume that brachytherapy is least likely to damage a man’s\nsexual function? No, no, no. The authors of the study explicitly warn that we cannot conclude\nthat brachytherapy is better at preserving sexual function, since the men who receive this\ntreatment are generally younger and fitter than men who receive the other treatment. The\npurpose of the study was merely to document the degree of sexual side effects across all types\nof treatment. A related source of bias, known as self-selection bias, will arise whenever individuals\nvolunteer to be in a treatment group. For example, prisoners who volunteer for a drug treatment\ngroup are different from other prisoners because they have volunteered to be in a drug\ntreatment program. If the participants in this program are more likely to stay out of prison\nafter release than other prisoners, that’s great---but it tells us absolutely nothing about the\nvalue of the drug treatment program. These former inmates may have changed their lives\nbecause the program helped them kick drugs. Or they may have changed their lives because of\nother factors that also happened to make them more likely to volunteer for a drug treatment\nprogram (such as having a really strong desire not to go back to prison).", "tokens": 507, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 91, "segment_id": "00091", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000156"}
{"type": "chunk", "text": "If the participants in this program are more likely to stay out of prison\nafter release than other prisoners, that’s great---but it tells us absolutely nothing about the\nvalue of the drug treatment program. These former inmates may have changed their lives\nbecause the program helped them kick drugs. Or they may have changed their lives because of\nother factors that also happened to make them more likely to volunteer for a drug treatment\nprogram (such as having a really strong desire not to go back to prison). We cannot separate\nthe causal impact of one (the drug treatment program) from the other (being the kind of person\nwho volunteers for a drug treatment program). Publication bias. Positive findings are more likely to be published than negative findings,\nwhich can skew the results that we see. Suppose you have just conducted a rigorous,\nlongitudinal study in which you find conclusively that playing video games does not prevent\ncolon cancer. You’ve followed a representative sample of 100,000 Americans for twenty\nyears; those participants who spend hours playing video games have roughly the same\nincidence of colon cancer as the participants who do not play video games at all. We’ll assume\nyour methodology is impeccable. Which prestigious medical journal is going to publish your\nresults? None, for two reasons. First, there is no strong scientific reason to believe that playing video\ngames has any impact on colon cancer, so it is not obvious why you were doing this study.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nIf the participants in this program are more likely to stay out of prison\nafter release than other prisoners, that’s great---but it tells us absolutely nothing about the\nvalue of the drug treatment program. These former inmates may have changed their lives\nbecause the program helped them kick drugs. Or they may have changed their lives because of\nother factors that also happened to make them more likely to volunteer for a drug treatment\nprogram (such as having a really strong desire not to go back to prison). We cannot separate\nthe causal impact of one (the drug treatment program) from the other (being the kind of person\nwho volunteers for a drug treatment program). Publication bias. Positive findings are more likely to be published than negative findings,\nwhich can skew the results that we see. Suppose you have just conducted a rigorous,\nlongitudinal study in which you find conclusively that playing video games does not prevent\ncolon cancer. You’ve followed a representative sample of 100,000 Americans for twenty\nyears; those participants who spend hours playing video games have roughly the same\nincidence of colon cancer as the participants who do not play video games at all. We’ll assume\nyour methodology is impeccable. Which prestigious medical journal is going to publish your\nresults? None, for two reasons. First, there is no strong scientific reason to believe that playing video\ngames has any impact on colon cancer, so it is not obvious why you were doing this study.", "tokens": 295, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 91, "segment_id": "00091", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000157"}
{"type": "chunk", "text": "Second, and more relevant here, the fact that something does not prevent cancer is not a\nparticularly interesting finding. After all, most things don’t prevent cancer. Negative findings\nare not especially sexy, in medicine or elsewhere. The net effect is to distort the research that we see, or do not see. Suppose that one of your\ngraduate school classmates has conducted a different longitudinal study. She finds that people\nwho spend a lot of time playing video games do have a lower incidence of colon cancer. Now\nthat is interesting! That is exactly the kind of finding that would catch the attention of a\nmedical journal, the popular press, bloggers, and video game makers (who would slap labels\non their products extolling the health benefits of their products). It wouldn’t be long before\nTiger Moms all over the country were “protecting” their children from cancer by snatching\nbooks out of their hands and forcing them to play video games instead. Of course, one important recurring idea in statistics is that unusual things happen every once\nin a while, just as a matter of chance. If you conduct 100 studies, one of them is likely to turn\nup results that are pure nonsense---like a statistical association between playing video games\nand a lower incidence of colon cancer. Here is the problem: The 99 studies that find no link\nbetween video games and colon cancer will not get published, because they are not very\ninteresting. The one study that does find a statistical link will make it into print and get loads\nof follow-on attention. The source of the bias stems not from the studies themselves but from\nthe skewed information that actually reaches the public. Someone reading the scientific\nliterature on video games and cancer would find only a single study, and that single study will\nsuggest that playing video games can prevent cancer. In fact, 99 studies out of 100 would have\nfound no such link. Yes, my example is absurd---but the problem is real and serious.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nSecond, and more relevant here, the fact that something does not prevent cancer is not a\nparticularly interesting finding. After all, most things don’t prevent cancer. Negative findings\nare not especially sexy, in medicine or elsewhere. The net effect is to distort the research that we see, or do not see. Suppose that one of your\ngraduate school classmates has conducted a different longitudinal study. She finds that people\nwho spend a lot of time playing video games do have a lower incidence of colon cancer. Now\nthat is interesting! That is exactly the kind of finding that would catch the attention of a\nmedical journal, the popular press, bloggers, and video game makers (who would slap labels\non their products extolling the health benefits of their products). It wouldn’t be long before\nTiger Moms all over the country were “protecting” their children from cancer by snatching\nbooks out of their hands and forcing them to play video games instead. Of course, one important recurring idea in statistics is that unusual things happen every once\nin a while, just as a matter of chance. If you conduct 100 studies, one of them is likely to turn\nup results that are pure nonsense---like a statistical association between playing video games\nand a lower incidence of colon cancer. Here is the problem: The 99 studies that find no link\nbetween video games and colon cancer will not get published, because they are not very\ninteresting. The one study that does find a statistical link will make it into print and get loads\nof follow-on attention. The source of the bias stems not from the studies themselves but from\nthe skewed information that actually reaches the public. Someone reading the scientific\nliterature on video games and cancer would find only a single study, and that single study will\nsuggest that playing video games can prevent cancer. In fact, 99 studies out of 100 would have\nfound no such link. Yes, my example is absurd---but the problem is real and serious.", "tokens": 409, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 92, "segment_id": "00092", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000158"}
{"type": "chunk", "text": "The one study that does find a statistical link will make it into print and get loads\nof follow-on attention. The source of the bias stems not from the studies themselves but from\nthe skewed information that actually reaches the public. Someone reading the scientific\nliterature on video games and cancer would find only a single study, and that single study will\nsuggest that playing video games can prevent cancer. In fact, 99 studies out of 100 would have\nfound no such link. Yes, my example is absurd---but the problem is real and serious. Here is the first sentence\nof a New York Times article on the publication bias surrounding drugs for treating depression:\n“The makers of antidepressants like Prozac and Paxil never published the results of about a\nthird of the drug trials that they conducted to win government approval, misleading doctors and\nconsumers about the drugs’ true effectiveness.” 4 It turns out that 94 percent of studies with\npositive findings on the effectiveness of these drugs were published, while only 14 percent of\nthe studies with nonpositive results were published. For patients dealing with depression, this\nis a big deal. When all the studies are included, the antidepressants are better than a placebo\nby only “a modest margin.”\n\nTo combat this problem, medical journals now typically require that any study be registered\nat the beginning of the project if it is to be eligible for publication later on. This gives the\neditors some evidence on the ratio of positive to nonpositive findings. If 100 studies are\nregistered that propose to examine the effect of skateboarding on heart disease, and only one is\nultimately submitted for publication with positive findings, the editors can infer that the other\nstudies had nonpositive findings (or they can at least investigate this possibility). Recall bias. Memory is a fascinating thing---though not always a great source of good data. We have a natural human impulse to understand the present as a logical consequence of things\nthat happened in the past---cause and effect. The problem is that our memories turn out to be\n“systematically fragile” when we are trying to explain some particularly good or bad outcome", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThe one study that does find a statistical link will make it into print and get loads\nof follow-on attention. The source of the bias stems not from the studies themselves but from\nthe skewed information that actually reaches the public. Someone reading the scientific\nliterature on video games and cancer would find only a single study, and that single study will\nsuggest that playing video games can prevent cancer. In fact, 99 studies out of 100 would have\nfound no such link. Yes, my example is absurd---but the problem is real and serious. Here is the first sentence\nof a New York Times article on the publication bias surrounding drugs for treating depression:\n“The makers of antidepressants like Prozac and Paxil never published the results of about a\nthird of the drug trials that they conducted to win government approval, misleading doctors and\nconsumers about the drugs’ true effectiveness.” 4 It turns out that 94 percent of studies with\npositive findings on the effectiveness of these drugs were published, while only 14 percent of\nthe studies with nonpositive results were published. For patients dealing with depression, this\nis a big deal. When all the studies are included, the antidepressants are better than a placebo\nby only “a modest margin.”\n\nTo combat this problem, medical journals now typically require that any study be registered\nat the beginning of the project if it is to be eligible for publication later on. This gives the\neditors some evidence on the ratio of positive to nonpositive findings. If 100 studies are\nregistered that propose to examine the effect of skateboarding on heart disease, and only one is\nultimately submitted for publication with positive findings, the editors can infer that the other\nstudies had nonpositive findings (or they can at least investigate this possibility). Recall bias. Memory is a fascinating thing---though not always a great source of good data. We have a natural human impulse to understand the present as a logical consequence of things\nthat happened in the past---cause and effect. The problem is that our memories turn out to be\n“systematically fragile” when we are trying to explain some particularly good or bad outcome", "tokens": 439, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 92, "segment_id": "00092", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000159"}
{"type": "chunk", "text": "in the present. Consider a study looking at the relationship between diet and cancer. In 1993, a\nHarvard researcher compiled a data set comprising a group of women with breast cancer and\nan age-matched group of women who had not been diagnosed with cancer. Women in both\ngroups were asked about their dietary habits earlier in life. The study produced clear results:\nThe women with breast cancer were significantly more likely to have had diets that were high\nin fat when they were younger. Ah, but this wasn’t actually a study of how diet affects the likelihood of getting cancer. This\nwas a study of how getting cancer affects a woman’s memory of her diet earlier in life. All\nof the women in the study had completed a dietary survey years earlier, before any of them had\nbeen diagnosed with cancer. The striking finding was that women with breast cancer recalled a\ndiet that was much higher in fat than what they actually consumed; the women with no cancer\ndid not. The New York Times Magazine described the insidious nature of this recall bias:\n\nThe diagnosis of breast cancer had not just changed a woman’s present and the future; it\nhad altered her past. Women with breast cancer had (unconsciously) decided that a higherfat diet was a likely predisposition for their disease and (unconsciously) recalled a highfat diet. It was a pattern poignantly familiar to anyone who knows the history of this\nstigmatized illness: these women, like thousands of women before them, had searched\ntheir own memories for a cause and then summoned that cause into memory.5\n\nRecall bias is one reason that longitudinal studies are often preferred to cross-sectional\nstudies. In a longitudinal study the data are collected contemporaneously. At age five, a\nparticipant can be asked about his attitudes toward school. Then, thirteen years later, we can\nrevisit that same participant and determine whether he has dropped out of high school. In a\ncross-sectional study, in which all the data are collected at one point in time, we must ask an\neighteen-year-old high school dropout how he or she felt about school at age five, which is\ninherently less reliable. Survivorship bias. Suppose a high school principal reports that test scores for a particular\ncohort of students has risen steadily for four years. The sophomore scores for this class were\nbetter than their freshman scores.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nin the present. Consider a study looking at the relationship between diet and cancer. In 1993, a\nHarvard researcher compiled a data set comprising a group of women with breast cancer and\nan age-matched group of women who had not been diagnosed with cancer. Women in both\ngroups were asked about their dietary habits earlier in life. The study produced clear results:\nThe women with breast cancer were significantly more likely to have had diets that were high\nin fat when they were younger. Ah, but this wasn’t actually a study of how diet affects the likelihood of getting cancer. This\nwas a study of how getting cancer affects a woman’s memory of her diet earlier in life. All\nof the women in the study had completed a dietary survey years earlier, before any of them had\nbeen diagnosed with cancer. The striking finding was that women with breast cancer recalled a\ndiet that was much higher in fat than what they actually consumed; the women with no cancer\ndid not. The New York Times Magazine described the insidious nature of this recall bias:\n\nThe diagnosis of breast cancer had not just changed a woman’s present and the future; it\nhad altered her past. Women with breast cancer had (unconsciously) decided that a higherfat diet was a likely predisposition for their disease and (unconsciously) recalled a highfat diet. It was a pattern poignantly familiar to anyone who knows the history of this\nstigmatized illness: these women, like thousands of women before them, had searched\ntheir own memories for a cause and then summoned that cause into memory.5\n\nRecall bias is one reason that longitudinal studies are often preferred to cross-sectional\nstudies. In a longitudinal study the data are collected contemporaneously. At age five, a\nparticipant can be asked about his attitudes toward school. Then, thirteen years later, we can\nrevisit that same participant and determine whether he has dropped out of high school. In a\ncross-sectional study, in which all the data are collected at one point in time, we must ask an\neighteen-year-old high school dropout how he or she felt about school at age five, which is\ninherently less reliable. Survivorship bias. Suppose a high school principal reports that test scores for a particular\ncohort of students has risen steadily for four years. The sophomore scores for this class were\nbetter than their freshman scores.", "tokens": 492, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 93, "segment_id": "00093", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000160"}
{"type": "chunk", "text": "Then, thirteen years later, we can\nrevisit that same participant and determine whether he has dropped out of high school. In a\ncross-sectional study, in which all the data are collected at one point in time, we must ask an\neighteen-year-old high school dropout how he or she felt about school at age five, which is\ninherently less reliable. Survivorship bias. Suppose a high school principal reports that test scores for a particular\ncohort of students has risen steadily for four years. The sophomore scores for this class were\nbetter than their freshman scores. The scores from junior year were better still, and the senior\nyear scores were best of all. We’ll stipulate that there is no cheating going on, and not even any\ncreative use of descriptive statistics. Every year this cohort of students has done better than it\ndid the preceding year, by every possible measure: mean, median, percentage of students at\ngrade level, and so on. Would you (a) nominate this school leader for “principal of the year” or (b) demand more\n\ndata? I say “b.” I smell survivorship bias, which occurs when some or many of the observations\nare falling out of the sample, changing the composition of the observations that are left and\ntherefore affecting the results of any analysis. Let’s suppose that our principal is truly awful. The students in his school are learning nothing; each year half of them drop out. Well, that\ncould do very nice things for the school’s test scores---without any individual student testing", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThen, thirteen years later, we can\nrevisit that same participant and determine whether he has dropped out of high school. In a\ncross-sectional study, in which all the data are collected at one point in time, we must ask an\neighteen-year-old high school dropout how he or she felt about school at age five, which is\ninherently less reliable. Survivorship bias. Suppose a high school principal reports that test scores for a particular\ncohort of students has risen steadily for four years. The sophomore scores for this class were\nbetter than their freshman scores. The scores from junior year were better still, and the senior\nyear scores were best of all. We’ll stipulate that there is no cheating going on, and not even any\ncreative use of descriptive statistics. Every year this cohort of students has done better than it\ndid the preceding year, by every possible measure: mean, median, percentage of students at\ngrade level, and so on. Would you (a) nominate this school leader for “principal of the year” or (b) demand more\n\ndata? I say “b.” I smell survivorship bias, which occurs when some or many of the observations\nare falling out of the sample, changing the composition of the observations that are left and\ntherefore affecting the results of any analysis. Let’s suppose that our principal is truly awful. The students in his school are learning nothing; each year half of them drop out. Well, that\ncould do very nice things for the school’s test scores---without any individual student testing", "tokens": 321, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 93, "segment_id": "00093", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000161"}
{"type": "chunk", "text": "better. If we make the reasonable assumption that the worst students (with the lowest test\nscores) are the most likely to drop out, then the average test scores of those students left\nbehind will go up steadily as more and more students drop out. (If you have a room of people\nwith varying heights, forcing the short people to leave will raise the average height in the room,\nbut it doesn’t make anyone taller.)\n\nThe mutual fund industry has aggressively (and insidiously) seized on survivorship bias to\nmake its returns look better to investors than they really are. Mutual funds typically gauge\ntheir performance against a key benchmark for stocks, the Standard & Poor’s 500, which is an\nindex of 500 leading public companies in America.* If the S&P 500 is up 5.3 percent for the\nyear, a mutual fund is said to beat the index if it performs better than that, or trail the index if it\ndoes worse. One cheap and easy option for investors who don’t want to pay a mutual fund\nmanager is to buy an S&P 500 Index Fund, which is a mutual fund that simply buys shares in\nall 500 stocks in the index. Mutual fund managers like to believe that they are savvy investors,\ncapable of using their knowledge to pick stocks that will perform better than a simple index\nfund. In fact, it turns out to be relatively hard to beat the S&P 500 for any consistent stretch of\ntime. (The S&P 500 is essentially an average of all large stocks being traded, so just as a\nmatter of math we would expect roughly half the actively managed mutual funds to outperform\nthe S&P 500 in a given year and half to underperform.) Of course, it doesn’t look very good to\nlose to a mindless index that simply buys 500 stocks and holds them. No analysis. No fancy\nmacro forecasting. And, much to the delight of investors, no high management fees. What is a traditional mutual fund company to do? Bogus data to the rescue! Here is how\nthey can “beat the market” without beating the market. A large mutual company will open\nmany new actively managed funds (meaning that experts are picking the stocks, often with a\nparticular focus or strategy).", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nbetter. If we make the reasonable assumption that the worst students (with the lowest test\nscores) are the most likely to drop out, then the average test scores of those students left\nbehind will go up steadily as more and more students drop out. (If you have a room of people\nwith varying heights, forcing the short people to leave will raise the average height in the room,\nbut it doesn’t make anyone taller.)\n\nThe mutual fund industry has aggressively (and insidiously) seized on survivorship bias to\nmake its returns look better to investors than they really are. Mutual funds typically gauge\ntheir performance against a key benchmark for stocks, the Standard & Poor’s 500, which is an\nindex of 500 leading public companies in America.* If the S&P 500 is up 5.3 percent for the\nyear, a mutual fund is said to beat the index if it performs better than that, or trail the index if it\ndoes worse. One cheap and easy option for investors who don’t want to pay a mutual fund\nmanager is to buy an S&P 500 Index Fund, which is a mutual fund that simply buys shares in\nall 500 stocks in the index. Mutual fund managers like to believe that they are savvy investors,\ncapable of using their knowledge to pick stocks that will perform better than a simple index\nfund. In fact, it turns out to be relatively hard to beat the S&P 500 for any consistent stretch of\ntime. (The S&P 500 is essentially an average of all large stocks being traded, so just as a\nmatter of math we would expect roughly half the actively managed mutual funds to outperform\nthe S&P 500 in a given year and half to underperform.) Of course, it doesn’t look very good to\nlose to a mindless index that simply buys 500 stocks and holds them. No analysis. No fancy\nmacro forecasting. And, much to the delight of investors, no high management fees. What is a traditional mutual fund company to do? Bogus data to the rescue! Here is how\nthey can “beat the market” without beating the market. A large mutual company will open\nmany new actively managed funds (meaning that experts are picking the stocks, often with a\nparticular focus or strategy).", "tokens": 472, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 94, "segment_id": "00094", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000162"}
{"type": "chunk", "text": "No analysis. No fancy\nmacro forecasting. And, much to the delight of investors, no high management fees. What is a traditional mutual fund company to do? Bogus data to the rescue! Here is how\nthey can “beat the market” without beating the market. A large mutual company will open\nmany new actively managed funds (meaning that experts are picking the stocks, often with a\nparticular focus or strategy). For the sake of example, let’s assume that a mutual fund\ncompany opens twenty new funds, each of which has roughly a 50 percent chance of beating\nthe S&P 500 in a given year. (This assumption is consistent with long-term data.) Now, basic\nprobability suggests that only ten of the firm’s new funds will beat the S&P 500 the first year;\nfive funds will beat it two years in a row; and two or three will beat it three years in a row. Here comes the clever part. At that point, the new mutual funds with unimpressive returns\nrelative to the S&P 500 are quietly closed. (Their assets are folded into other existing funds.)\nThe company can then heavily advertise the two or three new funds that have “consistently\noutperformed the S&P 500”---even if that performance is the stock-picking equivalent of\nflipping three heads in a row. The subsequent performance of these funds is likely to revert to\nthe mean, albeit after investors have piled in. The number of mutual funds or investment gurus\nwho have consistently beaten the S&P 500 over a long period is shockingly small.*\n\nHealthy user bias. People who take vitamins regularly are likely to be healthy---because they\nare the kind of people who take vitamins regularly! Whether the vitamins have any impact is a\nseparate issue. Consider the following thought experiment. Suppose public health officials\npromulgate a theory that all new parents should put their children to bed only in purple\npajamas, because that helps stimulate brain development. Twenty years later, longitudinal\nresearch confirms that having worn purple pajamas as a child does have an overwhelmingly", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nNo analysis. No fancy\nmacro forecasting. And, much to the delight of investors, no high management fees. What is a traditional mutual fund company to do? Bogus data to the rescue! Here is how\nthey can “beat the market” without beating the market. A large mutual company will open\nmany new actively managed funds (meaning that experts are picking the stocks, often with a\nparticular focus or strategy). For the sake of example, let’s assume that a mutual fund\ncompany opens twenty new funds, each of which has roughly a 50 percent chance of beating\nthe S&P 500 in a given year. (This assumption is consistent with long-term data.) Now, basic\nprobability suggests that only ten of the firm’s new funds will beat the S&P 500 the first year;\nfive funds will beat it two years in a row; and two or three will beat it three years in a row. Here comes the clever part. At that point, the new mutual funds with unimpressive returns\nrelative to the S&P 500 are quietly closed. (Their assets are folded into other existing funds.)\nThe company can then heavily advertise the two or three new funds that have “consistently\noutperformed the S&P 500”---even if that performance is the stock-picking equivalent of\nflipping three heads in a row. The subsequent performance of these funds is likely to revert to\nthe mean, albeit after investors have piled in. The number of mutual funds or investment gurus\nwho have consistently beaten the S&P 500 over a long period is shockingly small.*\n\nHealthy user bias. People who take vitamins regularly are likely to be healthy---because they\nare the kind of people who take vitamins regularly! Whether the vitamins have any impact is a\nseparate issue. Consider the following thought experiment. Suppose public health officials\npromulgate a theory that all new parents should put their children to bed only in purple\npajamas, because that helps stimulate brain development. Twenty years later, longitudinal\nresearch confirms that having worn purple pajamas as a child does have an overwhelmingly", "tokens": 433, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 94, "segment_id": "00094", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000163"}
{"type": "chunk", "text": "large positive association with success in life. We find, for example, that 98 percent of entering\nHarvard freshmen wore purple pajamas as children (and many still do) compared with only 3\npercent of inmates in the Massachusetts state prison system. Of course, the purple pajamas do not matter; but having the kind of parents who put their\nchildren in purple pajamas does matter. Even when we try to control for factors like parental\neducation, we are still going to be left with unobservable differences between those parents\nwho obsess about putting their children in purple pajamas and those who don’t. As New York\nTimes health writer Gary Taubes explains, “At its simplest, the problem is that people who\nfaithfully engage in activities that are good for them---taking a drug as prescribed, for instance,\nor eating what they believe is a healthy diet---are fundamentally different from those who\ndon’t.”6 This effect can potentially confound any study trying to evaluate the real effect of\nactivities perceived to be healthful, such as exercising regularly or eating kale. We think we\nare comparing the health effects of two diets: kale versus no kale. In fact, if the treatment and\ncontrol groups are not randomly assigned, we are comparing two diets that are being eaten by\ntwo different kinds of people. We have a treatment group that is different from the control\ngroup in two respects, rather than just one. If statistics is detective work, then the data are the clues. My wife spent a year teaching high\nschool students in rural New Hampshire. One of her students was arrested for breaking into a\nhardware store and stealing some tools. The police were able to crack the case because (1) it\nhad just snowed and there were tracks in the snow leading from the hardware store to the\nstudent’s home; and (2) the stolen tools were found inside. Good clues help. Like good data. But first you have to get good data, and that is a lot harder than it seems. * At the time, the disease had a mean duration of forty-three days with a standard deviation of twenty-four days. * The S&P 500 is a nice example of what an index can and should do. The index is made up of the share prices of the 500\nleading U.S. companies, each weighted by its market value (so that bigger companies have more weight in the index than\nsmaller companies).", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nlarge positive association with success in life. We find, for example, that 98 percent of entering\nHarvard freshmen wore purple pajamas as children (and many still do) compared with only 3\npercent of inmates in the Massachusetts state prison system. Of course, the purple pajamas do not matter; but having the kind of parents who put their\nchildren in purple pajamas does matter. Even when we try to control for factors like parental\neducation, we are still going to be left with unobservable differences between those parents\nwho obsess about putting their children in purple pajamas and those who don’t. As New York\nTimes health writer Gary Taubes explains, “At its simplest, the problem is that people who\nfaithfully engage in activities that are good for them---taking a drug as prescribed, for instance,\nor eating what they believe is a healthy diet---are fundamentally different from those who\ndon’t.”6 This effect can potentially confound any study trying to evaluate the real effect of\nactivities perceived to be healthful, such as exercising regularly or eating kale. We think we\nare comparing the health effects of two diets: kale versus no kale. In fact, if the treatment and\ncontrol groups are not randomly assigned, we are comparing two diets that are being eaten by\ntwo different kinds of people. We have a treatment group that is different from the control\ngroup in two respects, rather than just one. If statistics is detective work, then the data are the clues. My wife spent a year teaching high\nschool students in rural New Hampshire. One of her students was arrested for breaking into a\nhardware store and stealing some tools. The police were able to crack the case because (1) it\nhad just snowed and there were tracks in the snow leading from the hardware store to the\nstudent’s home; and (2) the stolen tools were found inside. Good clues help. Like good data. But first you have to get good data, and that is a lot harder than it seems. * At the time, the disease had a mean duration of forty-three days with a standard deviation of twenty-four days. * The S&P 500 is a nice example of what an index can and should do. The index is made up of the share prices of the 500\nleading U.S. companies, each weighted by its market value (so that bigger companies have more weight in the index than\nsmaller companies).", "tokens": 500, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 95, "segment_id": "00095", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000164"}
{"type": "chunk", "text": "Good clues help. Like good data. But first you have to get good data, and that is a lot harder than it seems. * At the time, the disease had a mean duration of forty-three days with a standard deviation of twenty-four days. * The S&P 500 is a nice example of what an index can and should do. The index is made up of the share prices of the 500\nleading U.S. companies, each weighted by its market value (so that bigger companies have more weight in the index than\nsmaller companies). The index is a simple and accurate gauge of what is happening to the share prices of the largest\nAmerican companies at any given time. * For a very nice discussion of why you should probably buy index funds rather than trying to beat the market, read A\nRandom Walk Down Wall Street, by my former professor Burton Malkiel.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nGood clues help. Like good data. But first you have to get good data, and that is a lot harder than it seems. * At the time, the disease had a mean duration of forty-three days with a standard deviation of twenty-four days. * The S&P 500 is a nice example of what an index can and should do. The index is made up of the share prices of the 500\nleading U.S. companies, each weighted by its market value (so that bigger companies have more weight in the index than\nsmaller companies). The index is a simple and accurate gauge of what is happening to the share prices of the largest\nAmerican companies at any given time. * For a very nice discussion of why you should probably buy index funds rather than trying to beat the market, read A\nRandom Walk Down Wall Street, by my former professor Burton Malkiel.", "tokens": 179, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 95, "segment_id": "00095", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000165"}
{"type": "chunk", "text": "CHAPTER 8\nThe Central Limit Theorem\nThe Lebron James of statistics\n\nAt times, statistics seems almost like magic. We are able to draw sweeping and powerful\n\nconclusions from relatively little data. Somehow we can gain meaningful insight into a\npresidential election by calling a mere one thousand American voters. We can test a hundred\nchicken breasts for salmonella at a poultry processing plant and conclude from that sample\nalone that the entire plant is safe or unsafe. Where does this extraordinary power to\ngeneralize come from? Much of it comes from the central limit theorem, which is the Lebron James of statistics---if\nLebron were also a supermodel, a Harvard professor, and the winner of the Nobel Peace Prize. The central limit theorem is the “power source” for many of the statistical activities that\ninvolve using a sample to make inferences about a large population (like a poll, or a test for\nsalmonella). These kinds of inferences may seem mystical; in fact, they are just a combination\nof two tools that we’ve already explored: probability and proper sampling. Before plunging\ninto the mechanics of the central limit theorem (which aren’t all that tricky), here is an example\nto give you the general intuition. Suppose you live in a city that is hosting a marathon. Runners from all over the world will\nbe competing, which means that many of them do not speak English. The logistics of the race\nrequire that runners check in on the morning of the race, after which they are randomly assigned\nto buses to take them to the starting line. Unfortunately one of the buses gets lost on the way to\nthe race. (Okay, you’re going to have to assume that no one has a cell phone and that the driver\ndoes not have a GPS navigation device; unless you want to do a lot of unpleasant math right\nnow, just go with it.) As a civic leader in this city, you join the search team. As luck would have it, you stumble upon a broken-down bus near your home with a large\ngroup of unhappy international passengers, none of whom speaks English. This must be the\nmissing bus! You’re going to be a hero! Except you have one lingering doubt . . . the\npassengers on this bus are, well, very large. Based on a quick glance, you reckon that the\naverage weight for this group of passengers has got to be over 220 pounds.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nCHAPTER 8\nThe Central Limit Theorem\nThe Lebron James of statistics\n\nAt times, statistics seems almost like magic. We are able to draw sweeping and powerful\n\nconclusions from relatively little data. Somehow we can gain meaningful insight into a\npresidential election by calling a mere one thousand American voters. We can test a hundred\nchicken breasts for salmonella at a poultry processing plant and conclude from that sample\nalone that the entire plant is safe or unsafe. Where does this extraordinary power to\ngeneralize come from? Much of it comes from the central limit theorem, which is the Lebron James of statistics---if\nLebron were also a supermodel, a Harvard professor, and the winner of the Nobel Peace Prize. The central limit theorem is the “power source” for many of the statistical activities that\ninvolve using a sample to make inferences about a large population (like a poll, or a test for\nsalmonella). These kinds of inferences may seem mystical; in fact, they are just a combination\nof two tools that we’ve already explored: probability and proper sampling. Before plunging\ninto the mechanics of the central limit theorem (which aren’t all that tricky), here is an example\nto give you the general intuition. Suppose you live in a city that is hosting a marathon. Runners from all over the world will\nbe competing, which means that many of them do not speak English. The logistics of the race\nrequire that runners check in on the morning of the race, after which they are randomly assigned\nto buses to take them to the starting line. Unfortunately one of the buses gets lost on the way to\nthe race. (Okay, you’re going to have to assume that no one has a cell phone and that the driver\ndoes not have a GPS navigation device; unless you want to do a lot of unpleasant math right\nnow, just go with it.) As a civic leader in this city, you join the search team. As luck would have it, you stumble upon a broken-down bus near your home with a large\ngroup of unhappy international passengers, none of whom speaks English. This must be the\nmissing bus! You’re going to be a hero! Except you have one lingering doubt . . . the\npassengers on this bus are, well, very large. Based on a quick glance, you reckon that the\naverage weight for this group of passengers has got to be over 220 pounds.", "tokens": 505, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 96, "segment_id": "00096", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000166"}
{"type": "chunk", "text": "As luck would have it, you stumble upon a broken-down bus near your home with a large\ngroup of unhappy international passengers, none of whom speaks English. This must be the\nmissing bus! You’re going to be a hero! Except you have one lingering doubt . . . the\npassengers on this bus are, well, very large. Based on a quick glance, you reckon that the\naverage weight for this group of passengers has got to be over 220 pounds. There is no way\nthat a random group of marathon runners could all be this heavy. You radio your message to\nsearch headquarters: “I think it’s the wrong bus. Keep looking.”\n\nFurther analysis confirms your initial impression. When a translator arrives, you discover\nthat this disabled bus was headed to the International Festival of Sausage, which is also being\nhosted by your city on the same weekend. (For the sake of verisimilitude, it is entirely possible\nthat sausage festival participants might also be wearing sweat pants.)\n\nCongratulations. If you can grasp how someone who takes a quick look at the weights of\npassengers on a bus can infer that they are probably not on their way to the starting line of a", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nAs luck would have it, you stumble upon a broken-down bus near your home with a large\ngroup of unhappy international passengers, none of whom speaks English. This must be the\nmissing bus! You’re going to be a hero! Except you have one lingering doubt . . . the\npassengers on this bus are, well, very large. Based on a quick glance, you reckon that the\naverage weight for this group of passengers has got to be over 220 pounds. There is no way\nthat a random group of marathon runners could all be this heavy. You radio your message to\nsearch headquarters: “I think it’s the wrong bus. Keep looking.”\n\nFurther analysis confirms your initial impression. When a translator arrives, you discover\nthat this disabled bus was headed to the International Festival of Sausage, which is also being\nhosted by your city on the same weekend. (For the sake of verisimilitude, it is entirely possible\nthat sausage festival participants might also be wearing sweat pants.)\n\nCongratulations. If you can grasp how someone who takes a quick look at the weights of\npassengers on a bus can infer that they are probably not on their way to the starting line of a", "tokens": 248, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 96, "segment_id": "00096", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000167"}
{"type": "chunk", "text": "marathon, then you now understand the basic idea of the central limit theorem. The rest is just\nfleshing out the details. And if you understand the central limit theorem, most forms of\nstatistical inference will seem relatively intuitive. The core principle underlying the central limit theorem is that a large, properly drawn sample\nwill resemble the population from which it is drawn. Obviously there will be variation from\nsample to sample (e.g., each bus headed to the start of the marathon will have a slightly\ndifferent mix of passengers), but the probability that any sample will deviate massively from\nthe underlying population is very low. This logic is what enabled your snap judgment when\nyou boarded the broken-down bus and saw the average girth of the passengers on board. Lots\nof big people run marathons; there are likely to be hundreds of people who weigh over 200\npounds in any given race. But the majority of marathon runners are relatively thin. Thus, the\nlikelihood that so many of the largest runners were randomly assigned to the same bus is very,\nvery low. You could conclude with a reasonable degree of confidence that this was not the\nmissing marathon bus. Yes, you could have been wrong, but probability tells us that most of\nthe time you would have been right. That’s the basic intuition behind the central limit theorem. When we add some statistical\nbells and whistles, we can quantify the likelihood that you will be right or wrong. For example,\nwe might calculate that in a marathon field of 10,000 runners with a mean weight of 155\npounds, there is less than a 1 in 100 chance that a random sample of 60 of those runners (our\nlost bus) would have a mean weight of 220 pounds or more. For now, let’s stick with the\nintuition; there will be plenty of time for calculations later. The central limit theorem enables\nus to make the following inferences, all of which will be explored in greater depth in the next\nchapter. 1. If we have detailed information about some population, then we can make powerful\ninferences about any properly drawn sample from that population. For example, assume\nthat a school principal has detailed information on the standardized test scores for all the\nstudents in his school (mean, standard deviation, etc.). That is the relevant population.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nmarathon, then you now understand the basic idea of the central limit theorem. The rest is just\nfleshing out the details. And if you understand the central limit theorem, most forms of\nstatistical inference will seem relatively intuitive. The core principle underlying the central limit theorem is that a large, properly drawn sample\nwill resemble the population from which it is drawn. Obviously there will be variation from\nsample to sample (e.g., each bus headed to the start of the marathon will have a slightly\ndifferent mix of passengers), but the probability that any sample will deviate massively from\nthe underlying population is very low. This logic is what enabled your snap judgment when\nyou boarded the broken-down bus and saw the average girth of the passengers on board. Lots\nof big people run marathons; there are likely to be hundreds of people who weigh over 200\npounds in any given race. But the majority of marathon runners are relatively thin. Thus, the\nlikelihood that so many of the largest runners were randomly assigned to the same bus is very,\nvery low. You could conclude with a reasonable degree of confidence that this was not the\nmissing marathon bus. Yes, you could have been wrong, but probability tells us that most of\nthe time you would have been right. That’s the basic intuition behind the central limit theorem. When we add some statistical\nbells and whistles, we can quantify the likelihood that you will be right or wrong. For example,\nwe might calculate that in a marathon field of 10,000 runners with a mean weight of 155\npounds, there is less than a 1 in 100 chance that a random sample of 60 of those runners (our\nlost bus) would have a mean weight of 220 pounds or more. For now, let’s stick with the\nintuition; there will be plenty of time for calculations later. The central limit theorem enables\nus to make the following inferences, all of which will be explored in greater depth in the next\nchapter. 1. If we have detailed information about some population, then we can make powerful\ninferences about any properly drawn sample from that population. For example, assume\nthat a school principal has detailed information on the standardized test scores for all the\nstudents in his school (mean, standard deviation, etc.). That is the relevant population.", "tokens": 487, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 97, "segment_id": "00097", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000168"}
{"type": "chunk", "text": "For now, let’s stick with the\nintuition; there will be plenty of time for calculations later. The central limit theorem enables\nus to make the following inferences, all of which will be explored in greater depth in the next\nchapter. 1. If we have detailed information about some population, then we can make powerful\ninferences about any properly drawn sample from that population. For example, assume\nthat a school principal has detailed information on the standardized test scores for all the\nstudents in his school (mean, standard deviation, etc.). That is the relevant population. Now assume that a bureaucrat from the school district will be arriving next week to give\na similar standardized test to 100 randomly selected students. The performance of those\n100 students, the sample, will be used to evaluate the performance of the school overall. How much confidence can the principal have that the performance of those randomly\nchosen 100 students will accurately reflect how the entire student body has been\nperforming on similar standardized tests? Quite a bit. According to the central limit\ntheorem, the average test score for the random sample of 100 students will not typically\ndeviate sharply from the average test score for the whole school. 2. If we have detailed information about a properly drawn sample (mean and standard\ndeviation), we can make strikingly accurate inferences about the population from which\nthat sample was drawn. This is essentially working in the opposite direction from the\nexample above, putting ourselves in the shoes of the school district bureaucrat who is\nevaluating various schools in the district. Unlike the school principal, this bureaucrat\ndoes not have (or does not trust) the standardized test score data that the principal has", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nFor now, let’s stick with the\nintuition; there will be plenty of time for calculations later. The central limit theorem enables\nus to make the following inferences, all of which will be explored in greater depth in the next\nchapter. 1. If we have detailed information about some population, then we can make powerful\ninferences about any properly drawn sample from that population. For example, assume\nthat a school principal has detailed information on the standardized test scores for all the\nstudents in his school (mean, standard deviation, etc.). That is the relevant population. Now assume that a bureaucrat from the school district will be arriving next week to give\na similar standardized test to 100 randomly selected students. The performance of those\n100 students, the sample, will be used to evaluate the performance of the school overall. How much confidence can the principal have that the performance of those randomly\nchosen 100 students will accurately reflect how the entire student body has been\nperforming on similar standardized tests? Quite a bit. According to the central limit\ntheorem, the average test score for the random sample of 100 students will not typically\ndeviate sharply from the average test score for the whole school. 2. If we have detailed information about a properly drawn sample (mean and standard\ndeviation), we can make strikingly accurate inferences about the population from which\nthat sample was drawn. This is essentially working in the opposite direction from the\nexample above, putting ourselves in the shoes of the school district bureaucrat who is\nevaluating various schools in the district. Unlike the school principal, this bureaucrat\ndoes not have (or does not trust) the standardized test score data that the principal has", "tokens": 351, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 97, "segment_id": "00097", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000169"}
{"type": "chunk", "text": "for all the students in a particular school, which is the relevant population. Instead, he\nwill be administering a similar test of his own to a random sample of 100 students in\neach school. Can this administrator be reasonably certain that the overall performance of any given\nschool can be evaluated fairly based on the test scores of a sample of just 100 students\nfrom that school? Yes. The central limit theorem tells us that a large sample will not\ntypically deviate sharply from its underlying population---which means that the sample\nresults (scores for the 100 randomly chosen students) are a good proxy for the results of\nthe population overall (the student body at a particular school). Of course, this is how\npolling works. A methodologically sound poll of 1,200 Americans can tell us a great deal\nabout how the entire country is thinking. Think about it: if no. 1 above is true, no. 2 must also be true---and vice versa. If a\nsample usually looks like the population from which it’s drawn, it must also be true that a\npopulation will usually look like a sample drawn from that population. (If children\ntypically look like their parents, parents must also typically look like their children.)\n3. If we have data describing a particular sample, and data on a particular population, we\ncan infer whether or not that sample is consistent with a sample that is likely to be drawn\nfrom that population. This is the missing-bus example described at the beginning of the\nchapter. We know the mean weight (more or less) for the participants in the marathon. And we know the mean weight (more or less) for the passengers on the broken-down bus. The central limit theorem enables us to calculate the probability that a particular sample\n(the rotund people on the bus) was drawn from a given population (the marathon field). If\nthat probability is low, then we can conclude with a high degree of confidence that the\nsample was not drawn from the population in question (e.g., the people on this bus really\ndon’t look like a group of marathon runners headed to the starting line). 4. Last, if we know the underlying characteristics of two samples, we can infer whether or\nnot both samples were likely drawn from the same population. Let us return to our\n(increasingly absurd) bus example. We now know that a marathon is going on in the city,\nas well as the International Festival of Sausage.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nfor all the students in a particular school, which is the relevant population. Instead, he\nwill be administering a similar test of his own to a random sample of 100 students in\neach school. Can this administrator be reasonably certain that the overall performance of any given\nschool can be evaluated fairly based on the test scores of a sample of just 100 students\nfrom that school? Yes. The central limit theorem tells us that a large sample will not\ntypically deviate sharply from its underlying population---which means that the sample\nresults (scores for the 100 randomly chosen students) are a good proxy for the results of\nthe population overall (the student body at a particular school). Of course, this is how\npolling works. A methodologically sound poll of 1,200 Americans can tell us a great deal\nabout how the entire country is thinking. Think about it: if no. 1 above is true, no. 2 must also be true---and vice versa. If a\nsample usually looks like the population from which it’s drawn, it must also be true that a\npopulation will usually look like a sample drawn from that population. (If children\ntypically look like their parents, parents must also typically look like their children.)\n3. If we have data describing a particular sample, and data on a particular population, we\ncan infer whether or not that sample is consistent with a sample that is likely to be drawn\nfrom that population. This is the missing-bus example described at the beginning of the\nchapter. We know the mean weight (more or less) for the participants in the marathon. And we know the mean weight (more or less) for the passengers on the broken-down bus. The central limit theorem enables us to calculate the probability that a particular sample\n(the rotund people on the bus) was drawn from a given population (the marathon field). If\nthat probability is low, then we can conclude with a high degree of confidence that the\nsample was not drawn from the population in question (e.g., the people on this bus really\ndon’t look like a group of marathon runners headed to the starting line). 4. Last, if we know the underlying characteristics of two samples, we can infer whether or\nnot both samples were likely drawn from the same population. Let us return to our\n(increasingly absurd) bus example. We now know that a marathon is going on in the city,\nas well as the International Festival of Sausage.", "tokens": 512, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 98, "segment_id": "00098", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000170"}
{"type": "chunk", "text": "If\nthat probability is low, then we can conclude with a high degree of confidence that the\nsample was not drawn from the population in question (e.g., the people on this bus really\ndon’t look like a group of marathon runners headed to the starting line). 4. Last, if we know the underlying characteristics of two samples, we can infer whether or\nnot both samples were likely drawn from the same population. Let us return to our\n(increasingly absurd) bus example. We now know that a marathon is going on in the city,\nas well as the International Festival of Sausage. Assume that both groups have thousands\nof participants, and that both groups are operating buses, all loaded with random samples\nof either marathon runners or sausage enthusiasts. Further assume that two buses collide. (I already conceded that the example is absurd, so just read on.) In your capacity as a\ncivic leader, you arrive on the scene and are tasked with determining whether or not both\nbuses were headed to the same event (sausage festival or marathon). Miraculously, no\none on either bus speaks English, but paramedics provide you with detailed information\non the weights of all the passengers on each bus. From that alone, you can infer whether the two buses were likely headed to the same\nevent, or to different events. Again, think about this intuitively. Suppose that the average\nweight of the passengers on one bus is 157 pounds, with a standard deviation of 11\npounds (meaning that a high proportion of the passengers weigh between 146 pounds and\n168 pounds). Now suppose that the passengers on the second bus have a mean weight of", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nIf\nthat probability is low, then we can conclude with a high degree of confidence that the\nsample was not drawn from the population in question (e.g., the people on this bus really\ndon’t look like a group of marathon runners headed to the starting line). 4. Last, if we know the underlying characteristics of two samples, we can infer whether or\nnot both samples were likely drawn from the same population. Let us return to our\n(increasingly absurd) bus example. We now know that a marathon is going on in the city,\nas well as the International Festival of Sausage. Assume that both groups have thousands\nof participants, and that both groups are operating buses, all loaded with random samples\nof either marathon runners or sausage enthusiasts. Further assume that two buses collide. (I already conceded that the example is absurd, so just read on.) In your capacity as a\ncivic leader, you arrive on the scene and are tasked with determining whether or not both\nbuses were headed to the same event (sausage festival or marathon). Miraculously, no\none on either bus speaks English, but paramedics provide you with detailed information\non the weights of all the passengers on each bus. From that alone, you can infer whether the two buses were likely headed to the same\nevent, or to different events. Again, think about this intuitively. Suppose that the average\nweight of the passengers on one bus is 157 pounds, with a standard deviation of 11\npounds (meaning that a high proportion of the passengers weigh between 146 pounds and\n168 pounds). Now suppose that the passengers on the second bus have a mean weight of", "tokens": 345, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 98, "segment_id": "00098", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000171"}
{"type": "chunk", "text": "211 pounds with a standard deviation of 21 pounds (meaning that a high proportion of the\npassengers weigh between 190 pounds and 232 pounds). Forget statistical formulas for a\nmoment, and just use logic: Does it seem likely that the passengers on those two buses\nwere randomly drawn from the same population? No. It seems far more likely that one bus is full of marathon runners and the other bus is\nfull of sausage enthusiasts. In addition to the difference in average weight between the two\nbuses, you can also see that the variation in weights between the two buses is very large\ncompared with the variation in weights within each bus. The folks who weigh one standard\ndeviation above the mean on the “skinny” bus are 168 pounds, which is less than the folks\nwho are one standard deviation below the mean on the “other” bus (190 pounds). This is a\ntelltale sign (both statistically and logically) that the two samples likely came from\ndifferent populations. If all of this makes intuitive sense, then you are 93.2 percent of the way to understanding the\ncentral limit theorem.* We need to go one step further to put some technical heft behind the\nintuition. Obviously when you stuck your head inside the broken-down bus and saw a group of\nlarge people in sweatpants, you had a “hunch” that they weren’t marathoners. The central limit\ntheorem allows us to go beyond that hunch and assign a degree of confidence to your\nconclusion. For example, some basic calculations will enable me to conclude that 99 times out of 100\nthe mean weight of any randomly selected bus of marathoners will be within nine pounds of the\nmean weight of the entire marathon field. That’s what gives statistical heft to my hunch when I\nstumble across the broken-down bus. These passengers have a mean weight that is twenty-one\npounds higher than the mean weight for the marathon field, something that should only occur by\nchance less than 1 time in 100. As a result, I can reject the hypothesis that this is a missing\nmarathon bus with 99 percent confidence---meaning I should expect my inference to be correct\n99 times out of 100. And yes, probability suggests that on average I’ll be wrong 1 time in 100.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\n211 pounds with a standard deviation of 21 pounds (meaning that a high proportion of the\npassengers weigh between 190 pounds and 232 pounds). Forget statistical formulas for a\nmoment, and just use logic: Does it seem likely that the passengers on those two buses\nwere randomly drawn from the same population? No. It seems far more likely that one bus is full of marathon runners and the other bus is\nfull of sausage enthusiasts. In addition to the difference in average weight between the two\nbuses, you can also see that the variation in weights between the two buses is very large\ncompared with the variation in weights within each bus. The folks who weigh one standard\ndeviation above the mean on the “skinny” bus are 168 pounds, which is less than the folks\nwho are one standard deviation below the mean on the “other” bus (190 pounds). This is a\ntelltale sign (both statistically and logically) that the two samples likely came from\ndifferent populations. If all of this makes intuitive sense, then you are 93.2 percent of the way to understanding the\ncentral limit theorem.* We need to go one step further to put some technical heft behind the\nintuition. Obviously when you stuck your head inside the broken-down bus and saw a group of\nlarge people in sweatpants, you had a “hunch” that they weren’t marathoners. The central limit\ntheorem allows us to go beyond that hunch and assign a degree of confidence to your\nconclusion. For example, some basic calculations will enable me to conclude that 99 times out of 100\nthe mean weight of any randomly selected bus of marathoners will be within nine pounds of the\nmean weight of the entire marathon field. That’s what gives statistical heft to my hunch when I\nstumble across the broken-down bus. These passengers have a mean weight that is twenty-one\npounds higher than the mean weight for the marathon field, something that should only occur by\nchance less than 1 time in 100. As a result, I can reject the hypothesis that this is a missing\nmarathon bus with 99 percent confidence---meaning I should expect my inference to be correct\n99 times out of 100. And yes, probability suggests that on average I’ll be wrong 1 time in 100.", "tokens": 485, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 99, "segment_id": "00099", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000172"}
{"type": "chunk", "text": "That’s what gives statistical heft to my hunch when I\nstumble across the broken-down bus. These passengers have a mean weight that is twenty-one\npounds higher than the mean weight for the marathon field, something that should only occur by\nchance less than 1 time in 100. As a result, I can reject the hypothesis that this is a missing\nmarathon bus with 99 percent confidence---meaning I should expect my inference to be correct\n99 times out of 100. And yes, probability suggests that on average I’ll be wrong 1 time in 100. This kind of analysis all stems from the central limit theorem, which, from a statistical\nstandpoint, has Lebron James--like power and elegance. According to the central limit theorem,\nthe sample means for any population will be distributed roughly as a normal distribution\naround the population mean. Hang on for a moment as we unpack that statement. 1. Suppose we have a population, like our marathon field, and we are interested in the\nweights of its members. Any sample of runners, such as each bus of sixty runners, will\nhave a mean. 2. If we take repeated samples, such as picking random groups of sixty runners from the\nfield over and over, then each of those samples will have its own mean weight. These are\nthe sample means. 3. Most of the sample means will be very close to the population mean. Some will be a\nlittle higher. Some will be a little lower. Just as a matter of chance, a very few will be", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThat’s what gives statistical heft to my hunch when I\nstumble across the broken-down bus. These passengers have a mean weight that is twenty-one\npounds higher than the mean weight for the marathon field, something that should only occur by\nchance less than 1 time in 100. As a result, I can reject the hypothesis that this is a missing\nmarathon bus with 99 percent confidence---meaning I should expect my inference to be correct\n99 times out of 100. And yes, probability suggests that on average I’ll be wrong 1 time in 100. This kind of analysis all stems from the central limit theorem, which, from a statistical\nstandpoint, has Lebron James--like power and elegance. According to the central limit theorem,\nthe sample means for any population will be distributed roughly as a normal distribution\naround the population mean. Hang on for a moment as we unpack that statement. 1. Suppose we have a population, like our marathon field, and we are interested in the\nweights of its members. Any sample of runners, such as each bus of sixty runners, will\nhave a mean. 2. If we take repeated samples, such as picking random groups of sixty runners from the\nfield over and over, then each of those samples will have its own mean weight. These are\nthe sample means. 3. Most of the sample means will be very close to the population mean. Some will be a\nlittle higher. Some will be a little lower. Just as a matter of chance, a very few will be", "tokens": 325, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 99, "segment_id": "00099", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000173"}
{"type": "chunk", "text": "significantly higher than the population mean, and a very few will be significantly lower. Cue the music, because this is where everything comes together in a powerful crescendo . . . 4. The central limit theorem tells us that the sample means will be distributed roughly as a\nnormal distribution around the population mean. The normal distribution, as you may\nremember from Chapter 2, is the bell-shaped distribution (e.g., adult men’s heights) in\nwhich 68 percent of the observations lie within one standard deviation of the mean, 95\npercent lie within two standard deviations, and so on. 5. All of this will be true no matter what the distribution of the underlying population looks\nlike. The population from which the samples are being drawn does not have to have a\nnormal distribution in order for the sample means to be distributed normally. Let’s think about some real data, say, the household income distribution in the United States. Household income is not distributed normally in America; instead, it tends to be skewed to the\nright. No household can earn less than $0 in a given year, so that must be the lower bound for\nthe distribution. Meanwhile, a small group of households can earn staggeringly large annual\nincomes---hundreds of millions or even billions of dollars in some cases. As a result, we\nwould expect the distribution of household incomes to have a long right tail---something like\nthis:\n\nThe median household income in the United States is roughly $51,900; the mean household\nincome is $70,900.1 (People like Bill Gates pull the mean household income to the right, just\nas he did when he walked in to the bar in Chapter 2.) Now suppose we take a random sample\nof 1,000 U.S. households and gather information on annual household income. On the basis of\nthe information above, and the central limit theorem, what can we infer about this sample? Quite a lot, it turns out. First of all, our best guess for what the mean of any sample will be\nis the mean of the population from which it’s drawn. The whole point of a representative\nsample is that it looks like the underlying population. A properly drawn sample will, on\naverage, look like America. There will be hedge fund managers and homeless people and\npolice officers and everyone else---all roughly in proportion to their frequency in the\npopulation.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nsignificantly higher than the population mean, and a very few will be significantly lower. Cue the music, because this is where everything comes together in a powerful crescendo . . . 4. The central limit theorem tells us that the sample means will be distributed roughly as a\nnormal distribution around the population mean. The normal distribution, as you may\nremember from Chapter 2, is the bell-shaped distribution (e.g., adult men’s heights) in\nwhich 68 percent of the observations lie within one standard deviation of the mean, 95\npercent lie within two standard deviations, and so on. 5. All of this will be true no matter what the distribution of the underlying population looks\nlike. The population from which the samples are being drawn does not have to have a\nnormal distribution in order for the sample means to be distributed normally. Let’s think about some real data, say, the household income distribution in the United States. Household income is not distributed normally in America; instead, it tends to be skewed to the\nright. No household can earn less than $0 in a given year, so that must be the lower bound for\nthe distribution. Meanwhile, a small group of households can earn staggeringly large annual\nincomes---hundreds of millions or even billions of dollars in some cases. As a result, we\nwould expect the distribution of household incomes to have a long right tail---something like\nthis:\n\nThe median household income in the United States is roughly $51,900; the mean household\nincome is $70,900.1 (People like Bill Gates pull the mean household income to the right, just\nas he did when he walked in to the bar in Chapter 2.) Now suppose we take a random sample\nof 1,000 U.S. households and gather information on annual household income. On the basis of\nthe information above, and the central limit theorem, what can we infer about this sample? Quite a lot, it turns out. First of all, our best guess for what the mean of any sample will be\nis the mean of the population from which it’s drawn. The whole point of a representative\nsample is that it looks like the underlying population. A properly drawn sample will, on\naverage, look like America. There will be hedge fund managers and homeless people and\npolice officers and everyone else---all roughly in proportion to their frequency in the\npopulation.", "tokens": 498, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 100, "segment_id": "00100", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000174"}
{"type": "chunk", "text": "On the basis of\nthe information above, and the central limit theorem, what can we infer about this sample? Quite a lot, it turns out. First of all, our best guess for what the mean of any sample will be\nis the mean of the population from which it’s drawn. The whole point of a representative\nsample is that it looks like the underlying population. A properly drawn sample will, on\naverage, look like America. There will be hedge fund managers and homeless people and\npolice officers and everyone else---all roughly in proportion to their frequency in the\npopulation. Therefore, we would expect the mean household income for a representative\nsample of 1,000 American households to be about $70,900. Will it be exactly that? No. But it\nshouldn’t be wildly different either.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nOn the basis of\nthe information above, and the central limit theorem, what can we infer about this sample? Quite a lot, it turns out. First of all, our best guess for what the mean of any sample will be\nis the mean of the population from which it’s drawn. The whole point of a representative\nsample is that it looks like the underlying population. A properly drawn sample will, on\naverage, look like America. There will be hedge fund managers and homeless people and\npolice officers and everyone else---all roughly in proportion to their frequency in the\npopulation. Therefore, we would expect the mean household income for a representative\nsample of 1,000 American households to be about $70,900. Will it be exactly that? No. But it\nshouldn’t be wildly different either.", "tokens": 169, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 100, "segment_id": "00100", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000175"}
{"type": "chunk", "text": "If we took multiple samples of 1,000 households, we would expect the different sample\nmeans to cluster around the population mean, $70,900. We would expect some means to be\nhigher, and some to be lower. Might we get a sample of 1,000 households with a mean\nhousehold income of $427,000? Sure, that’s possible---but highly unlikely. (Remember, our\nsampling methodology is sound; we are not conducting a survey in the parking lot of the\nGreenwich Country Club.) It’s also highly unlikely that a proper sample of 1,000 American\nhouseholds would have a mean income of $8,000. That’s all just basic logic. The central limit theorem enables us to go one step further by\ndescribing the expected distribution of those different sample means as they cluster around the\npopulation mean. Specifically, the sample means will form a normal distribution around the\npopulation mean, which in this case is $70,900. Remember, the shape of the underlying\npopulation doesn’t matter. The household income distribution in the United States is plenty\nskewed, but the distribution of the sample means will not be skewed. If we were to take 100\ndifferent samples, each with 1,000 households, and plotted the frequency of our results, we\nwould expect those sample means to form the familiar “bell-shaped” distribution around\n$70,900. The larger the number of samples, the more closely the distribution will approximate the\nnormal distribution. And the larger the size of each sample, the tighter that distribution will be. To test this result, let’s do a fun experiment with real data on the weights of real Americans. The University of Michigan conducts a longitudinal study called Americans’ Changing Lives,\nwhich consists of detailed observations on several thousand American adults, including their\nweights. The weight distribution is skewed slightly right, because it’s biologically easier to be\n100 pounds overweight than it is to be 100 pounds underweight. The mean weight for all adults\nin the study is 162 pounds. Using basic statistical software, we can direct the computer to take a random sample of 100\nindividuals from the Changing Lives data. In fact, we can do this over and over again to see\nhow the results fit with what the central limit theorem would predict. Here is a graph of the\ndistribution of 100 sample means (rounded to the nearest pound) randomly generated from the\nChanging Lives data.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nIf we took multiple samples of 1,000 households, we would expect the different sample\nmeans to cluster around the population mean, $70,900. We would expect some means to be\nhigher, and some to be lower. Might we get a sample of 1,000 households with a mean\nhousehold income of $427,000? Sure, that’s possible---but highly unlikely. (Remember, our\nsampling methodology is sound; we are not conducting a survey in the parking lot of the\nGreenwich Country Club.) It’s also highly unlikely that a proper sample of 1,000 American\nhouseholds would have a mean income of $8,000. That’s all just basic logic. The central limit theorem enables us to go one step further by\ndescribing the expected distribution of those different sample means as they cluster around the\npopulation mean. Specifically, the sample means will form a normal distribution around the\npopulation mean, which in this case is $70,900. Remember, the shape of the underlying\npopulation doesn’t matter. The household income distribution in the United States is plenty\nskewed, but the distribution of the sample means will not be skewed. If we were to take 100\ndifferent samples, each with 1,000 households, and plotted the frequency of our results, we\nwould expect those sample means to form the familiar “bell-shaped” distribution around\n$70,900. The larger the number of samples, the more closely the distribution will approximate the\nnormal distribution. And the larger the size of each sample, the tighter that distribution will be. To test this result, let’s do a fun experiment with real data on the weights of real Americans. The University of Michigan conducts a longitudinal study called Americans’ Changing Lives,\nwhich consists of detailed observations on several thousand American adults, including their\nweights. The weight distribution is skewed slightly right, because it’s biologically easier to be\n100 pounds overweight than it is to be 100 pounds underweight. The mean weight for all adults\nin the study is 162 pounds. Using basic statistical software, we can direct the computer to take a random sample of 100\nindividuals from the Changing Lives data. In fact, we can do this over and over again to see\nhow the results fit with what the central limit theorem would predict. Here is a graph of the\ndistribution of 100 sample means (rounded to the nearest pound) randomly generated from the\nChanging Lives data.", "tokens": 508, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 101, "segment_id": "00101", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000176"}
{"type": "chunk", "text": "The mean weight for all adults\nin the study is 162 pounds. Using basic statistical software, we can direct the computer to take a random sample of 100\nindividuals from the Changing Lives data. In fact, we can do this over and over again to see\nhow the results fit with what the central limit theorem would predict. Here is a graph of the\ndistribution of 100 sample means (rounded to the nearest pound) randomly generated from the\nChanging Lives data. 100 Sample Means, n = 100\n\nThe larger the sample size and the more samples taken, the more closely the distribution of\nsample means will approximate the normal curve. (As a rule of thumb, the sample size must be", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThe mean weight for all adults\nin the study is 162 pounds. Using basic statistical software, we can direct the computer to take a random sample of 100\nindividuals from the Changing Lives data. In fact, we can do this over and over again to see\nhow the results fit with what the central limit theorem would predict. Here is a graph of the\ndistribution of 100 sample means (rounded to the nearest pound) randomly generated from the\nChanging Lives data. 100 Sample Means, n = 100\n\nThe larger the sample size and the more samples taken, the more closely the distribution of\nsample means will approximate the normal curve. (As a rule of thumb, the sample size must be", "tokens": 146, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 101, "segment_id": "00101", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000177"}
{"type": "chunk", "text": "at least 30 for the central limit theorem to hold true.) This makes sense. A larger sample is less\nlikely to be affected by random variation. A sample of 2 can be highly skewed by 1\nparticularly large or small person. In contrast, a sample of 500 will not be unduly affected by a\nfew particularly large or small people. We are now very close to making all of our statistical dreams come true! The sample means\nare distributed roughly as a normal curve, as described above. The power of a normal\ndistribution derives from the fact that we know roughly what proportion of observations will lie\nwithin one standard deviation above or below the mean (68 percent); what proportion of\nobservations will lie within two standard deviations above or below the mean (95 percent); and\nso on. This is powerful stuff. Earlier in this chapter, I pointed out that we could infer intuitively that a busload of\npassengers with a mean weight twenty-five pounds higher than the mean weight for the whole\nmarathon field was probably not the lost bus of runners. To quantify that intuition---to be able\nto say that this inference will be correct 95 percent of the time, or 99 percent, or 99.9 percent\n---we need just one more technical concept: the standard error. The standard error measures the dispersion of the sample means. How tightly do we expect\nthe sample means to cluster around the population mean? There is some potential confusion\nhere, as we have now introduced two different measures of dispersion: the standard deviation\nand the standard error. Here is what you need to remember to keep them straight:\n\n1. The standard deviation measures dispersion in the underlying population. In this case, it\nmight measure the dispersion of the weights of all the participants in the Framingham\nHeart Study, or the dispersion around the mean for the entire marathon field. 2. The standard error measures the dispersion of the sample means. If we draw repeated\nsamples of 100 participants from the Framingham Heart Study, what will the dispersion\nof those sample means look like? 3. Here is what ties the two concepts together: The standard error is the standard deviation\n\nof the sample means! Isn’t that kind of cool? A large standard error means that the sample means are spread out widely around the\npopulation mean; a small standard error means that they are clustered relatively tightly. Here\nare three real examples from the Changing Lives data. 100 Sample Means, n = 20", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nat least 30 for the central limit theorem to hold true.) This makes sense. A larger sample is less\nlikely to be affected by random variation. A sample of 2 can be highly skewed by 1\nparticularly large or small person. In contrast, a sample of 500 will not be unduly affected by a\nfew particularly large or small people. We are now very close to making all of our statistical dreams come true! The sample means\nare distributed roughly as a normal curve, as described above. The power of a normal\ndistribution derives from the fact that we know roughly what proportion of observations will lie\nwithin one standard deviation above or below the mean (68 percent); what proportion of\nobservations will lie within two standard deviations above or below the mean (95 percent); and\nso on. This is powerful stuff. Earlier in this chapter, I pointed out that we could infer intuitively that a busload of\npassengers with a mean weight twenty-five pounds higher than the mean weight for the whole\nmarathon field was probably not the lost bus of runners. To quantify that intuition---to be able\nto say that this inference will be correct 95 percent of the time, or 99 percent, or 99.9 percent\n---we need just one more technical concept: the standard error. The standard error measures the dispersion of the sample means. How tightly do we expect\nthe sample means to cluster around the population mean? There is some potential confusion\nhere, as we have now introduced two different measures of dispersion: the standard deviation\nand the standard error. Here is what you need to remember to keep them straight:\n\n1. The standard deviation measures dispersion in the underlying population. In this case, it\nmight measure the dispersion of the weights of all the participants in the Framingham\nHeart Study, or the dispersion around the mean for the entire marathon field. 2. The standard error measures the dispersion of the sample means. If we draw repeated\nsamples of 100 participants from the Framingham Heart Study, what will the dispersion\nof those sample means look like? 3. Here is what ties the two concepts together: The standard error is the standard deviation\n\nof the sample means! Isn’t that kind of cool? A large standard error means that the sample means are spread out widely around the\npopulation mean; a small standard error means that they are clustered relatively tightly. Here\nare three real examples from the Changing Lives data. 100 Sample Means, n = 20", "tokens": 512, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 102, "segment_id": "00102", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000178"}
{"type": "chunk", "text": "100 Sample Means, n = 100\n\nFemale Population Only/100 Sample Means, n = 100\n\nThe second distribution, which has a larger sample size, is more tightly clustered around the\nmean than the first distribution. The larger sample size makes it less likely that a sample mean\nwill deviate sharply from the population mean. The final set of sample means is drawn only\nfrom a subset of the population, women in the study. Since the weights of women in the data\nset are less diffuse than the weights of all persons in the population, it stands to reason that the\nweights of samples drawn just from the women would be less dispersed than samples drawn\nfrom the whole Changing Lives population. (These samples are also clustered around a slightly\ndifferent population mean, since the mean weight for all females in the Changing Lives study is\ndifferent from the mean weight for the entire population in the study.)\n\nThe pattern that you saw above holds true in general. Sample means will cluster more tightly\naround the population mean as the size of each sample gets larger (e.g., our sample means\nwere more tightly clustered when we took samples of 100 rather than 30). And the sample\nmeans will cluster less tightly around the population mean when the underlying population is\nmore spread out (e.g., our sample means for the entire Changing Lives population were more\ndispersed than the sample means for just the females in the study). If you’ve followed the logic this far, then the formula for the standard error follows naturally:\n where s is the standard deviation of the population from which the sample is drawn,\nSE\nand n is the size of the sample. Keep your head about you! Don’t let the appearance of letters\nmess up the basic intuition. The standard error will be large when the standard deviation of the\nunderlying distribution is large. A large sample drawn from a highly dispersed population is\nalso likely to be highly dispersed; a large sample from a population clustered tightly around the\nmean is also likely to be clustered tightly around the mean. If we are still looking at weight, we", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\n100 Sample Means, n = 100\n\nFemale Population Only/100 Sample Means, n = 100\n\nThe second distribution, which has a larger sample size, is more tightly clustered around the\nmean than the first distribution. The larger sample size makes it less likely that a sample mean\nwill deviate sharply from the population mean. The final set of sample means is drawn only\nfrom a subset of the population, women in the study. Since the weights of women in the data\nset are less diffuse than the weights of all persons in the population, it stands to reason that the\nweights of samples drawn just from the women would be less dispersed than samples drawn\nfrom the whole Changing Lives population. (These samples are also clustered around a slightly\ndifferent population mean, since the mean weight for all females in the Changing Lives study is\ndifferent from the mean weight for the entire population in the study.)\n\nThe pattern that you saw above holds true in general. Sample means will cluster more tightly\naround the population mean as the size of each sample gets larger (e.g., our sample means\nwere more tightly clustered when we took samples of 100 rather than 30). And the sample\nmeans will cluster less tightly around the population mean when the underlying population is\nmore spread out (e.g., our sample means for the entire Changing Lives population were more\ndispersed than the sample means for just the females in the study). If you’ve followed the logic this far, then the formula for the standard error follows naturally:\n where s is the standard deviation of the population from which the sample is drawn,\nSE\nand n is the size of the sample. Keep your head about you! Don’t let the appearance of letters\nmess up the basic intuition. The standard error will be large when the standard deviation of the\nunderlying distribution is large. A large sample drawn from a highly dispersed population is\nalso likely to be highly dispersed; a large sample from a population clustered tightly around the\nmean is also likely to be clustered tightly around the mean. If we are still looking at weight, we", "tokens": 425, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 103, "segment_id": "00103", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000179"}
{"type": "chunk", "text": "would expect the standard error for a sample drawn from the entire Changing Lives population\nto be larger than the standard error for a sample drawn only from the men in their twenties. This\nis why the standard deviation (s) is in the numerator. Similarly, we would expect the standard error to get smaller as the sample size gets larger,\nsince large samples are less prone to distortion by extreme outliers. This is why the sample\nsize (n) is in the denominator. (The reason we take the square root of n will be left for a more\nadvanced text; the basic relationship is what’s important here.)\n\nIn the case of the Changing Lives data, we actually know the standard deviation of the\npopulation; often that is not the case. For large samples, we can assume that the standard\ndeviation of the sample is reasonably close to the standard deviation of the population.*\n\nFinally, we have arrived at the payoff for all of this. Because the sample means are\ndistributed normally (thanks to the central limit theorem), we can harness the power of the\nnormal curve. We expect that roughly 68 percent of all sample means will lie within one\nstandard error of the population mean; 95 percent of the sample means will lie within two\nstandard errors of the population mean; and 99.7 percent of the sample means will lie within\nthree standard errors of the population mean. Frequency Distribution of Sample Means\n\nSo let’s return to a variation on our lost-bus example, only now we can substitute numbers\nfor intuition. (The example itself will remain absurd; the next chapter will have plenty of less\nabsurd, real-world examples.) Suppose that the Changing Lives study has invited all of the\nindividuals in the study to meet in Boston for a weekend of data gathering and revelry. The\nparticipants are loaded randomly onto buses and ferried among the buildings at the testing\nfacility where they are weighed, measured, poked, prodded, and so on. Shockingly, one bus\ngoes missing, a fact that is broadcast on the local news. At around that time, you are driving\nback from the Festival of Sausage when you see a crashed bus on the side of the road. Apparently the bus swerved to miss a wild fox crossing the road, and all of the passengers are", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nwould expect the standard error for a sample drawn from the entire Changing Lives population\nto be larger than the standard error for a sample drawn only from the men in their twenties. This\nis why the standard deviation (s) is in the numerator. Similarly, we would expect the standard error to get smaller as the sample size gets larger,\nsince large samples are less prone to distortion by extreme outliers. This is why the sample\nsize (n) is in the denominator. (The reason we take the square root of n will be left for a more\nadvanced text; the basic relationship is what’s important here.)\n\nIn the case of the Changing Lives data, we actually know the standard deviation of the\npopulation; often that is not the case. For large samples, we can assume that the standard\ndeviation of the sample is reasonably close to the standard deviation of the population.*\n\nFinally, we have arrived at the payoff for all of this. Because the sample means are\ndistributed normally (thanks to the central limit theorem), we can harness the power of the\nnormal curve. We expect that roughly 68 percent of all sample means will lie within one\nstandard error of the population mean; 95 percent of the sample means will lie within two\nstandard errors of the population mean; and 99.7 percent of the sample means will lie within\nthree standard errors of the population mean. Frequency Distribution of Sample Means\n\nSo let’s return to a variation on our lost-bus example, only now we can substitute numbers\nfor intuition. (The example itself will remain absurd; the next chapter will have plenty of less\nabsurd, real-world examples.) Suppose that the Changing Lives study has invited all of the\nindividuals in the study to meet in Boston for a weekend of data gathering and revelry. The\nparticipants are loaded randomly onto buses and ferried among the buildings at the testing\nfacility where they are weighed, measured, poked, prodded, and so on. Shockingly, one bus\ngoes missing, a fact that is broadcast on the local news. At around that time, you are driving\nback from the Festival of Sausage when you see a crashed bus on the side of the road. Apparently the bus swerved to miss a wild fox crossing the road, and all of the passengers are", "tokens": 474, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 104, "segment_id": "00104", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000180"}
{"type": "chunk", "text": "unconscious but not seriously hurt. (I need them to be uncommunicative for the example to\nwork, but I don’t want their injuries to be too disturbing.) Paramedics on the scene inform you\nthat the mean weight of the 62 passengers on the bus is 194 pounds. Also, the fox that the bus\nswerved to avoid was clipped slightly and appears to have a broken hind leg. Fortunately you know the mean weight and standard deviation for the entire Changing Lives\npopulation, you have a working knowledge of the central limit theorem, and you know how to\nadminister first aid to a wild fox. The mean weight for the Changing Lives participants is 162;\nthe standard deviation is 36. From that information, we can calculate the standard error for a\n62-person sample (the number of unconscious passengers on the bus):\n\nThe difference between the sample mean (194 pounds) and the population mean (162\npounds) is 32 pounds, or well more than three standard errors. We know from the central limit\ntheorem that 99.7 percent of all sample means will lie within three standard errors of the\npopulation mean. That makes it extremely unlikely that this bus represents a random group of\nChanging Lives participants. In your duty as a civic leader, you call the study officials to tell\nthem that this is probably not their missing bus, only now you can offer statistical evidence,\nrather than just “a hunch.” You report to the Changing Lives folks that you can reject the\npossibility that this is the missing bus at the 99.7 percent confidence level. And since you are\ntalking to researchers, they actually understand what you are talking about. Your analysis is further confirmed when paramedics conduct blood tests on the bus\npassengers and discover that the mean cholesterol level for the busload of passengers is five\nstandard errors above the mean cholesterol level for the Changing Lives study participants. That suggests, correctly it later turns out, that the unconscious passengers are involved with the\nFestival of Sausage. [There is a happy ending. When the bus passengers regained consciousness, Changing Lives\nstudy officials offered them counseling on the dangers of a diet high in saturated fats, causing\nmany of them to adopt more heart-healthy eating habits. Meanwhile, the fox was nurtured back\nto health at a local wildlife preserve and was eventually released back into the wild.]*\n\nI’ve tried to stick with the basics in this chapter.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nunconscious but not seriously hurt. (I need them to be uncommunicative for the example to\nwork, but I don’t want their injuries to be too disturbing.) Paramedics on the scene inform you\nthat the mean weight of the 62 passengers on the bus is 194 pounds. Also, the fox that the bus\nswerved to avoid was clipped slightly and appears to have a broken hind leg. Fortunately you know the mean weight and standard deviation for the entire Changing Lives\npopulation, you have a working knowledge of the central limit theorem, and you know how to\nadminister first aid to a wild fox. The mean weight for the Changing Lives participants is 162;\nthe standard deviation is 36. From that information, we can calculate the standard error for a\n62-person sample (the number of unconscious passengers on the bus):\n\nThe difference between the sample mean (194 pounds) and the population mean (162\npounds) is 32 pounds, or well more than three standard errors. We know from the central limit\ntheorem that 99.7 percent of all sample means will lie within three standard errors of the\npopulation mean. That makes it extremely unlikely that this bus represents a random group of\nChanging Lives participants. In your duty as a civic leader, you call the study officials to tell\nthem that this is probably not their missing bus, only now you can offer statistical evidence,\nrather than just “a hunch.” You report to the Changing Lives folks that you can reject the\npossibility that this is the missing bus at the 99.7 percent confidence level. And since you are\ntalking to researchers, they actually understand what you are talking about. Your analysis is further confirmed when paramedics conduct blood tests on the bus\npassengers and discover that the mean cholesterol level for the busload of passengers is five\nstandard errors above the mean cholesterol level for the Changing Lives study participants. That suggests, correctly it later turns out, that the unconscious passengers are involved with the\nFestival of Sausage. [There is a happy ending. When the bus passengers regained consciousness, Changing Lives\nstudy officials offered them counseling on the dangers of a diet high in saturated fats, causing\nmany of them to adopt more heart-healthy eating habits. Meanwhile, the fox was nurtured back\nto health at a local wildlife preserve and was eventually released back into the wild.]*\n\nI’ve tried to stick with the basics in this chapter.", "tokens": 503, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 105, "segment_id": "00105", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000181"}
{"type": "chunk", "text": "That suggests, correctly it later turns out, that the unconscious passengers are involved with the\nFestival of Sausage. [There is a happy ending. When the bus passengers regained consciousness, Changing Lives\nstudy officials offered them counseling on the dangers of a diet high in saturated fats, causing\nmany of them to adopt more heart-healthy eating habits. Meanwhile, the fox was nurtured back\nto health at a local wildlife preserve and was eventually released back into the wild.]*\n\nI’ve tried to stick with the basics in this chapter. You should note that for the central limit\ntheorem to apply, the sample sizes need to be relatively large (over 30 as a rule of thumb). We\nalso need a relatively large sample if we are going to assume that the standard deviation of the\nsample is roughly the same as the standard deviation of the population from which it is drawn. There are plenty of statistical fixes that can be applied when these conditions are not met---but\nthat is all frosting on the cake (and maybe even sprinkles on the frosting on the cake). The “big\npicture” here is simple and massively powerful:\n\n1. If you draw large, random samples from any population, the means of those samples will\nbe distributed normally around the population mean (regardless of what the distribution\nof the underlying population looks like). 2. Most sample means will lie reasonably close to the population mean; the standard error\n\nis what defines “reasonably close.”\n\n3. The central limit theorem tells us the probability that a sample mean will lie within a", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThat suggests, correctly it later turns out, that the unconscious passengers are involved with the\nFestival of Sausage. [There is a happy ending. When the bus passengers regained consciousness, Changing Lives\nstudy officials offered them counseling on the dangers of a diet high in saturated fats, causing\nmany of them to adopt more heart-healthy eating habits. Meanwhile, the fox was nurtured back\nto health at a local wildlife preserve and was eventually released back into the wild.]*\n\nI’ve tried to stick with the basics in this chapter. You should note that for the central limit\ntheorem to apply, the sample sizes need to be relatively large (over 30 as a rule of thumb). We\nalso need a relatively large sample if we are going to assume that the standard deviation of the\nsample is roughly the same as the standard deviation of the population from which it is drawn. There are plenty of statistical fixes that can be applied when these conditions are not met---but\nthat is all frosting on the cake (and maybe even sprinkles on the frosting on the cake). The “big\npicture” here is simple and massively powerful:\n\n1. If you draw large, random samples from any population, the means of those samples will\nbe distributed normally around the population mean (regardless of what the distribution\nof the underlying population looks like). 2. Most sample means will lie reasonably close to the population mean; the standard error\n\nis what defines “reasonably close.”\n\n3. The central limit theorem tells us the probability that a sample mean will lie within a", "tokens": 320, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 105, "segment_id": "00105", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000182"}
{"type": "chunk", "text": "certain distance of the population mean. It is relatively unlikely that a sample mean will\nlie more than two standard errors from the population mean, and extremely unlikely that\nit will lie three or more standard errors from the population mean. 4. The less likely it is that an outcome has been observed by chance, the more confident\n\nwe can be in surmising that some other factor is in play. That’s pretty much what statistical inference is about. The central limit theorem is what\nmakes most of it possible. And until Lebron James wins as many NBA championships as\nMichael Jordan (six), the central limit theorem will be far more impressive than he is. * Note the clever use of false precision here. * When the standard deviation for the population is calculated from a smaller sample, the formula is tweaked slightly:\n This helps to account for the fact that the dispersion in a small sample may understate the dispersion of the full\n\npopulation. This is not highly relevant to the bigger points in this chapter. * My University of Chicago colleague Jim Sallee makes a very important critique of the missing-bus examples. He points\nout that very few buses ever go missing. So if we happen to be looking for a missing bus, any bus that turns up lost or\ncrashed is likely to be that bus, regardless of the weight of the passengers on the bus . He’s right. (Think about it: if you\nlose your child in a supermarket, and the store manager tells you that there happens to be a lost child standing near register\nsix, you would conclude immediately that it’s probably your child.) We’re therefore going to have to add one more element\nof absurdity to these examples and pretend that buses go missing all the time.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\ncertain distance of the population mean. It is relatively unlikely that a sample mean will\nlie more than two standard errors from the population mean, and extremely unlikely that\nit will lie three or more standard errors from the population mean. 4. The less likely it is that an outcome has been observed by chance, the more confident\n\nwe can be in surmising that some other factor is in play. That’s pretty much what statistical inference is about. The central limit theorem is what\nmakes most of it possible. And until Lebron James wins as many NBA championships as\nMichael Jordan (six), the central limit theorem will be far more impressive than he is. * Note the clever use of false precision here. * When the standard deviation for the population is calculated from a smaller sample, the formula is tweaked slightly:\n This helps to account for the fact that the dispersion in a small sample may understate the dispersion of the full\n\npopulation. This is not highly relevant to the bigger points in this chapter. * My University of Chicago colleague Jim Sallee makes a very important critique of the missing-bus examples. He points\nout that very few buses ever go missing. So if we happen to be looking for a missing bus, any bus that turns up lost or\ncrashed is likely to be that bus, regardless of the weight of the passengers on the bus . He’s right. (Think about it: if you\nlose your child in a supermarket, and the store manager tells you that there happens to be a lost child standing near register\nsix, you would conclude immediately that it’s probably your child.) We’re therefore going to have to add one more element\nof absurdity to these examples and pretend that buses go missing all the time.", "tokens": 356, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 106, "segment_id": "00106", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000183"}
{"type": "chunk", "text": "CHAPTER 9\nInference\nWhy my statistics professor\nthought I might have cheated\n\nIn the spring of my senior year of college, I took a statistics class. I wasn’t particularly\n\nenamored of statistics or of most math-based disciplines at that time, but I had promised my\ndad that I would take the course if I could leave school for ten days to go on a family trip to\nthe Soviet Union. So, I basically took stats in exchange for a trip to the USSR. This turned out\nto be a great deal, both because I liked statistics more than I thought I would and because I got\nto visit the USSR in the spring of 1988. Who knew that the country wouldn’t be around in its\ncommunist form for much longer? This story is actually relevant to the chapter; the point is that I wasn’t as devoted to my\nstatistics course during the term as I might have been. Among other responsibilities, I was also\nwriting a senior honors thesis that was due about halfway through the term. We had regular\nquizzes in the statistics course, many of which I ignored or failed. I studied a little for the\nmidterm and did passably well---literally. But a few weeks before the end of the term, two\nthings happened. First, I finished my thesis, giving me copious amounts of new free time. And\nsecond, I realized that statistics wasn’t nearly as difficult as I had been making it out to be. I\nbegan studying the stats book and doing the work from earlier in the course. I earned an A on\nthe final exam. That’s when my statistics professor, whose name I’ve long since forgotten, called me into his\noffice. I don’t remember exactly what he said, but it was something along the lines of “You\nreally did much better on the final than you did on the midterm.” This was not a congratulatory\nvisit during which I was recognized for finally doing serious work in the class. There was an\nimplicit accusation (though not an explicit one) in his summons; the expectation was that I\nwould explain why I did so much better on the final exam than the midterm. In short, this guy\nsuspected that I might have cheated. Now that I’ve taught for many years, I’m more\nsympathetic to his line of thinking.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nCHAPTER 9\nInference\nWhy my statistics professor\nthought I might have cheated\n\nIn the spring of my senior year of college, I took a statistics class. I wasn’t particularly\n\nenamored of statistics or of most math-based disciplines at that time, but I had promised my\ndad that I would take the course if I could leave school for ten days to go on a family trip to\nthe Soviet Union. So, I basically took stats in exchange for a trip to the USSR. This turned out\nto be a great deal, both because I liked statistics more than I thought I would and because I got\nto visit the USSR in the spring of 1988. Who knew that the country wouldn’t be around in its\ncommunist form for much longer? This story is actually relevant to the chapter; the point is that I wasn’t as devoted to my\nstatistics course during the term as I might have been. Among other responsibilities, I was also\nwriting a senior honors thesis that was due about halfway through the term. We had regular\nquizzes in the statistics course, many of which I ignored or failed. I studied a little for the\nmidterm and did passably well---literally. But a few weeks before the end of the term, two\nthings happened. First, I finished my thesis, giving me copious amounts of new free time. And\nsecond, I realized that statistics wasn’t nearly as difficult as I had been making it out to be. I\nbegan studying the stats book and doing the work from earlier in the course. I earned an A on\nthe final exam. That’s when my statistics professor, whose name I’ve long since forgotten, called me into his\noffice. I don’t remember exactly what he said, but it was something along the lines of “You\nreally did much better on the final than you did on the midterm.” This was not a congratulatory\nvisit during which I was recognized for finally doing serious work in the class. There was an\nimplicit accusation (though not an explicit one) in his summons; the expectation was that I\nwould explain why I did so much better on the final exam than the midterm. In short, this guy\nsuspected that I might have cheated. Now that I’ve taught for many years, I’m more\nsympathetic to his line of thinking.", "tokens": 488, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 107, "segment_id": "00107", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000184"}
{"type": "chunk", "text": "There was an\nimplicit accusation (though not an explicit one) in his summons; the expectation was that I\nwould explain why I did so much better on the final exam than the midterm. In short, this guy\nsuspected that I might have cheated. Now that I’ve taught for many years, I’m more\nsympathetic to his line of thinking. In nearly every course I’ve taught, there is a striking degree\nof correlation between a student’s performance on the midterm and on the final. It is highly\nunusual for a student to score below average on the midterm and then near the top of the class\non the final. I explained that I had finished my thesis and gotten serious about the class (by doing things\nlike reading the assigned textbook chapters and doing the homework). He seemed content with\nthis explanation, and I left, still somewhat unsettled by the implicit accusation. Believe it or not, this anecdote embodies much of what you need to know about statistical\ninference, including both its strengths and its potential weaknesses. Statistics cannot prove", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThere was an\nimplicit accusation (though not an explicit one) in his summons; the expectation was that I\nwould explain why I did so much better on the final exam than the midterm. In short, this guy\nsuspected that I might have cheated. Now that I’ve taught for many years, I’m more\nsympathetic to his line of thinking. In nearly every course I’ve taught, there is a striking degree\nof correlation between a student’s performance on the midterm and on the final. It is highly\nunusual for a student to score below average on the midterm and then near the top of the class\non the final. I explained that I had finished my thesis and gotten serious about the class (by doing things\nlike reading the assigned textbook chapters and doing the homework). He seemed content with\nthis explanation, and I left, still somewhat unsettled by the implicit accusation. Believe it or not, this anecdote embodies much of what you need to know about statistical\ninference, including both its strengths and its potential weaknesses. Statistics cannot prove", "tokens": 219, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 107, "segment_id": "00107", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000185"}
{"type": "chunk", "text": "anything with certainty. Instead, the power of statistical inference derives from observing some\npattern or outcome and then using probability to determine the most likely explanation for that\noutcome. Suppose a strange gambler arrives in town and offers you a wager: He wins $1,000 if\nhe rolls a six with a single die; you win $500 if he rolls anything else---a pretty good bet from\nyour standpoint. He then proceeds to roll ten sixes in a row, taking $10,000 from you. One possible explanation is that he was lucky. An alternative explanation is that he cheated\nsomehow. The probability of rolling ten sixes in a row with a fair die is roughly 1 in 60 million. You can’t prove that he cheated, but you ought at least to inspect the die. Of course, the most likely explanation is not always the right explanation. Extremely rare\nthings happen. Linda Cooper is a South Carolina woman who has been struck by lightning four\ntimes.1 (The Federal Emergency Management Administration estimates the probability of\ngetting hit by lightning just once as 1 in 600,000.) Linda Cooper’s insurance company cannot\ndeny her coverage simply because her injuries are statistically improbable. To return to my\nundergraduate statistics exam, the professor had reasonable cause to be suspicious. He saw a\npattern that was highly unlikely; this is exactly how investigators spot cheating on standardized\nexams and how the SEC catches insider trading. But an unlikely pattern is just an unlikely\npattern unless it is corroborated by additional evidence. Later in the chapter we will discuss\nerrors that can arise when probability steers us wrong. For now, we should appreciate that statistical inference uses data to address important\nquestions. Is a new drug effective in treating heart disease? Do cell phones cause brain cancer? Please note that I’m not claiming that statistics can answer these kinds of questions\nunequivocally; instead, inference tells us what is likely, and what is unlikely. Researchers\ncannot prove that a new drug is effective in treating heart disease, even when they have data\nfrom a carefully controlled clinical trial. After all, it is entirely possible that there will be\nrandom variation in the outcomes of patients in the treatment and control groups that are\nunrelated to the new drug.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nanything with certainty. Instead, the power of statistical inference derives from observing some\npattern or outcome and then using probability to determine the most likely explanation for that\noutcome. Suppose a strange gambler arrives in town and offers you a wager: He wins $1,000 if\nhe rolls a six with a single die; you win $500 if he rolls anything else---a pretty good bet from\nyour standpoint. He then proceeds to roll ten sixes in a row, taking $10,000 from you. One possible explanation is that he was lucky. An alternative explanation is that he cheated\nsomehow. The probability of rolling ten sixes in a row with a fair die is roughly 1 in 60 million. You can’t prove that he cheated, but you ought at least to inspect the die. Of course, the most likely explanation is not always the right explanation. Extremely rare\nthings happen. Linda Cooper is a South Carolina woman who has been struck by lightning four\ntimes.1 (The Federal Emergency Management Administration estimates the probability of\ngetting hit by lightning just once as 1 in 600,000.) Linda Cooper’s insurance company cannot\ndeny her coverage simply because her injuries are statistically improbable. To return to my\nundergraduate statistics exam, the professor had reasonable cause to be suspicious. He saw a\npattern that was highly unlikely; this is exactly how investigators spot cheating on standardized\nexams and how the SEC catches insider trading. But an unlikely pattern is just an unlikely\npattern unless it is corroborated by additional evidence. Later in the chapter we will discuss\nerrors that can arise when probability steers us wrong. For now, we should appreciate that statistical inference uses data to address important\nquestions. Is a new drug effective in treating heart disease? Do cell phones cause brain cancer? Please note that I’m not claiming that statistics can answer these kinds of questions\nunequivocally; instead, inference tells us what is likely, and what is unlikely. Researchers\ncannot prove that a new drug is effective in treating heart disease, even when they have data\nfrom a carefully controlled clinical trial. After all, it is entirely possible that there will be\nrandom variation in the outcomes of patients in the treatment and control groups that are\nunrelated to the new drug.", "tokens": 466, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 108, "segment_id": "00108", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000186"}
{"type": "chunk", "text": "Is a new drug effective in treating heart disease? Do cell phones cause brain cancer? Please note that I’m not claiming that statistics can answer these kinds of questions\nunequivocally; instead, inference tells us what is likely, and what is unlikely. Researchers\ncannot prove that a new drug is effective in treating heart disease, even when they have data\nfrom a carefully controlled clinical trial. After all, it is entirely possible that there will be\nrandom variation in the outcomes of patients in the treatment and control groups that are\nunrelated to the new drug. If 53 out of 100 patients taking the new heart disease medication\nshowed marked improvement compared with 49 patients out of 100 receiving a placebo, we\nwould not immediately conclude that the new medication is effective. This is an outcome that\ncan easily be explained by chance variation between the two groups rather than by the new\ndrug. But suppose instead that 91 out of 100 patients receiving the new drug show marked\nimprovement, compared with 49 out of 100 patients in the control group. It is still possible that\nthis impressive result is unrelated to the new drug; the patients in the treatment group may be\nparticularly lucky or resilient. But that is now a much less likely explanation. In the formal\nlanguage of statistical inference, researchers would likely conclude the following: (1) If the\nexperimental drug has no effect, we would rarely see this amount of variation in outcomes\nbetween those who are receiving the drug and those who are taking the placebo. (2) It is\ntherefore highly improbable that the drug has no positive effect. (3) The alternative---and more\nlikely---explanation for the pattern of data observed is that the experimental drug has a positive\neffect. Statistical inference is the process by which the data speak to us, enabling us to draw", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nIs a new drug effective in treating heart disease? Do cell phones cause brain cancer? Please note that I’m not claiming that statistics can answer these kinds of questions\nunequivocally; instead, inference tells us what is likely, and what is unlikely. Researchers\ncannot prove that a new drug is effective in treating heart disease, even when they have data\nfrom a carefully controlled clinical trial. After all, it is entirely possible that there will be\nrandom variation in the outcomes of patients in the treatment and control groups that are\nunrelated to the new drug. If 53 out of 100 patients taking the new heart disease medication\nshowed marked improvement compared with 49 patients out of 100 receiving a placebo, we\nwould not immediately conclude that the new medication is effective. This is an outcome that\ncan easily be explained by chance variation between the two groups rather than by the new\ndrug. But suppose instead that 91 out of 100 patients receiving the new drug show marked\nimprovement, compared with 49 out of 100 patients in the control group. It is still possible that\nthis impressive result is unrelated to the new drug; the patients in the treatment group may be\nparticularly lucky or resilient. But that is now a much less likely explanation. In the formal\nlanguage of statistical inference, researchers would likely conclude the following: (1) If the\nexperimental drug has no effect, we would rarely see this amount of variation in outcomes\nbetween those who are receiving the drug and those who are taking the placebo. (2) It is\ntherefore highly improbable that the drug has no positive effect. (3) The alternative---and more\nlikely---explanation for the pattern of data observed is that the experimental drug has a positive\neffect. Statistical inference is the process by which the data speak to us, enabling us to draw", "tokens": 380, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 108, "segment_id": "00108", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000187"}
{"type": "chunk", "text": "meaningful conclusions. This is the payoff! The point of statistics is not to do myriad rigorous\nmathematical calculations; the point is to gain insight into meaningful social phenomena. Statistical inference is really just the marriage of two concepts that we’ve already discussed:\ndata and probability (with a little help from the central limit theorem). I have taken one major\nmethodological shortcut in this chapter. All of the examples will assume that we are working\nwith large, properly drawn samples. This assumption means that the central limit theorem\napplies, and that the mean and standard deviation for any sample will be roughly the same as\nthe mean and standard deviation for the population from which it is drawn. Both of these things\nmake our calculations easier. Statistical inference is not dependent on this simplifying assumption, but the assorted\nmethodological fixes for dealing with small samples or imperfect data often get in the way of\nunderstanding the big picture. The purpose here is to introduce the power of statistical\ninference and to explain how it works. Once you get that, it’s easy enough to layer on\ncomplexity. One of the most common tools in statistical inference is hypothesis testing. Actually, I’ve\nalready introduced this concept---just without the fancy terminology. As noted above, statistics\nalone cannot prove anything; instead, we use statistical inference to accept or reject\nexplanations on the basis of their relative likelihood. To be more precise, any statistical\ninference begins with an implicit or explicit null hypothesis. This is our starting assumption,\nwhich will be rejected or not on the basis of subsequent statistical analysis. If we reject the\nnull hypothesis, then we typically accept some alternative hypothesis that is more consistent\nwith the data observed. For example, in a court of law the starting assumption, or null\nhypothesis, is that the defendant is innocent. The job of the prosecution is to persuade the\njudge or jury to reject that assumption and accept the alternative hypothesis, which is that the\ndefendant is guilty. As a matter of logic, the alternative hypothesis is a conclusion that must be\ntrue if we can reject the null hypothesis. Consider some examples. Null hypothesis: This new experimental drug is no more effective at preventing malaria than\n\na placebo. Alternative hypothesis: This new experimental drug can help to prevent malaria. The data: One group is randomly chosen to receive the new experimental drug, and a control\ngroup receives a placebo.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nmeaningful conclusions. This is the payoff! The point of statistics is not to do myriad rigorous\nmathematical calculations; the point is to gain insight into meaningful social phenomena. Statistical inference is really just the marriage of two concepts that we’ve already discussed:\ndata and probability (with a little help from the central limit theorem). I have taken one major\nmethodological shortcut in this chapter. All of the examples will assume that we are working\nwith large, properly drawn samples. This assumption means that the central limit theorem\napplies, and that the mean and standard deviation for any sample will be roughly the same as\nthe mean and standard deviation for the population from which it is drawn. Both of these things\nmake our calculations easier. Statistical inference is not dependent on this simplifying assumption, but the assorted\nmethodological fixes for dealing with small samples or imperfect data often get in the way of\nunderstanding the big picture. The purpose here is to introduce the power of statistical\ninference and to explain how it works. Once you get that, it’s easy enough to layer on\ncomplexity. One of the most common tools in statistical inference is hypothesis testing. Actually, I’ve\nalready introduced this concept---just without the fancy terminology. As noted above, statistics\nalone cannot prove anything; instead, we use statistical inference to accept or reject\nexplanations on the basis of their relative likelihood. To be more precise, any statistical\ninference begins with an implicit or explicit null hypothesis. This is our starting assumption,\nwhich will be rejected or not on the basis of subsequent statistical analysis. If we reject the\nnull hypothesis, then we typically accept some alternative hypothesis that is more consistent\nwith the data observed. For example, in a court of law the starting assumption, or null\nhypothesis, is that the defendant is innocent. The job of the prosecution is to persuade the\njudge or jury to reject that assumption and accept the alternative hypothesis, which is that the\ndefendant is guilty. As a matter of logic, the alternative hypothesis is a conclusion that must be\ntrue if we can reject the null hypothesis. Consider some examples. Null hypothesis: This new experimental drug is no more effective at preventing malaria than\n\na placebo. Alternative hypothesis: This new experimental drug can help to prevent malaria. The data: One group is randomly chosen to receive the new experimental drug, and a control\ngroup receives a placebo.", "tokens": 495, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 109, "segment_id": "00109", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000188"}
{"type": "chunk", "text": "The job of the prosecution is to persuade the\njudge or jury to reject that assumption and accept the alternative hypothesis, which is that the\ndefendant is guilty. As a matter of logic, the alternative hypothesis is a conclusion that must be\ntrue if we can reject the null hypothesis. Consider some examples. Null hypothesis: This new experimental drug is no more effective at preventing malaria than\n\na placebo. Alternative hypothesis: This new experimental drug can help to prevent malaria. The data: One group is randomly chosen to receive the new experimental drug, and a control\ngroup receives a placebo. At the end of some period of time, the group receiving the\nexperimental drug has far fewer cases of malaria than the control group. This would be an\nextremely unlikely outcome if the experimental drug had no medical impact. As a result, we\nreject the null hypothesis that the new drug has no impact (beyond that of a placebo), and we\naccept the logical alternative, which is our alternative hypothesis: This new experimental drug\ncan help to prevent malaria. This methodological approach is strange enough that we should do one more example. Again, note that the null hypothesis and alternative hypothesis are logical complements. If one\nis true, the other is not true. Or, if we reject one statement, we must accept the other. Null hypothesis: Substance abuse treatment for prisoners does not reduce their rearrest rate\n\nafter leaving prison.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThe job of the prosecution is to persuade the\njudge or jury to reject that assumption and accept the alternative hypothesis, which is that the\ndefendant is guilty. As a matter of logic, the alternative hypothesis is a conclusion that must be\ntrue if we can reject the null hypothesis. Consider some examples. Null hypothesis: This new experimental drug is no more effective at preventing malaria than\n\na placebo. Alternative hypothesis: This new experimental drug can help to prevent malaria. The data: One group is randomly chosen to receive the new experimental drug, and a control\ngroup receives a placebo. At the end of some period of time, the group receiving the\nexperimental drug has far fewer cases of malaria than the control group. This would be an\nextremely unlikely outcome if the experimental drug had no medical impact. As a result, we\nreject the null hypothesis that the new drug has no impact (beyond that of a placebo), and we\naccept the logical alternative, which is our alternative hypothesis: This new experimental drug\ncan help to prevent malaria. This methodological approach is strange enough that we should do one more example. Again, note that the null hypothesis and alternative hypothesis are logical complements. If one\nis true, the other is not true. Or, if we reject one statement, we must accept the other. Null hypothesis: Substance abuse treatment for prisoners does not reduce their rearrest rate\n\nafter leaving prison.", "tokens": 289, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 109, "segment_id": "00109", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000189"}
{"type": "chunk", "text": "Alternative hypothesis: Substance abuse treatment for prisoners will make them less likely\n\nto be rearrested after they are released. The (hypothetical) data: Prisoners were randomly assigned into two groups; the “treatment”\ngroup received substance abuse treatment and the control group did not. (This is one of those\ncool occasions when the treatment group actually gets treatment!) At the end of five years, both\ngroups have similar rearrest rates. In this case, we cannot reject the null hypothesis.* The data\nhave given us no reason to discard our beginning assumption that substance abuse treatment is\nnot an effective tool for keeping ex-offenders from returning to prison. It may seem counterintuitive, but researchers often create a null hypothesis in hopes of being\nable to reject it. In both of the examples above, a research “success” (finding a new malaria\ndrug or reducing recidivism) involved rejecting the null hypothesis. The data made that\npossible in only one of the cases (the malaria drug). In a courtroom, the threshold for rejecting the presumption of innocence is the qualitative\nassessment that the defendant is “guilty beyond a reasonable doubt.” The judge or jury is left\nto define what exactly that means. Statistics harnesses the same basic idea, but “guilty beyond\na reasonable doubt” is defined quantitatively instead. Researchers typically ask, If the null\nhypothesis is true, how likely is it that we would observe this pattern of data by chance? To\nuse a familiar example, medical researchers might ask, If this experimental drug has no effect\non heart disease (our null hypothesis), how likely is it that 91 out of 100 patients getting the\ndrug would show improvement compared with only 49 out of 100 patients getting a placebo? If\nthe data suggest that the null hypothesis is extremely unlikely---as in this medical example---\nthen we must reject it and accept the alternative hypothesis (that the drug is effective in treating\nheart disease). In that vein, let us revisit the Atlanta standardized cheating scandal alluded to at several\npoints in the book. The Atlanta test score results were first flagged because of a high number\nof “wrong-to-right” erasures. Obviously students taking standardized exams erase answers all\nthe time. And some groups of students may be particularly lucky in their changes, without any\ncheating necessarily being involved.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nAlternative hypothesis: Substance abuse treatment for prisoners will make them less likely\n\nto be rearrested after they are released. The (hypothetical) data: Prisoners were randomly assigned into two groups; the “treatment”\ngroup received substance abuse treatment and the control group did not. (This is one of those\ncool occasions when the treatment group actually gets treatment!) At the end of five years, both\ngroups have similar rearrest rates. In this case, we cannot reject the null hypothesis.* The data\nhave given us no reason to discard our beginning assumption that substance abuse treatment is\nnot an effective tool for keeping ex-offenders from returning to prison. It may seem counterintuitive, but researchers often create a null hypothesis in hopes of being\nable to reject it. In both of the examples above, a research “success” (finding a new malaria\ndrug or reducing recidivism) involved rejecting the null hypothesis. The data made that\npossible in only one of the cases (the malaria drug). In a courtroom, the threshold for rejecting the presumption of innocence is the qualitative\nassessment that the defendant is “guilty beyond a reasonable doubt.” The judge or jury is left\nto define what exactly that means. Statistics harnesses the same basic idea, but “guilty beyond\na reasonable doubt” is defined quantitatively instead. Researchers typically ask, If the null\nhypothesis is true, how likely is it that we would observe this pattern of data by chance? To\nuse a familiar example, medical researchers might ask, If this experimental drug has no effect\non heart disease (our null hypothesis), how likely is it that 91 out of 100 patients getting the\ndrug would show improvement compared with only 49 out of 100 patients getting a placebo? If\nthe data suggest that the null hypothesis is extremely unlikely---as in this medical example---\nthen we must reject it and accept the alternative hypothesis (that the drug is effective in treating\nheart disease). In that vein, let us revisit the Atlanta standardized cheating scandal alluded to at several\npoints in the book. The Atlanta test score results were first flagged because of a high number\nof “wrong-to-right” erasures. Obviously students taking standardized exams erase answers all\nthe time. And some groups of students may be particularly lucky in their changes, without any\ncheating necessarily being involved.", "tokens": 486, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 110, "segment_id": "00110", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000190"}
{"type": "chunk", "text": "If\nthe data suggest that the null hypothesis is extremely unlikely---as in this medical example---\nthen we must reject it and accept the alternative hypothesis (that the drug is effective in treating\nheart disease). In that vein, let us revisit the Atlanta standardized cheating scandal alluded to at several\npoints in the book. The Atlanta test score results were first flagged because of a high number\nof “wrong-to-right” erasures. Obviously students taking standardized exams erase answers all\nthe time. And some groups of students may be particularly lucky in their changes, without any\ncheating necessarily being involved. For that reason, the null hypothesis is that the standardized\ntest scores for any particular school district are legitimate and that any irregular patterns of\nerasures are merely a product of chance. We certainly do not want to be punishing students or\nadministrators because an unusually high proportion of students happened to make sensible\nchanges to their answer sheets in the final minutes of an important state exam. But “unusually high” does not begin to describe what was happening in Atlanta. Some\nclassrooms had answer sheets on which the number of wrong-to-right erasures were twenty to\nfifty standard deviations above the state norm. (To put this in perspective, remember that most\nobservations in a distribution typically fall within two standard deviations of the mean.) So\nhow likely was it that Atlanta students happened to erase massive numbers of wrong answers\nand replace them with correct answers just as a matter of chance? The official who analyzed\nthe data described the probability of the Atlanta pattern occurring without cheating as roughly\nequal to the chance of having 70,000 people show up for a football game at the Georgia Dome", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nIf\nthe data suggest that the null hypothesis is extremely unlikely---as in this medical example---\nthen we must reject it and accept the alternative hypothesis (that the drug is effective in treating\nheart disease). In that vein, let us revisit the Atlanta standardized cheating scandal alluded to at several\npoints in the book. The Atlanta test score results were first flagged because of a high number\nof “wrong-to-right” erasures. Obviously students taking standardized exams erase answers all\nthe time. And some groups of students may be particularly lucky in their changes, without any\ncheating necessarily being involved. For that reason, the null hypothesis is that the standardized\ntest scores for any particular school district are legitimate and that any irregular patterns of\nerasures are merely a product of chance. We certainly do not want to be punishing students or\nadministrators because an unusually high proportion of students happened to make sensible\nchanges to their answer sheets in the final minutes of an important state exam. But “unusually high” does not begin to describe what was happening in Atlanta. Some\nclassrooms had answer sheets on which the number of wrong-to-right erasures were twenty to\nfifty standard deviations above the state norm. (To put this in perspective, remember that most\nobservations in a distribution typically fall within two standard deviations of the mean.) So\nhow likely was it that Atlanta students happened to erase massive numbers of wrong answers\nand replace them with correct answers just as a matter of chance? The official who analyzed\nthe data described the probability of the Atlanta pattern occurring without cheating as roughly\nequal to the chance of having 70,000 people show up for a football game at the Georgia Dome", "tokens": 342, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 110, "segment_id": "00110", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000191"}
{"type": "chunk", "text": "who all happen to be over seven feet tall.2 Could it happen? Yes. Is it likely? Not so much. Georgia officials still could not convict anybody of wrongdoing, just as my professor could\nnot (and should not) have had me thrown out of school because my final exam grade in\nstatistics was out of sync with my midterm grade. Atlanta officials could not prove that\ncheating was going on. They could, however, reject the null hypothesis that the results were\nlegitimate. And they could do so with a “high degree of confidence,” meaning that the observed\npattern was nearly impossible among normal test takers. They therefore explicitly accepted the\nalternative hypothesis, which is that something fishy was going on. (I suspect they used more\nofficial-sounding language.) Subsequent investigation did in fact uncover the “smoking\nerasers.” There were reports of teachers changing answers, giving out answers, allowing lowscoring children to copy from high-scoring children, and even pointing to answers while\nstanding over students’ desks. The most egregious cheating involved a group of teachers who\nheld a weekend pizza party during which they went through exam sheets and changed students’\nanswers. In the Atlanta example, we could reject the null hypothesis of “no cheating” because the\npattern of test results was so wildly improbable in the absence of foul play. But how\nimplausible does the null hypothesis have to be before we can reject it and invite some\nalternative explanation? One of the most common thresholds that researchers use for rejecting a null hypothesis is 5\npercent, which is often written in decimal form: .05. This probability is known as a significance\nlevel, and it represents the upper bound for the likelihood of observing some pattern of data if\nthe null hypothesis were true. Stick with me for a moment, because it’s not really that\ncomplicated. Let’s think about a significance level of .05. We can reject a null hypothesis at the .05 level\nif there is less than a 5 percent chance of getting an outcome at least as extreme as what we’ve\nobserved if the null hypothesis were true. A simple example can make this much clearer. I hate\nto do this to you, but assume once again that you’ve been put on missing-bus duty (in part\nbecause of your valiant efforts in the last chapter).", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nwho all happen to be over seven feet tall.2 Could it happen? Yes. Is it likely? Not so much. Georgia officials still could not convict anybody of wrongdoing, just as my professor could\nnot (and should not) have had me thrown out of school because my final exam grade in\nstatistics was out of sync with my midterm grade. Atlanta officials could not prove that\ncheating was going on. They could, however, reject the null hypothesis that the results were\nlegitimate. And they could do so with a “high degree of confidence,” meaning that the observed\npattern was nearly impossible among normal test takers. They therefore explicitly accepted the\nalternative hypothesis, which is that something fishy was going on. (I suspect they used more\nofficial-sounding language.) Subsequent investigation did in fact uncover the “smoking\nerasers.” There were reports of teachers changing answers, giving out answers, allowing lowscoring children to copy from high-scoring children, and even pointing to answers while\nstanding over students’ desks. The most egregious cheating involved a group of teachers who\nheld a weekend pizza party during which they went through exam sheets and changed students’\nanswers. In the Atlanta example, we could reject the null hypothesis of “no cheating” because the\npattern of test results was so wildly improbable in the absence of foul play. But how\nimplausible does the null hypothesis have to be before we can reject it and invite some\nalternative explanation? One of the most common thresholds that researchers use for rejecting a null hypothesis is 5\npercent, which is often written in decimal form: .05. This probability is known as a significance\nlevel, and it represents the upper bound for the likelihood of observing some pattern of data if\nthe null hypothesis were true. Stick with me for a moment, because it’s not really that\ncomplicated. Let’s think about a significance level of .05. We can reject a null hypothesis at the .05 level\nif there is less than a 5 percent chance of getting an outcome at least as extreme as what we’ve\nobserved if the null hypothesis were true. A simple example can make this much clearer. I hate\nto do this to you, but assume once again that you’ve been put on missing-bus duty (in part\nbecause of your valiant efforts in the last chapter).", "tokens": 484, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 111, "segment_id": "00111", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000192"}
{"type": "chunk", "text": "Stick with me for a moment, because it’s not really that\ncomplicated. Let’s think about a significance level of .05. We can reject a null hypothesis at the .05 level\nif there is less than a 5 percent chance of getting an outcome at least as extreme as what we’ve\nobserved if the null hypothesis were true. A simple example can make this much clearer. I hate\nto do this to you, but assume once again that you’ve been put on missing-bus duty (in part\nbecause of your valiant efforts in the last chapter). Only now you are working full-time for the\nresearchers at the Changing Lives study, and they have given you some excellent data to help\ninform your work. Each bus operated by the organizers of the study has roughly 60 passengers,\nso we can treat the passengers on any bus as a random sample drawn from the entire Changing\nLives population. You are awakened early one morning by the news that a bus in the Boston\narea has been hijacked by a pro-obesity terrorist group.* Your job is to drop from a helicopter\nonto the roof of the moving bus, sneak inside through the emergency exit, and then stealthily\ndetermine whether the passengers are Changing Lives participants, solely on the basis of their\nweights. (Seriously, this is no more implausible than most action-adventure plots, and it’s a lot\nmore educational.)\n\nAs the helicopter takes off from the commando base, you are given a machine gun, several\ngrenades, a watch that also functions as a high-resolution video camera, and the data that we\ncalculated in the last chapter on the mean weight and standard error for samples drawn from the\nChanging Lives participants. Any random sample of 60 participants will have an expected", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nStick with me for a moment, because it’s not really that\ncomplicated. Let’s think about a significance level of .05. We can reject a null hypothesis at the .05 level\nif there is less than a 5 percent chance of getting an outcome at least as extreme as what we’ve\nobserved if the null hypothesis were true. A simple example can make this much clearer. I hate\nto do this to you, but assume once again that you’ve been put on missing-bus duty (in part\nbecause of your valiant efforts in the last chapter). Only now you are working full-time for the\nresearchers at the Changing Lives study, and they have given you some excellent data to help\ninform your work. Each bus operated by the organizers of the study has roughly 60 passengers,\nso we can treat the passengers on any bus as a random sample drawn from the entire Changing\nLives population. You are awakened early one morning by the news that a bus in the Boston\narea has been hijacked by a pro-obesity terrorist group.* Your job is to drop from a helicopter\nonto the roof of the moving bus, sneak inside through the emergency exit, and then stealthily\ndetermine whether the passengers are Changing Lives participants, solely on the basis of their\nweights. (Seriously, this is no more implausible than most action-adventure plots, and it’s a lot\nmore educational.)\n\nAs the helicopter takes off from the commando base, you are given a machine gun, several\ngrenades, a watch that also functions as a high-resolution video camera, and the data that we\ncalculated in the last chapter on the mean weight and standard error for samples drawn from the\nChanging Lives participants. Any random sample of 60 participants will have an expected", "tokens": 367, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 111, "segment_id": "00111", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000193"}
{"type": "chunk", "text": "mean weight of 162 pounds and standard deviation of 36 pounds, since that is the mean and\nstandard deviation for all participants in the study (the population). With those data, we can\ncalculate the standard error for the sample mean:\n At mission control, the\nfollowing distribution is scanned onto the inside of your right retina, so that you can refer to it\nafter penetrating the moving bus and secretly weighing all the passengers inside. Distribution of Sample Means\n\nAs the distribution above shows, we would expect roughly 95 percent of all 60-person\nsamples drawn from the Changing Lives participants to have a mean weight within two standard\nerrors of the population mean, or roughly between 153 pounds and 171 pounds.* Conversely,\nonly 5 times out of 100 would a sample of 60 persons randomly drawn from the Changing\nLives participants have a mean weight that is greater than 171 pounds or less than 153 pounds. (You are conducting what is known as a “two-tailed” hypothesis test; the difference between\nthis and a “one-tailed” test will be covered in an appendix at the end of the chapter.) Your\nhandlers on the counterterrorism task force have decided that .05 is the significance level for\nyour mission. If the mean weight of the 60 passengers on the hijacked bus is above 171 or\nbelow 153, then you will reject the null hypothesis that the bus contains Changing Lives\nparticipants, accept the alternative hypothesis that the bus contains 60 people headed\nsomewhere else, and await further orders. You successfully drop into the moving bus and secretly weigh all the passengers. The mean\nweight for this 60-person sample is 136 pounds, which falls more than two standard errors\nbelow the mean. (Another important clue is that all of the passengers are children wearing\n“Glendale Hockey Camp” T-shirts.)\n\nPer your mission instructions, you can reject the null hypothesis that this bus contains a\nrandom sample of 60 Changing Lives study participants at the .05 significance level.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nmean weight of 162 pounds and standard deviation of 36 pounds, since that is the mean and\nstandard deviation for all participants in the study (the population). With those data, we can\ncalculate the standard error for the sample mean:\n At mission control, the\nfollowing distribution is scanned onto the inside of your right retina, so that you can refer to it\nafter penetrating the moving bus and secretly weighing all the passengers inside. Distribution of Sample Means\n\nAs the distribution above shows, we would expect roughly 95 percent of all 60-person\nsamples drawn from the Changing Lives participants to have a mean weight within two standard\nerrors of the population mean, or roughly between 153 pounds and 171 pounds.* Conversely,\nonly 5 times out of 100 would a sample of 60 persons randomly drawn from the Changing\nLives participants have a mean weight that is greater than 171 pounds or less than 153 pounds. (You are conducting what is known as a “two-tailed” hypothesis test; the difference between\nthis and a “one-tailed” test will be covered in an appendix at the end of the chapter.) Your\nhandlers on the counterterrorism task force have decided that .05 is the significance level for\nyour mission. If the mean weight of the 60 passengers on the hijacked bus is above 171 or\nbelow 153, then you will reject the null hypothesis that the bus contains Changing Lives\nparticipants, accept the alternative hypothesis that the bus contains 60 people headed\nsomewhere else, and await further orders. You successfully drop into the moving bus and secretly weigh all the passengers. The mean\nweight for this 60-person sample is 136 pounds, which falls more than two standard errors\nbelow the mean. (Another important clue is that all of the passengers are children wearing\n“Glendale Hockey Camp” T-shirts.)\n\nPer your mission instructions, you can reject the null hypothesis that this bus contains a\nrandom sample of 60 Changing Lives study participants at the .05 significance level.", "tokens": 414, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 112, "segment_id": "00112", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000194"}
{"type": "chunk", "text": "You successfully drop into the moving bus and secretly weigh all the passengers. The mean\nweight for this 60-person sample is 136 pounds, which falls more than two standard errors\nbelow the mean. (Another important clue is that all of the passengers are children wearing\n“Glendale Hockey Camp” T-shirts.)\n\nPer your mission instructions, you can reject the null hypothesis that this bus contains a\nrandom sample of 60 Changing Lives study participants at the .05 significance level. This\nmeans (1) the mean weight on the bus falls into a range that we would expect to observe only 5\ntimes in 100 if the null hypothesis were true and this were really a bus full of Changing Lives\npassengers; (2) you can reject the null hypothesis at the .05 significance level; and (3) on\naverage, 95 times out of 100 you will have correctly rejected the null hypothesis, and 5 times\nout of 100 you will be wrong, meaning that you have concluded that this is not a bus of", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nYou successfully drop into the moving bus and secretly weigh all the passengers. The mean\nweight for this 60-person sample is 136 pounds, which falls more than two standard errors\nbelow the mean. (Another important clue is that all of the passengers are children wearing\n“Glendale Hockey Camp” T-shirts.)\n\nPer your mission instructions, you can reject the null hypothesis that this bus contains a\nrandom sample of 60 Changing Lives study participants at the .05 significance level. This\nmeans (1) the mean weight on the bus falls into a range that we would expect to observe only 5\ntimes in 100 if the null hypothesis were true and this were really a bus full of Changing Lives\npassengers; (2) you can reject the null hypothesis at the .05 significance level; and (3) on\naverage, 95 times out of 100 you will have correctly rejected the null hypothesis, and 5 times\nout of 100 you will be wrong, meaning that you have concluded that this is not a bus of", "tokens": 213, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 112, "segment_id": "00112", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000195"}
{"type": "chunk", "text": "Changing Lives participants, when in fact it is. This sample of Changing Lives folks just\nhappens to have a mean weight that is particularly high or low relative to the mean for the\nstudy participants overall. The mission is not quite over. Your handler at mission control (played by Angelina Jolie in\nthe film version of this example) asks you to calculate a p-value for your result. The p-value is\nthe specific probability of getting a result at least as extreme as the one you’ve observed if the\nnull hypothesis is true. The mean weight for the passengers on this bus is 136, which is 5.7\nstandard errors below the mean for the Changing Lives study participants. The probability of\ngetting a result at least that extreme if this really were a sample of Changing Lives participants\nis less than .0001. (In a research document, this would be reported as p<.0001.) With your\nmission complete, you leap from the moving bus and land safely in the passenger seat of a\nconvertible driving in an adjacent lane. [This story has a happy ending as well. Once the pro--obesity terrorists learn more about\nyour city’s International Festival of Sausage, they agree to abandon violence and work\npeacefully to promote obesity by expanding and promoting sausage festivals around the\nworld.]\n\nIf the .05 significance level seems somewhat arbitrary, that’s because it is. There is no single\nstandardized statistical threshold for rejecting a null hypothesis. Both .01 and .1 are also\nreasonably common thresholds for doing the kind of analysis described above. Obviously rejecting the null hypothesis at the .01 level (meaning that there is less than a 1 in\n100 chance of observing a result in this range if the null hypothesis were true) carries more\nstatistical heft than rejecting the null hypothesis at the .1 level (meaning that there is less than a\n1 in 10 chance of observing this result if the null hypothesis were true). The pros and cons of\ndifferent significance levels will be discussed later in the chapter. For now, the important point\nis that when we can reject a null hypothesis at some reasonable significance level, the results\nare said to be “statistically significant.”\n\nHere is what that means in real life.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nChanging Lives participants, when in fact it is. This sample of Changing Lives folks just\nhappens to have a mean weight that is particularly high or low relative to the mean for the\nstudy participants overall. The mission is not quite over. Your handler at mission control (played by Angelina Jolie in\nthe film version of this example) asks you to calculate a p-value for your result. The p-value is\nthe specific probability of getting a result at least as extreme as the one you’ve observed if the\nnull hypothesis is true. The mean weight for the passengers on this bus is 136, which is 5.7\nstandard errors below the mean for the Changing Lives study participants. The probability of\ngetting a result at least that extreme if this really were a sample of Changing Lives participants\nis less than .0001. (In a research document, this would be reported as p<.0001.) With your\nmission complete, you leap from the moving bus and land safely in the passenger seat of a\nconvertible driving in an adjacent lane. [This story has a happy ending as well. Once the pro--obesity terrorists learn more about\nyour city’s International Festival of Sausage, they agree to abandon violence and work\npeacefully to promote obesity by expanding and promoting sausage festivals around the\nworld.]\n\nIf the .05 significance level seems somewhat arbitrary, that’s because it is. There is no single\nstandardized statistical threshold for rejecting a null hypothesis. Both .01 and .1 are also\nreasonably common thresholds for doing the kind of analysis described above. Obviously rejecting the null hypothesis at the .01 level (meaning that there is less than a 1 in\n100 chance of observing a result in this range if the null hypothesis were true) carries more\nstatistical heft than rejecting the null hypothesis at the .1 level (meaning that there is less than a\n1 in 10 chance of observing this result if the null hypothesis were true). The pros and cons of\ndifferent significance levels will be discussed later in the chapter. For now, the important point\nis that when we can reject a null hypothesis at some reasonable significance level, the results\nare said to be “statistically significant.”\n\nHere is what that means in real life.", "tokens": 468, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 113, "segment_id": "00113", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000196"}
{"type": "chunk", "text": "The pros and cons of\ndifferent significance levels will be discussed later in the chapter. For now, the important point\nis that when we can reject a null hypothesis at some reasonable significance level, the results\nare said to be “statistically significant.”\n\nHere is what that means in real life. When you read in the newspaper that people who eat\ntwenty bran muffins a day have lower rates of colon cancer than people who don’t eat\nprodigious amounts of bran, the underlying academic research probably looked something like\nthis: (1) In some large data set, researchers determined that individuals who ate at least twenty\nbran muffins a day had a lower incidence of colon cancer than individuals who did not report\neating much bran. (2) The researchers’ null hypothesis was that eating bran muffins has no\nimpact on colon cancer. (3) The disparity in colon cancer outcomes between those who ate lots\nof bran muffins and those who didn’t could not easily be explained by chance alone. More\nspecifically, if eating bran muffins has no true association with colon cancer, the probability of\ngetting such a wide gap in cancer incidence between bran eaters and non--bran eaters by chance\nalone is lower than some threshold, such as .05. (This threshold should be established by the\nresearchers before they do their statistical analysis to avoid choosing a threshold after the fact\nthat is convenient for making the results look significant.) (4) The academic paper probably\ncontains a conclusion that says something along these lines: “We find a statistically significant\nassociation between daily consumption of twenty or more bran muffins and a reduced", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThe pros and cons of\ndifferent significance levels will be discussed later in the chapter. For now, the important point\nis that when we can reject a null hypothesis at some reasonable significance level, the results\nare said to be “statistically significant.”\n\nHere is what that means in real life. When you read in the newspaper that people who eat\ntwenty bran muffins a day have lower rates of colon cancer than people who don’t eat\nprodigious amounts of bran, the underlying academic research probably looked something like\nthis: (1) In some large data set, researchers determined that individuals who ate at least twenty\nbran muffins a day had a lower incidence of colon cancer than individuals who did not report\neating much bran. (2) The researchers’ null hypothesis was that eating bran muffins has no\nimpact on colon cancer. (3) The disparity in colon cancer outcomes between those who ate lots\nof bran muffins and those who didn’t could not easily be explained by chance alone. More\nspecifically, if eating bran muffins has no true association with colon cancer, the probability of\ngetting such a wide gap in cancer incidence between bran eaters and non--bran eaters by chance\nalone is lower than some threshold, such as .05. (This threshold should be established by the\nresearchers before they do their statistical analysis to avoid choosing a threshold after the fact\nthat is convenient for making the results look significant.) (4) The academic paper probably\ncontains a conclusion that says something along these lines: “We find a statistically significant\nassociation between daily consumption of twenty or more bran muffins and a reduced", "tokens": 336, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 113, "segment_id": "00113", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000197"}
{"type": "chunk", "text": "incidence of colon cancer. These results are significant at the .05 level.”\n\nWhen I subsequently read about that study in the Chicago Sun-Times while eating my\nbreakfast of bacon and eggs, the headline is probably more direct and interesting: “20 Bran\nMuffins a Day Help Keep Colon Cancer Away.” However, that newspaper headline, while\nmuch more interesting to read than the academic paper, may also be introducing a serious\ninaccuracy. The study does not actually claim that eating bran muffins lowers an individual’s\nrisk of getting colon cancer; it merely shows a negative correlation between the consumption of\nbran muffins and the incidence of colon cancer in one large data set. This statistical association\nis not sufficient to prove that the bran muffins cause the improved health outcome. After all,\nthe kind of people who eat bran muffins (particularly twenty a day!) may do lots of other things\nthat lower their cancer risk, such as eating less red meat, exercising regularly, getting screened\nfor cancer, and so on. (This is the “healthy user bias” from Chapter 7.) Is it the bran muffins at\nwork here, or is it other behaviors or personal attributes that happen to be shared by people\nwho eat a lot of bran muffins? This distinction between correlation and causation is crucial to\nthe proper interpretation of statistical results. We will revisit the idea that “correlation does not\nequal causation” later in the book. I should also point out that statistical significance says nothing about the size of the\nassociation. People who eat lots of bran muffins may have a lower incidence of colon cancer---\nbut how much lower? The difference in colon cancer rates for bran muffin eaters and non--bran\nmuffin eaters may be trivial; the finding of statistical significance means only that the observed\neffect, however tiny, is not likely to be a coincidence. Suppose you stumble across a welldesigned study that has found a statistically significant positive relationship between eating a\nbanana before the SAT and achieving a higher score on the math portion of the test. One of the\nfirst questions you want to ask is, How big is this effect? It could easily be .9 points; on a test\nwith a mean score of 500, that is not a life-changing figure. In Chapter 11, we will return to this\ncrucial distinction between size and significance when it comes to interpreting statistical\nresults.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nincidence of colon cancer. These results are significant at the .05 level.”\n\nWhen I subsequently read about that study in the Chicago Sun-Times while eating my\nbreakfast of bacon and eggs, the headline is probably more direct and interesting: “20 Bran\nMuffins a Day Help Keep Colon Cancer Away.” However, that newspaper headline, while\nmuch more interesting to read than the academic paper, may also be introducing a serious\ninaccuracy. The study does not actually claim that eating bran muffins lowers an individual’s\nrisk of getting colon cancer; it merely shows a negative correlation between the consumption of\nbran muffins and the incidence of colon cancer in one large data set. This statistical association\nis not sufficient to prove that the bran muffins cause the improved health outcome. After all,\nthe kind of people who eat bran muffins (particularly twenty a day!) may do lots of other things\nthat lower their cancer risk, such as eating less red meat, exercising regularly, getting screened\nfor cancer, and so on. (This is the “healthy user bias” from Chapter 7.) Is it the bran muffins at\nwork here, or is it other behaviors or personal attributes that happen to be shared by people\nwho eat a lot of bran muffins? This distinction between correlation and causation is crucial to\nthe proper interpretation of statistical results. We will revisit the idea that “correlation does not\nequal causation” later in the book. I should also point out that statistical significance says nothing about the size of the\nassociation. People who eat lots of bran muffins may have a lower incidence of colon cancer---\nbut how much lower? The difference in colon cancer rates for bran muffin eaters and non--bran\nmuffin eaters may be trivial; the finding of statistical significance means only that the observed\neffect, however tiny, is not likely to be a coincidence. Suppose you stumble across a welldesigned study that has found a statistically significant positive relationship between eating a\nbanana before the SAT and achieving a higher score on the math portion of the test. One of the\nfirst questions you want to ask is, How big is this effect? It could easily be .9 points; on a test\nwith a mean score of 500, that is not a life-changing figure. In Chapter 11, we will return to this\ncrucial distinction between size and significance when it comes to interpreting statistical\nresults.", "tokens": 505, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 114, "segment_id": "00114", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000198"}
{"type": "chunk", "text": "Suppose you stumble across a welldesigned study that has found a statistically significant positive relationship between eating a\nbanana before the SAT and achieving a higher score on the math portion of the test. One of the\nfirst questions you want to ask is, How big is this effect? It could easily be .9 points; on a test\nwith a mean score of 500, that is not a life-changing figure. In Chapter 11, we will return to this\ncrucial distinction between size and significance when it comes to interpreting statistical\nresults. Meanwhile, a finding that there is “no statistically significant association” between two\nvariables means that any relationship between the two variables can reasonably be explained\nby chance alone. The New York Times recently ran an exposé on technology companies\npeddling software that they claim improves student performance, when the data suggest\notherwise.3 According to the article, Carnegie Mellon University sells a software program\ncalled Cognitive Tutor with this bold claim: “Revolutionary Math Curricula. Revolutionary\nResults.” Yet an assessment of Cognitive Tutor conducted by the U.S. Department of\nEducation concluded that the product had “no discernible effects” on the test scores of high\nschool students. (The Times suggested that the appropriate marketing campaign should be\n“Undistinguished Math Curricula. Unproven Results.”) In fact, a study of ten software products\ndesigned to teach skills such as math or reading found that nine of them “did not have\nstatistically significant effects on test scores.” In other words, federal researchers cannot rule\nout mere chance as the cause of any variation in the performance of students who use these\nsoftware products and students who do not.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nSuppose you stumble across a welldesigned study that has found a statistically significant positive relationship between eating a\nbanana before the SAT and achieving a higher score on the math portion of the test. One of the\nfirst questions you want to ask is, How big is this effect? It could easily be .9 points; on a test\nwith a mean score of 500, that is not a life-changing figure. In Chapter 11, we will return to this\ncrucial distinction between size and significance when it comes to interpreting statistical\nresults. Meanwhile, a finding that there is “no statistically significant association” between two\nvariables means that any relationship between the two variables can reasonably be explained\nby chance alone. The New York Times recently ran an exposé on technology companies\npeddling software that they claim improves student performance, when the data suggest\notherwise.3 According to the article, Carnegie Mellon University sells a software program\ncalled Cognitive Tutor with this bold claim: “Revolutionary Math Curricula. Revolutionary\nResults.” Yet an assessment of Cognitive Tutor conducted by the U.S. Department of\nEducation concluded that the product had “no discernible effects” on the test scores of high\nschool students. (The Times suggested that the appropriate marketing campaign should be\n“Undistinguished Math Curricula. Unproven Results.”) In fact, a study of ten software products\ndesigned to teach skills such as math or reading found that nine of them “did not have\nstatistically significant effects on test scores.” In other words, federal researchers cannot rule\nout mere chance as the cause of any variation in the performance of students who use these\nsoftware products and students who do not.", "tokens": 347, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 114, "segment_id": "00114", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000199"}
{"type": "chunk", "text": "Let me pause here to remind you why all of this matters. An article in the Wall Street Journal\nin May of 2011 carried the headline “Link in Autism, Brain Size.” This is an important\nbreakthrough, as the causes of autism spectrum disorder remain elusive. The first sentence of\nthe Wall Street Journal story, which summarized a paper published in the Archives of General\nPsychiatry, reports, “Children with autism have larger brains than children without the disorder,\nand the growth appears to occur before age 2, according to a new study released on Monday.”4\nOn the basis of brain imaging conducted on 59 children with autism and 38 children without\nautism, researchers at the University of North Carolina reported that children with autism have\nbrains that are up to 10 percent larger than those of children of the same age without autism. Here is the relevant medical question: Is there a physiological difference in the brains of\nyoung children who have autism spectrum disorder? If so, this insight might lead to a better\nunderstanding of what causes the disorder and how it can be treated or prevented. And here is the relevant statistical question: Can researchers make sweeping inferences\nabout autism spectrum disorder in general that are based on a study of a seemingly small group\nof children with autism (59) and an even smaller control group (38)---a mere 97 subjects in all? The answer is yes. The researchers concluded that the probability of observing the differences\nin total brain size that they found in their two samples would be a mere 2 in 1,000 (p = .002) if\nthere is in fact no real difference in brain size between children with and without autism\nspectrum disorder in the overall population. I tracked down the original study in the Archives of General Psychiatry.5 The methods used\nby these researchers are no more sophisticated than the concepts we’ve covered so far. I will\ngive you a quick tour of the underpinnings of this socially and statistically significant result. First, you should recognize that each group of children, the 59 with autism and the 38 without\nautism, constitutes a reasonably large sample drawn from their respective populations---all\nchildren with and without autism spectrum disorder. The samples are large enough that the\ncentral limit will apply.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nLet me pause here to remind you why all of this matters. An article in the Wall Street Journal\nin May of 2011 carried the headline “Link in Autism, Brain Size.” This is an important\nbreakthrough, as the causes of autism spectrum disorder remain elusive. The first sentence of\nthe Wall Street Journal story, which summarized a paper published in the Archives of General\nPsychiatry, reports, “Children with autism have larger brains than children without the disorder,\nand the growth appears to occur before age 2, according to a new study released on Monday.”4\nOn the basis of brain imaging conducted on 59 children with autism and 38 children without\nautism, researchers at the University of North Carolina reported that children with autism have\nbrains that are up to 10 percent larger than those of children of the same age without autism. Here is the relevant medical question: Is there a physiological difference in the brains of\nyoung children who have autism spectrum disorder? If so, this insight might lead to a better\nunderstanding of what causes the disorder and how it can be treated or prevented. And here is the relevant statistical question: Can researchers make sweeping inferences\nabout autism spectrum disorder in general that are based on a study of a seemingly small group\nof children with autism (59) and an even smaller control group (38)---a mere 97 subjects in all? The answer is yes. The researchers concluded that the probability of observing the differences\nin total brain size that they found in their two samples would be a mere 2 in 1,000 (p = .002) if\nthere is in fact no real difference in brain size between children with and without autism\nspectrum disorder in the overall population. I tracked down the original study in the Archives of General Psychiatry.5 The methods used\nby these researchers are no more sophisticated than the concepts we’ve covered so far. I will\ngive you a quick tour of the underpinnings of this socially and statistically significant result. First, you should recognize that each group of children, the 59 with autism and the 38 without\nautism, constitutes a reasonably large sample drawn from their respective populations---all\nchildren with and without autism spectrum disorder. The samples are large enough that the\ncentral limit will apply.", "tokens": 471, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 115, "segment_id": "00115", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000200"}
{"type": "chunk", "text": "I tracked down the original study in the Archives of General Psychiatry.5 The methods used\nby these researchers are no more sophisticated than the concepts we’ve covered so far. I will\ngive you a quick tour of the underpinnings of this socially and statistically significant result. First, you should recognize that each group of children, the 59 with autism and the 38 without\nautism, constitutes a reasonably large sample drawn from their respective populations---all\nchildren with and without autism spectrum disorder. The samples are large enough that the\ncentral limit will apply. If you’ve already tried to block the last chapter out of your mind, I will\nremind you of what the central limit theorem tells us: (1) the sample means for any population\nwill be distributed roughly as a normal distribution around the true population mean; (2) we\nwould expect the sample mean and the sample standard deviation to be roughly equal to the\nmean and standard deviation for the population from which it is drawn; and (3) roughly 68\npercent of sample means will lie within one standard error of the population mean, roughly 95\npercent will lie within two standard errors of the population mean, and so on. In less technical language, this all means that any sample should look a lot like the\npopulation from which it is drawn; while every sample will be different, it would be relatively\nrare for the mean of a properly drawn sample to deviate by a huge amount from the mean for\nthe relevant underlying population. Similarly, we would also expect two samples drawn from\nthe same population to look a lot like each other. Or, to think about the situation somewhat\ndifferently, if we have two samples that have extremely dissimilar means, the most likely\nexplanation is that they came from different populations. Here is a quick intuitive example. Suppose your null hypothesis is that male professional\nbasketball players have the same mean height as the rest of the adult male population. You", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nI tracked down the original study in the Archives of General Psychiatry.5 The methods used\nby these researchers are no more sophisticated than the concepts we’ve covered so far. I will\ngive you a quick tour of the underpinnings of this socially and statistically significant result. First, you should recognize that each group of children, the 59 with autism and the 38 without\nautism, constitutes a reasonably large sample drawn from their respective populations---all\nchildren with and without autism spectrum disorder. The samples are large enough that the\ncentral limit will apply. If you’ve already tried to block the last chapter out of your mind, I will\nremind you of what the central limit theorem tells us: (1) the sample means for any population\nwill be distributed roughly as a normal distribution around the true population mean; (2) we\nwould expect the sample mean and the sample standard deviation to be roughly equal to the\nmean and standard deviation for the population from which it is drawn; and (3) roughly 68\npercent of sample means will lie within one standard error of the population mean, roughly 95\npercent will lie within two standard errors of the population mean, and so on. In less technical language, this all means that any sample should look a lot like the\npopulation from which it is drawn; while every sample will be different, it would be relatively\nrare for the mean of a properly drawn sample to deviate by a huge amount from the mean for\nthe relevant underlying population. Similarly, we would also expect two samples drawn from\nthe same population to look a lot like each other. Or, to think about the situation somewhat\ndifferently, if we have two samples that have extremely dissimilar means, the most likely\nexplanation is that they came from different populations. Here is a quick intuitive example. Suppose your null hypothesis is that male professional\nbasketball players have the same mean height as the rest of the adult male population. You", "tokens": 406, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 115, "segment_id": "00115", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000201"}
{"type": "chunk", "text": "randomly select a sample of 50 professional basketball players and a sample of 50 men who\ndo not play professional basketball. Suppose the mean height of your basketball sample is 6\nfeet 7 inches, and the mean height of the non--basketball players is 5 feet 10 inches (a 9-inch\ndifference). What is the probability of observing such a large difference in mean height\nbetween the two samples if in fact there is no difference in average height between professional\nbasketball players and all other men in the overall population? The nontechnical answer: very,\nvery, very low.*\n\nThe autism research paper has the same basic methodology. The paper compares several\nmeasures of brain size between the samples of children. (The brain measurements were done\nwith magnetic resonance imaging at age two, and again between ages four and five.) I’ll focus\non just one measurement, the total brain volume. The researchers’ null hypothesis was\npresumably that there are no anatomical differences in the brains of children with and without\nautism. The alternative hypothesis is that the brains of children with autism spectrum disorder\nare fundamentally different. Such a finding would still leave lots of questions, but it would\npoint to a direction for further inquiry. In this study, the children with autism spectrum disorder had a mean brain volume of 1310.4\ncubic centimeters; the children in the control group had a mean brain volume of 1238.8 cubic\ncentimeters. Thus, the difference in average brain volume between the two groups is 71.6\ncubic centimeters. How likely would this result be if in fact there were no difference in average\nbrain size in the general population between children who have autism spectrum disorder and\nchildren who do not? You may recall from the last chapter that we can create a standard error for each of our\n where s is the standard deviation of the sample and n is the number of\nsamples:\nobservations. The research paper gives us these figures. The standard error for the total brain\nvolume of the 59 children in the autism spectrum disorder sample is 13 cubic centimeters; the\nstandard error for the total brain volume of the 38 children in the control group is 18 cubic\ncentimeters. You will recall that the central limit theorem tells us that for 95 samples out of\n100, the sample mean is going to lie within two standard errors of the true population mean, in\none direction or the other.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nrandomly select a sample of 50 professional basketball players and a sample of 50 men who\ndo not play professional basketball. Suppose the mean height of your basketball sample is 6\nfeet 7 inches, and the mean height of the non--basketball players is 5 feet 10 inches (a 9-inch\ndifference). What is the probability of observing such a large difference in mean height\nbetween the two samples if in fact there is no difference in average height between professional\nbasketball players and all other men in the overall population? The nontechnical answer: very,\nvery, very low.*\n\nThe autism research paper has the same basic methodology. The paper compares several\nmeasures of brain size between the samples of children. (The brain measurements were done\nwith magnetic resonance imaging at age two, and again between ages four and five.) I’ll focus\non just one measurement, the total brain volume. The researchers’ null hypothesis was\npresumably that there are no anatomical differences in the brains of children with and without\nautism. The alternative hypothesis is that the brains of children with autism spectrum disorder\nare fundamentally different. Such a finding would still leave lots of questions, but it would\npoint to a direction for further inquiry. In this study, the children with autism spectrum disorder had a mean brain volume of 1310.4\ncubic centimeters; the children in the control group had a mean brain volume of 1238.8 cubic\ncentimeters. Thus, the difference in average brain volume between the two groups is 71.6\ncubic centimeters. How likely would this result be if in fact there were no difference in average\nbrain size in the general population between children who have autism spectrum disorder and\nchildren who do not? You may recall from the last chapter that we can create a standard error for each of our\n where s is the standard deviation of the sample and n is the number of\nsamples:\nobservations. The research paper gives us these figures. The standard error for the total brain\nvolume of the 59 children in the autism spectrum disorder sample is 13 cubic centimeters; the\nstandard error for the total brain volume of the 38 children in the control group is 18 cubic\ncentimeters. You will recall that the central limit theorem tells us that for 95 samples out of\n100, the sample mean is going to lie within two standard errors of the true population mean, in\none direction or the other.", "tokens": 511, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 116, "segment_id": "00116", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000202"}
{"type": "chunk", "text": "The research paper gives us these figures. The standard error for the total brain\nvolume of the 59 children in the autism spectrum disorder sample is 13 cubic centimeters; the\nstandard error for the total brain volume of the 38 children in the control group is 18 cubic\ncentimeters. You will recall that the central limit theorem tells us that for 95 samples out of\n100, the sample mean is going to lie within two standard errors of the true population mean, in\none direction or the other. As a result, we can infer from our sample that 95 times out of 100 the interval of 1310.4\ncubic centimeters ± 26 (which is two standard errors) will contain the average brain volume for\nall children with autism spectrum disorder. This expression is called a confidence interval. We\ncan say with 95 percent confidence that the range 1284.4 to 1336.4 cubic centimeters contains\nthe average total brain volume for children in the general population with autism spectrum\ndisorder. Using the same methodology, we can say with 95 percent confidence that the interval of\n1238.8 ± 36, or between 1202.8 and 1274.8 cubic centimeters, will include the average brain\nvolume for children in the general population who do not have autism spectrum disorder. Yes, there are a lot of numbers here. Perhaps you’ve just hurled the book across the room. *\nIf not, or if you then went and retrieved the book, what you should notice is that our confidence\nintervals do not overlap. The lower bound of our 95 percent confidence interval for the", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThe research paper gives us these figures. The standard error for the total brain\nvolume of the 59 children in the autism spectrum disorder sample is 13 cubic centimeters; the\nstandard error for the total brain volume of the 38 children in the control group is 18 cubic\ncentimeters. You will recall that the central limit theorem tells us that for 95 samples out of\n100, the sample mean is going to lie within two standard errors of the true population mean, in\none direction or the other. As a result, we can infer from our sample that 95 times out of 100 the interval of 1310.4\ncubic centimeters ± 26 (which is two standard errors) will contain the average brain volume for\nall children with autism spectrum disorder. This expression is called a confidence interval. We\ncan say with 95 percent confidence that the range 1284.4 to 1336.4 cubic centimeters contains\nthe average total brain volume for children in the general population with autism spectrum\ndisorder. Using the same methodology, we can say with 95 percent confidence that the interval of\n1238.8 ± 36, or between 1202.8 and 1274.8 cubic centimeters, will include the average brain\nvolume for children in the general population who do not have autism spectrum disorder. Yes, there are a lot of numbers here. Perhaps you’ve just hurled the book across the room. *\nIf not, or if you then went and retrieved the book, what you should notice is that our confidence\nintervals do not overlap. The lower bound of our 95 percent confidence interval for the", "tokens": 343, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 116, "segment_id": "00116", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000203"}
{"type": "chunk", "text": "average brain size of children with autism in the general population (1284.4 cubic centimeters)\nis still higher than the upper bound for the 95 percent confidence interval for the average brain\nsize for young children in the population without autism (1274.8 cubic centimeters), as the\nfollowing diagram illustrates. This is the first clue that there may be an underlying anatomical difference in the brains of\nyoung children with autism spectrum disorder. Still, it’s just a clue. All of these inferences are\nbased on data from fewer than 100 children. Maybe we just have wacky samples. One final statistical procedure can bring all this to fruition. If statistics were an Olympic\nevent like figure skating, this would be the last program, after which elated fans throw\nbouquets of flowers onto the ice. We can calculate the exact probability of observing a\ndifference of means at least this large (1310.4 cubic centimeters versus 1238.8 cubic\ncentimeters) if there is really no difference in brain size between children with autism spectrum\nand all other children in the general population. We can find a p-value for the observed\ndifference in means. Lest you hurl the book across the room again, I have put the formula in an appendix. The\nintuition is quite straightforward. If we draw two large samples from the same population, we\nwould expect them to have very similar means. In fact, our best guess is that they will have\nidentical means. For example, if I were to select 100 players from the NBA and they had an\naverage height of 6 feet 7 inches, then I would expect another random sample of 100 players\nfrom the NBA to have a mean height close to 6 feet 7 inches. Okay, maybe the two samples\nwould be an inch or 2 apart. But it’s less likely that the means of the two samples will be 4\ninches apart---and even less likely that there will be a difference of 6 or 8 inches. It turns out\nthat we can calculate a standard error for the difference between two sample means; this\nstandard error gives us a measure of the dispersion we can expect, on average, when we\nsubtract one sample mean from the other. (As noted earlier, the formula is in the chapter\nappendix.) The important thing is that we can use this standard error to calculate the\nprobability that two samples come from the same population.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\naverage brain size of children with autism in the general population (1284.4 cubic centimeters)\nis still higher than the upper bound for the 95 percent confidence interval for the average brain\nsize for young children in the population without autism (1274.8 cubic centimeters), as the\nfollowing diagram illustrates. This is the first clue that there may be an underlying anatomical difference in the brains of\nyoung children with autism spectrum disorder. Still, it’s just a clue. All of these inferences are\nbased on data from fewer than 100 children. Maybe we just have wacky samples. One final statistical procedure can bring all this to fruition. If statistics were an Olympic\nevent like figure skating, this would be the last program, after which elated fans throw\nbouquets of flowers onto the ice. We can calculate the exact probability of observing a\ndifference of means at least this large (1310.4 cubic centimeters versus 1238.8 cubic\ncentimeters) if there is really no difference in brain size between children with autism spectrum\nand all other children in the general population. We can find a p-value for the observed\ndifference in means. Lest you hurl the book across the room again, I have put the formula in an appendix. The\nintuition is quite straightforward. If we draw two large samples from the same population, we\nwould expect them to have very similar means. In fact, our best guess is that they will have\nidentical means. For example, if I were to select 100 players from the NBA and they had an\naverage height of 6 feet 7 inches, then I would expect another random sample of 100 players\nfrom the NBA to have a mean height close to 6 feet 7 inches. Okay, maybe the two samples\nwould be an inch or 2 apart. But it’s less likely that the means of the two samples will be 4\ninches apart---and even less likely that there will be a difference of 6 or 8 inches. It turns out\nthat we can calculate a standard error for the difference between two sample means; this\nstandard error gives us a measure of the dispersion we can expect, on average, when we\nsubtract one sample mean from the other. (As noted earlier, the formula is in the chapter\nappendix.) The important thing is that we can use this standard error to calculate the\nprobability that two samples come from the same population.", "tokens": 509, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 117, "segment_id": "00117", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000204"}
{"type": "chunk", "text": "But it’s less likely that the means of the two samples will be 4\ninches apart---and even less likely that there will be a difference of 6 or 8 inches. It turns out\nthat we can calculate a standard error for the difference between two sample means; this\nstandard error gives us a measure of the dispersion we can expect, on average, when we\nsubtract one sample mean from the other. (As noted earlier, the formula is in the chapter\nappendix.) The important thing is that we can use this standard error to calculate the\nprobability that two samples come from the same population. Here is how it works:\n\n1. If two samples are drawn from the same population, our best guess for the difference\n\nbetween their means is zero. 2. The central limit theorem tells us that in repeated samples, the difference between the\ntwo means will be distributed roughly as a normal distribution. (Okay, have you come to\nlove the central limit theorem yet or not?)\n\n3. If the two samples really have come from the same population, then in roughly 68 cases\nout of 100, the difference between the two sample means will be within one standard\nerror of zero. And in roughly 95 cases out of 100, the difference between the two sample", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nBut it’s less likely that the means of the two samples will be 4\ninches apart---and even less likely that there will be a difference of 6 or 8 inches. It turns out\nthat we can calculate a standard error for the difference between two sample means; this\nstandard error gives us a measure of the dispersion we can expect, on average, when we\nsubtract one sample mean from the other. (As noted earlier, the formula is in the chapter\nappendix.) The important thing is that we can use this standard error to calculate the\nprobability that two samples come from the same population. Here is how it works:\n\n1. If two samples are drawn from the same population, our best guess for the difference\n\nbetween their means is zero. 2. The central limit theorem tells us that in repeated samples, the difference between the\ntwo means will be distributed roughly as a normal distribution. (Okay, have you come to\nlove the central limit theorem yet or not?)\n\n3. If the two samples really have come from the same population, then in roughly 68 cases\nout of 100, the difference between the two sample means will be within one standard\nerror of zero. And in roughly 95 cases out of 100, the difference between the two sample", "tokens": 265, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 117, "segment_id": "00117", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000205"}
{"type": "chunk", "text": "means will be within two standard errors of zero. And in 99.7 cases out of 100, the\ndifference will be within three standard errors of zero---which turns out to be what\nmotivates the conclusion in the autism research paper that we started with. As noted earlier, the difference in the mean brain size between the sample of children with\nautism spectrum disorder and the control group is 71.6 cubic centimeters. The standard error\non that difference is 22.7, meaning that the difference in means between the two samples is\nmore than three standard errors from zero; we would expect an outcome this extreme (or more\nso) only 2 times in 1,000 if these samples are drawn from an identical population. In the paper published in the Archives of General Psychiatry, the authors report a p-value of\n\n.002, as I mentioned earlier. Now you know where it came from! For all the wonders of statistical inference, there are some significant pitfalls. They derive from\nthe example that introduced the chapter: my suspicious statistics professor. The powerful\nprocess of statistical inference is based on probability, not on some kind of cosmic certainty. We don’t want to be sending people to jail just for doing the equivalent of drawing two royal\nflushes in a row; it can happen, even if someone is not cheating. As a result, we have a\nfundamental dilemma when it comes to any kind of hypothesis testing. This statistical reality came to a head in 2011 when the Journal of Personality and Social\nPsychology prepared to publish an academic paper that, on the surface, seemed like thousands\nof other academic papers.6 A Cornell professor explicitly proposed a null hypothesis,\nconducted an experiment to test his null hypothesis, and then rejected the null hypothesis at the\n.05 significance on the basis of the experimental results. The result was uproar, in scientific\ncircles as well as mainstream media outlets like the New York Times. Suffice it to say that articles in the Journal of Personality and Social Psychology don’t\nusually attract big headlines. What exactly made this study so controversial? The researcher in\nquestion was testing humans’ capacity to exercise extrasensory perception, or ESP. The null\nhypothesis was that ESP does not exist; the alternative hypothesis was that humans do have\nextrasensory powers. To study this question, the researcher recruited a large sample of\nparticipants to examine two “curtains” posted on a computer screen.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nmeans will be within two standard errors of zero. And in 99.7 cases out of 100, the\ndifference will be within three standard errors of zero---which turns out to be what\nmotivates the conclusion in the autism research paper that we started with. As noted earlier, the difference in the mean brain size between the sample of children with\nautism spectrum disorder and the control group is 71.6 cubic centimeters. The standard error\non that difference is 22.7, meaning that the difference in means between the two samples is\nmore than three standard errors from zero; we would expect an outcome this extreme (or more\nso) only 2 times in 1,000 if these samples are drawn from an identical population. In the paper published in the Archives of General Psychiatry, the authors report a p-value of\n\n.002, as I mentioned earlier. Now you know where it came from! For all the wonders of statistical inference, there are some significant pitfalls. They derive from\nthe example that introduced the chapter: my suspicious statistics professor. The powerful\nprocess of statistical inference is based on probability, not on some kind of cosmic certainty. We don’t want to be sending people to jail just for doing the equivalent of drawing two royal\nflushes in a row; it can happen, even if someone is not cheating. As a result, we have a\nfundamental dilemma when it comes to any kind of hypothesis testing. This statistical reality came to a head in 2011 when the Journal of Personality and Social\nPsychology prepared to publish an academic paper that, on the surface, seemed like thousands\nof other academic papers.6 A Cornell professor explicitly proposed a null hypothesis,\nconducted an experiment to test his null hypothesis, and then rejected the null hypothesis at the\n.05 significance on the basis of the experimental results. The result was uproar, in scientific\ncircles as well as mainstream media outlets like the New York Times. Suffice it to say that articles in the Journal of Personality and Social Psychology don’t\nusually attract big headlines. What exactly made this study so controversial? The researcher in\nquestion was testing humans’ capacity to exercise extrasensory perception, or ESP. The null\nhypothesis was that ESP does not exist; the alternative hypothesis was that humans do have\nextrasensory powers. To study this question, the researcher recruited a large sample of\nparticipants to examine two “curtains” posted on a computer screen.", "tokens": 510, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 118, "segment_id": "00118", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000206"}
{"type": "chunk", "text": "Suffice it to say that articles in the Journal of Personality and Social Psychology don’t\nusually attract big headlines. What exactly made this study so controversial? The researcher in\nquestion was testing humans’ capacity to exercise extrasensory perception, or ESP. The null\nhypothesis was that ESP does not exist; the alternative hypothesis was that humans do have\nextrasensory powers. To study this question, the researcher recruited a large sample of\nparticipants to examine two “curtains” posted on a computer screen. A software program\nrandomly put an erotic photo behind one curtain or the other. In repeated trials, study\nparticipants were able to pick the curtain with the erotic photo behind it 53 percent of the time,\nwhereas probability says they would be right only 50 percent of the time. Because of the large\nsample size, the researcher was able to reject the null hypothesis that extrasensory perception\ndoes not exist and accept instead the alternative hypothesis that extrasensory perception can\nenable individuals to sense future events. The decision to publish the paper was widely\ncriticized on the grounds that a single statistically significant event can easily be a product of\nchance, especially when there is no other evidence corroborating or even explaining the finding. The New York Times summarized the critiques: “Claims that defy almost every law of science\nare by definition extraordinary and thus require extraordinary evidence. Neglecting to take this\ninto account---as conventional social science analyses do---makes many findings look far more\nsignificant than they really are.”", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nSuffice it to say that articles in the Journal of Personality and Social Psychology don’t\nusually attract big headlines. What exactly made this study so controversial? The researcher in\nquestion was testing humans’ capacity to exercise extrasensory perception, or ESP. The null\nhypothesis was that ESP does not exist; the alternative hypothesis was that humans do have\nextrasensory powers. To study this question, the researcher recruited a large sample of\nparticipants to examine two “curtains” posted on a computer screen. A software program\nrandomly put an erotic photo behind one curtain or the other. In repeated trials, study\nparticipants were able to pick the curtain with the erotic photo behind it 53 percent of the time,\nwhereas probability says they would be right only 50 percent of the time. Because of the large\nsample size, the researcher was able to reject the null hypothesis that extrasensory perception\ndoes not exist and accept instead the alternative hypothesis that extrasensory perception can\nenable individuals to sense future events. The decision to publish the paper was widely\ncriticized on the grounds that a single statistically significant event can easily be a product of\nchance, especially when there is no other evidence corroborating or even explaining the finding. The New York Times summarized the critiques: “Claims that defy almost every law of science\nare by definition extraordinary and thus require extraordinary evidence. Neglecting to take this\ninto account---as conventional social science analyses do---makes many findings look far more\nsignificant than they really are.”", "tokens": 316, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 118, "segment_id": "00118", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000207"}
{"type": "chunk", "text": "One answer to this kind of nonsense would appear to be a more rigorous threshold for\ndefining statistical significance, such as .001.* But that creates problems of its own. Choosing\nan appropriate significance level involves an inherent trade-off. If our burden of proof for rejecting the null hypothesis is too low (e.g., .1), we are going to\nfind ourselves periodically rejecting the null hypothesis when in fact it is true (as I suspect was\nthe case with the ESP study). In statistical parlance, this is known as a Type I error. Consider\nthe example of an American courtroom, where the null hypothesis is that a defendant is not\nguilty and the threshold for rejecting that null hypothesis is “guilty beyond a reasonable\ndoubt.” Suppose we were to relax that threshold to something like “a strong hunch that the guy\ndid it.” This is going to ensure that more criminals go to jail---and also more innocent people. In a statistical context, this is the equivalent of having a relatively low significance level, such\nas .1. Well, 1 in 10 is not exactly wildly improbable. Consider this challenge in the context of\napproving a new cancer drug. For every ten drugs that we approve with this relatively low\nburden of statistical proof, one of them does not actually work and showed promising results in\nthe trial just by chance. (Or, in the courtroom example, for every ten defendants that we find\nguilty, one of them was actually innocent.) A Type I error involves wrongly rejecting a null\nhypothesis. Though the terminology is somewhat counterintuitive, this is also known as a “false\npositive.” Here is one way to reconcile the jargon. When you go to the doctor to get tested for\nsome disease, the null hypothesis is that you do not have that disease. If the lab results can be\nused to reject the null hypothesis, then you are said to test positive. And if you test positive\nbut are not really sick, then it’s a false positive. In any case, the lower our statistical burden for rejecting the null hypothesis, the more likely\nit is to happen. Obviously we would prefer not to approve ineffective cancer drugs, or send\ninnocent defendants to prison. But there is a tension here. The higher the threshold for rejecting the null hypothesis, the\nmore likely it is that we will fail to reject a null hypothesis that ought to be rejected.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nOne answer to this kind of nonsense would appear to be a more rigorous threshold for\ndefining statistical significance, such as .001.* But that creates problems of its own. Choosing\nan appropriate significance level involves an inherent trade-off. If our burden of proof for rejecting the null hypothesis is too low (e.g., .1), we are going to\nfind ourselves periodically rejecting the null hypothesis when in fact it is true (as I suspect was\nthe case with the ESP study). In statistical parlance, this is known as a Type I error. Consider\nthe example of an American courtroom, where the null hypothesis is that a defendant is not\nguilty and the threshold for rejecting that null hypothesis is “guilty beyond a reasonable\ndoubt.” Suppose we were to relax that threshold to something like “a strong hunch that the guy\ndid it.” This is going to ensure that more criminals go to jail---and also more innocent people. In a statistical context, this is the equivalent of having a relatively low significance level, such\nas .1. Well, 1 in 10 is not exactly wildly improbable. Consider this challenge in the context of\napproving a new cancer drug. For every ten drugs that we approve with this relatively low\nburden of statistical proof, one of them does not actually work and showed promising results in\nthe trial just by chance. (Or, in the courtroom example, for every ten defendants that we find\nguilty, one of them was actually innocent.) A Type I error involves wrongly rejecting a null\nhypothesis. Though the terminology is somewhat counterintuitive, this is also known as a “false\npositive.” Here is one way to reconcile the jargon. When you go to the doctor to get tested for\nsome disease, the null hypothesis is that you do not have that disease. If the lab results can be\nused to reject the null hypothesis, then you are said to test positive. And if you test positive\nbut are not really sick, then it’s a false positive. In any case, the lower our statistical burden for rejecting the null hypothesis, the more likely\nit is to happen. Obviously we would prefer not to approve ineffective cancer drugs, or send\ninnocent defendants to prison. But there is a tension here. The higher the threshold for rejecting the null hypothesis, the\nmore likely it is that we will fail to reject a null hypothesis that ought to be rejected.", "tokens": 502, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 119, "segment_id": "00119", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000208"}
{"type": "chunk", "text": "If the lab results can be\nused to reject the null hypothesis, then you are said to test positive. And if you test positive\nbut are not really sick, then it’s a false positive. In any case, the lower our statistical burden for rejecting the null hypothesis, the more likely\nit is to happen. Obviously we would prefer not to approve ineffective cancer drugs, or send\ninnocent defendants to prison. But there is a tension here. The higher the threshold for rejecting the null hypothesis, the\nmore likely it is that we will fail to reject a null hypothesis that ought to be rejected. If we\nrequire five eyewitnesses in order to convict every criminal defendant, then a lot of guilty\ndefendants are wrongly going to be set free. (Of course, fewer innocents will go to prison.) If\nwe adopt a .001 significance level in the clinical trials for all new cancer drugs, then we will\nindeed minimize the approval of ineffective drugs. (There is only a 1 in 1,000 chance of\nwrongly rejecting the null hypothesis that the drug is no more effective than a placebo.) Yet\nnow we introduce the risk of not approving many effective drugs because we have set the bar\nfor approval so high. This is known as a Type II error, or false negative. Which kind of error is worse? That depends on the circumstances. The most important point\nis that you recognize the trade-off. There is no statistical “free lunch.” Consider these\nnonstatistical situations, all of which involve a trade-off between Type I and Type II errors. 1. Spam filters. The null hypothesis is that any particular e-mail message is not spam. Your spam filter looks for clues that can be used to reject that null hypothesis for any\nparticular e-mail, such as huge distribution lists or phrases like “penis enlargement.” A\nType I error would involve screening out an e-mail message that is not actually spam (a", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nIf the lab results can be\nused to reject the null hypothesis, then you are said to test positive. And if you test positive\nbut are not really sick, then it’s a false positive. In any case, the lower our statistical burden for rejecting the null hypothesis, the more likely\nit is to happen. Obviously we would prefer not to approve ineffective cancer drugs, or send\ninnocent defendants to prison. But there is a tension here. The higher the threshold for rejecting the null hypothesis, the\nmore likely it is that we will fail to reject a null hypothesis that ought to be rejected. If we\nrequire five eyewitnesses in order to convict every criminal defendant, then a lot of guilty\ndefendants are wrongly going to be set free. (Of course, fewer innocents will go to prison.) If\nwe adopt a .001 significance level in the clinical trials for all new cancer drugs, then we will\nindeed minimize the approval of ineffective drugs. (There is only a 1 in 1,000 chance of\nwrongly rejecting the null hypothesis that the drug is no more effective than a placebo.) Yet\nnow we introduce the risk of not approving many effective drugs because we have set the bar\nfor approval so high. This is known as a Type II error, or false negative. Which kind of error is worse? That depends on the circumstances. The most important point\nis that you recognize the trade-off. There is no statistical “free lunch.” Consider these\nnonstatistical situations, all of which involve a trade-off between Type I and Type II errors. 1. Spam filters. The null hypothesis is that any particular e-mail message is not spam. Your spam filter looks for clues that can be used to reject that null hypothesis for any\nparticular e-mail, such as huge distribution lists or phrases like “penis enlargement.” A\nType I error would involve screening out an e-mail message that is not actually spam (a", "tokens": 402, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 119, "segment_id": "00119", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000209"}
{"type": "chunk", "text": "false positive). A Type II error would involve letting spam through the filter into your\ninbox (a false negative). Given the costs of missing an important e-mail relative to the\ncosts of getting the occasional message about herbal vitamins, most people would\nprobably err on the side of allowing Type II errors. An optimally designed spam filter\nshould require a relatively high degree of certainty before rejecting the null hypothesis\nthat an incoming e-mail is legitimate and blocking it. 2. Screening for cancer. We have numerous tests for the early detection of cancer, such as\nmammograms (breast cancer), the PSA test (prostate cancer), and even full-body MRI\nscans for anything else that might look suspicious. The null hypothesis for anyone\nundergoing this kind of screening is that no cancer is present. The screening is used to\nreject this null hypothesis if the results are suspicious. The assumption has always been\nthat a Type I error (a false positive that turns out to be nothing) is far preferable to a\nType II error (a false negative that misses a cancer diagnosis). Historically, cancer\nscreening has been the opposite of the spam example. Doctors and patients are willing to\ntolerate a fair number of Type I errors (false positives) in order to avoid the possibility\nof a Type II error (missing a cancer diagnosis). More recently, health policy experts have\nbegun to challenge this view because of the high costs and serious side effects\nassociated with false positives. 3. Capturing terrorists. Neither a Type I nor a Type II error is acceptable in this situation,\nwhich is why society continues to debate about the appropriate balance between fighting\nterrorism and protecting civil liberties. The null hypothesis is that an individual is not a\nterrorist. As in a regular criminal context, we do not want to commit a Type I error and\nsend innocent people to Guantánamo Bay. Yet in a world with weapons of mass\ndestruction, letting even a single terrorist go free (a Type II error) can be literally\ncatastrophic. This is why---whether you approve of it or not---the United States is\nholding suspected terrorists at Guantánamo Bay on the basis of less evidence than might\nbe required to convict them in a regular criminal court. Statistical inference is not magic, nor is it infallible, but it is an extraordinary tool for making\nsense of the world.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nfalse positive). A Type II error would involve letting spam through the filter into your\ninbox (a false negative). Given the costs of missing an important e-mail relative to the\ncosts of getting the occasional message about herbal vitamins, most people would\nprobably err on the side of allowing Type II errors. An optimally designed spam filter\nshould require a relatively high degree of certainty before rejecting the null hypothesis\nthat an incoming e-mail is legitimate and blocking it. 2. Screening for cancer. We have numerous tests for the early detection of cancer, such as\nmammograms (breast cancer), the PSA test (prostate cancer), and even full-body MRI\nscans for anything else that might look suspicious. The null hypothesis for anyone\nundergoing this kind of screening is that no cancer is present. The screening is used to\nreject this null hypothesis if the results are suspicious. The assumption has always been\nthat a Type I error (a false positive that turns out to be nothing) is far preferable to a\nType II error (a false negative that misses a cancer diagnosis). Historically, cancer\nscreening has been the opposite of the spam example. Doctors and patients are willing to\ntolerate a fair number of Type I errors (false positives) in order to avoid the possibility\nof a Type II error (missing a cancer diagnosis). More recently, health policy experts have\nbegun to challenge this view because of the high costs and serious side effects\nassociated with false positives. 3. Capturing terrorists. Neither a Type I nor a Type II error is acceptable in this situation,\nwhich is why society continues to debate about the appropriate balance between fighting\nterrorism and protecting civil liberties. The null hypothesis is that an individual is not a\nterrorist. As in a regular criminal context, we do not want to commit a Type I error and\nsend innocent people to Guantánamo Bay. Yet in a world with weapons of mass\ndestruction, letting even a single terrorist go free (a Type II error) can be literally\ncatastrophic. This is why---whether you approve of it or not---the United States is\nholding suspected terrorists at Guantánamo Bay on the basis of less evidence than might\nbe required to convict them in a regular criminal court. Statistical inference is not magic, nor is it infallible, but it is an extraordinary tool for making\nsense of the world.", "tokens": 498, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 120, "segment_id": "00120", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000210"}
{"type": "chunk", "text": "Yet in a world with weapons of mass\ndestruction, letting even a single terrorist go free (a Type II error) can be literally\ncatastrophic. This is why---whether you approve of it or not---the United States is\nholding suspected terrorists at Guantánamo Bay on the basis of less evidence than might\nbe required to convict them in a regular criminal court. Statistical inference is not magic, nor is it infallible, but it is an extraordinary tool for making\nsense of the world. We can gain great insight into many life phenomena just by determining the\nmost likely explanation. Most of us do this all the time (e.g., “I think that college student\npassed out on the floor surrounded by beer cans has had too much to drink” rather than “I think\nthat college student passed out on the floor surrounded by beer cans has been poisoned by\nterrorists”). Statistical inference merely formalizes the process. APPENDIX TO CHAPTER 9\nCalculating the standard error for a difference of means\n\nFormula for comparing two means", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nYet in a world with weapons of mass\ndestruction, letting even a single terrorist go free (a Type II error) can be literally\ncatastrophic. This is why---whether you approve of it or not---the United States is\nholding suspected terrorists at Guantánamo Bay on the basis of less evidence than might\nbe required to convict them in a regular criminal court. Statistical inference is not magic, nor is it infallible, but it is an extraordinary tool for making\nsense of the world. We can gain great insight into many life phenomena just by determining the\nmost likely explanation. Most of us do this all the time (e.g., “I think that college student\npassed out on the floor surrounded by beer cans has had too much to drink” rather than “I think\nthat college student passed out on the floor surrounded by beer cans has been poisoned by\nterrorists”). Statistical inference merely formalizes the process. APPENDIX TO CHAPTER 9\nCalculating the standard error for a difference of means\n\nFormula for comparing two means", "tokens": 218, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 120, "segment_id": "00120", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000211"}
{"type": "chunk", "text": "where = mean for sample x\n\n = mean for sample y\nsx = standard deviation for sample x\nsy = standard deviation for sample y\nnx = number of observations in sample x\nny = number of observations in sample y\n\nOur null hypothesis is that the two sample means are the same. The formula above calculates\nthe observed difference in means relative to the size of the standard error for the difference in\nmeans. Again, we lean heavily on the normal distribution. If the underlying population means\nare truly the same, then we would expect the difference in sample means to be less than one\nstandard error about 68 percent of the time; less than two standard errors about 95 percent of\nthe time; and so on. In the autism example from the chapter, the difference in the mean between the two samples\nwas 71.6 cubic centimeters with a standard error of 22.7. The ratio of that observed difference\nis 3.15, meaning that the two samples have means that are more than 3 standard errors apart. As noted in the chapter, the probability of getting samples with such different means if the\nunderlying populations have the same mean is very, very low. Specifically, the probability of\nobserving a difference of means that is 3.15 standard errors or larger is .002. Difference in Sample Means\n\nOne- and Two-Tailed Hypothesis Testing\nThis chapter introduced the idea of using samples to test whether male professional basketball\nplayers are the same height as the general population. I finessed one detail. Our null\nhypothesis is that male basketball players have the same mean height as men in the general\npopulation. What I glossed over is that we have two possible alternative hypotheses. One alternative hypothesis is that male professional basketball players have a different mean", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nwhere = mean for sample x\n\n = mean for sample y\nsx = standard deviation for sample x\nsy = standard deviation for sample y\nnx = number of observations in sample x\nny = number of observations in sample y\n\nOur null hypothesis is that the two sample means are the same. The formula above calculates\nthe observed difference in means relative to the size of the standard error for the difference in\nmeans. Again, we lean heavily on the normal distribution. If the underlying population means\nare truly the same, then we would expect the difference in sample means to be less than one\nstandard error about 68 percent of the time; less than two standard errors about 95 percent of\nthe time; and so on. In the autism example from the chapter, the difference in the mean between the two samples\nwas 71.6 cubic centimeters with a standard error of 22.7. The ratio of that observed difference\nis 3.15, meaning that the two samples have means that are more than 3 standard errors apart. As noted in the chapter, the probability of getting samples with such different means if the\nunderlying populations have the same mean is very, very low. Specifically, the probability of\nobserving a difference of means that is 3.15 standard errors or larger is .002. Difference in Sample Means\n\nOne- and Two-Tailed Hypothesis Testing\nThis chapter introduced the idea of using samples to test whether male professional basketball\nplayers are the same height as the general population. I finessed one detail. Our null\nhypothesis is that male basketball players have the same mean height as men in the general\npopulation. What I glossed over is that we have two possible alternative hypotheses. One alternative hypothesis is that male professional basketball players have a different mean", "tokens": 370, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 121, "segment_id": "00121", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000212"}
{"type": "chunk", "text": "height than the overall male population; they could be taller than other men in the population,\nor shorter. This was the approach that you took when you dropped into the hijacked bus and\nweighed the passengers to determine whether they were participants in the Changing Lives\nstudy. You could reject the null hypothesis that the bus participants were part of the study if\nthe passengers’ mean weight was significantly higher than the overall mean for Changing Lives\nparticipants or if it was significantly lower (as turned out to be the case). Our second\nalternative hypothesis is that male professional basketball players are taller on average than\nother men in the population. In this case, the background knowledge that we bring to this\nquestion tells us that basketball players cannot possibly be shorter than the general population. The distinction between these two alternative hypotheses will determine whether we do a onetailed hypothesis test or a two-tailed hypothesis test. In both cases, let’s assume that we are going to do a significance test at the .05 level. We\nwill reject our null hypothesis if we observe a difference in heights between the two samples\nthat would occur 5 times in 100 or less if all these guys really are the same height. So far, so\ngood. Here is where things get a little more nuanced. When our alternative hypothesis is that\nbasketball players are taller than other men, we are going to do a one-tailed hypothesis test. We will measure the difference in mean height between our sample of male basketball players\nand our sample of regular men. We know that if our null hypothesis is true, then we will\nobserve a difference that is 1.64 standard errors or greater only 5 times in 100. We reject our\nnull hypothesis if our result falls in this range, as the following diagram shows. Difference in Sample Means\n(Measured in Standard Errors)\n\nNow let’s revisit the other alternative hypothesis---that male basketball players could be\ntaller or shorter than the general population. Our general approach is the same. Again, we will\nreject our null hypothesis that basketball players are the same height as the general population\nif we get a result that would occur 5 times in 100 or less if there really is no difference in\nheights. The difference, however, is that we must now entertain the possibility that basketball\nplayers are shorter than the general population. We will therefore reject our null hypothesis if", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nheight than the overall male population; they could be taller than other men in the population,\nor shorter. This was the approach that you took when you dropped into the hijacked bus and\nweighed the passengers to determine whether they were participants in the Changing Lives\nstudy. You could reject the null hypothesis that the bus participants were part of the study if\nthe passengers’ mean weight was significantly higher than the overall mean for Changing Lives\nparticipants or if it was significantly lower (as turned out to be the case). Our second\nalternative hypothesis is that male professional basketball players are taller on average than\nother men in the population. In this case, the background knowledge that we bring to this\nquestion tells us that basketball players cannot possibly be shorter than the general population. The distinction between these two alternative hypotheses will determine whether we do a onetailed hypothesis test or a two-tailed hypothesis test. In both cases, let’s assume that we are going to do a significance test at the .05 level. We\nwill reject our null hypothesis if we observe a difference in heights between the two samples\nthat would occur 5 times in 100 or less if all these guys really are the same height. So far, so\ngood. Here is where things get a little more nuanced. When our alternative hypothesis is that\nbasketball players are taller than other men, we are going to do a one-tailed hypothesis test. We will measure the difference in mean height between our sample of male basketball players\nand our sample of regular men. We know that if our null hypothesis is true, then we will\nobserve a difference that is 1.64 standard errors or greater only 5 times in 100. We reject our\nnull hypothesis if our result falls in this range, as the following diagram shows. Difference in Sample Means\n(Measured in Standard Errors)\n\nNow let’s revisit the other alternative hypothesis---that male basketball players could be\ntaller or shorter than the general population. Our general approach is the same. Again, we will\nreject our null hypothesis that basketball players are the same height as the general population\nif we get a result that would occur 5 times in 100 or less if there really is no difference in\nheights. The difference, however, is that we must now entertain the possibility that basketball\nplayers are shorter than the general population. We will therefore reject our null hypothesis if", "tokens": 488, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 122, "segment_id": "00122", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000213"}
{"type": "chunk", "text": "our sample of male basketball players has a mean height that is significantly higher or lower\nthan the mean height for our sample of normal men. This requires a two-tailed hypothesis test. The cutoff points for rejecting our null hypothesis will be different because we must now\naccount for the possibility of a large difference in sample means in both directions: positive or\nnegative. More specifically, the range in which we will reject our null hypothesis has been split\nbetween the two tails. We will still reject our null hypothesis if we get an outcome that would\noccur 5 percent of the time or less if basketball players are the same height as the general\npopulation; only now we have two different ways that we can end up rejecting the null\nhypothesis. We will reject our null hypothesis if the mean height for the sample of male basketball\nplayers is so much larger than the mean for the normal men that we would observe such an\noutcome only 2.5 times in 100 if basketball players are really the same height as everyone\nelse. And we will reject our null hypothesis if the mean height for the sample of male basketball\nplayers is so much smaller than the mean for the normal men that we would observe such an\noutcome only 2.5 times in 100 if basketball players are really the same height as everyone\nelse. Together, these two contingencies add up to 5 percent, as the diagram below illustrates. Difference in Sample Means\n(Measured in Standard Errors)\n\nJudgment should inform whether a one- or a two-tailed hypothesis is more appropriate for\n\nthe analysis being conducted. * As a matter of semantics, we have not proved the null hypothesis to be true (that substance abuse treatment has no\neffect). It may turn out to be extremely effective for another group of prisoners. Or perhaps many more of the prisoners in\nthis treatment group would have been rearrested if they had not received the treatment. In any case, on the basis of the data\ncollected, we have merely failed to reject our null hypothesis. There is a similar distinction between “failing to reject” a\nnull hypothesis and accepting that null hypothesis. Just because one study could not disprove that substance abuse\ntreatment has no effect (yes, a double negative) does not mean that one must accept that substance abuse treatment is\nuseless. There is a meaningful statistical distinction here.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nour sample of male basketball players has a mean height that is significantly higher or lower\nthan the mean height for our sample of normal men. This requires a two-tailed hypothesis test. The cutoff points for rejecting our null hypothesis will be different because we must now\naccount for the possibility of a large difference in sample means in both directions: positive or\nnegative. More specifically, the range in which we will reject our null hypothesis has been split\nbetween the two tails. We will still reject our null hypothesis if we get an outcome that would\noccur 5 percent of the time or less if basketball players are the same height as the general\npopulation; only now we have two different ways that we can end up rejecting the null\nhypothesis. We will reject our null hypothesis if the mean height for the sample of male basketball\nplayers is so much larger than the mean for the normal men that we would observe such an\noutcome only 2.5 times in 100 if basketball players are really the same height as everyone\nelse. And we will reject our null hypothesis if the mean height for the sample of male basketball\nplayers is so much smaller than the mean for the normal men that we would observe such an\noutcome only 2.5 times in 100 if basketball players are really the same height as everyone\nelse. Together, these two contingencies add up to 5 percent, as the diagram below illustrates. Difference in Sample Means\n(Measured in Standard Errors)\n\nJudgment should inform whether a one- or a two-tailed hypothesis is more appropriate for\n\nthe analysis being conducted. * As a matter of semantics, we have not proved the null hypothesis to be true (that substance abuse treatment has no\neffect). It may turn out to be extremely effective for another group of prisoners. Or perhaps many more of the prisoners in\nthis treatment group would have been rearrested if they had not received the treatment. In any case, on the basis of the data\ncollected, we have merely failed to reject our null hypothesis. There is a similar distinction between “failing to reject” a\nnull hypothesis and accepting that null hypothesis. Just because one study could not disprove that substance abuse\ntreatment has no effect (yes, a double negative) does not mean that one must accept that substance abuse treatment is\nuseless. There is a meaningful statistical distinction here.", "tokens": 487, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 123, "segment_id": "00123", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000214"}
{"type": "chunk", "text": "Or perhaps many more of the prisoners in\nthis treatment group would have been rearrested if they had not received the treatment. In any case, on the basis of the data\ncollected, we have merely failed to reject our null hypothesis. There is a similar distinction between “failing to reject” a\nnull hypothesis and accepting that null hypothesis. Just because one study could not disprove that substance abuse\ntreatment has no effect (yes, a double negative) does not mean that one must accept that substance abuse treatment is\nuseless. There is a meaningful statistical distinction here. That said, research is often designed to inform policy, and prison\nofficials, who have to decide where to allocate resources, might reasonably accept the position that substance treatment is", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nOr perhaps many more of the prisoners in\nthis treatment group would have been rearrested if they had not received the treatment. In any case, on the basis of the data\ncollected, we have merely failed to reject our null hypothesis. There is a similar distinction between “failing to reject” a\nnull hypothesis and accepting that null hypothesis. Just because one study could not disprove that substance abuse\ntreatment has no effect (yes, a double negative) does not mean that one must accept that substance abuse treatment is\nuseless. There is a meaningful statistical distinction here. That said, research is often designed to inform policy, and prison\nofficials, who have to decide where to allocate resources, might reasonably accept the position that substance treatment is", "tokens": 155, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 123, "segment_id": "00123", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000215"}
{"type": "chunk", "text": "ineffective until they are persuaded otherwise. Here, as in so many other areas of statistics, judgment matters. * This example is inspired by real events. Obviously many details have been changed for national security reasons. I can\nneither confirm nor deny my own involvement. * To be precise, 95 percent of all sample means will lie within 1.96 standard errors above or below the population mean. * There are two possible alternative hypotheses. One is that male professional basketball players are taller than the overall\nmale population. The other is merely that male professional basketball players have a different mean height than the overall\nmale population (leaving open the possibility that male basketball players may actually be shorter than other men). This\ndistinction has a small impact when one performs significance tests and calculates p-values. It is explained in more\nadvanced texts and is not important to our general discussion here. * I will admit that I did once tear a statistics book in half out of frustration. * Another answer is to attempt to replicate the results in additional studies.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nineffective until they are persuaded otherwise. Here, as in so many other areas of statistics, judgment matters. * This example is inspired by real events. Obviously many details have been changed for national security reasons. I can\nneither confirm nor deny my own involvement. * To be precise, 95 percent of all sample means will lie within 1.96 standard errors above or below the population mean. * There are two possible alternative hypotheses. One is that male professional basketball players are taller than the overall\nmale population. The other is merely that male professional basketball players have a different mean height than the overall\nmale population (leaving open the possibility that male basketball players may actually be shorter than other men). This\ndistinction has a small impact when one performs significance tests and calculates p-values. It is explained in more\nadvanced texts and is not important to our general discussion here. * I will admit that I did once tear a statistics book in half out of frustration. * Another answer is to attempt to replicate the results in additional studies.", "tokens": 212, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 124, "segment_id": "00124", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000216"}
{"type": "chunk", "text": "CHAPTER 10\nPolling\nHow we know that 64 percent of\nAmericans support the death penalty\n(with a sampling error ± 3 percent)\n\nIn late 2011, the New York Times ran a front-page story reporting that “a deep sense of\nanxiety and doubt about the future hangs over the nation.”1 The story delved into the psyche of\nAmerica, offering insights into public opinion on topics ranging from the performance of the\nObama administration to the distribution of wealth. Here is a snapshot of what Americans had\nto say in the fall of 2011:\n\n• A shocking 89 percent of Americans said that they distrust government to do the right\n\nthing, the highest level of distrust ever recorded. • Two-thirds of the public said that wealth should be more evenly distributed in the\n\ncountry. • Forty-three percent of Americans said that they generally agreed with the views of the\nOccupy Wall Street movement, an amorphous protest movement that began near Wall\nStreet in New York and was spreading to other cities around the country. * A slightly\nhigher percentage, 46 percent, said that the views of the people involved in the Occupy\nWall Street movement “generally reflect the views of most Americans.”\n\n• Forty-six percent of Americans approved of Barack Obama’s handling of his job as\n\npresident---and an identical 46 percent disapproved of his job performance. • A mere 9 percent of the public approved of the way Congress was handling its job. • Even though the presidential primaries would begin in only two months, roughly 80\npercent of Republican primary voters said “it was still too early to tell whom they will\nsupport.”\n\nThese are fascinating figures that provided meaningful insight into American opinions one\nyear in advance of a presidential race. Still, one might reasonably ask, How do we know all\nthis? How can we draw such sweeping conclusions about the attitudes of hundreds of millions\nof adults? And how do we know whether these sweeping conclusions are accurate? The answer, of course, is that we conduct polls. Or in the example above, the New York\nTimes and CBS News can do a poll. (The fact that two competing news organizations would\ncollaborate on a project like this is the first clue that conducting a methodologically sound\nnational poll is not cheap.) I have no doubt that you are familiar with polling results. It may be\nless obvious that the methodology of polling is just one more form of statistical inference. A", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nCHAPTER 10\nPolling\nHow we know that 64 percent of\nAmericans support the death penalty\n(with a sampling error ± 3 percent)\n\nIn late 2011, the New York Times ran a front-page story reporting that “a deep sense of\nanxiety and doubt about the future hangs over the nation.”1 The story delved into the psyche of\nAmerica, offering insights into public opinion on topics ranging from the performance of the\nObama administration to the distribution of wealth. Here is a snapshot of what Americans had\nto say in the fall of 2011:\n\n• A shocking 89 percent of Americans said that they distrust government to do the right\n\nthing, the highest level of distrust ever recorded. • Two-thirds of the public said that wealth should be more evenly distributed in the\n\ncountry. • Forty-three percent of Americans said that they generally agreed with the views of the\nOccupy Wall Street movement, an amorphous protest movement that began near Wall\nStreet in New York and was spreading to other cities around the country. * A slightly\nhigher percentage, 46 percent, said that the views of the people involved in the Occupy\nWall Street movement “generally reflect the views of most Americans.”\n\n• Forty-six percent of Americans approved of Barack Obama’s handling of his job as\n\npresident---and an identical 46 percent disapproved of his job performance. • A mere 9 percent of the public approved of the way Congress was handling its job. • Even though the presidential primaries would begin in only two months, roughly 80\npercent of Republican primary voters said “it was still too early to tell whom they will\nsupport.”\n\nThese are fascinating figures that provided meaningful insight into American opinions one\nyear in advance of a presidential race. Still, one might reasonably ask, How do we know all\nthis? How can we draw such sweeping conclusions about the attitudes of hundreds of millions\nof adults? And how do we know whether these sweeping conclusions are accurate? The answer, of course, is that we conduct polls. Or in the example above, the New York\nTimes and CBS News can do a poll. (The fact that two competing news organizations would\ncollaborate on a project like this is the first clue that conducting a methodologically sound\nnational poll is not cheap.) I have no doubt that you are familiar with polling results. It may be\nless obvious that the methodology of polling is just one more form of statistical inference. A", "tokens": 504, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 125, "segment_id": "00125", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000217"}
{"type": "chunk", "text": "poll (or survey) is an inference about the opinions of some population that is based on the\nviews expressed by some sample drawn from that population. The power of polling stems from the same source as our previous sampling examples: the\ncentral limit theorem. If we take a large, representative sample of American voters (or any\nother group), we can reasonably assume that our sample will look a lot like the population\nfrom which it is drawn. If exactly half of American adults disapprove of gay marriage, then our\nbest guess about the attitudes of a representative sample of 1,000 Americans is that about half\nof them will disapprove of gay marriage. Conversely---and more important from the standpoint of polling---if we have a representative\nsample of 1,000 Americans who feel a certain way, such as the 46 percent who disapprove of\nPresident Obama’s job performance, then we can infer from that sample that the general\npopulation is likely to feel the same way. In fact, we can calculate the probability that our\nsample results will deviate wildly from the true attitudes of the population. When you read that\na poll has a “margin of error” of ± 3 percent, this is really just the same kind of 95 percent\nconfidence interval that we calculated in the last chapter. Our “95 percent confidence” means\nthat if we conducted 100 different polls on samples drawn from the same population, we would\nexpect the answers we get from our sample in 95 of those polls to be within 3 percentage\npoints in one direction or the other of the population’s true sentiment. In the context of the job\napproval question in the New York Times /CBS poll, we can be 95 percent confident that the\ntrue proportion of all Americans who disapprove of President Obama’s job rating lies in the\nrange of 46 percent ± 3 percent, or between 43 percent and 49 percent.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\npoll (or survey) is an inference about the opinions of some population that is based on the\nviews expressed by some sample drawn from that population. The power of polling stems from the same source as our previous sampling examples: the\ncentral limit theorem. If we take a large, representative sample of American voters (or any\nother group), we can reasonably assume that our sample will look a lot like the population\nfrom which it is drawn. If exactly half of American adults disapprove of gay marriage, then our\nbest guess about the attitudes of a representative sample of 1,000 Americans is that about half\nof them will disapprove of gay marriage. Conversely---and more important from the standpoint of polling---if we have a representative\nsample of 1,000 Americans who feel a certain way, such as the 46 percent who disapprove of\nPresident Obama’s job performance, then we can infer from that sample that the general\npopulation is likely to feel the same way. In fact, we can calculate the probability that our\nsample results will deviate wildly from the true attitudes of the population. When you read that\na poll has a “margin of error” of ± 3 percent, this is really just the same kind of 95 percent\nconfidence interval that we calculated in the last chapter. Our “95 percent confidence” means\nthat if we conducted 100 different polls on samples drawn from the same population, we would\nexpect the answers we get from our sample in 95 of those polls to be within 3 percentage\npoints in one direction or the other of the population’s true sentiment. In the context of the job\napproval question in the New York Times /CBS poll, we can be 95 percent confident that the\ntrue proportion of all Americans who disapprove of President Obama’s job rating lies in the\nrange of 46 percent ± 3 percent, or between 43 percent and 49 percent.", "tokens": 396, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 126, "segment_id": "00126", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000218"}
{"type": "chunk", "text": "Our “95 percent confidence” means\nthat if we conducted 100 different polls on samples drawn from the same population, we would\nexpect the answers we get from our sample in 95 of those polls to be within 3 percentage\npoints in one direction or the other of the population’s true sentiment. In the context of the job\napproval question in the New York Times /CBS poll, we can be 95 percent confident that the\ntrue proportion of all Americans who disapprove of President Obama’s job rating lies in the\nrange of 46 percent ± 3 percent, or between 43 percent and 49 percent. If you read the small\nprint on the New York Times /CBS poll (as I urge you to do), that’s pretty much what it says:\n“In theory, in 19 cases out of 20, overall results based on such samples will differ by no more\nthan 3 percentage points in either direction from what would have been obtained by seeking to\ninterview all American adults.”\n\nOne fundamental difference between a poll and other forms of sampling is that the sample\nstatistic we care about will be not a mean (e.g., 187 pounds) but rather a percentage or\nproportion (e.g., 47 percent of voters, or .47). In other respects, the process is identical. When\nwe have a large, representative sample (the poll), we would expect the proportion of\nrespondents who feel a certain way in the sample (e.g., the 9 percent who think Congress is\ndoing a good job) to be roughly equal to the proportion of all Americans who feel that way. This is no different from assuming that the mean weight for a sample of 1,000 American men\nshould be roughly equal to the mean weight for all American men. Still, we expect some\nvariation in the percentage who approve of Congress from sample to sample, just as we would\nexpect some variation in mean weight as we took different random samples of 1,000 men. If the\nNew York Times and CBS had conducted a second poll---asking the same questions to a new\nsample of 1,000 U.S. adults---it is highly unlikely that the results of the second poll would\nhave been identical to the results of the first. On the other hand, we should not expect the\nanswers from our second sample to diverge widely from the answers given by the first.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nOur “95 percent confidence” means\nthat if we conducted 100 different polls on samples drawn from the same population, we would\nexpect the answers we get from our sample in 95 of those polls to be within 3 percentage\npoints in one direction or the other of the population’s true sentiment. In the context of the job\napproval question in the New York Times /CBS poll, we can be 95 percent confident that the\ntrue proportion of all Americans who disapprove of President Obama’s job rating lies in the\nrange of 46 percent ± 3 percent, or between 43 percent and 49 percent. If you read the small\nprint on the New York Times /CBS poll (as I urge you to do), that’s pretty much what it says:\n“In theory, in 19 cases out of 20, overall results based on such samples will differ by no more\nthan 3 percentage points in either direction from what would have been obtained by seeking to\ninterview all American adults.”\n\nOne fundamental difference between a poll and other forms of sampling is that the sample\nstatistic we care about will be not a mean (e.g., 187 pounds) but rather a percentage or\nproportion (e.g., 47 percent of voters, or .47). In other respects, the process is identical. When\nwe have a large, representative sample (the poll), we would expect the proportion of\nrespondents who feel a certain way in the sample (e.g., the 9 percent who think Congress is\ndoing a good job) to be roughly equal to the proportion of all Americans who feel that way. This is no different from assuming that the mean weight for a sample of 1,000 American men\nshould be roughly equal to the mean weight for all American men. Still, we expect some\nvariation in the percentage who approve of Congress from sample to sample, just as we would\nexpect some variation in mean weight as we took different random samples of 1,000 men. If the\nNew York Times and CBS had conducted a second poll---asking the same questions to a new\nsample of 1,000 U.S. adults---it is highly unlikely that the results of the second poll would\nhave been identical to the results of the first. On the other hand, we should not expect the\nanswers from our second sample to diverge widely from the answers given by the first.", "tokens": 496, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 126, "segment_id": "00126", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000219"}
{"type": "chunk", "text": "Still, we expect some\nvariation in the percentage who approve of Congress from sample to sample, just as we would\nexpect some variation in mean weight as we took different random samples of 1,000 men. If the\nNew York Times and CBS had conducted a second poll---asking the same questions to a new\nsample of 1,000 U.S. adults---it is highly unlikely that the results of the second poll would\nhave been identical to the results of the first. On the other hand, we should not expect the\nanswers from our second sample to diverge widely from the answers given by the first. (To\nreturn to a metaphor used earlier, if you taste a spoonful of soup, stir the pot, and then taste\nagain, the two spoonfuls are going to taste similar.) The standard error is what tells us how", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nStill, we expect some\nvariation in the percentage who approve of Congress from sample to sample, just as we would\nexpect some variation in mean weight as we took different random samples of 1,000 men. If the\nNew York Times and CBS had conducted a second poll---asking the same questions to a new\nsample of 1,000 U.S. adults---it is highly unlikely that the results of the second poll would\nhave been identical to the results of the first. On the other hand, we should not expect the\nanswers from our second sample to diverge widely from the answers given by the first. (To\nreturn to a metaphor used earlier, if you taste a spoonful of soup, stir the pot, and then taste\nagain, the two spoonfuls are going to taste similar.) The standard error is what tells us how", "tokens": 175, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 126, "segment_id": "00126", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000220"}
{"type": "chunk", "text": "much dispersion we can expect in our results from sample to sample, which in this case means\npoll to poll. The formula for calculating a standard error for a percentage or proportion is slightly\ndifferent from the formula introduced earlier; the intuition is exactly the same. For any properly\ndrawn random sample, the standard error is equal to\n where p is the proportion of\nrespondents expressing a particular view, (1 -- p) is the proportion of respondents expressing a\ndifferent view, and n is the total number of respondents in the sample. You should see that the\nstandard error will fall as the sample size gets larger, since n is in the denominator. The\nstandard error also tends to be smaller when p and (1 -- p) are far apart. For example, the\nstandard error will be smaller for a poll in which 95 percent of respondents express a certain\nview than for a poll in which opinions tend to split 50-50. This is just math, since (.05)(.95) =\n.047, while (.5)(.5) = .25; a smaller number in the numerator of the formula leads to a smaller\nstandard error. As an example, assume that a simple “exit poll” of 500 representative voters on election day\nfinds that 53 percent voted for the Republican candidate; 45 percent of voters voted for the\nDemocrat; and 2 percent supported a third-party candidate. If we use the Republican candidate\nthis exit poll would be\nthe standard error\nas our proportion of\n\ninterest,\n\nfor\n\nFor simplicity, we’ll round the standard error for this exit poll to .02. So far, that’s just a\nnumber. Let’s work through why that number matters. Assume the polls have just closed, and\nyou work for a television network that is keen to declare a winner in the race before the full\nresults are available. You are now the official network data cruncher (having read two-thirds of\nthis book), and your producer wants to know whether it is possible to “call the race” on the\nbasis of this exit poll. You explain that the answer depends on how confident the network people would like to be\nin the announcement---or, more specifically, what risk they are willing to take that they will get\nit wrong.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nmuch dispersion we can expect in our results from sample to sample, which in this case means\npoll to poll. The formula for calculating a standard error for a percentage or proportion is slightly\ndifferent from the formula introduced earlier; the intuition is exactly the same. For any properly\ndrawn random sample, the standard error is equal to\n where p is the proportion of\nrespondents expressing a particular view, (1 -- p) is the proportion of respondents expressing a\ndifferent view, and n is the total number of respondents in the sample. You should see that the\nstandard error will fall as the sample size gets larger, since n is in the denominator. The\nstandard error also tends to be smaller when p and (1 -- p) are far apart. For example, the\nstandard error will be smaller for a poll in which 95 percent of respondents express a certain\nview than for a poll in which opinions tend to split 50-50. This is just math, since (.05)(.95) =\n.047, while (.5)(.5) = .25; a smaller number in the numerator of the formula leads to a smaller\nstandard error. As an example, assume that a simple “exit poll” of 500 representative voters on election day\nfinds that 53 percent voted for the Republican candidate; 45 percent of voters voted for the\nDemocrat; and 2 percent supported a third-party candidate. If we use the Republican candidate\nthis exit poll would be\nthe standard error\nas our proportion of\n\ninterest,\n\nfor\n\nFor simplicity, we’ll round the standard error for this exit poll to .02. So far, that’s just a\nnumber. Let’s work through why that number matters. Assume the polls have just closed, and\nyou work for a television network that is keen to declare a winner in the race before the full\nresults are available. You are now the official network data cruncher (having read two-thirds of\nthis book), and your producer wants to know whether it is possible to “call the race” on the\nbasis of this exit poll. You explain that the answer depends on how confident the network people would like to be\nin the announcement---or, more specifically, what risk they are willing to take that they will get\nit wrong.", "tokens": 474, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 127, "segment_id": "00127", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000221"}
{"type": "chunk", "text": "Let’s work through why that number matters. Assume the polls have just closed, and\nyou work for a television network that is keen to declare a winner in the race before the full\nresults are available. You are now the official network data cruncher (having read two-thirds of\nthis book), and your producer wants to know whether it is possible to “call the race” on the\nbasis of this exit poll. You explain that the answer depends on how confident the network people would like to be\nin the announcement---or, more specifically, what risk they are willing to take that they will get\nit wrong. Remember, the standard error gives us a sense of how often we can expect our\nsample proportion (the exit poll) to lie reasonably close to the true population proportion (the\nelection outcome). We know that roughly 68 percent of the time we can expect the sample\nproportion---the 53 percent of voters who said they voted for the Republican in this case---to\nbe within one standard error of the true final tally. As a result, you tell your producer “with 68\npercent confidence” that your sample, which shows the Republican getting 53 percent of the\nvote ± 2 percent, or between 51 and 55 percent, has captured the Republican candidate’s true\ntally. Meanwhile, the same exit poll shows that the Democratic candidate has received 45\npercent of the vote. If we assume that the vote tally for the Democratic candidate has the same\nstandard error (a simplification that I’ll explain in a minute), we can say with 68 percent\nconfidence that the exit poll sample, which shows the Democrat with 45 percent of the vote ± 2\npercent, or between 43 and 47 percent, contains the Democrat’s true tally. According to this\ncalculation, the Republican is the winner. The graphics department rushes to do a fancy three-dimensional image that you can flash on\n\nthe screen for your viewers:", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nLet’s work through why that number matters. Assume the polls have just closed, and\nyou work for a television network that is keen to declare a winner in the race before the full\nresults are available. You are now the official network data cruncher (having read two-thirds of\nthis book), and your producer wants to know whether it is possible to “call the race” on the\nbasis of this exit poll. You explain that the answer depends on how confident the network people would like to be\nin the announcement---or, more specifically, what risk they are willing to take that they will get\nit wrong. Remember, the standard error gives us a sense of how often we can expect our\nsample proportion (the exit poll) to lie reasonably close to the true population proportion (the\nelection outcome). We know that roughly 68 percent of the time we can expect the sample\nproportion---the 53 percent of voters who said they voted for the Republican in this case---to\nbe within one standard error of the true final tally. As a result, you tell your producer “with 68\npercent confidence” that your sample, which shows the Republican getting 53 percent of the\nvote ± 2 percent, or between 51 and 55 percent, has captured the Republican candidate’s true\ntally. Meanwhile, the same exit poll shows that the Democratic candidate has received 45\npercent of the vote. If we assume that the vote tally for the Democratic candidate has the same\nstandard error (a simplification that I’ll explain in a minute), we can say with 68 percent\nconfidence that the exit poll sample, which shows the Democrat with 45 percent of the vote ± 2\npercent, or between 43 and 47 percent, contains the Democrat’s true tally. According to this\ncalculation, the Republican is the winner. The graphics department rushes to do a fancy three-dimensional image that you can flash on\n\nthe screen for your viewers:", "tokens": 406, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 127, "segment_id": "00127", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000222"}
{"type": "chunk", "text": "Republican 53%\nDemocrat 45%\nIndependent 2%\n(Margin of Error 2%)\n\nAt first, your producer is impressed and excited, in large part because the above graphic is\n3-D, multicolored, and able to spin around on the screen. However, when you explain that\nroughly 68 times out of 100 your exit poll results will be within one standard error of the true\nelection outcome, your producer, who has twice been sent by the courts to anger management\nprograms, points out the obvious math---32 times out of 100 your exit poll will not be within\none standard error of the true election outcome. Then what? You explain that there are two possibilities: (1) the Republican candidate could have\nreceived even more votes than your poll predicted, in which case you still will have called the\nelection correctly. Or (2) there is a reasonably high probability that the Democratic candidate\nhas received far more votes than your poll has reported, in which case your fancy 3-D,\nmulticolored, spinning graphic will have reported the wrong winner. Your producer hurls a coffee mug across the room and uses several phrases that violate her\nprobation. She screams, “How can we be [deleted] sure that we have the right [deleted]\nresult?”\n\nEver the statistics guru, you point out that you cannot be certain of any result until all of the\nvotes are counted. However, you can offer a 95 percent confidence interval instead. In this\ncase, your spinning, 3-D, multicolored graphic will be wrong, on average, only 5 times out of\n100. Your producer lights a cigarette and seems to relax. You decide not to mention the ban on\nsmoking in the workplace, as that turned out disastrously last time. However, you do share\nsome bad news. The only way the station can be more confident of its polling results is by\nbroadening the “margin of error.” And when you do that, there is no longer a clear winner in the\nelection. You show your boss the new fancy graphic:\n\nRepublican 53%\nDemocrat 45%\nIndependent 2%\n(Margin of Error 4%)\n\nWe know from the central limit theorem that roughly 95 percent of sample proportions will\nlie within two standard errors of the true population proportion (which is 4% in this case).", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nRepublican 53%\nDemocrat 45%\nIndependent 2%\n(Margin of Error 2%)\n\nAt first, your producer is impressed and excited, in large part because the above graphic is\n3-D, multicolored, and able to spin around on the screen. However, when you explain that\nroughly 68 times out of 100 your exit poll results will be within one standard error of the true\nelection outcome, your producer, who has twice been sent by the courts to anger management\nprograms, points out the obvious math---32 times out of 100 your exit poll will not be within\none standard error of the true election outcome. Then what? You explain that there are two possibilities: (1) the Republican candidate could have\nreceived even more votes than your poll predicted, in which case you still will have called the\nelection correctly. Or (2) there is a reasonably high probability that the Democratic candidate\nhas received far more votes than your poll has reported, in which case your fancy 3-D,\nmulticolored, spinning graphic will have reported the wrong winner. Your producer hurls a coffee mug across the room and uses several phrases that violate her\nprobation. She screams, “How can we be [deleted] sure that we have the right [deleted]\nresult?”\n\nEver the statistics guru, you point out that you cannot be certain of any result until all of the\nvotes are counted. However, you can offer a 95 percent confidence interval instead. In this\ncase, your spinning, 3-D, multicolored graphic will be wrong, on average, only 5 times out of\n100. Your producer lights a cigarette and seems to relax. You decide not to mention the ban on\nsmoking in the workplace, as that turned out disastrously last time. However, you do share\nsome bad news. The only way the station can be more confident of its polling results is by\nbroadening the “margin of error.” And when you do that, there is no longer a clear winner in the\nelection. You show your boss the new fancy graphic:\n\nRepublican 53%\nDemocrat 45%\nIndependent 2%\n(Margin of Error 4%)\n\nWe know from the central limit theorem that roughly 95 percent of sample proportions will\nlie within two standard errors of the true population proportion (which is 4% in this case).", "tokens": 490, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 128, "segment_id": "00128", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000223"}
{"type": "chunk", "text": "However, you do share\nsome bad news. The only way the station can be more confident of its polling results is by\nbroadening the “margin of error.” And when you do that, there is no longer a clear winner in the\nelection. You show your boss the new fancy graphic:\n\nRepublican 53%\nDemocrat 45%\nIndependent 2%\n(Margin of Error 4%)\n\nWe know from the central limit theorem that roughly 95 percent of sample proportions will\nlie within two standard errors of the true population proportion (which is 4% in this case). Therefore, if we want to be more confident of our polling results, we have to be less ambitious\nin what we are predicting. As the above graphic illustrates (without the 3-D and color), at the\n95 percent confidence level, the television station can announce that the Republican candidate\nhas earned 53 percent of the vote ± 4 percent, or between 49 and 57 percent of the votes cast. Meanwhile, the Democratic candidate has earned 45 percent ± 4 percent, or between 41 and 49\npercent of the votes cast. And, yes, now you have a new problem. At the 95 percent confidence level, you cannot", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nHowever, you do share\nsome bad news. The only way the station can be more confident of its polling results is by\nbroadening the “margin of error.” And when you do that, there is no longer a clear winner in the\nelection. You show your boss the new fancy graphic:\n\nRepublican 53%\nDemocrat 45%\nIndependent 2%\n(Margin of Error 4%)\n\nWe know from the central limit theorem that roughly 95 percent of sample proportions will\nlie within two standard errors of the true population proportion (which is 4% in this case). Therefore, if we want to be more confident of our polling results, we have to be less ambitious\nin what we are predicting. As the above graphic illustrates (without the 3-D and color), at the\n95 percent confidence level, the television station can announce that the Republican candidate\nhas earned 53 percent of the vote ± 4 percent, or between 49 and 57 percent of the votes cast. Meanwhile, the Democratic candidate has earned 45 percent ± 4 percent, or between 41 and 49\npercent of the votes cast. And, yes, now you have a new problem. At the 95 percent confidence level, you cannot", "tokens": 254, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 128, "segment_id": "00128", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000224"}
{"type": "chunk", "text": "reject the possibility that the two candidates may be tied with 49 percent of the vote each. This\nis an inevitable trade-off; the only way to become more certain that your polling results will be\nconsistent with the election outcome without new data is to become more timid in your\nprediction. Think about a nonstatistical context. Suppose you tell a friend that you are “pretty\nsure” that Thomas Jefferson was the third or fourth president. How can you become more\nconfident of your historical knowledge? By being less specific. You are “absolutely positive”\nthat Thomas Jefferson was one of the first five presidents. Your producer tells you to order a pizza and prepare to stay at work all night. At that point,\nstatistical good fortune shines upon you. The results of a second exit poll come across your\ndesk with a sample of 2,000 voters. These results show the following: Republican (52 percent);\nDemocrat (45 percent); Independent (3 percent). Your producer is now thoroughly exasperated,\nsince this poll suggests that the gap between the candidates has narrowed, making it even\nharder for you to call the race in a timely manner. But wait! You point out (heroically) that the\nsample size (2,000) is four times as large as the sample in the first poll. As a result, the\nstandard error will shrink significantly. The new standard error for the Republican candidate is\n\n which is .01. If your producer is still comfortable with a 95 percent confidence level, you can declare the\nRepublican candidate the winner. With your new .01 standard error, the 95 percent confidence\nintervals for the candidates are the following: Republican: 52 ± 2, or between 50 and 54\npercent of the votes cast; Democrat: 45 ± 2, or between 43 and 47 percent of the votes cast. There is no longer any overlap between the two confidence intervals. You can predict on air\nthat the Republican candidate is the winner; more than 95 times out of 100 you will be\ncorrect.*\n\nBut this case is even better than that. The central limit theorem tells us that 99.7 percent of\nthe time a sample proportion will be within three standard errors of the true population\nproportion.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nreject the possibility that the two candidates may be tied with 49 percent of the vote each. This\nis an inevitable trade-off; the only way to become more certain that your polling results will be\nconsistent with the election outcome without new data is to become more timid in your\nprediction. Think about a nonstatistical context. Suppose you tell a friend that you are “pretty\nsure” that Thomas Jefferson was the third or fourth president. How can you become more\nconfident of your historical knowledge? By being less specific. You are “absolutely positive”\nthat Thomas Jefferson was one of the first five presidents. Your producer tells you to order a pizza and prepare to stay at work all night. At that point,\nstatistical good fortune shines upon you. The results of a second exit poll come across your\ndesk with a sample of 2,000 voters. These results show the following: Republican (52 percent);\nDemocrat (45 percent); Independent (3 percent). Your producer is now thoroughly exasperated,\nsince this poll suggests that the gap between the candidates has narrowed, making it even\nharder for you to call the race in a timely manner. But wait! You point out (heroically) that the\nsample size (2,000) is four times as large as the sample in the first poll. As a result, the\nstandard error will shrink significantly. The new standard error for the Republican candidate is\n\n which is .01. If your producer is still comfortable with a 95 percent confidence level, you can declare the\nRepublican candidate the winner. With your new .01 standard error, the 95 percent confidence\nintervals for the candidates are the following: Republican: 52 ± 2, or between 50 and 54\npercent of the votes cast; Democrat: 45 ± 2, or between 43 and 47 percent of the votes cast. There is no longer any overlap between the two confidence intervals. You can predict on air\nthat the Republican candidate is the winner; more than 95 times out of 100 you will be\ncorrect.*\n\nBut this case is even better than that. The central limit theorem tells us that 99.7 percent of\nthe time a sample proportion will be within three standard errors of the true population\nproportion.", "tokens": 471, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 129, "segment_id": "00129", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000225"}
{"type": "chunk", "text": "There is no longer any overlap between the two confidence intervals. You can predict on air\nthat the Republican candidate is the winner; more than 95 times out of 100 you will be\ncorrect.*\n\nBut this case is even better than that. The central limit theorem tells us that 99.7 percent of\nthe time a sample proportion will be within three standard errors of the true population\nproportion. In this election example, our 99.7 percent confidence intervals for the two\ncandidates are the following: Republican, 52 ± 3 percent, or between 49 and 55 percent;\nDemocrat, 45 ± 3 percent, or between 42 and 48 percent. If you report that the Republican\ncandidate has won, there is only a tiny chance that you and your producer will be fired, thanks\nto your new 2,000-voter sample. You should see that a bigger sample makes for a shrinking standard error, which is how large\nnational polls can end up with shockingly accurate results. On the other hand, smaller samples\nobviously make for larger standard errors and therefore a larger confidence interval (or “margin\nof sampling error,” to use the polling lingo). The fine print in the New York Times /CBS poll\npoints out that the margin of error for the questions about the Republican primary is 5\npercentage points, compared with 3 percentage points for other questions in the poll. Only selfdescribed Republican primary and caucus voters were asked these questions, so the sample\nsize for this subgroup of questions fell to 455 (compared with 1,650 adults for the balance of\nthe poll). As usual, I’ve simplified lots of things in this chapter. You might have recognized that in my", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThere is no longer any overlap between the two confidence intervals. You can predict on air\nthat the Republican candidate is the winner; more than 95 times out of 100 you will be\ncorrect.*\n\nBut this case is even better than that. The central limit theorem tells us that 99.7 percent of\nthe time a sample proportion will be within three standard errors of the true population\nproportion. In this election example, our 99.7 percent confidence intervals for the two\ncandidates are the following: Republican, 52 ± 3 percent, or between 49 and 55 percent;\nDemocrat, 45 ± 3 percent, or between 42 and 48 percent. If you report that the Republican\ncandidate has won, there is only a tiny chance that you and your producer will be fired, thanks\nto your new 2,000-voter sample. You should see that a bigger sample makes for a shrinking standard error, which is how large\nnational polls can end up with shockingly accurate results. On the other hand, smaller samples\nobviously make for larger standard errors and therefore a larger confidence interval (or “margin\nof sampling error,” to use the polling lingo). The fine print in the New York Times /CBS poll\npoints out that the margin of error for the questions about the Republican primary is 5\npercentage points, compared with 3 percentage points for other questions in the poll. Only selfdescribed Republican primary and caucus voters were asked these questions, so the sample\nsize for this subgroup of questions fell to 455 (compared with 1,650 adults for the balance of\nthe poll). As usual, I’ve simplified lots of things in this chapter. You might have recognized that in my", "tokens": 359, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 129, "segment_id": "00129", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000226"}
{"type": "chunk", "text": "election example above, the Republican and Democratic candidates should each have their\n The size of the sample, n, is the\nown standard error. Think again about the formula:\nsame for both candidates, but p and (1 -- p) will be slightly different. In the second exit poll\n for the\n(with the 2,000-voter sample), the standard error for the Republican is\nDemocrat,\n Of course, for all intents and purposes, those two numbers are\nthe same. For that reason, I have adopted a common convention, which is to take the higher\nstandard error of the two and use that for all of the candidates. If anything, this introduces a\nlittle extra caution into our confidence intervals. Many national polls that ask multiple questions will go one step further. In the case of the\nNew York Times/CBS poll, the standard error should technically be different for each question,\ndepending on the response. For example, the standard error for the finding that 9 percent of the\npublic approves of the way Congress is handling its job should be lower than the standard error\nfor the question finding that 46 percent of the public approves of the way President Obama has\nhandled his job, since .09 × (.91) is less than .46 × (.54)---.0819 versus .2484. (The intuition\nbehind this formula is explained in a chapter appendix.)\n\nSince it would be both confusing and inconvenient to have a different standard error for each\nquestion, polls of this nature will typically assume that the sample proportion for each question\nis .5 (or 50 percent)---generating the largest possible standard error for any given sample size\n---and then adopt that standard error to calculate the margin of sampling error for the entire\npoll.*\n\nWhen done properly, polls are uncanny instruments. According to Frank Newport, editor in\nchief of the Gallup Organization, a poll of 1,000 people can offer meaningful and accurate\ninsights into the attitudes of the entire country. Statistically speaking, he’s right. But to get\nthose meaningful and accurate results, we have to conduct a proper poll and then interpret the\nresults correctly, both of which are much easier said than done. Bad polling results do not\ntypically stem from bad math when calculating the standard errors. Bad polling results\ntypically stem from a biased sample, or bad questions, or both. The mantra “garbage in,\ngarbage out” applies in spades when it comes to sampling public opinion.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nelection example above, the Republican and Democratic candidates should each have their\n The size of the sample, n, is the\nown standard error. Think again about the formula:\nsame for both candidates, but p and (1 -- p) will be slightly different. In the second exit poll\n for the\n(with the 2,000-voter sample), the standard error for the Republican is\nDemocrat,\n Of course, for all intents and purposes, those two numbers are\nthe same. For that reason, I have adopted a common convention, which is to take the higher\nstandard error of the two and use that for all of the candidates. If anything, this introduces a\nlittle extra caution into our confidence intervals. Many national polls that ask multiple questions will go one step further. In the case of the\nNew York Times/CBS poll, the standard error should technically be different for each question,\ndepending on the response. For example, the standard error for the finding that 9 percent of the\npublic approves of the way Congress is handling its job should be lower than the standard error\nfor the question finding that 46 percent of the public approves of the way President Obama has\nhandled his job, since .09 × (.91) is less than .46 × (.54)---.0819 versus .2484. (The intuition\nbehind this formula is explained in a chapter appendix.)\n\nSince it would be both confusing and inconvenient to have a different standard error for each\nquestion, polls of this nature will typically assume that the sample proportion for each question\nis .5 (or 50 percent)---generating the largest possible standard error for any given sample size\n---and then adopt that standard error to calculate the margin of sampling error for the entire\npoll.*\n\nWhen done properly, polls are uncanny instruments. According to Frank Newport, editor in\nchief of the Gallup Organization, a poll of 1,000 people can offer meaningful and accurate\ninsights into the attitudes of the entire country. Statistically speaking, he’s right. But to get\nthose meaningful and accurate results, we have to conduct a proper poll and then interpret the\nresults correctly, both of which are much easier said than done. Bad polling results do not\ntypically stem from bad math when calculating the standard errors. Bad polling results\ntypically stem from a biased sample, or bad questions, or both. The mantra “garbage in,\ngarbage out” applies in spades when it comes to sampling public opinion.", "tokens": 512, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 130, "segment_id": "00130", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000227"}
{"type": "chunk", "text": "Statistically speaking, he’s right. But to get\nthose meaningful and accurate results, we have to conduct a proper poll and then interpret the\nresults correctly, both of which are much easier said than done. Bad polling results do not\ntypically stem from bad math when calculating the standard errors. Bad polling results\ntypically stem from a biased sample, or bad questions, or both. The mantra “garbage in,\ngarbage out” applies in spades when it comes to sampling public opinion. Below are the key\nmethodological questions one ought to ask when conducting a poll, or when reviewing the work\nof others. Is this an accurate sample of the population whose opinions we are trying to measure? Many common data-related challenges were discussed in Chapter 7. Nonetheless, I will point\nout once again the danger of selection bias, particularly self-selection. Any poll that depends\non individuals who select into the sample, such as a radio call-in show or a voluntary Internet\nsurvey, will capture only the views of those who make the effort to voice their opinions. These\nare likely to be the people who feel particularly strongly about an issue, or those who happen\nto have a lot of free time on their hands. Neither of these groups is likely to be representative\nof the public at large. I once appeared as a guest on a call-in radio show. One of the callers to\nthe program declared emphatically on air that my views were “so wrong” that he had pulled his\ncar off the highway and found a pay phone in order to call the show and register his dissent. I’d\nlike to think that the listeners who did not pull their cars off the highway to call the show felt", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nStatistically speaking, he’s right. But to get\nthose meaningful and accurate results, we have to conduct a proper poll and then interpret the\nresults correctly, both of which are much easier said than done. Bad polling results do not\ntypically stem from bad math when calculating the standard errors. Bad polling results\ntypically stem from a biased sample, or bad questions, or both. The mantra “garbage in,\ngarbage out” applies in spades when it comes to sampling public opinion. Below are the key\nmethodological questions one ought to ask when conducting a poll, or when reviewing the work\nof others. Is this an accurate sample of the population whose opinions we are trying to measure? Many common data-related challenges were discussed in Chapter 7. Nonetheless, I will point\nout once again the danger of selection bias, particularly self-selection. Any poll that depends\non individuals who select into the sample, such as a radio call-in show or a voluntary Internet\nsurvey, will capture only the views of those who make the effort to voice their opinions. These\nare likely to be the people who feel particularly strongly about an issue, or those who happen\nto have a lot of free time on their hands. Neither of these groups is likely to be representative\nof the public at large. I once appeared as a guest on a call-in radio show. One of the callers to\nthe program declared emphatically on air that my views were “so wrong” that he had pulled his\ncar off the highway and found a pay phone in order to call the show and register his dissent. I’d\nlike to think that the listeners who did not pull their cars off the highway to call the show felt", "tokens": 348, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 130, "segment_id": "00130", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000228"}
{"type": "chunk", "text": "differently. Any method of gathering opinion that systematically excludes some segment of the\npopulation is also prone to bias. For example, mobile phones have introduced a host of new\nmethodological complexities. Professional polling organizations go to great lengths to poll a\nrepresentative sample of the relevant population. The New York Times/CBS poll was based on\ntelephone interviews conducted over six days with 1,650 adults, 1,475 of whom said they were\nregistered to vote. I can only guess at the rest of the methodology, but most professional polls use some\nvariation on the following techniques. To ensure that the adults who pick up the phone are\nrepresentative of the population, the process starts with probability---a variation on picking\nmarbles out of an urn. A computer randomly selects a set of landline telephone exchanges. (An\nexchange is an area code plus the first three digits of a phone number.) By randomly choosing\nfrom the 69,000 residential exchanges in the country, each in proportion to its share of all\ntelephone numbers, the survey is likely to get a generally representative geographic distribution\nof the population. As the small print explains, “The exchanges were chosen so as to ensure that\neach region of the country was represented in proportion to its share of all telephone numbers.”\nFor each exchange selected, the computer added four random digits. As a result, both listed\nand unlisted numbers will end up on the final list of households to be called. The survey also\nincluded a “random dialing of cell phone numbers.”\n\nFor each number dialed, one adult is designated to be the respondent by a “random\nprocedure,” such as asking for the youngest adult who is currently at home. This process has\nbeen refined to produce a sample of respondents that resembles the adult population in terms\nof age and gender. Most important, the interviewer will attempt to make multiple calls at\ndifferent times of day and evening in order to reach each selected phone number. These\nrepeated attempts---as many as ten or twelve calls to the same number---are an important part\nof getting an unbiased sample. Obviously it would be cheaper and easier to make random calls\nto different numbers until a sufficiently large sample of adults have picked up the phone and\nanswered the relevant questions. However, such a sample would be biased toward people who\nare likely to be home and to answer the phone: the unemployed, the elderly, and so on.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\ndifferently. Any method of gathering opinion that systematically excludes some segment of the\npopulation is also prone to bias. For example, mobile phones have introduced a host of new\nmethodological complexities. Professional polling organizations go to great lengths to poll a\nrepresentative sample of the relevant population. The New York Times/CBS poll was based on\ntelephone interviews conducted over six days with 1,650 adults, 1,475 of whom said they were\nregistered to vote. I can only guess at the rest of the methodology, but most professional polls use some\nvariation on the following techniques. To ensure that the adults who pick up the phone are\nrepresentative of the population, the process starts with probability---a variation on picking\nmarbles out of an urn. A computer randomly selects a set of landline telephone exchanges. (An\nexchange is an area code plus the first three digits of a phone number.) By randomly choosing\nfrom the 69,000 residential exchanges in the country, each in proportion to its share of all\ntelephone numbers, the survey is likely to get a generally representative geographic distribution\nof the population. As the small print explains, “The exchanges were chosen so as to ensure that\neach region of the country was represented in proportion to its share of all telephone numbers.”\nFor each exchange selected, the computer added four random digits. As a result, both listed\nand unlisted numbers will end up on the final list of households to be called. The survey also\nincluded a “random dialing of cell phone numbers.”\n\nFor each number dialed, one adult is designated to be the respondent by a “random\nprocedure,” such as asking for the youngest adult who is currently at home. This process has\nbeen refined to produce a sample of respondents that resembles the adult population in terms\nof age and gender. Most important, the interviewer will attempt to make multiple calls at\ndifferent times of day and evening in order to reach each selected phone number. These\nrepeated attempts---as many as ten or twelve calls to the same number---are an important part\nof getting an unbiased sample. Obviously it would be cheaper and easier to make random calls\nto different numbers until a sufficiently large sample of adults have picked up the phone and\nanswered the relevant questions. However, such a sample would be biased toward people who\nare likely to be home and to answer the phone: the unemployed, the elderly, and so on.", "tokens": 499, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 131, "segment_id": "00131", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000229"}
{"type": "chunk", "text": "Most important, the interviewer will attempt to make multiple calls at\ndifferent times of day and evening in order to reach each selected phone number. These\nrepeated attempts---as many as ten or twelve calls to the same number---are an important part\nof getting an unbiased sample. Obviously it would be cheaper and easier to make random calls\nto different numbers until a sufficiently large sample of adults have picked up the phone and\nanswered the relevant questions. However, such a sample would be biased toward people who\nare likely to be home and to answer the phone: the unemployed, the elderly, and so on. That’s\njust fine as long as you’re willing to qualify your poll results in the following way: President\nObama’s approval rating stands at 46 percent among the unemployed, old people, and others\nwho are eager to answer random phone calls. One indicator of a poll’s validity is the response rate: What proportion of respondents who\nwere chosen to be contacted ultimately completed the poll or survey? A low response rate can\nbe a warning sign for potential sampling bias. The more people there are who opt not to answer\nthe poll, or who just can’t be reached, the greater the possibility that this large group is\ndifferent in some material way from those who did answer the questions. Pollsters can test for\n“nonresponse bias” by analyzing available data on the respondents whom they were not able to\ncontact. Do they live in a particular area? Are they refusing to answer for a particular reason? Are they more likely to be from a particular racial, ethnic, or income group? This kind of\nanalysis can determine whether or not a low response rate will affect the results of the poll.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nMost important, the interviewer will attempt to make multiple calls at\ndifferent times of day and evening in order to reach each selected phone number. These\nrepeated attempts---as many as ten or twelve calls to the same number---are an important part\nof getting an unbiased sample. Obviously it would be cheaper and easier to make random calls\nto different numbers until a sufficiently large sample of adults have picked up the phone and\nanswered the relevant questions. However, such a sample would be biased toward people who\nare likely to be home and to answer the phone: the unemployed, the elderly, and so on. That’s\njust fine as long as you’re willing to qualify your poll results in the following way: President\nObama’s approval rating stands at 46 percent among the unemployed, old people, and others\nwho are eager to answer random phone calls. One indicator of a poll’s validity is the response rate: What proportion of respondents who\nwere chosen to be contacted ultimately completed the poll or survey? A low response rate can\nbe a warning sign for potential sampling bias. The more people there are who opt not to answer\nthe poll, or who just can’t be reached, the greater the possibility that this large group is\ndifferent in some material way from those who did answer the questions. Pollsters can test for\n“nonresponse bias” by analyzing available data on the respondents whom they were not able to\ncontact. Do they live in a particular area? Are they refusing to answer for a particular reason? Are they more likely to be from a particular racial, ethnic, or income group? This kind of\nanalysis can determine whether or not a low response rate will affect the results of the poll.", "tokens": 349, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 131, "segment_id": "00131", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000230"}
{"type": "chunk", "text": "Have the questions been posed in a way that elicits accurate information on the topic of\ninterest? Soliciting public opinion requires more nuance than measuring test scores or putting\nrespondents on a scale to determine their weight. Survey results can be extremely sensitive to\nthe way a question is asked. Let’s take a seemingly simple example: What proportion of\nAmericans support capital punishment? As the chapter title suggests, a solid and consistent\nmajority of Americans approve of the death penalty. According to Gallup, in every year since\n2002 over 60 percent of Americans have said they favor the death penalty for a person\nconvicted of murder. The percentage of Americans supporting capital punishment has\nfluctuated in a relatively narrow range from a high of 70 percent in 2003 to a low of 64 percent\nat several different points. The polling data are clear: Americans support the death penalty by\na wide margin. Or not. American support for the death penalty plummets when life imprisonment without\nparole is offered as an alternative . A 2006 Gallup poll found that only 47 percent of\nAmericans judged the death penalty as the appropriate penalty for murder, as opposed to 48\npercent who preferred life imprisonment.2 That’s not just a statistical factoid to amuse guests\nat a dinner party; it means that there is no longer majority support for capital punishment when\nlife in prison without parole is a credible alternative. When we solicit public opinion, the\nphrasing of the question and the choice of language can matter enormously. Politicians will often exploit this phenomenon by using polls and focus groups to test “words\nthat work.” For example, voters are more inclined to support “tax relief” than “tax cuts,” even\nthough the two phrases describe the same thing. Similarly, voters are less concerned about\n“climate change” than they are about “global warming,” even though global warming is a form\nof climate change. Obviously politicians are trying to manipulate voters’ responses by\nchoosing nonneutral words. If pollsters are to be considered honest brokers generating\nlegitimate results, they must guard against language that is prone to affect the accuracy of the\ninformation collected. Similarly, if answers are to be compared over time---such as how\nconsumers feel about the economy today compared with how they felt a year ago---then the\nquestions eliciting that information over time must be the same, or very similar.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nHave the questions been posed in a way that elicits accurate information on the topic of\ninterest? Soliciting public opinion requires more nuance than measuring test scores or putting\nrespondents on a scale to determine their weight. Survey results can be extremely sensitive to\nthe way a question is asked. Let’s take a seemingly simple example: What proportion of\nAmericans support capital punishment? As the chapter title suggests, a solid and consistent\nmajority of Americans approve of the death penalty. According to Gallup, in every year since\n2002 over 60 percent of Americans have said they favor the death penalty for a person\nconvicted of murder. The percentage of Americans supporting capital punishment has\nfluctuated in a relatively narrow range from a high of 70 percent in 2003 to a low of 64 percent\nat several different points. The polling data are clear: Americans support the death penalty by\na wide margin. Or not. American support for the death penalty plummets when life imprisonment without\nparole is offered as an alternative . A 2006 Gallup poll found that only 47 percent of\nAmericans judged the death penalty as the appropriate penalty for murder, as opposed to 48\npercent who preferred life imprisonment.2 That’s not just a statistical factoid to amuse guests\nat a dinner party; it means that there is no longer majority support for capital punishment when\nlife in prison without parole is a credible alternative. When we solicit public opinion, the\nphrasing of the question and the choice of language can matter enormously. Politicians will often exploit this phenomenon by using polls and focus groups to test “words\nthat work.” For example, voters are more inclined to support “tax relief” than “tax cuts,” even\nthough the two phrases describe the same thing. Similarly, voters are less concerned about\n“climate change” than they are about “global warming,” even though global warming is a form\nof climate change. Obviously politicians are trying to manipulate voters’ responses by\nchoosing nonneutral words. If pollsters are to be considered honest brokers generating\nlegitimate results, they must guard against language that is prone to affect the accuracy of the\ninformation collected. Similarly, if answers are to be compared over time---such as how\nconsumers feel about the economy today compared with how they felt a year ago---then the\nquestions eliciting that information over time must be the same, or very similar.", "tokens": 499, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 132, "segment_id": "00132", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000231"}
{"type": "chunk", "text": "Obviously politicians are trying to manipulate voters’ responses by\nchoosing nonneutral words. If pollsters are to be considered honest brokers generating\nlegitimate results, they must guard against language that is prone to affect the accuracy of the\ninformation collected. Similarly, if answers are to be compared over time---such as how\nconsumers feel about the economy today compared with how they felt a year ago---then the\nquestions eliciting that information over time must be the same, or very similar. Polling organizations like Gallup will often conduct “split sample testing,” in which\nvariations of a question are tested on different samples to gauge how small changes in wording\naffect respondents’ answers. To experts like Gallup’s Frank Newport, the answers to every\nquestion present meaningful data, even when those answers may appear to be inconsistent.3\nThe fact that American attitudes toward capital punishment change dramatically when life\nwithout parole is offered as an option tells us something important. The key point, says\nNewport, is to view any polling result in context. No single question or poll can capture the\nfull depth of public opinion on a complex issue. Are respondents telling the truth? Polling is like Internet dating: There is a little wiggle room\nin the veracity of information provided. We know that people shade the truth, particularly when\nthe questions asked are embarrassing or sensitive. Respondents may overstate their income, or\ninflate the number of times they have sex in a typical month. They may not admit that they do", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nObviously politicians are trying to manipulate voters’ responses by\nchoosing nonneutral words. If pollsters are to be considered honest brokers generating\nlegitimate results, they must guard against language that is prone to affect the accuracy of the\ninformation collected. Similarly, if answers are to be compared over time---such as how\nconsumers feel about the economy today compared with how they felt a year ago---then the\nquestions eliciting that information over time must be the same, or very similar. Polling organizations like Gallup will often conduct “split sample testing,” in which\nvariations of a question are tested on different samples to gauge how small changes in wording\naffect respondents’ answers. To experts like Gallup’s Frank Newport, the answers to every\nquestion present meaningful data, even when those answers may appear to be inconsistent.3\nThe fact that American attitudes toward capital punishment change dramatically when life\nwithout parole is offered as an option tells us something important. The key point, says\nNewport, is to view any polling result in context. No single question or poll can capture the\nfull depth of public opinion on a complex issue. Are respondents telling the truth? Polling is like Internet dating: There is a little wiggle room\nin the veracity of information provided. We know that people shade the truth, particularly when\nthe questions asked are embarrassing or sensitive. Respondents may overstate their income, or\ninflate the number of times they have sex in a typical month. They may not admit that they do", "tokens": 310, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 132, "segment_id": "00132", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000232"}
{"type": "chunk", "text": "not vote. They may hesitate to express views that are unpopular or socially unacceptable. For\nall these reasons, even the most carefully designed poll is dependent on the integrity of the\nrespondents’ answers. Election polls depend crucially on sorting those who will vote on Election Day from those\nwho will not. (If we are trying to gauge the likely winner of an election, we do not care about\nthe opinions of anyone who is not going to vote.) Individuals often say they are going to vote\nbecause they think that is what pollsters want to hear. Studies that have compared self-reported\nvoting behavior to election records consistently find that one-quarter to one-third of\nrespondents say they voted when in fact they did not.4 One way to minimize this potential bias\nis to ask whether a respondent voted in the last election, or in the last several elections. Respondents who have voted consistently in the past are most likely to vote in the future. Similarly, if there are concerns that respondents may be hesitant to express a socially\nunacceptable answer, such as a negative view of a racial or ethnic group, the question may be\nphrased in a more subtle way, such as by asking “if people you know” hold such an opinion. One of the most sensitive surveys of all time was a study conducted by the National\nOpinion Research Center (NORC) at the University of Chicago called “The Social\nOrganization of Sexuality: Sexual Practices in the United States,” which quickly became\nknown as the “Sex Study.”5 The formal description of the study included phrases like “the\norganization of the behaviors constituting sexual transactions” and “sexual partnering and\nbehavior across the lifecourse.” (I’m not even sure what a “lifecourse” is; spell-check says it’s\nnot a word.) I’m oversimplifying when I write that the survey sought to document who is doing\nwhat to whom---and how often. The purpose of the study, which was published in 1995, was\nnot merely to enlighten us all about the sexual behavior of our neighbors (though that was part\nof it) but also to gauge how sexual behavior in the United States was likely to affect the spread\nof HIV/AIDS. If Americans are hesitant to admit when they don’t vote, you can imagine how keen they are\nto describe their sexual behavior, particularly when it may involve illicit activity, infidelity, or\njust really weird stuff.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nnot vote. They may hesitate to express views that are unpopular or socially unacceptable. For\nall these reasons, even the most carefully designed poll is dependent on the integrity of the\nrespondents’ answers. Election polls depend crucially on sorting those who will vote on Election Day from those\nwho will not. (If we are trying to gauge the likely winner of an election, we do not care about\nthe opinions of anyone who is not going to vote.) Individuals often say they are going to vote\nbecause they think that is what pollsters want to hear. Studies that have compared self-reported\nvoting behavior to election records consistently find that one-quarter to one-third of\nrespondents say they voted when in fact they did not.4 One way to minimize this potential bias\nis to ask whether a respondent voted in the last election, or in the last several elections. Respondents who have voted consistently in the past are most likely to vote in the future. Similarly, if there are concerns that respondents may be hesitant to express a socially\nunacceptable answer, such as a negative view of a racial or ethnic group, the question may be\nphrased in a more subtle way, such as by asking “if people you know” hold such an opinion. One of the most sensitive surveys of all time was a study conducted by the National\nOpinion Research Center (NORC) at the University of Chicago called “The Social\nOrganization of Sexuality: Sexual Practices in the United States,” which quickly became\nknown as the “Sex Study.”5 The formal description of the study included phrases like “the\norganization of the behaviors constituting sexual transactions” and “sexual partnering and\nbehavior across the lifecourse.” (I’m not even sure what a “lifecourse” is; spell-check says it’s\nnot a word.) I’m oversimplifying when I write that the survey sought to document who is doing\nwhat to whom---and how often. The purpose of the study, which was published in 1995, was\nnot merely to enlighten us all about the sexual behavior of our neighbors (though that was part\nof it) but also to gauge how sexual behavior in the United States was likely to affect the spread\nof HIV/AIDS. If Americans are hesitant to admit when they don’t vote, you can imagine how keen they are\nto describe their sexual behavior, particularly when it may involve illicit activity, infidelity, or\njust really weird stuff.", "tokens": 507, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 133, "segment_id": "00133", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000233"}
{"type": "chunk", "text": "The purpose of the study, which was published in 1995, was\nnot merely to enlighten us all about the sexual behavior of our neighbors (though that was part\nof it) but also to gauge how sexual behavior in the United States was likely to affect the spread\nof HIV/AIDS. If Americans are hesitant to admit when they don’t vote, you can imagine how keen they are\nto describe their sexual behavior, particularly when it may involve illicit activity, infidelity, or\njust really weird stuff. The Sex Study methodology was impressive. The research was based\non ninety-minute interviews of 3,342 adults chosen to be representative of the U.S. adult\npopulation. Nearly 80 percent of the selected respondents completed the survey, leading the\nauthors to conclude that the findings are an accurate reporting of America’s sexual behavior (or\nat least what we were doing in 1995). Since you’ve suffered through a chapter on polling methodology, and now nearly an entire\nbook on statistics, you are entitled to a glimpse at what they found (none of which is\nparticularly shocking). As one reviewer noted, “There is much less sexual behavior going on\nthan we might think.”6\n\n• People generally have sex with others like themselves. Ninety percent of couples shared\n\nthe same race, religion, social class, and general age group. • The typical respondent was engaging in sexual activity “a few times a month,” though\nthere was wide variation. The number of sexual partners since age eighteen ranged from", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThe purpose of the study, which was published in 1995, was\nnot merely to enlighten us all about the sexual behavior of our neighbors (though that was part\nof it) but also to gauge how sexual behavior in the United States was likely to affect the spread\nof HIV/AIDS. If Americans are hesitant to admit when they don’t vote, you can imagine how keen they are\nto describe their sexual behavior, particularly when it may involve illicit activity, infidelity, or\njust really weird stuff. The Sex Study methodology was impressive. The research was based\non ninety-minute interviews of 3,342 adults chosen to be representative of the U.S. adult\npopulation. Nearly 80 percent of the selected respondents completed the survey, leading the\nauthors to conclude that the findings are an accurate reporting of America’s sexual behavior (or\nat least what we were doing in 1995). Since you’ve suffered through a chapter on polling methodology, and now nearly an entire\nbook on statistics, you are entitled to a glimpse at what they found (none of which is\nparticularly shocking). As one reviewer noted, “There is much less sexual behavior going on\nthan we might think.”6\n\n• People generally have sex with others like themselves. Ninety percent of couples shared\n\nthe same race, religion, social class, and general age group. • The typical respondent was engaging in sexual activity “a few times a month,” though\nthere was wide variation. The number of sexual partners since age eighteen ranged from", "tokens": 310, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 133, "segment_id": "00133", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000234"}
{"type": "chunk", "text": "zero to over 1,000. • Roughly 5 percent of men and 4 percent of women reported some sexual activity with a\n\nsame-gender partner. • Eighty percent of respondents had either one sexual partner in the previous year or none\n\nat all. • Respondents with one sexual partner were happier than those with none or with multiple\n\npartners.7\n\n• A quarter of the married men and 10 percent of the married women reported having\n\nextramarital sexual activity. • Most people are doing it the old-fashioned way: vaginal intercourse was the most\n\nappealing sexual activity for men and women. One review of the Sex Study made a simple but potent critique: The conclusion that the\naccuracy of the survey represents the sexual practices of adults in the United States “assumes\nthat respondents to the NORC survey both mirrored the population from which they were\ndrawn and gave accurate answers.”8 That sentence could also be the takeaway for this entire\nchapter. At first glance, the most suspicious thing about polling is that the opinions of so few\ncan tell us about the opinions of so many. But that’s the easy part . One of the most basic\nstatistical principles is that a proper sample will look like the population from which it is\ndrawn. The real challenge of polling is twofold: finding and reaching that proper sample; and\neliciting information from that representative group in a way that accurately reflects what its\nmembers believe. APPENDIX TO CHAPTER 10\nWhy is the standard error larger when\np (and 1 -- p) are close to 50 percent? Here is the intuition for why the standard error is highest when the proportion answering a\nparticular way (p) is near 50 percent (which, just as a matter of math, means that 1 -- p will also\nbe close to 50 percent). Let’s imagine that you are conducting two polls in North Dakota. The\nfirst poll is designed to measure the mix of Republicans and Democrats in the state. Assume\nthat the true political mix in the North Dakota population is evenly split 50-50 but that your\npoll finds 60 percent Republicans and 40 percent Democrats. Your results are off by 10\npercentage points, which is a large margin. Yet, you have generated this large error without\nmaking an unimaginably large data-collecting mistake.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nzero to over 1,000. • Roughly 5 percent of men and 4 percent of women reported some sexual activity with a\n\nsame-gender partner. • Eighty percent of respondents had either one sexual partner in the previous year or none\n\nat all. • Respondents with one sexual partner were happier than those with none or with multiple\n\npartners.7\n\n• A quarter of the married men and 10 percent of the married women reported having\n\nextramarital sexual activity. • Most people are doing it the old-fashioned way: vaginal intercourse was the most\n\nappealing sexual activity for men and women. One review of the Sex Study made a simple but potent critique: The conclusion that the\naccuracy of the survey represents the sexual practices of adults in the United States “assumes\nthat respondents to the NORC survey both mirrored the population from which they were\ndrawn and gave accurate answers.”8 That sentence could also be the takeaway for this entire\nchapter. At first glance, the most suspicious thing about polling is that the opinions of so few\ncan tell us about the opinions of so many. But that’s the easy part . One of the most basic\nstatistical principles is that a proper sample will look like the population from which it is\ndrawn. The real challenge of polling is twofold: finding and reaching that proper sample; and\neliciting information from that representative group in a way that accurately reflects what its\nmembers believe. APPENDIX TO CHAPTER 10\nWhy is the standard error larger when\np (and 1 -- p) are close to 50 percent? Here is the intuition for why the standard error is highest when the proportion answering a\nparticular way (p) is near 50 percent (which, just as a matter of math, means that 1 -- p will also\nbe close to 50 percent). Let’s imagine that you are conducting two polls in North Dakota. The\nfirst poll is designed to measure the mix of Republicans and Democrats in the state. Assume\nthat the true political mix in the North Dakota population is evenly split 50-50 but that your\npoll finds 60 percent Republicans and 40 percent Democrats. Your results are off by 10\npercentage points, which is a large margin. Yet, you have generated this large error without\nmaking an unimaginably large data-collecting mistake.", "tokens": 488, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 134, "segment_id": "00134", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000235"}
{"type": "chunk", "text": "Let’s imagine that you are conducting two polls in North Dakota. The\nfirst poll is designed to measure the mix of Republicans and Democrats in the state. Assume\nthat the true political mix in the North Dakota population is evenly split 50-50 but that your\npoll finds 60 percent Republicans and 40 percent Democrats. Your results are off by 10\npercentage points, which is a large margin. Yet, you have generated this large error without\nmaking an unimaginably large data-collecting mistake. You have overcounted the Republicans\nrelative to their true incidence in the population by 20 percent [(60 -- 50)/50]. And in so doing,\nyou have also undercounted the Democrats by 20 percent [(40 -- 50)/50]. That could happen,\neven with a decent polling methodology. Your second poll is designed to measure the fraction of Native Americans in the North\nDakota population. Assume that the true proportion of Native Americans in North Dakota is\n10 percent while non--Native Americans make up 90 percent of the state population. Now let’s", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nLet’s imagine that you are conducting two polls in North Dakota. The\nfirst poll is designed to measure the mix of Republicans and Democrats in the state. Assume\nthat the true political mix in the North Dakota population is evenly split 50-50 but that your\npoll finds 60 percent Republicans and 40 percent Democrats. Your results are off by 10\npercentage points, which is a large margin. Yet, you have generated this large error without\nmaking an unimaginably large data-collecting mistake. You have overcounted the Republicans\nrelative to their true incidence in the population by 20 percent [(60 -- 50)/50]. And in so doing,\nyou have also undercounted the Democrats by 20 percent [(40 -- 50)/50]. That could happen,\neven with a decent polling methodology. Your second poll is designed to measure the fraction of Native Americans in the North\nDakota population. Assume that the true proportion of Native Americans in North Dakota is\n10 percent while non--Native Americans make up 90 percent of the state population. Now let’s", "tokens": 222, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 134, "segment_id": "00134", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000236"}
{"type": "chunk", "text": "discuss how bad your data collecting would have to be in order to produce a poll with a\nsampling error of 10 percentage points. This could happen in two ways. First, you could find\nthat 0 percent of the population is Native American and 100 percent is non--Native American. Or you could find that 20 percent of the population is Native American and 80 percent is non--\nNative American. In one case you have missed all of the Native Americans; and in the other,\nyou have found double their true incidence in the population. These are really bad sampling\nmistakes. In both cases, your estimate is off by 100 percent: either [(0 -- 10)/10] or [(20 --\n10)/10]. And if you missed just 20 percent of the Native Americans---the same degree of error\nthat you had in the Republican-Democrat poll---your results would find 8 percent Native\nAmericans and 92 percent non--Native Americans, which is only 2 percentage points from the\ntrue split in the population. When p and 1 -- p are close to 50 percent, relatively small sampling errors are magnified into\n\nlarge absolute errors in the outcome of the poll. When either p or 1 -- p is closer to zero, the opposite is true. Even relatively large sampling\n\nerrors produce small absolute errors in the outcome of the poll. The same 20 percent sampling error distorted the outcome of the Democrat-Republican poll\nby 10 percentage points while distorting the Native American poll by only 2 percentage points. Since the standard error in a poll is measured in absolute terms (e.g., ± 5 percent), the formula\nrecognizes that this error is likely to be largest when p and 1 -- p are close to 50 percent. * According to its website, “Occupy Wall Street is a people-powered movement that began on September 17, 2011, in\nLiberty Square in Manhattan’s Financial District, and has spread to over 100 cities in the United States and actions in over\n1,500 cities globally. Occupy Wall Street is fighting back against the corrosive power of major banks and multinational\ncorporations over the democratic process, and the role of Wall Street in creating an economic collapse that has caused the\ngreatest recession in generations.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\ndiscuss how bad your data collecting would have to be in order to produce a poll with a\nsampling error of 10 percentage points. This could happen in two ways. First, you could find\nthat 0 percent of the population is Native American and 100 percent is non--Native American. Or you could find that 20 percent of the population is Native American and 80 percent is non--\nNative American. In one case you have missed all of the Native Americans; and in the other,\nyou have found double their true incidence in the population. These are really bad sampling\nmistakes. In both cases, your estimate is off by 100 percent: either [(0 -- 10)/10] or [(20 --\n10)/10]. And if you missed just 20 percent of the Native Americans---the same degree of error\nthat you had in the Republican-Democrat poll---your results would find 8 percent Native\nAmericans and 92 percent non--Native Americans, which is only 2 percentage points from the\ntrue split in the population. When p and 1 -- p are close to 50 percent, relatively small sampling errors are magnified into\n\nlarge absolute errors in the outcome of the poll. When either p or 1 -- p is closer to zero, the opposite is true. Even relatively large sampling\n\nerrors produce small absolute errors in the outcome of the poll. The same 20 percent sampling error distorted the outcome of the Democrat-Republican poll\nby 10 percentage points while distorting the Native American poll by only 2 percentage points. Since the standard error in a poll is measured in absolute terms (e.g., ± 5 percent), the formula\nrecognizes that this error is likely to be largest when p and 1 -- p are close to 50 percent. * According to its website, “Occupy Wall Street is a people-powered movement that began on September 17, 2011, in\nLiberty Square in Manhattan’s Financial District, and has spread to over 100 cities in the United States and actions in over\n1,500 cities globally. Occupy Wall Street is fighting back against the corrosive power of major banks and multinational\ncorporations over the democratic process, and the role of Wall Street in creating an economic collapse that has caused the\ngreatest recession in generations.", "tokens": 477, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 135, "segment_id": "00135", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000237"}
{"type": "chunk", "text": "* According to its website, “Occupy Wall Street is a people-powered movement that began on September 17, 2011, in\nLiberty Square in Manhattan’s Financial District, and has spread to over 100 cities in the United States and actions in over\n1,500 cities globally. Occupy Wall Street is fighting back against the corrosive power of major banks and multinational\ncorporations over the democratic process, and the role of Wall Street in creating an economic collapse that has caused the\ngreatest recession in generations. The movement is inspired by popular uprisings in Egypt and Tunisia, and aims to expose\nhow the richest 1% of people are writing the rules of an unfair global economy that is foreclosing on our future.”\n* We would expect the Republican candidate’s true vote tally to be outside of the confidence interval of the poll roughly 5\npercent of the time. In those cases, his true vote tally would be less than 50 percent or greater than 54 percent. However, if\nhe gets more than 54 percent of the vote, your station has not made an error in declaring him the winner. (You’ve only\nunderstated the margin of his victory.) As a result, the probability that your poll will cause you to mistakenly declare the\nRepublican candidate the winner is only 2.5 percent. * The formula for calculating the standard error of a poll that I have introduced here assumes that the poll is conducted on a\nrandom sample of the population. Sophisticated polling organizations may deviate from this sampling method, in which\ncase the formula for calculating the standard error will also change slightly. The basic methodology remains the same,\nhowever.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\n* According to its website, “Occupy Wall Street is a people-powered movement that began on September 17, 2011, in\nLiberty Square in Manhattan’s Financial District, and has spread to over 100 cities in the United States and actions in over\n1,500 cities globally. Occupy Wall Street is fighting back against the corrosive power of major banks and multinational\ncorporations over the democratic process, and the role of Wall Street in creating an economic collapse that has caused the\ngreatest recession in generations. The movement is inspired by popular uprisings in Egypt and Tunisia, and aims to expose\nhow the richest 1% of people are writing the rules of an unfair global economy that is foreclosing on our future.”\n* We would expect the Republican candidate’s true vote tally to be outside of the confidence interval of the poll roughly 5\npercent of the time. In those cases, his true vote tally would be less than 50 percent or greater than 54 percent. However, if\nhe gets more than 54 percent of the vote, your station has not made an error in declaring him the winner. (You’ve only\nunderstated the margin of his victory.) As a result, the probability that your poll will cause you to mistakenly declare the\nRepublican candidate the winner is only 2.5 percent. * The formula for calculating the standard error of a poll that I have introduced here assumes that the poll is conducted on a\nrandom sample of the population. Sophisticated polling organizations may deviate from this sampling method, in which\ncase the formula for calculating the standard error will also change slightly. The basic methodology remains the same,\nhowever.", "tokens": 344, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 135, "segment_id": "00135", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000238"}
{"type": "chunk", "text": "CHAPTER 11\nRegression Analysis\nThe miracle elixir\n\nCan stress on the job kill you? Yes. There is compelling evidence that rigors on the job can\n\nlead to premature death, particularly of heart disease. But it’s not the kind of stress you are\nprobably imagining. CEOs, who must routinely make massively important decisions that\ndetermine the fate of their companies, are at significantly less risk than their secretaries, who\ndutifully answer the phone and perform other tasks as instructed. How can that possibly make\nsense? It turns out that the most dangerous kind of job stress stems from having “low control”\nover one’s responsibilities. Several studies of thousands of British civil servants (the Whitehall\nstudies) have found that workers who have little control over their jobs---meaning they have\nminimal say over what tasks are performed or how those tasks are carried out---have a\nsignificantly higher mortality rate than other workers in the civil service with more decisionmaking authority. According to this research, it is not the stress associated with major\nresponsibilities that will kill you; it is the stress associated with being told what to do while\nhaving little say in how or when it gets done. This is not a chapter about job stress, heart disease, or British civil servants. The relevant\nquestion regarding the Whitehall studies (and others like them) is how researchers can possibly\ncome to such a conclusion. Clearly this cannot be a randomized experiment. We cannot\narbitrarily assign human beings to different jobs, force them to work in those jobs for many\nyears, and then measure who dies at the highest rate. (Ethical concerns aside, we would\npresumably wreak havoc on the British civil service by randomly distributing jobs.) Instead,\nresearchers have collected detailed longitudinal data on thousands of individuals in the British\ncivil service; these data can be analyzed to identify meaningful associations, such as the\nconnection between “low control” jobs and coronary heart disease. A simple association is not enough to conclude that certain kinds of jobs are bad for your\nhealth. If we merely observe that low-ranking workers in the British civil service hierarchy\nhave higher rates of heart disease, our results would be confounded by other factors. For\nexample, we would expect low-level workers to have less education than senior officials in the\nbureaucracy. They may be more likely to smoke (perhaps because of their job frustration).", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nCHAPTER 11\nRegression Analysis\nThe miracle elixir\n\nCan stress on the job kill you? Yes. There is compelling evidence that rigors on the job can\n\nlead to premature death, particularly of heart disease. But it’s not the kind of stress you are\nprobably imagining. CEOs, who must routinely make massively important decisions that\ndetermine the fate of their companies, are at significantly less risk than their secretaries, who\ndutifully answer the phone and perform other tasks as instructed. How can that possibly make\nsense? It turns out that the most dangerous kind of job stress stems from having “low control”\nover one’s responsibilities. Several studies of thousands of British civil servants (the Whitehall\nstudies) have found that workers who have little control over their jobs---meaning they have\nminimal say over what tasks are performed or how those tasks are carried out---have a\nsignificantly higher mortality rate than other workers in the civil service with more decisionmaking authority. According to this research, it is not the stress associated with major\nresponsibilities that will kill you; it is the stress associated with being told what to do while\nhaving little say in how or when it gets done. This is not a chapter about job stress, heart disease, or British civil servants. The relevant\nquestion regarding the Whitehall studies (and others like them) is how researchers can possibly\ncome to such a conclusion. Clearly this cannot be a randomized experiment. We cannot\narbitrarily assign human beings to different jobs, force them to work in those jobs for many\nyears, and then measure who dies at the highest rate. (Ethical concerns aside, we would\npresumably wreak havoc on the British civil service by randomly distributing jobs.) Instead,\nresearchers have collected detailed longitudinal data on thousands of individuals in the British\ncivil service; these data can be analyzed to identify meaningful associations, such as the\nconnection between “low control” jobs and coronary heart disease. A simple association is not enough to conclude that certain kinds of jobs are bad for your\nhealth. If we merely observe that low-ranking workers in the British civil service hierarchy\nhave higher rates of heart disease, our results would be confounded by other factors. For\nexample, we would expect low-level workers to have less education than senior officials in the\nbureaucracy. They may be more likely to smoke (perhaps because of their job frustration).", "tokens": 498, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 136, "segment_id": "00136", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000239"}
{"type": "chunk", "text": "A simple association is not enough to conclude that certain kinds of jobs are bad for your\nhealth. If we merely observe that low-ranking workers in the British civil service hierarchy\nhave higher rates of heart disease, our results would be confounded by other factors. For\nexample, we would expect low-level workers to have less education than senior officials in the\nbureaucracy. They may be more likely to smoke (perhaps because of their job frustration). They may have had less healthy childhoods, which diminished their job prospects. Or their\nlower pay may limit their access to health care. And so on. The point is that any study simply\ncomparing health outcomes across a large group of British workers---or across any other large\ngroup---will not really tell us much. Other sources of variation in the data are likely to obscure\nthe relationship that we care about. Is “low job control” really causing heart disease? Or is it\nsome combination of other factors that happen to be shared by people with low job control, in\nwhich case we may be completely missing the real public health threat.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nA simple association is not enough to conclude that certain kinds of jobs are bad for your\nhealth. If we merely observe that low-ranking workers in the British civil service hierarchy\nhave higher rates of heart disease, our results would be confounded by other factors. For\nexample, we would expect low-level workers to have less education than senior officials in the\nbureaucracy. They may be more likely to smoke (perhaps because of their job frustration). They may have had less healthy childhoods, which diminished their job prospects. Or their\nlower pay may limit their access to health care. And so on. The point is that any study simply\ncomparing health outcomes across a large group of British workers---or across any other large\ngroup---will not really tell us much. Other sources of variation in the data are likely to obscure\nthe relationship that we care about. Is “low job control” really causing heart disease? Or is it\nsome combination of other factors that happen to be shared by people with low job control, in\nwhich case we may be completely missing the real public health threat.", "tokens": 226, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 136, "segment_id": "00136", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000240"}
{"type": "chunk", "text": "Regression analysis is the statistical tool that helps us deal with this challenge. Specifically,\nregression analysis allows us to quantify the relationship between a particular variable and an\noutcome that we care about while controlling for other factors. In other words, we can isolate\nthe effect of one variable, such as having a certain kind of job, while holding the effects of\nother variables constant. The Whitehall studies used regression analysis to measure the health\nimpacts of low job control among people who are similar in other ways, such as smoking\nbehavior. (Low-level workers do in fact smoke more than their superiors; this explains a\nrelatively small amount of the variation in heart disease across the Whitehall hierarchy.)\n\nMost of the studies that you read about in the newspaper are based on regression analysis. When researchers conclude that children who spend a lot of time in day care are more prone to\nbehavioral problems in elementary school than children who spend that time at home, the study\nhas not randomly assigned thousands of infants either to day care or to home care with a\nparent. Nor has the study simply compared the elementary school behavior of children who had\ndifferent early childhood experiences without recognizing that these populations are likely to\nbe different in other fundamental ways. Different families make different child care decisions\nbecause they are different. Some households have two parents present; some don’t. Some\nhave two parents working; some don’t. Some households are wealthier or more educated than\nothers. All of these things affect child care decisions, and they affect how children in those\nfamilies will perform in elementary school. When done properly, regression analysis can help\nus estimate the effects of day care apart from other things that affect young children: family\nincome, family structure, parental education, and so on. Now, there are two key phrases in that last sentence. The first is “when done properly.”\nGiven adequate data and access to a personal computer, a six-year-old could use a basic\nstatistics program to generate regression results. Personal computing has made the mechanics\nof regression analysis almost effortless. The problem is that the mechanics of regression\nanalysis are not the hard part; the hard part is determining which variables ought to be\nconsidered in the analysis and how that can best be done. Regression analysis is like one of\nthose fancy power tools. It is relatively easy to use, but hard to use well---and potentially\ndangerous when used improperly.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nRegression analysis is the statistical tool that helps us deal with this challenge. Specifically,\nregression analysis allows us to quantify the relationship between a particular variable and an\noutcome that we care about while controlling for other factors. In other words, we can isolate\nthe effect of one variable, such as having a certain kind of job, while holding the effects of\nother variables constant. The Whitehall studies used regression analysis to measure the health\nimpacts of low job control among people who are similar in other ways, such as smoking\nbehavior. (Low-level workers do in fact smoke more than their superiors; this explains a\nrelatively small amount of the variation in heart disease across the Whitehall hierarchy.)\n\nMost of the studies that you read about in the newspaper are based on regression analysis. When researchers conclude that children who spend a lot of time in day care are more prone to\nbehavioral problems in elementary school than children who spend that time at home, the study\nhas not randomly assigned thousands of infants either to day care or to home care with a\nparent. Nor has the study simply compared the elementary school behavior of children who had\ndifferent early childhood experiences without recognizing that these populations are likely to\nbe different in other fundamental ways. Different families make different child care decisions\nbecause they are different. Some households have two parents present; some don’t. Some\nhave two parents working; some don’t. Some households are wealthier or more educated than\nothers. All of these things affect child care decisions, and they affect how children in those\nfamilies will perform in elementary school. When done properly, regression analysis can help\nus estimate the effects of day care apart from other things that affect young children: family\nincome, family structure, parental education, and so on. Now, there are two key phrases in that last sentence. The first is “when done properly.”\nGiven adequate data and access to a personal computer, a six-year-old could use a basic\nstatistics program to generate regression results. Personal computing has made the mechanics\nof regression analysis almost effortless. The problem is that the mechanics of regression\nanalysis are not the hard part; the hard part is determining which variables ought to be\nconsidered in the analysis and how that can best be done. Regression analysis is like one of\nthose fancy power tools. It is relatively easy to use, but hard to use well---and potentially\ndangerous when used improperly.", "tokens": 496, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 137, "segment_id": "00137", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000241"}
{"type": "chunk", "text": "The first is “when done properly.”\nGiven adequate data and access to a personal computer, a six-year-old could use a basic\nstatistics program to generate regression results. Personal computing has made the mechanics\nof regression analysis almost effortless. The problem is that the mechanics of regression\nanalysis are not the hard part; the hard part is determining which variables ought to be\nconsidered in the analysis and how that can best be done. Regression analysis is like one of\nthose fancy power tools. It is relatively easy to use, but hard to use well---and potentially\ndangerous when used improperly. The second important phrase above is “help us estimate.” Our child care study does not give\nus a “right” answer for the relationship between day care and subsequent school performance. Instead, it quantifies the relationship observed for a particular group of children over a\nparticular stretch of time. Can we draw conclusions that might apply to the broader\npopulation? Yes, but we will have the same limitations and qualifications as we do with any\nother kind of inference. First, our sample has to be representative of the population that we\ncare about. A study of 2,000 young children in Sweden will not tell us much about the best\npolicies for early childhood education in rural Mexico. And second, there will be variation\nfrom sample to sample. If we do multiple studies of children and child care, each study will\nproduce slightly different findings, even if the methodologies are all sound and similar. Regression analysis is similar to polling. The good news is that if we have a large\nrepresentative sample and solid methodology, the relationship we observe for our sample data", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThe first is “when done properly.”\nGiven adequate data and access to a personal computer, a six-year-old could use a basic\nstatistics program to generate regression results. Personal computing has made the mechanics\nof regression analysis almost effortless. The problem is that the mechanics of regression\nanalysis are not the hard part; the hard part is determining which variables ought to be\nconsidered in the analysis and how that can best be done. Regression analysis is like one of\nthose fancy power tools. It is relatively easy to use, but hard to use well---and potentially\ndangerous when used improperly. The second important phrase above is “help us estimate.” Our child care study does not give\nus a “right” answer for the relationship between day care and subsequent school performance. Instead, it quantifies the relationship observed for a particular group of children over a\nparticular stretch of time. Can we draw conclusions that might apply to the broader\npopulation? Yes, but we will have the same limitations and qualifications as we do with any\nother kind of inference. First, our sample has to be representative of the population that we\ncare about. A study of 2,000 young children in Sweden will not tell us much about the best\npolicies for early childhood education in rural Mexico. And second, there will be variation\nfrom sample to sample. If we do multiple studies of children and child care, each study will\nproduce slightly different findings, even if the methodologies are all sound and similar. Regression analysis is similar to polling. The good news is that if we have a large\nrepresentative sample and solid methodology, the relationship we observe for our sample data", "tokens": 338, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 137, "segment_id": "00137", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000242"}
{"type": "chunk", "text": "is not likely to deviate wildly from the true relationship for the whole population. If 10,000\npeople who exercise three or more times a week have sharply lower rates of cardiovascular\ndisease than 10,000 people who don’t exercise (but are similar in all other important respects),\nthen the chances are pretty good that we will see a similar association between exercise and\ncardiovascular health for the broader population. That’s why we do these studies. (The point is\nnot to tell those nonexercisers who are sick at the end of the study that they should have\nexercised.)\n\nThe bad news is that we are not proving definitively that exercise prevents heart disease. We\nare instead rejecting the null hypothesis that exercise has no association with heart disease, on\nthe basis of some statistical threshold that was chosen before the study was conducted. Specifically, the authors of the study would report that if exercise is unrelated to\ncardiovascular health, the likelihood of observing such a marked difference in heart disease\nbetween the exercisers and nonexercisers in this large sample would be less than 5 in 100, or\nbelow some other threshold for statistical significance. Let’s pause for a moment and wave our first giant yellow flag. Suppose that this particular\nstudy compared a large group of individuals who play squash regularly with those of an equalsized group who get no exercise at all. Playing squash does provide a good cardiovascular\nworkout. However, we also know that squash players tend to be affluent enough to belong to\nclubs with squash courts. Wealthy individuals may have great access to health care, which can\nalso improve cardiovascular health. If our analysis is sloppy, we may attribute health benefits\nto playing squash when in fact the real benefit comes from being wealthy enough to play\nsquash (in which case playing polo would also be associated with better heart health, even\nthough the horse is doing most of the work). Or perhaps causality goes the other direction. Could having a healthy heart “cause”\nexercise? Yes. Individuals who are infirm, particularly those who have some incipient form of\nheart disease, will find it much harder to exercise. They will certainly be less likely to play\nsquash regularly. Again, if the analysis is sloppy or oversimplified, the claim that exercise is\ngood for your health may simply reflect the fact that people who start out unhealthy find it hard\nto exercise.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nis not likely to deviate wildly from the true relationship for the whole population. If 10,000\npeople who exercise three or more times a week have sharply lower rates of cardiovascular\ndisease than 10,000 people who don’t exercise (but are similar in all other important respects),\nthen the chances are pretty good that we will see a similar association between exercise and\ncardiovascular health for the broader population. That’s why we do these studies. (The point is\nnot to tell those nonexercisers who are sick at the end of the study that they should have\nexercised.)\n\nThe bad news is that we are not proving definitively that exercise prevents heart disease. We\nare instead rejecting the null hypothesis that exercise has no association with heart disease, on\nthe basis of some statistical threshold that was chosen before the study was conducted. Specifically, the authors of the study would report that if exercise is unrelated to\ncardiovascular health, the likelihood of observing such a marked difference in heart disease\nbetween the exercisers and nonexercisers in this large sample would be less than 5 in 100, or\nbelow some other threshold for statistical significance. Let’s pause for a moment and wave our first giant yellow flag. Suppose that this particular\nstudy compared a large group of individuals who play squash regularly with those of an equalsized group who get no exercise at all. Playing squash does provide a good cardiovascular\nworkout. However, we also know that squash players tend to be affluent enough to belong to\nclubs with squash courts. Wealthy individuals may have great access to health care, which can\nalso improve cardiovascular health. If our analysis is sloppy, we may attribute health benefits\nto playing squash when in fact the real benefit comes from being wealthy enough to play\nsquash (in which case playing polo would also be associated with better heart health, even\nthough the horse is doing most of the work). Or perhaps causality goes the other direction. Could having a healthy heart “cause”\nexercise? Yes. Individuals who are infirm, particularly those who have some incipient form of\nheart disease, will find it much harder to exercise. They will certainly be less likely to play\nsquash regularly. Again, if the analysis is sloppy or oversimplified, the claim that exercise is\ngood for your health may simply reflect the fact that people who start out unhealthy find it hard\nto exercise.", "tokens": 496, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 138, "segment_id": "00138", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000243"}
{"type": "chunk", "text": "Or perhaps causality goes the other direction. Could having a healthy heart “cause”\nexercise? Yes. Individuals who are infirm, particularly those who have some incipient form of\nheart disease, will find it much harder to exercise. They will certainly be less likely to play\nsquash regularly. Again, if the analysis is sloppy or oversimplified, the claim that exercise is\ngood for your health may simply reflect the fact that people who start out unhealthy find it hard\nto exercise. In this case, playing squash doesn’t make anyone healthier; it merely separates the\nhealthy from the unhealthy. There are so many potential regression pitfalls that I’ve devoted the next chapter to the most\negregious errors. For now, we’ll focus on what can go right. Regression analysis has the\namazing capacity to isolate a statistical relationship that we care about, such as that between\njob control and heart disease, while taking into account other factors that might confuse the\nrelationship. How exactly does this work? If we know that low-level British civil servants smoke more\nthan their superiors, how can we discern which part of their poor cardiovascular health is due\nto their low-level jobs, and which part is due to the smoking? These two factors seem\ninextricably intertwined. Regression analysis (done properly!) can untangle them. To explain the intuition, I need to\nbegin with the basic idea that underlies all forms of regression analysis---from the simplest", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nOr perhaps causality goes the other direction. Could having a healthy heart “cause”\nexercise? Yes. Individuals who are infirm, particularly those who have some incipient form of\nheart disease, will find it much harder to exercise. They will certainly be less likely to play\nsquash regularly. Again, if the analysis is sloppy or oversimplified, the claim that exercise is\ngood for your health may simply reflect the fact that people who start out unhealthy find it hard\nto exercise. In this case, playing squash doesn’t make anyone healthier; it merely separates the\nhealthy from the unhealthy. There are so many potential regression pitfalls that I’ve devoted the next chapter to the most\negregious errors. For now, we’ll focus on what can go right. Regression analysis has the\namazing capacity to isolate a statistical relationship that we care about, such as that between\njob control and heart disease, while taking into account other factors that might confuse the\nrelationship. How exactly does this work? If we know that low-level British civil servants smoke more\nthan their superiors, how can we discern which part of their poor cardiovascular health is due\nto their low-level jobs, and which part is due to the smoking? These two factors seem\ninextricably intertwined. Regression analysis (done properly!) can untangle them. To explain the intuition, I need to\nbegin with the basic idea that underlies all forms of regression analysis---from the simplest", "tokens": 299, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 138, "segment_id": "00138", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000244"}
{"type": "chunk", "text": "statistical relationships to the complex models cobbled together by Nobel Prize winners. At its\ncore, regression analysis seeks to find the “best fit” for a linear relationship between two\nvariables. A simple example is the relationship between height and weight. People who are\ntaller tend to weigh more---though that is obviously not always the case. If we were to plot the\nheights and weights of a group of graduate students, you might recall what it looked like from\nChapter 4:\n\nScatter Plot for Height and Weight\n\nIf you were asked to describe the pattern, you might say something along the lines of\n“Weight seems to increase with height.” This is not a terribly insightful or specific statement. Regression analysis enables us to go one step further and “fit a line” that best describes a\nlinear relationship between the two variables. Many possible lines are broadly consistent with the height and weight data. But how do we\nknow which is the best line for these data? In fact, how exactly would we define “best”? Regression analysis typically uses a methodology called ordinary least squares, or OLS. The\ntechnical details, including why OLS produces the best fit, will have to be left to a more\nadvanced book. The key point lies in the “least squares” part of the name; OLS fits the line\nthat minimizes the sum of the squared residuals. That’s not as awfully complicated as it\nsounds. Each observation in our height and weight data set has a residual, which is its vertical\ndistance from the regression line, except for those observations that lie directly on the line, for\nwhich the residual equals zero. (On the diagram below, the residual is marked for a\nhypothetical person A.) It should be intuitive that the larger the sum of residuals overall, the\nworse the fit of the line. The only nonintuitive twist with OLS is that the formula takes the\nsquare of each residual before adding them all up (which increases the weight given to\nobservations that lie particularly far from the regression line, or the “outliers”). Ordinary least squares “fits” the line that minimizes the sum of the squared residuals, as\n\nillustrated below. Line of Best Fit for Height and Weight", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nstatistical relationships to the complex models cobbled together by Nobel Prize winners. At its\ncore, regression analysis seeks to find the “best fit” for a linear relationship between two\nvariables. A simple example is the relationship between height and weight. People who are\ntaller tend to weigh more---though that is obviously not always the case. If we were to plot the\nheights and weights of a group of graduate students, you might recall what it looked like from\nChapter 4:\n\nScatter Plot for Height and Weight\n\nIf you were asked to describe the pattern, you might say something along the lines of\n“Weight seems to increase with height.” This is not a terribly insightful or specific statement. Regression analysis enables us to go one step further and “fit a line” that best describes a\nlinear relationship between the two variables. Many possible lines are broadly consistent with the height and weight data. But how do we\nknow which is the best line for these data? In fact, how exactly would we define “best”? Regression analysis typically uses a methodology called ordinary least squares, or OLS. The\ntechnical details, including why OLS produces the best fit, will have to be left to a more\nadvanced book. The key point lies in the “least squares” part of the name; OLS fits the line\nthat minimizes the sum of the squared residuals. That’s not as awfully complicated as it\nsounds. Each observation in our height and weight data set has a residual, which is its vertical\ndistance from the regression line, except for those observations that lie directly on the line, for\nwhich the residual equals zero. (On the diagram below, the residual is marked for a\nhypothetical person A.) It should be intuitive that the larger the sum of residuals overall, the\nworse the fit of the line. The only nonintuitive twist with OLS is that the formula takes the\nsquare of each residual before adding them all up (which increases the weight given to\nobservations that lie particularly far from the regression line, or the “outliers”). Ordinary least squares “fits” the line that minimizes the sum of the squared residuals, as\n\nillustrated below. Line of Best Fit for Height and Weight", "tokens": 461, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 139, "segment_id": "00139", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000245"}
{"type": "chunk", "text": "If the technical details have given you a headache, you can be forgiven for just grasping at\nthe bottom line, which is that ordinary least squares gives us the best description of a linear\nrelationship between two variables. The result is not only a line but, as you may recall from\nhigh school geometry, an equation describing that line. This is known as the regression\nequation, and it takes the following form: y = a + bx, where y is weight in pounds; a is the yintercept of the line (the value for y when x = 0); b is the slope of the line; and x is height in\ninches. The slope of the line we’ve fitted, b, describes the “best” linear relationship between\nheight and weight for this sample, as defined by ordinary least squares. The regression line certainly does not describe every observation in the data set perfectly. But it is the best description we can muster for what is clearly a meaningful relationship\nbetween height and weight. It also means that every observation can be explained as WEIGHT\n= a + b(HEIGHT) + e, where e is a “residual” that catches the variation in weight for each\nindividual that is not explained by height. Finally, it means that our best guess for the weight of\nany person in the data set would be a + b(HEIGHT). Even though most observations do not lie\nexactly on the regression line, the residual still has an expected value of zero since any person\nin our sample is just as likely to weigh more than the regression equation predicts as he is to\nweigh less. Enough of this theoretical jargon! Let’s look at some real height and weight data from the\nChanging Lives study, though I should first clarify some basic terminology. The variable that is\nbeing explained---weight in this case---is known as the dependent variable (because it depends\non other factors). The variables that we are using to explain our dependent variable are known\nas explanatory variables since they explain the outcome that we care about. (Just to make\nthings hard, the explanatory variables are also sometimes called independent variables or\ncontrol variables.) Let’s start by using height to explain weight among the Changing Lives\nparticipants; later we’ll add other potential explanatory factors.* There are 3,537 adult\nparticipants in the Changing Lives study. This is our number of observations, or n.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nIf the technical details have given you a headache, you can be forgiven for just grasping at\nthe bottom line, which is that ordinary least squares gives us the best description of a linear\nrelationship between two variables. The result is not only a line but, as you may recall from\nhigh school geometry, an equation describing that line. This is known as the regression\nequation, and it takes the following form: y = a + bx, where y is weight in pounds; a is the yintercept of the line (the value for y when x = 0); b is the slope of the line; and x is height in\ninches. The slope of the line we’ve fitted, b, describes the “best” linear relationship between\nheight and weight for this sample, as defined by ordinary least squares. The regression line certainly does not describe every observation in the data set perfectly. But it is the best description we can muster for what is clearly a meaningful relationship\nbetween height and weight. It also means that every observation can be explained as WEIGHT\n= a + b(HEIGHT) + e, where e is a “residual” that catches the variation in weight for each\nindividual that is not explained by height. Finally, it means that our best guess for the weight of\nany person in the data set would be a + b(HEIGHT). Even though most observations do not lie\nexactly on the regression line, the residual still has an expected value of zero since any person\nin our sample is just as likely to weigh more than the regression equation predicts as he is to\nweigh less. Enough of this theoretical jargon! Let’s look at some real height and weight data from the\nChanging Lives study, though I should first clarify some basic terminology. The variable that is\nbeing explained---weight in this case---is known as the dependent variable (because it depends\non other factors). The variables that we are using to explain our dependent variable are known\nas explanatory variables since they explain the outcome that we care about. (Just to make\nthings hard, the explanatory variables are also sometimes called independent variables or\ncontrol variables.) Let’s start by using height to explain weight among the Changing Lives\nparticipants; later we’ll add other potential explanatory factors.* There are 3,537 adult\nparticipants in the Changing Lives study. This is our number of observations, or n.", "tokens": 492, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 140, "segment_id": "00140", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000246"}
{"type": "chunk", "text": "The variable that is\nbeing explained---weight in this case---is known as the dependent variable (because it depends\non other factors). The variables that we are using to explain our dependent variable are known\nas explanatory variables since they explain the outcome that we care about. (Just to make\nthings hard, the explanatory variables are also sometimes called independent variables or\ncontrol variables.) Let’s start by using height to explain weight among the Changing Lives\nparticipants; later we’ll add other potential explanatory factors.* There are 3,537 adult\nparticipants in the Changing Lives study. This is our number of observations, or n. (Sometimes\na research paper might note that n = 3,537.) When we run a simple regression on the Changing\nLives data with weight as the dependent variable and height as the only explanatory variable,\nwe get the following results:\n\nWEIGHT = --135 + (4.5) × HEIGHT IN INCHES", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThe variable that is\nbeing explained---weight in this case---is known as the dependent variable (because it depends\non other factors). The variables that we are using to explain our dependent variable are known\nas explanatory variables since they explain the outcome that we care about. (Just to make\nthings hard, the explanatory variables are also sometimes called independent variables or\ncontrol variables.) Let’s start by using height to explain weight among the Changing Lives\nparticipants; later we’ll add other potential explanatory factors.* There are 3,537 adult\nparticipants in the Changing Lives study. This is our number of observations, or n. (Sometimes\na research paper might note that n = 3,537.) When we run a simple regression on the Changing\nLives data with weight as the dependent variable and height as the only explanatory variable,\nwe get the following results:\n\nWEIGHT = --135 + (4.5) × HEIGHT IN INCHES", "tokens": 193, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 140, "segment_id": "00140", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000247"}
{"type": "chunk", "text": "a = --135. This is the y-intercept, which has no particular meaning on its own. (If you\ninterpret it literally, a person who measures zero inches would weigh negative 135 pounds;\nobviously this is nonsense on several levels.) This figure is also known as the constant,\nbecause it is the starting point for calculating the weight of all observations in the study. b = 4.5. Our estimate for b, 4.5, is known as a regression coefficient, or in statistics jargon,\n“the coefficient on height,” because it gives us the best estimate of the relationship between\nheight and weight among the Changing Lives participants. The regression coefficient has a\nconvenient interpretation: a one-unit increase in the independent variable (height) is associated\nwith an increase of 4.5 units in the dependent variable (weight). For our data sample, this\nmeans that a 1-inch increase in height is associated with a 4.5 pound increase in weight. Thus,\nif we had no other information, our best guess for the weight of a person who is 5 feet 10\ninches tall (70 inches) in the Changing Lives study would be --135 + 4.5 (70) = 180 pounds. This is our payoff, as we have now quantified the best fit for the linear relationship between\nheight and weight for the Changing Lives participants. The same basic tools can be used to\nexplore more complex relationships and more socially significant questions. For any regression\ncoefficient, you will generally be interested in three things: sign, size, and significance. Sign. The sign (positive or negative) on the coefficient for an independent variable tells us\nthe direction of its association with the dependent variable (the outcome we are trying to\nexplain). In the simple case above, the coefficient on height is positive. Taller people tend to\nweigh more. Some relationships will work in the other direction. I would expect the association\nbetween exercise and weight to be negative. If the Changing Lives study included data on\nsomething like “miles run per month,” I am fairly certain that the coefficient on “miles run”\nwould be negative. Running more is associated with weighing less. Size. How big is the observed effect between the independent variable and the dependent\nvariable? Is it of a magnitude that matters?", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\na = --135. This is the y-intercept, which has no particular meaning on its own. (If you\ninterpret it literally, a person who measures zero inches would weigh negative 135 pounds;\nobviously this is nonsense on several levels.) This figure is also known as the constant,\nbecause it is the starting point for calculating the weight of all observations in the study. b = 4.5. Our estimate for b, 4.5, is known as a regression coefficient, or in statistics jargon,\n“the coefficient on height,” because it gives us the best estimate of the relationship between\nheight and weight among the Changing Lives participants. The regression coefficient has a\nconvenient interpretation: a one-unit increase in the independent variable (height) is associated\nwith an increase of 4.5 units in the dependent variable (weight). For our data sample, this\nmeans that a 1-inch increase in height is associated with a 4.5 pound increase in weight. Thus,\nif we had no other information, our best guess for the weight of a person who is 5 feet 10\ninches tall (70 inches) in the Changing Lives study would be --135 + 4.5 (70) = 180 pounds. This is our payoff, as we have now quantified the best fit for the linear relationship between\nheight and weight for the Changing Lives participants. The same basic tools can be used to\nexplore more complex relationships and more socially significant questions. For any regression\ncoefficient, you will generally be interested in three things: sign, size, and significance. Sign. The sign (positive or negative) on the coefficient for an independent variable tells us\nthe direction of its association with the dependent variable (the outcome we are trying to\nexplain). In the simple case above, the coefficient on height is positive. Taller people tend to\nweigh more. Some relationships will work in the other direction. I would expect the association\nbetween exercise and weight to be negative. If the Changing Lives study included data on\nsomething like “miles run per month,” I am fairly certain that the coefficient on “miles run”\nwould be negative. Running more is associated with weighing less. Size. How big is the observed effect between the independent variable and the dependent\nvariable? Is it of a magnitude that matters?", "tokens": 482, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 141, "segment_id": "00141", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000248"}
{"type": "chunk", "text": "In the simple case above, the coefficient on height is positive. Taller people tend to\nweigh more. Some relationships will work in the other direction. I would expect the association\nbetween exercise and weight to be negative. If the Changing Lives study included data on\nsomething like “miles run per month,” I am fairly certain that the coefficient on “miles run”\nwould be negative. Running more is associated with weighing less. Size. How big is the observed effect between the independent variable and the dependent\nvariable? Is it of a magnitude that matters? In this case, every one inch in height is associated\nwith 4.5 pounds, which is a sizable percentage of a typical person’s body weight. In an\nexplanation of why some people weigh more than others, height is clearly an important factor. In other studies, we may find an explanatory variable that has a statistically significant impact\non our outcome of interest---meaning that the observed effect is not likely to be a product of\nchance---but that effect may be so small as to be trivial or socially insignificant. For example,\nsuppose that we are examining determinants of income. Why do some people make more\nmoney than others? The explanatory variables are likely to be things like education, years of\nwork experience, and so on. In a large data set, researchers might also find that people with\nwhiter teeth earn $86 more per year than other workers, ceteris paribus. (“Ceteris paribus”\ncomes from the Latin meaning “other things being equal.”) The positive and statistically\nsignificant coefficient on the “white teeth” variable assumes that the individuals being\ncompared are similar in other respects: same education, same work experience, and so on. (I\nwill explain in a moment how we pull off this tantalizing feat.) Our statistical analysis has\ndemonstrated that whiter teeth are associated with $86 in additional annual income per year\nand that this finding is not likely to be a mere coincidence. This means (1) we’ve rejected the\nnull hypothesis that really white teeth have no association with income with a high degree of", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nIn the simple case above, the coefficient on height is positive. Taller people tend to\nweigh more. Some relationships will work in the other direction. I would expect the association\nbetween exercise and weight to be negative. If the Changing Lives study included data on\nsomething like “miles run per month,” I am fairly certain that the coefficient on “miles run”\nwould be negative. Running more is associated with weighing less. Size. How big is the observed effect between the independent variable and the dependent\nvariable? Is it of a magnitude that matters? In this case, every one inch in height is associated\nwith 4.5 pounds, which is a sizable percentage of a typical person’s body weight. In an\nexplanation of why some people weigh more than others, height is clearly an important factor. In other studies, we may find an explanatory variable that has a statistically significant impact\non our outcome of interest---meaning that the observed effect is not likely to be a product of\nchance---but that effect may be so small as to be trivial or socially insignificant. For example,\nsuppose that we are examining determinants of income. Why do some people make more\nmoney than others? The explanatory variables are likely to be things like education, years of\nwork experience, and so on. In a large data set, researchers might also find that people with\nwhiter teeth earn $86 more per year than other workers, ceteris paribus. (“Ceteris paribus”\ncomes from the Latin meaning “other things being equal.”) The positive and statistically\nsignificant coefficient on the “white teeth” variable assumes that the individuals being\ncompared are similar in other respects: same education, same work experience, and so on. (I\nwill explain in a moment how we pull off this tantalizing feat.) Our statistical analysis has\ndemonstrated that whiter teeth are associated with $86 in additional annual income per year\nand that this finding is not likely to be a mere coincidence. This means (1) we’ve rejected the\nnull hypothesis that really white teeth have no association with income with a high degree of", "tokens": 440, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 141, "segment_id": "00141", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000249"}
{"type": "chunk", "text": "confidence; and (2) if we analyze other data samples, we are likely to find a similar\nrelationship between good-looking teeth and higher income. So what? We’ve found a statistically significant result, but not one that is particularly\nmeaningful. To begin with, $86 per year is not a life-changing sum of money. From a public\npolicy standpoint, $86 is also probably less than it would cost to whiten an individual’s teeth\nevery year, so we can’t even recommend that young workers make such an investment. And,\nalthough I’m getting a chapter ahead of myself, I’d also be worried about some serious\nmethodological problems. For example, having perfect teeth may be associated with other\npersonality traits that explain the earnings advantage; the earnings effect may be caused by the\nkind of people who care about their teeth, not the teeth themselves. For now, the point is that\nwe should take note of the size of the association that we observe between the explanatory\nvariable and our outcome of interest. Significance. Is the observed result an aberration based on a quirky sample of data, or does\nit reflect a meaningful association that is likely to be observed for the population as a whole? This is the same basic question that we have been asking for the last several chapters. In the\ncontext of height and weight, do we think that we would observe a similar positive association\nin other samples that are representative of the population? To answer this question, we use the\nbasic tools of inference that have already been introduced. Our regression coefficient is based\non an observed relationship between height and weight for a particular sample of data. If we\nwere to test another large sample of data, we would almost certainly get a slightly different\nassociation between height and weight and therefore a different coefficient. The relationship\nbetween height and weight observed in the Whitehall data (the British civil servants) is likely to\nbe different from the relationship observed between height and weight for the participants in the\nChanging Lives study. However, we know from the central limit theorem that the mean for a\nlarge, properly drawn sample will not typically deviate wildly from the mean for the population\nas a whole. Similarly, we can assume that the observed relationship between variables like\nheight and weight will not typically bounce around wildly from sample to sample, assuming\nthat these samples are large and properly drawn from the same population.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nconfidence; and (2) if we analyze other data samples, we are likely to find a similar\nrelationship between good-looking teeth and higher income. So what? We’ve found a statistically significant result, but not one that is particularly\nmeaningful. To begin with, $86 per year is not a life-changing sum of money. From a public\npolicy standpoint, $86 is also probably less than it would cost to whiten an individual’s teeth\nevery year, so we can’t even recommend that young workers make such an investment. And,\nalthough I’m getting a chapter ahead of myself, I’d also be worried about some serious\nmethodological problems. For example, having perfect teeth may be associated with other\npersonality traits that explain the earnings advantage; the earnings effect may be caused by the\nkind of people who care about their teeth, not the teeth themselves. For now, the point is that\nwe should take note of the size of the association that we observe between the explanatory\nvariable and our outcome of interest. Significance. Is the observed result an aberration based on a quirky sample of data, or does\nit reflect a meaningful association that is likely to be observed for the population as a whole? This is the same basic question that we have been asking for the last several chapters. In the\ncontext of height and weight, do we think that we would observe a similar positive association\nin other samples that are representative of the population? To answer this question, we use the\nbasic tools of inference that have already been introduced. Our regression coefficient is based\non an observed relationship between height and weight for a particular sample of data. If we\nwere to test another large sample of data, we would almost certainly get a slightly different\nassociation between height and weight and therefore a different coefficient. The relationship\nbetween height and weight observed in the Whitehall data (the British civil servants) is likely to\nbe different from the relationship observed between height and weight for the participants in the\nChanging Lives study. However, we know from the central limit theorem that the mean for a\nlarge, properly drawn sample will not typically deviate wildly from the mean for the population\nas a whole. Similarly, we can assume that the observed relationship between variables like\nheight and weight will not typically bounce around wildly from sample to sample, assuming\nthat these samples are large and properly drawn from the same population.", "tokens": 492, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 142, "segment_id": "00142", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000250"}
{"type": "chunk", "text": "The relationship\nbetween height and weight observed in the Whitehall data (the British civil servants) is likely to\nbe different from the relationship observed between height and weight for the participants in the\nChanging Lives study. However, we know from the central limit theorem that the mean for a\nlarge, properly drawn sample will not typically deviate wildly from the mean for the population\nas a whole. Similarly, we can assume that the observed relationship between variables like\nheight and weight will not typically bounce around wildly from sample to sample, assuming\nthat these samples are large and properly drawn from the same population. Think about the intuition: It’s highly unlikely (though still possible) that we would find that\nevery inch of height is associated with 4.5 additional pounds among the Changing Lives\nparticipants but that there is no association between height and weight in a different\nrepresentative sample of 3,000 adult Americans. This should give you the first inkling of how we’ll test whether our regression results are\nstatistically significant or not. As with polling and other forms of inference, we can calculate a\nstandard error for the regression coefficient. The standard error is a measure of the likely\ndispersion we would observe in the coefficient if we were to conduct the regression analysis on\nrepeated samples drawn from the same population. If we were to measure and weigh a different\nsample of 3,000 Americans, we might find in the subsequent analysis that each inch of height is\nassociated with 4.3 pounds. If we did it again for another sample of 3,000 Americans, we might\nfind that each inch is associated with 5.2 pounds. Once again, the normal distribution is our\nfriend. For large samples of data, such as our Changing Lives data set, we can assume that our", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThe relationship\nbetween height and weight observed in the Whitehall data (the British civil servants) is likely to\nbe different from the relationship observed between height and weight for the participants in the\nChanging Lives study. However, we know from the central limit theorem that the mean for a\nlarge, properly drawn sample will not typically deviate wildly from the mean for the population\nas a whole. Similarly, we can assume that the observed relationship between variables like\nheight and weight will not typically bounce around wildly from sample to sample, assuming\nthat these samples are large and properly drawn from the same population. Think about the intuition: It’s highly unlikely (though still possible) that we would find that\nevery inch of height is associated with 4.5 additional pounds among the Changing Lives\nparticipants but that there is no association between height and weight in a different\nrepresentative sample of 3,000 adult Americans. This should give you the first inkling of how we’ll test whether our regression results are\nstatistically significant or not. As with polling and other forms of inference, we can calculate a\nstandard error for the regression coefficient. The standard error is a measure of the likely\ndispersion we would observe in the coefficient if we were to conduct the regression analysis on\nrepeated samples drawn from the same population. If we were to measure and weigh a different\nsample of 3,000 Americans, we might find in the subsequent analysis that each inch of height is\nassociated with 4.3 pounds. If we did it again for another sample of 3,000 Americans, we might\nfind that each inch is associated with 5.2 pounds. Once again, the normal distribution is our\nfriend. For large samples of data, such as our Changing Lives data set, we can assume that our", "tokens": 370, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 142, "segment_id": "00142", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000251"}
{"type": "chunk", "text": "various coefficients will be distributed normally around the “true” association between height\nand weight in the American adult population. On that assumption, we can calculate a standard\nerror for the regression coefficient that gives us a sense of how much dispersion we should\nexpect in the coefficients from sample to sample. I will not delve into the formula for\ncalculating the standard error here, both because it will take us off in a direction that involves a\nlot of math and because all basic statistical packages will calculate it for you. However, I must warn that when we are working with a small sample of data---such as a\ngroup of 20 adults rather than the 3,000+ persons in the Changing Lives study---the normal\ndistribution is no longer willing to be our friend. Specifically, if we repeatedly conduct\nregression analysis on different small samples, we can no longer assume that our various\ncoefficients will be distributed normally around the “true” association between height and\nweight in the American adult population. Instead, our coefficients will still be distributed\naround the “true” association between height and weight for the American adult population in\nwhat is known as a t-distribution. (Basically the t-distribution is more dispersed than the\nnormal distribution and therefore has “fatter tails.”) Nothing else changes; any basic statistical\nsoftware package will easily manage the additional complexity associated with using the tdistributions. For this reason, the t-distribution will be explained in greater detail in the chapter\nappendix. Sticking with large samples for now (and the normal distribution), the most important thing to\nunderstand is why the standard error matters. As with polling and other forms of inference, we\nexpect that more than half of our observed regression coefficients will lie within one standard\nerror of the true population parameter.* Roughly 95 percent will lie within two standard errors. And so on. With that, we’re just about home, because now we can do a little hypothesis\ntesting. (Seriously, did you think you were already done with hypothesis testing?) Once we\nhave a coefficient and a standard error, we can test the null hypothesis that there is in fact no\nrelationship between the explanatory variable and the dependent variable (meaning that the true\nassociation between the two variables in the population is zero).", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nvarious coefficients will be distributed normally around the “true” association between height\nand weight in the American adult population. On that assumption, we can calculate a standard\nerror for the regression coefficient that gives us a sense of how much dispersion we should\nexpect in the coefficients from sample to sample. I will not delve into the formula for\ncalculating the standard error here, both because it will take us off in a direction that involves a\nlot of math and because all basic statistical packages will calculate it for you. However, I must warn that when we are working with a small sample of data---such as a\ngroup of 20 adults rather than the 3,000+ persons in the Changing Lives study---the normal\ndistribution is no longer willing to be our friend. Specifically, if we repeatedly conduct\nregression analysis on different small samples, we can no longer assume that our various\ncoefficients will be distributed normally around the “true” association between height and\nweight in the American adult population. Instead, our coefficients will still be distributed\naround the “true” association between height and weight for the American adult population in\nwhat is known as a t-distribution. (Basically the t-distribution is more dispersed than the\nnormal distribution and therefore has “fatter tails.”) Nothing else changes; any basic statistical\nsoftware package will easily manage the additional complexity associated with using the tdistributions. For this reason, the t-distribution will be explained in greater detail in the chapter\nappendix. Sticking with large samples for now (and the normal distribution), the most important thing to\nunderstand is why the standard error matters. As with polling and other forms of inference, we\nexpect that more than half of our observed regression coefficients will lie within one standard\nerror of the true population parameter.* Roughly 95 percent will lie within two standard errors. And so on. With that, we’re just about home, because now we can do a little hypothesis\ntesting. (Seriously, did you think you were already done with hypothesis testing?) Once we\nhave a coefficient and a standard error, we can test the null hypothesis that there is in fact no\nrelationship between the explanatory variable and the dependent variable (meaning that the true\nassociation between the two variables in the population is zero).", "tokens": 470, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 143, "segment_id": "00143", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000252"}
{"type": "chunk", "text": "And so on. With that, we’re just about home, because now we can do a little hypothesis\ntesting. (Seriously, did you think you were already done with hypothesis testing?) Once we\nhave a coefficient and a standard error, we can test the null hypothesis that there is in fact no\nrelationship between the explanatory variable and the dependent variable (meaning that the true\nassociation between the two variables in the population is zero). In our simple height and weight example, we can test how likely it is that we would find in\nour Changing Lives sample that every inch of height is associated with 4.5 pounds if there is\nreally no association between height and weight in the general population. I’ve run the\nregression by using a basic statistics program; the standard error on the height coefficient is\n.13. This means that if we were to do this analysis repeatedly---say with 100 different samples\n---then we would expect our observed regression coefficient to be within two standard errors of\nthe true population parameter roughly 95 times out of 100. We can therefore express our results in two different but related ways. First, we can build a\n95 percent confidence interval. We can say that 95 times out of 100, we expect our confidence\ninterval, which is 4.5 ± .26, to contain the true population parameter. This is the range between\n4.24 and 4.76. A basic statistics package will calculate this interval as well. Second, we can\nsee that our 95 percent confidence interval for the true association between height and weight\ndoes not include zero. Thus, we can reject the null hypothesis that there is no association\nbetween height and weight for the general population at the 95 percent confidence level. This", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nAnd so on. With that, we’re just about home, because now we can do a little hypothesis\ntesting. (Seriously, did you think you were already done with hypothesis testing?) Once we\nhave a coefficient and a standard error, we can test the null hypothesis that there is in fact no\nrelationship between the explanatory variable and the dependent variable (meaning that the true\nassociation between the two variables in the population is zero). In our simple height and weight example, we can test how likely it is that we would find in\nour Changing Lives sample that every inch of height is associated with 4.5 pounds if there is\nreally no association between height and weight in the general population. I’ve run the\nregression by using a basic statistics program; the standard error on the height coefficient is\n.13. This means that if we were to do this analysis repeatedly---say with 100 different samples\n---then we would expect our observed regression coefficient to be within two standard errors of\nthe true population parameter roughly 95 times out of 100. We can therefore express our results in two different but related ways. First, we can build a\n95 percent confidence interval. We can say that 95 times out of 100, we expect our confidence\ninterval, which is 4.5 ± .26, to contain the true population parameter. This is the range between\n4.24 and 4.76. A basic statistics package will calculate this interval as well. Second, we can\nsee that our 95 percent confidence interval for the true association between height and weight\ndoes not include zero. Thus, we can reject the null hypothesis that there is no association\nbetween height and weight for the general population at the 95 percent confidence level. This", "tokens": 362, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 143, "segment_id": "00143", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000253"}
{"type": "chunk", "text": "result can also be expressed as being statistically significant at the .05 level; there is only a 5\npercent chance that we are wrongly rejecting the null hypothesis. In fact, our results are even more extreme than that. The standard error (.13) is extremely\nlow relative to the size of the coefficient (4.5). One rough rule of thumb is that the coefficient\nis likely to be statistically significant when the coefficient is at least twice the size of the\nstandard error.* A statistics package also calculates a p-value, which is .000 in this case,\nmeaning that there is essentially zero chance of getting an outcome as extreme as what we’ve\nobserved (or more so) if there is no true association between height and weight in the general\npopulation. Remember, we have not proved that taller people weigh more in the general\npopulation; we have merely shown that our results for the Changing Lives sample would be\nhighly anomalous if that were not the case. Our basic regression analysis produces one other statistic of note: the R2, which is a measure\nof the total amount of variation explained by the regression equation. We know that we have a\nbroad variation in weight in our Changing Lives sample. Many of the persons in the sample\nweigh more than the mean for the group overall; many weigh less. The R2 tells us how much of\nthat variation around the mean is associated with differences in height alone. The answer in our\ncase is .25, or 25 percent. The more significant point may be that 75 percent of the variation in\nweight for our sample remains unexplained. There are clearly factors other than height that\nmight help us understand the weights of the Changing Lives participants. This is where things\nget more interesting. I’ll admit that I began this chapter by selling regression analysis as the miracle elixir of\nsocial science research. So far all I’ve done is use a statistics package and an impressive data\nset to demonstrate that tall people tend to weigh more than short people. A short trip to a\nshopping mall would probably have convinced you of the same thing. Now that you understand\nthe basics, we can unleash the real power of regression analysis. It’s time to take off the\ntraining wheels! As I’ve promised, regression analysis allows us to unravel complex relationships in which\nmultiple factors affect some outcome that we care about, such as income, or test scores, or\nheart disease.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nresult can also be expressed as being statistically significant at the .05 level; there is only a 5\npercent chance that we are wrongly rejecting the null hypothesis. In fact, our results are even more extreme than that. The standard error (.13) is extremely\nlow relative to the size of the coefficient (4.5). One rough rule of thumb is that the coefficient\nis likely to be statistically significant when the coefficient is at least twice the size of the\nstandard error.* A statistics package also calculates a p-value, which is .000 in this case,\nmeaning that there is essentially zero chance of getting an outcome as extreme as what we’ve\nobserved (or more so) if there is no true association between height and weight in the general\npopulation. Remember, we have not proved that taller people weigh more in the general\npopulation; we have merely shown that our results for the Changing Lives sample would be\nhighly anomalous if that were not the case. Our basic regression analysis produces one other statistic of note: the R2, which is a measure\nof the total amount of variation explained by the regression equation. We know that we have a\nbroad variation in weight in our Changing Lives sample. Many of the persons in the sample\nweigh more than the mean for the group overall; many weigh less. The R2 tells us how much of\nthat variation around the mean is associated with differences in height alone. The answer in our\ncase is .25, or 25 percent. The more significant point may be that 75 percent of the variation in\nweight for our sample remains unexplained. There are clearly factors other than height that\nmight help us understand the weights of the Changing Lives participants. This is where things\nget more interesting. I’ll admit that I began this chapter by selling regression analysis as the miracle elixir of\nsocial science research. So far all I’ve done is use a statistics package and an impressive data\nset to demonstrate that tall people tend to weigh more than short people. A short trip to a\nshopping mall would probably have convinced you of the same thing. Now that you understand\nthe basics, we can unleash the real power of regression analysis. It’s time to take off the\ntraining wheels! As I’ve promised, regression analysis allows us to unravel complex relationships in which\nmultiple factors affect some outcome that we care about, such as income, or test scores, or\nheart disease.", "tokens": 501, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 144, "segment_id": "00144", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000254"}
{"type": "chunk", "text": "So far all I’ve done is use a statistics package and an impressive data\nset to demonstrate that tall people tend to weigh more than short people. A short trip to a\nshopping mall would probably have convinced you of the same thing. Now that you understand\nthe basics, we can unleash the real power of regression analysis. It’s time to take off the\ntraining wheels! As I’ve promised, regression analysis allows us to unravel complex relationships in which\nmultiple factors affect some outcome that we care about, such as income, or test scores, or\nheart disease. When we include multiple variables in the regression equation, the analysis\ngives us an estimate of the linear association between each explanatory variable and the\ndependent variable while holding other dependent variables constant, or “controlling for” these\nother factors. Let’s stick with weight for a while. We’ve found an association between height\nand weight; we know there are other factors that can help to explain weight (age, sex, diet,\nexercise, and so on). Regression analysis (often called multiple regression analysis when more\nthan one explanatory variable is involved, or multivariate regression analysis) will give us a\ncoefficient for each explanatory variable included in the regression equation. In other words,\namong people who are the same sex and height, what is the relationship between age and\nweight? Once we have more than one explanatory variable, we can no longer plot these data in\ntwo dimensions. (Try to imagine a graph that represents the weight, sex, height, and age of each\nparticipant in the Changing Lives study.) Yet the basic methodology is the same as in our", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nSo far all I’ve done is use a statistics package and an impressive data\nset to demonstrate that tall people tend to weigh more than short people. A short trip to a\nshopping mall would probably have convinced you of the same thing. Now that you understand\nthe basics, we can unleash the real power of regression analysis. It’s time to take off the\ntraining wheels! As I’ve promised, regression analysis allows us to unravel complex relationships in which\nmultiple factors affect some outcome that we care about, such as income, or test scores, or\nheart disease. When we include multiple variables in the regression equation, the analysis\ngives us an estimate of the linear association between each explanatory variable and the\ndependent variable while holding other dependent variables constant, or “controlling for” these\nother factors. Let’s stick with weight for a while. We’ve found an association between height\nand weight; we know there are other factors that can help to explain weight (age, sex, diet,\nexercise, and so on). Regression analysis (often called multiple regression analysis when more\nthan one explanatory variable is involved, or multivariate regression analysis) will give us a\ncoefficient for each explanatory variable included in the regression equation. In other words,\namong people who are the same sex and height, what is the relationship between age and\nweight? Once we have more than one explanatory variable, we can no longer plot these data in\ntwo dimensions. (Try to imagine a graph that represents the weight, sex, height, and age of each\nparticipant in the Changing Lives study.) Yet the basic methodology is the same as in our", "tokens": 333, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 144, "segment_id": "00144", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000255"}
{"type": "chunk", "text": "simple height and weight example. As we add explanatory variables, a statistical package will\ncalculate the regression coefficients that minimize the total sum of the squared residuals for the\nregression equation. Let’s work with the Changing Lives data for now; then I’ll go back and give an intuitive\nexplanation for how this statistical parting of the Red Sea could possibly work. We can start by\nadding one more variable to the equation that explains the weights of the Changing Lives\nparticipants: age. When we run the regression including both height and age as explanatory\nvariables for weight, here is what we get. WEIGHT = --145 + 4.6 × (HEIGHT IN INCHES) + .1 × (AGE IN YEARS)\n\nThe coefficient on age is .1. That can be interpreted to mean that every additional year in\nage is associated with .1 additional pounds in weight, holding height constant. For any group\nof people who are the same height, on average those who are ten years older will weigh one\npound more. This is not a huge effect, but it’s consistent with what we tend to see in life. The\ncoefficient is significant at the .05 level. You may have noticed that the coefficient on height has increased slightly. Once age is in\nour regression, we have a more refined understanding of the relationship between height and\nweight. Among people who are the same age in our sample, or “holding age constant,” every\nadditional inch in height is associated with 4.6 pounds in weight. Let’s add one more variable: sex. This will be slightly different because sex can only take\non two possibilities, male or female. How does one put M or F into a regression? The answer\nis that we use what is called a binary variable, or dummy variable. In our data set, we enter a 1\nfor those participants who are female and a 0 for those who are male. (This is not meant to be\na value judgment.) The sex coefficient can then be interpreted as the effect on weight of being\nfemale, ceteris paribus. The coefficient is --4.8, which is not surprising. We can interpret that to\nmean that for individuals who are the same height and age, women typically weigh 4.8 pounds\nless than men. Now we can begin to see some of the power of multiple regression analysis.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nsimple height and weight example. As we add explanatory variables, a statistical package will\ncalculate the regression coefficients that minimize the total sum of the squared residuals for the\nregression equation. Let’s work with the Changing Lives data for now; then I’ll go back and give an intuitive\nexplanation for how this statistical parting of the Red Sea could possibly work. We can start by\nadding one more variable to the equation that explains the weights of the Changing Lives\nparticipants: age. When we run the regression including both height and age as explanatory\nvariables for weight, here is what we get. WEIGHT = --145 + 4.6 × (HEIGHT IN INCHES) + .1 × (AGE IN YEARS)\n\nThe coefficient on age is .1. That can be interpreted to mean that every additional year in\nage is associated with .1 additional pounds in weight, holding height constant. For any group\nof people who are the same height, on average those who are ten years older will weigh one\npound more. This is not a huge effect, but it’s consistent with what we tend to see in life. The\ncoefficient is significant at the .05 level. You may have noticed that the coefficient on height has increased slightly. Once age is in\nour regression, we have a more refined understanding of the relationship between height and\nweight. Among people who are the same age in our sample, or “holding age constant,” every\nadditional inch in height is associated with 4.6 pounds in weight. Let’s add one more variable: sex. This will be slightly different because sex can only take\non two possibilities, male or female. How does one put M or F into a regression? The answer\nis that we use what is called a binary variable, or dummy variable. In our data set, we enter a 1\nfor those participants who are female and a 0 for those who are male. (This is not meant to be\na value judgment.) The sex coefficient can then be interpreted as the effect on weight of being\nfemale, ceteris paribus. The coefficient is --4.8, which is not surprising. We can interpret that to\nmean that for individuals who are the same height and age, women typically weigh 4.8 pounds\nless than men. Now we can begin to see some of the power of multiple regression analysis.", "tokens": 491, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 145, "segment_id": "00145", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000256"}
{"type": "chunk", "text": "In our data set, we enter a 1\nfor those participants who are female and a 0 for those who are male. (This is not meant to be\na value judgment.) The sex coefficient can then be interpreted as the effect on weight of being\nfemale, ceteris paribus. The coefficient is --4.8, which is not surprising. We can interpret that to\nmean that for individuals who are the same height and age, women typically weigh 4.8 pounds\nless than men. Now we can begin to see some of the power of multiple regression analysis. We\nknow that women tend to be shorter than men, but our coefficient takes this into account since\nwe have already controlled for height. What we have isolated here is the effect of being\nfemale. The new regression becomes:\n\nWEIGHT = --118 + 4.3 × (HEIGHT IN INCHES) + .12 (AGE IN YEARS) -- 4.8 (IF\nSEX IS FEMALE)\n\nOur best estimate of the weight of a fifty-three-year-old woman who is 5 feet 5 inches is: --\n\n118 + 4.3 (65) + .12 (53) -- 4.8 = 163 pounds. And our best guess for a thirty-five-year-old male who is 6 feet 3 inches is --118 + 4.3 (75)\n+ .12 (35) = 209 pounds. We skip the last term in our regression result (--4.8) since this person\nis not female. Now we can start to test things that are more interesting and less predictable. What about\neducation? How might that affect weight? I would hypothesize that better-educated individuals", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nIn our data set, we enter a 1\nfor those participants who are female and a 0 for those who are male. (This is not meant to be\na value judgment.) The sex coefficient can then be interpreted as the effect on weight of being\nfemale, ceteris paribus. The coefficient is --4.8, which is not surprising. We can interpret that to\nmean that for individuals who are the same height and age, women typically weigh 4.8 pounds\nless than men. Now we can begin to see some of the power of multiple regression analysis. We\nknow that women tend to be shorter than men, but our coefficient takes this into account since\nwe have already controlled for height. What we have isolated here is the effect of being\nfemale. The new regression becomes:\n\nWEIGHT = --118 + 4.3 × (HEIGHT IN INCHES) + .12 (AGE IN YEARS) -- 4.8 (IF\nSEX IS FEMALE)\n\nOur best estimate of the weight of a fifty-three-year-old woman who is 5 feet 5 inches is: --\n\n118 + 4.3 (65) + .12 (53) -- 4.8 = 163 pounds. And our best guess for a thirty-five-year-old male who is 6 feet 3 inches is --118 + 4.3 (75)\n+ .12 (35) = 209 pounds. We skip the last term in our regression result (--4.8) since this person\nis not female. Now we can start to test things that are more interesting and less predictable. What about\neducation? How might that affect weight? I would hypothesize that better-educated individuals", "tokens": 352, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 145, "segment_id": "00145", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000257"}
{"type": "chunk", "text": "are more health conscious and therefore will weigh less, ceteris paribus. We also haven’t\ntested any measure of exercise; I assume that, holding other factors constant, the people in the\nsample who get more exercise will weigh less. What about poverty? Does being low-income in America have effects that manifest\nthemselves in weight? The Changing Lives study asks whether the participants are receiving\nfood stamps, which is a good measure of poverty in America. Finally, I’m interested in race. We know that people of color have different life experiences in the United States because of\ntheir race. There are cultural and residential factors associated with race in America that have\nimplications for weight. Many cities are still characterized by a high degree of racial\nsegregation; African Americans might be more likely than other residents to live in “food\ndeserts,” which are areas with limited access to grocery stores that carry fruits, vegetables, and\nother fresh produce. We can use regression analysis to separate out the independent effect of each of the\npotential explanatory factors described above. For example, we can isolate the association\nbetween race and weight, holding constant other socioeconomic factors like educational\nbackground and poverty. Among people who are high school graduates and eligible for food\nstamps, what is the statistical association between weight and being black? At this point, our regression equation is so long that it would be cumbersome to print the\nresults in their entirety here. Academic papers typically insert large tables that summarize the\nresults of various regression equations. I have included a table with the complete results of this\nregression equation in the appendix to this chapter. In the meantime, here are the highlights of\nwhat happens when we add education, exercise, poverty (as measured by receiving food\nstamps), and race to our equation. All of our original variables (height, age, and sex) are still significant. The coefficients\nchange little as we add explanatory variables. All of our new variables are statistically\nsignificant at the .05 level. The R2 on the regression has climbed from .25 to .29. (Remember,\nan R2 of zero means that our regression equation does no better than the mean at predicting the\nweight of any individual in the sample; an R2 of 1 means that the regression equation perfectly\npredicts the weight of every person in the sample.) A lot of the variation in weight across\nindividuals remains unexplained.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nare more health conscious and therefore will weigh less, ceteris paribus. We also haven’t\ntested any measure of exercise; I assume that, holding other factors constant, the people in the\nsample who get more exercise will weigh less. What about poverty? Does being low-income in America have effects that manifest\nthemselves in weight? The Changing Lives study asks whether the participants are receiving\nfood stamps, which is a good measure of poverty in America. Finally, I’m interested in race. We know that people of color have different life experiences in the United States because of\ntheir race. There are cultural and residential factors associated with race in America that have\nimplications for weight. Many cities are still characterized by a high degree of racial\nsegregation; African Americans might be more likely than other residents to live in “food\ndeserts,” which are areas with limited access to grocery stores that carry fruits, vegetables, and\nother fresh produce. We can use regression analysis to separate out the independent effect of each of the\npotential explanatory factors described above. For example, we can isolate the association\nbetween race and weight, holding constant other socioeconomic factors like educational\nbackground and poverty. Among people who are high school graduates and eligible for food\nstamps, what is the statistical association between weight and being black? At this point, our regression equation is so long that it would be cumbersome to print the\nresults in their entirety here. Academic papers typically insert large tables that summarize the\nresults of various regression equations. I have included a table with the complete results of this\nregression equation in the appendix to this chapter. In the meantime, here are the highlights of\nwhat happens when we add education, exercise, poverty (as measured by receiving food\nstamps), and race to our equation. All of our original variables (height, age, and sex) are still significant. The coefficients\nchange little as we add explanatory variables. All of our new variables are statistically\nsignificant at the .05 level. The R2 on the regression has climbed from .25 to .29. (Remember,\nan R2 of zero means that our regression equation does no better than the mean at predicting the\nweight of any individual in the sample; an R2 of 1 means that the regression equation perfectly\npredicts the weight of every person in the sample.) A lot of the variation in weight across\nindividuals remains unexplained.", "tokens": 497, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 146, "segment_id": "00146", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000258"}
{"type": "chunk", "text": "The coefficients\nchange little as we add explanatory variables. All of our new variables are statistically\nsignificant at the .05 level. The R2 on the regression has climbed from .25 to .29. (Remember,\nan R2 of zero means that our regression equation does no better than the mean at predicting the\nweight of any individual in the sample; an R2 of 1 means that the regression equation perfectly\npredicts the weight of every person in the sample.) A lot of the variation in weight across\nindividuals remains unexplained. Education turns out to be negatively associated with weight, as I had hypothesized. Among\nparticipants in the Changing Lives study, each year of education is associated with --1.3\npounds. Not surprisingly, exercise is also negatively associated with weight. The Changing Lives\nstudy includes an index that evaluates each participant in the study on his or her level of\nphysical activity. Those individuals who are in the bottom quintile of physical activity weigh,\non average, 4.5 pounds more than other adults in the sample, ceteris paribus. Those in the\nbottom quintile for physical activity weigh, on average, nearly 9 pounds more than adults in the\ntop quintile for physical activity. Individuals receiving food stamps (the proxy for poverty in this regression) are heavier\nthan other adults. Food stamp recipients weigh an average of 5.6 pounds more than other", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThe coefficients\nchange little as we add explanatory variables. All of our new variables are statistically\nsignificant at the .05 level. The R2 on the regression has climbed from .25 to .29. (Remember,\nan R2 of zero means that our regression equation does no better than the mean at predicting the\nweight of any individual in the sample; an R2 of 1 means that the regression equation perfectly\npredicts the weight of every person in the sample.) A lot of the variation in weight across\nindividuals remains unexplained. Education turns out to be negatively associated with weight, as I had hypothesized. Among\nparticipants in the Changing Lives study, each year of education is associated with --1.3\npounds. Not surprisingly, exercise is also negatively associated with weight. The Changing Lives\nstudy includes an index that evaluates each participant in the study on his or her level of\nphysical activity. Those individuals who are in the bottom quintile of physical activity weigh,\non average, 4.5 pounds more than other adults in the sample, ceteris paribus. Those in the\nbottom quintile for physical activity weigh, on average, nearly 9 pounds more than adults in the\ntop quintile for physical activity. Individuals receiving food stamps (the proxy for poverty in this regression) are heavier\nthan other adults. Food stamp recipients weigh an average of 5.6 pounds more than other", "tokens": 290, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 146, "segment_id": "00146", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000259"}
{"type": "chunk", "text": "Changing Lives participants, ceteris paribus. The race variable turns out to be particularly interesting. Even after one controls for all the\nother variables described up to this point, race still matters a lot when it comes to explaining\nweight. The non-Hispanic black adults in the Changing Lives sample weigh, on average,\nroughly 10 pounds more than the other adults in the sample. Ten pounds is a lot of weight, both\nin absolute terms and compared with the effects of the other explanatory variables in the\nregression equation. This is not a quirk of the data. The p-value on the dummy variable for\nnon-Hispanic blacks is .000 and the 95 percent confidence interval stretches from 7.7 pounds\nto 16.1 pounds. What is going on? The honest answer is that I have no idea. Let me reiterate a point that was\nburied earlier in a footnote: I’m just playing around with data here to illustrate how regression\nanalysis works. The analytics presented here are to true academic research what street hockey\nis to the NHL. If this were a real research project, there would be weeks or months of followon analysis to probe this finding. What I can say is that I have demonstrated why multiple\nregression analysis is the best tool we have for finding meaningful patterns in large, complex\ndata sets. We started with a ridiculously banal exercise: quantifying the relationship between\nheight and weight. Before long, we were knee-deep in issues with real social significance. issue: gender discrimination\n\nthe workplace. The curious\n\nIn that vein, I can offer you a real study that used regression analysis to probe a socially\nsignificant\nthing about\nin\ndiscrimination is that it’s hard to observe directly. No employer ever states explicitly that\nsomeone is being paid less because of his or her race or gender or that someone has not been\nhired for discriminatory reasons (which would presumably leave the person in a different job\nwith a lower salary). Instead, what we observe are gaps in pay by race and gender that may be\nthe result of discrimination: whites earn more than blacks; men earn more than women; and so\non. The methodological challenge is that these observed gaps may also be the result of\nunderlying differences in workers that have nothing to do with workplace discrimination, such\nas the fact that women tend to choose more part-time work.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nChanging Lives participants, ceteris paribus. The race variable turns out to be particularly interesting. Even after one controls for all the\nother variables described up to this point, race still matters a lot when it comes to explaining\nweight. The non-Hispanic black adults in the Changing Lives sample weigh, on average,\nroughly 10 pounds more than the other adults in the sample. Ten pounds is a lot of weight, both\nin absolute terms and compared with the effects of the other explanatory variables in the\nregression equation. This is not a quirk of the data. The p-value on the dummy variable for\nnon-Hispanic blacks is .000 and the 95 percent confidence interval stretches from 7.7 pounds\nto 16.1 pounds. What is going on? The honest answer is that I have no idea. Let me reiterate a point that was\nburied earlier in a footnote: I’m just playing around with data here to illustrate how regression\nanalysis works. The analytics presented here are to true academic research what street hockey\nis to the NHL. If this were a real research project, there would be weeks or months of followon analysis to probe this finding. What I can say is that I have demonstrated why multiple\nregression analysis is the best tool we have for finding meaningful patterns in large, complex\ndata sets. We started with a ridiculously banal exercise: quantifying the relationship between\nheight and weight. Before long, we were knee-deep in issues with real social significance. issue: gender discrimination\n\nthe workplace. The curious\n\nIn that vein, I can offer you a real study that used regression analysis to probe a socially\nsignificant\nthing about\nin\ndiscrimination is that it’s hard to observe directly. No employer ever states explicitly that\nsomeone is being paid less because of his or her race or gender or that someone has not been\nhired for discriminatory reasons (which would presumably leave the person in a different job\nwith a lower salary). Instead, what we observe are gaps in pay by race and gender that may be\nthe result of discrimination: whites earn more than blacks; men earn more than women; and so\non. The methodological challenge is that these observed gaps may also be the result of\nunderlying differences in workers that have nothing to do with workplace discrimination, such\nas the fact that women tend to choose more part-time work.", "tokens": 497, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 147, "segment_id": "00147", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000260"}
{"type": "chunk", "text": "Instead, what we observe are gaps in pay by race and gender that may be\nthe result of discrimination: whites earn more than blacks; men earn more than women; and so\non. The methodological challenge is that these observed gaps may also be the result of\nunderlying differences in workers that have nothing to do with workplace discrimination, such\nas the fact that women tend to choose more part-time work. How much of the wage gap is due\nto factors associated with productivity on the job, and how much of the gap, if any, is due to\nlabor force discrimination? No one can claim that this is a trivial question. Regression analysis can help us answer it. However, our methodology will be slightly more\nroundabout than it was with our analysis explaining weight. Since we cannot measure\ndiscrimination directly, we will examine other factors that traditionally explain wages, such as\neducation, experience, occupational field, and so on. The case for discrimination is\ncircumstantial: If a significant wage gap remains after controlling for other factors that typically\nexplain wages, then discrimination is a likely culprit. The larger the unexplained portion of any\nwage gap, the more suspicious we should be. As an example, let’s look at a paper by three\neconomists examining the wage trajectories of a sample of roughly 2,500 men and women who\ngraduated with MBAs from the Booth School of Business at the University of Chicago.1 Upon\ngraduation, male and female graduates have very similar average starting salaries: $130,000\nfor men and $115,000 for women. After ten years in the workforce, however, a huge gap has\nopened up; women on average are earning a striking 45 percent less than their male classmates:", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nInstead, what we observe are gaps in pay by race and gender that may be\nthe result of discrimination: whites earn more than blacks; men earn more than women; and so\non. The methodological challenge is that these observed gaps may also be the result of\nunderlying differences in workers that have nothing to do with workplace discrimination, such\nas the fact that women tend to choose more part-time work. How much of the wage gap is due\nto factors associated with productivity on the job, and how much of the gap, if any, is due to\nlabor force discrimination? No one can claim that this is a trivial question. Regression analysis can help us answer it. However, our methodology will be slightly more\nroundabout than it was with our analysis explaining weight. Since we cannot measure\ndiscrimination directly, we will examine other factors that traditionally explain wages, such as\neducation, experience, occupational field, and so on. The case for discrimination is\ncircumstantial: If a significant wage gap remains after controlling for other factors that typically\nexplain wages, then discrimination is a likely culprit. The larger the unexplained portion of any\nwage gap, the more suspicious we should be. As an example, let’s look at a paper by three\neconomists examining the wage trajectories of a sample of roughly 2,500 men and women who\ngraduated with MBAs from the Booth School of Business at the University of Chicago.1 Upon\ngraduation, male and female graduates have very similar average starting salaries: $130,000\nfor men and $115,000 for women. After ten years in the workforce, however, a huge gap has\nopened up; women on average are earning a striking 45 percent less than their male classmates:", "tokens": 364, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 147, "segment_id": "00147", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000261"}
{"type": "chunk", "text": "$243,000 versus $442,000. In a broader sample of more than 18,000 MBA graduates who\nentered the workforce between 1990 and 2006, being female is associated with 29 percent\nlower earnings. What is happening to women once they enter the labor force? According to the authors of the study (Marianne Bertrand of the Booth School of Business\nand Claudia Goldin and Lawrence Katz of Harvard), discrimination is not a likely explanation\nfor most of the gap. The gender wage gap fades away as the authors add more explanatory\nvariables to the analysis. For example, men take more finance classes in the MBA program and\ngraduate with higher grade point averages. When these data are included as control variables in\nthe regression equation, the unexplained portion of the gap in male-female earnings drops to 19\npercent. When variables are added to the equation to account for post-MBA work experience,\nparticularly spells out of the labor force, the unexplained portion of the male-female wage gap\ndrops to 9 percent. And when explanatory variables are added for other work characteristics,\nsuch as employer type and hours worked, the unexplained portion of the gender wage gap falls\nto under 4 percent. For workers who have been in the labor force more than ten years, the authors can ultimately\nexplain all but 1 percent of the gender wage gap with factors unrelated to discrimination on the\njob.* They conclude, “We identify three proximate reasons for the large and rising gender gap\nin earnings: differences in training prior to MBA graduation; differences in career interruptions;\nand differences in weekly hours. These three determinants can explain the bulk of gender\ndifferences across the years following MBA completion.”\n\nI hope that I’ve convinced you of the value of multiple regression analysis, particularly the\nresearch insights that stem from being able to isolate the effect of one explanatory variable\nwhile controlling for other confounding factors. I have not yet provided an intuitive explanation\nfor how this statistical “miracle elixir” works. When we use regression analysis to evaluate the\nrelationship between education and weight, ceteris paribus, how does a statistical package\ncontrol for factors like height, sex, age, and income when we know that our Changing Lives\nparticipants are not identical in these other respects? To get your mind around how we can isolate the effect on weight of a single variable, say,\neducation, imagine the following situation.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\n$243,000 versus $442,000. In a broader sample of more than 18,000 MBA graduates who\nentered the workforce between 1990 and 2006, being female is associated with 29 percent\nlower earnings. What is happening to women once they enter the labor force? According to the authors of the study (Marianne Bertrand of the Booth School of Business\nand Claudia Goldin and Lawrence Katz of Harvard), discrimination is not a likely explanation\nfor most of the gap. The gender wage gap fades away as the authors add more explanatory\nvariables to the analysis. For example, men take more finance classes in the MBA program and\ngraduate with higher grade point averages. When these data are included as control variables in\nthe regression equation, the unexplained portion of the gap in male-female earnings drops to 19\npercent. When variables are added to the equation to account for post-MBA work experience,\nparticularly spells out of the labor force, the unexplained portion of the male-female wage gap\ndrops to 9 percent. And when explanatory variables are added for other work characteristics,\nsuch as employer type and hours worked, the unexplained portion of the gender wage gap falls\nto under 4 percent. For workers who have been in the labor force more than ten years, the authors can ultimately\nexplain all but 1 percent of the gender wage gap with factors unrelated to discrimination on the\njob.* They conclude, “We identify three proximate reasons for the large and rising gender gap\nin earnings: differences in training prior to MBA graduation; differences in career interruptions;\nand differences in weekly hours. These three determinants can explain the bulk of gender\ndifferences across the years following MBA completion.”\n\nI hope that I’ve convinced you of the value of multiple regression analysis, particularly the\nresearch insights that stem from being able to isolate the effect of one explanatory variable\nwhile controlling for other confounding factors. I have not yet provided an intuitive explanation\nfor how this statistical “miracle elixir” works. When we use regression analysis to evaluate the\nrelationship between education and weight, ceteris paribus, how does a statistical package\ncontrol for factors like height, sex, age, and income when we know that our Changing Lives\nparticipants are not identical in these other respects? To get your mind around how we can isolate the effect on weight of a single variable, say,\neducation, imagine the following situation.", "tokens": 501, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 148, "segment_id": "00148", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000262"}
{"type": "chunk", "text": "I have not yet provided an intuitive explanation\nfor how this statistical “miracle elixir” works. When we use regression analysis to evaluate the\nrelationship between education and weight, ceteris paribus, how does a statistical package\ncontrol for factors like height, sex, age, and income when we know that our Changing Lives\nparticipants are not identical in these other respects? To get your mind around how we can isolate the effect on weight of a single variable, say,\neducation, imagine the following situation. Assume that all of the Changing Lives participants\nare convened in one place---say, Framingham, Massachusetts. Now assume that the men and\nwomen are separated. And then assume that both the men and the women are further divided\nby height. There will be a room of six-foot tall men. Next door, there will be a room of 6-foot\n1-inch men, and so on for both genders. If we have enough participants in our study, we can\nfurther subdivide each of those rooms by income. Eventually we will have lots of rooms, each\nof which contains individuals who are identical in all respects except for education and\nweight, which are the two variables we care about. There would be a room of forty-five-yearold 5-foot 5-inch men who earn $30,000 to $40,000 a year. Next door would be all the fortyfive-year-old 5-foot 5-inch women who earn $30,000 to $40,000 a year. And so on (and on and\non). There will still be some variation in weight in each room; people who are the same sex and\nheight and have the same income will still weigh different amounts---though presumably there", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nI have not yet provided an intuitive explanation\nfor how this statistical “miracle elixir” works. When we use regression analysis to evaluate the\nrelationship between education and weight, ceteris paribus, how does a statistical package\ncontrol for factors like height, sex, age, and income when we know that our Changing Lives\nparticipants are not identical in these other respects? To get your mind around how we can isolate the effect on weight of a single variable, say,\neducation, imagine the following situation. Assume that all of the Changing Lives participants\nare convened in one place---say, Framingham, Massachusetts. Now assume that the men and\nwomen are separated. And then assume that both the men and the women are further divided\nby height. There will be a room of six-foot tall men. Next door, there will be a room of 6-foot\n1-inch men, and so on for both genders. If we have enough participants in our study, we can\nfurther subdivide each of those rooms by income. Eventually we will have lots of rooms, each\nof which contains individuals who are identical in all respects except for education and\nweight, which are the two variables we care about. There would be a room of forty-five-yearold 5-foot 5-inch men who earn $30,000 to $40,000 a year. Next door would be all the fortyfive-year-old 5-foot 5-inch women who earn $30,000 to $40,000 a year. And so on (and on and\non). There will still be some variation in weight in each room; people who are the same sex and\nheight and have the same income will still weigh different amounts---though presumably there", "tokens": 356, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 148, "segment_id": "00148", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000263"}
{"type": "chunk", "text": "will be much less variation in weight in each room than there is for the overall sample. Our\ngoal now is to see how much of the remaining variation in weight in each room can be\nexplained by education. In other words, what is the best linear relationship between education\nand weight in each room? The final challenge, however, is that we do not want different coefficients in each “room.”\nThe whole point of this exercise is to calculate a single coefficient that best expresses the\nrelationship between education and weight for the entire sample, while holding other factors\nconstant. What we would like to calculate is the single coefficient for education that we can\nuse in every room to minimize the sum of the squared residuals for all of the rooms combined. What coefficient for education minimizes the square of the unexplained weight for every\nindividual across all the rooms? That becomes our regression coefficient because it is the best\nexplanation of the linear relationship between education and weight for this sample when we\nhold sex, height, and income constant. As an aside, you can see why large data sets are so useful. They allow us to control for\nmany factors while still having many observations in each “room.” Obviously a computer can\ndo all of this in a split second without herding thousands of people into different rooms. Let’s finish the chapter where we started, with the connection between stress on the job and\ncoronary heart disease. The Whitehall studies of British civil servants sought to measure the\nassociation between grade of employment and death from coronary heart disease over the\nensuing years. One of the early studies followed 17,530 civil servants for seven and a half\nyears.2 The authors concluded, “Men in the lower employment grades were shorter, heavier for\ntheir height, had higher blood pressure, higher plasma glucose, smoked more, and reported less\nleisure-time physical activity than men in the higher grades. Yet when allowance was made for\nthe influence on mortality of all of these factors plus plasma cholesterol, the inverse\nassociation between grade of employment and [coronary heart disease] mortality was still\nstrong.” The “allowance” they refer to for these other known risk factors is done by means of\nregression analysis.* The study demonstrates that holding other health factors constant\n(including height, which is a decent proxy for early childhood health and nutrition), working in a\n“low grade” job can literally kill you. Skepticism is always a good first response.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nwill be much less variation in weight in each room than there is for the overall sample. Our\ngoal now is to see how much of the remaining variation in weight in each room can be\nexplained by education. In other words, what is the best linear relationship between education\nand weight in each room? The final challenge, however, is that we do not want different coefficients in each “room.”\nThe whole point of this exercise is to calculate a single coefficient that best expresses the\nrelationship between education and weight for the entire sample, while holding other factors\nconstant. What we would like to calculate is the single coefficient for education that we can\nuse in every room to minimize the sum of the squared residuals for all of the rooms combined. What coefficient for education minimizes the square of the unexplained weight for every\nindividual across all the rooms? That becomes our regression coefficient because it is the best\nexplanation of the linear relationship between education and weight for this sample when we\nhold sex, height, and income constant. As an aside, you can see why large data sets are so useful. They allow us to control for\nmany factors while still having many observations in each “room.” Obviously a computer can\ndo all of this in a split second without herding thousands of people into different rooms. Let’s finish the chapter where we started, with the connection between stress on the job and\ncoronary heart disease. The Whitehall studies of British civil servants sought to measure the\nassociation between grade of employment and death from coronary heart disease over the\nensuing years. One of the early studies followed 17,530 civil servants for seven and a half\nyears.2 The authors concluded, “Men in the lower employment grades were shorter, heavier for\ntheir height, had higher blood pressure, higher plasma glucose, smoked more, and reported less\nleisure-time physical activity than men in the higher grades. Yet when allowance was made for\nthe influence on mortality of all of these factors plus plasma cholesterol, the inverse\nassociation between grade of employment and [coronary heart disease] mortality was still\nstrong.” The “allowance” they refer to for these other known risk factors is done by means of\nregression analysis.* The study demonstrates that holding other health factors constant\n(including height, which is a decent proxy for early childhood health and nutrition), working in a\n“low grade” job can literally kill you. Skepticism is always a good first response.", "tokens": 506, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 149, "segment_id": "00149", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000264"}
{"type": "chunk", "text": "Yet when allowance was made for\nthe influence on mortality of all of these factors plus plasma cholesterol, the inverse\nassociation between grade of employment and [coronary heart disease] mortality was still\nstrong.” The “allowance” they refer to for these other known risk factors is done by means of\nregression analysis.* The study demonstrates that holding other health factors constant\n(including height, which is a decent proxy for early childhood health and nutrition), working in a\n“low grade” job can literally kill you. Skepticism is always a good first response. I wrote at the outset of the chapter that “lowcontrol” jobs are bad for your health. That may or may not be synonymous with being low on\nthe administrative totem pole. A follow-up study using a second sample of 10,308 British civil\nservants sought to drill down on this distinction.3 The workers were once again divided into\nadministrative grades---high, intermediate, and low---only this time the participants were also\ngiven a fifteen-item questionnaire that evaluated their level of “decision latitude or control.”\nThese included questions such as “Do you have a choice in deciding how you do your job?”\nand categorical responses (ranging from “never” to “often”) to statements such as “I can decide\nwhen to take a break.” The researchers found that the “low-control” workers were at\nsignificantly higher risk of developing coronary heart disease over the course of the study than\n“high-control” workers. Yet researchers also found that workers with rigorous job demands", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nYet when allowance was made for\nthe influence on mortality of all of these factors plus plasma cholesterol, the inverse\nassociation between grade of employment and [coronary heart disease] mortality was still\nstrong.” The “allowance” they refer to for these other known risk factors is done by means of\nregression analysis.* The study demonstrates that holding other health factors constant\n(including height, which is a decent proxy for early childhood health and nutrition), working in a\n“low grade” job can literally kill you. Skepticism is always a good first response. I wrote at the outset of the chapter that “lowcontrol” jobs are bad for your health. That may or may not be synonymous with being low on\nthe administrative totem pole. A follow-up study using a second sample of 10,308 British civil\nservants sought to drill down on this distinction.3 The workers were once again divided into\nadministrative grades---high, intermediate, and low---only this time the participants were also\ngiven a fifteen-item questionnaire that evaluated their level of “decision latitude or control.”\nThese included questions such as “Do you have a choice in deciding how you do your job?”\nand categorical responses (ranging from “never” to “often”) to statements such as “I can decide\nwhen to take a break.” The researchers found that the “low-control” workers were at\nsignificantly higher risk of developing coronary heart disease over the course of the study than\n“high-control” workers. Yet researchers also found that workers with rigorous job demands", "tokens": 319, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 149, "segment_id": "00149", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000265"}
{"type": "chunk", "text": "were at no greater risk of developing heart disease, nor were workers who reported low levels\nof social support on the job. Lack of control seems to be the killer, literally. The Whitehall studies have two characteristics typically associated with strong research. First, the results have been replicated elsewhere. In the public health literature, the “lowcontrol” idea evolved into a term known as “job strain,” which characterizes jobs with “high\npsychological workload demands” and “low decision latitude.” Between 1981 and 1993, thirtysix studies were published on the subject; most found a significant positive association\nbetween job strain and heart disease.4\n\nSecond, researchers sought and found corroborating biological evidence to explain the\nmechanism by which this particular kind of stress on the job causes poor health. Work\nconditions that involve rigorous demands but low control can cause physiological responses\n(such as the release of stress-related hormones) that increase the risk of heart disease over the\nlong run. Even animal research plays a role; low-status monkeys and baboons (who bear some\nresemblance to civil servants at the bottom of the authority chain) have physiological\ndifferences from their high-status peers that put them at greater cardiovascular risk.5\n\nAll else equal, it’s better not to be a low-status baboon, which is a point I try to make to my\nchildren as often as possible, particularly my son. The larger message here is that regression\nanalysis is arguably the most important tool that researchers have for finding meaningful\npatterns in large data sets. We typically cannot do controlled experiments to learn about job\ndiscrimination or factors that cause heart disease. Our insights into these socially significant\nissues and many others come from the statistical tools covered in this chapter. In fact, it would\nnot be an exaggeration to say that a high proportion of all important research done in the social\nsciences over the past half century (particularly since the advent of cheap computing power)\ndraws on regression analysis. Regression analysis supersizes the scientific method; we are healthier, safer, and better\n\ninformed as a result. So what could possibly go wrong with this powerful and impressive tool? Read on. APPENDIX TO CHAPTER 11\nThe t-distribution\n\nLife gets a little trickier when we are doing our regression analysis (or other forms of statistical\ninference) with a small sample of data.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nwere at no greater risk of developing heart disease, nor were workers who reported low levels\nof social support on the job. Lack of control seems to be the killer, literally. The Whitehall studies have two characteristics typically associated with strong research. First, the results have been replicated elsewhere. In the public health literature, the “lowcontrol” idea evolved into a term known as “job strain,” which characterizes jobs with “high\npsychological workload demands” and “low decision latitude.” Between 1981 and 1993, thirtysix studies were published on the subject; most found a significant positive association\nbetween job strain and heart disease.4\n\nSecond, researchers sought and found corroborating biological evidence to explain the\nmechanism by which this particular kind of stress on the job causes poor health. Work\nconditions that involve rigorous demands but low control can cause physiological responses\n(such as the release of stress-related hormones) that increase the risk of heart disease over the\nlong run. Even animal research plays a role; low-status monkeys and baboons (who bear some\nresemblance to civil servants at the bottom of the authority chain) have physiological\ndifferences from their high-status peers that put them at greater cardiovascular risk.5\n\nAll else equal, it’s better not to be a low-status baboon, which is a point I try to make to my\nchildren as often as possible, particularly my son. The larger message here is that regression\nanalysis is arguably the most important tool that researchers have for finding meaningful\npatterns in large data sets. We typically cannot do controlled experiments to learn about job\ndiscrimination or factors that cause heart disease. Our insights into these socially significant\nissues and many others come from the statistical tools covered in this chapter. In fact, it would\nnot be an exaggeration to say that a high proportion of all important research done in the social\nsciences over the past half century (particularly since the advent of cheap computing power)\ndraws on regression analysis. Regression analysis supersizes the scientific method; we are healthier, safer, and better\n\ninformed as a result. So what could possibly go wrong with this powerful and impressive tool? Read on. APPENDIX TO CHAPTER 11\nThe t-distribution\n\nLife gets a little trickier when we are doing our regression analysis (or other forms of statistical\ninference) with a small sample of data.", "tokens": 494, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 150, "segment_id": "00150", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000266"}
{"type": "chunk", "text": "Regression analysis supersizes the scientific method; we are healthier, safer, and better\n\ninformed as a result. So what could possibly go wrong with this powerful and impressive tool? Read on. APPENDIX TO CHAPTER 11\nThe t-distribution\n\nLife gets a little trickier when we are doing our regression analysis (or other forms of statistical\ninference) with a small sample of data. Suppose we were analyzing the relationship between\nweight and height on the basis of a sample of only 25 adults, rather than using a huge data set\nlike the Changing Lives study. Logic suggests that we should be less confident about\ngeneralizing our results to the entire adult population from a sample of 25 than from a sample\nof 3,000. One of the themes throughout the book has been that smaller samples tend to\ngenerate more dispersion in outcomes. Our sample of 25 will still give us meaningful\ninformation, as would a sample of 5 or 10---but how meaningful? The t-distribution answers that question. If we analyze the association between height and\nweight for repeated samples of 25 adults, we can no longer assume that the various coefficients", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nRegression analysis supersizes the scientific method; we are healthier, safer, and better\n\ninformed as a result. So what could possibly go wrong with this powerful and impressive tool? Read on. APPENDIX TO CHAPTER 11\nThe t-distribution\n\nLife gets a little trickier when we are doing our regression analysis (or other forms of statistical\ninference) with a small sample of data. Suppose we were analyzing the relationship between\nweight and height on the basis of a sample of only 25 adults, rather than using a huge data set\nlike the Changing Lives study. Logic suggests that we should be less confident about\ngeneralizing our results to the entire adult population from a sample of 25 than from a sample\nof 3,000. One of the themes throughout the book has been that smaller samples tend to\ngenerate more dispersion in outcomes. Our sample of 25 will still give us meaningful\ninformation, as would a sample of 5 or 10---but how meaningful? The t-distribution answers that question. If we analyze the association between height and\nweight for repeated samples of 25 adults, we can no longer assume that the various coefficients", "tokens": 239, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 150, "segment_id": "00150", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000267"}
{"type": "chunk", "text": "we get for height will be distributed normally around the “true” coefficient for height in the\nadult population. They will still be distributed around the true coefficient for the whole\npopulation, but the shape of that distribution will not be our familiar bell-shaped normal\ncurve. Instead, we have to assume that repeated samples of just 25 will produce more\ndispersion around the true population coefficient---and therefore a distribution with “fatter\ntails.” And repeated samples of 10 will produce even more dispersion than that---and therefore\neven fatter tails. The t-distribution is actually a series, or “family,” of probability density\nfunctions that vary according to the size of our sample. Specifically, the more data we have in\nour sample, the more “degrees of freedom” we have when determining the appropriate\ndistribution against which to evaluate our results. In a more advanced class, you will learn\nexactly how to calculate degrees of freedom; for our purposes, they are roughly equal to the\nnumber of observations in our sample. For instance, a basic regression analysis with a sample\nof 10 and a single explanatory variable has 9 degrees of freedom. The more degrees of\nfreedom we have, the more confident we can be that our sample represents the true population,\nand the “tighter” our distribution will be, as the following diagram illustrates. When the number of degrees of freedom gets large, the t-distribution converges to the normal\ndistribution. That’s why when we are working with large data sets, we can use the normal\ndistribution for our assorted calculations. The t-distribution merely adds nuance to the same process of statistical inference that we\nhave been using throughout the book. We are still formulating a null hypothesis and then\ntesting it against some observed data. If the data we observe would be highly unlikely if the\nnull hypothesis were true, then we reject the null hypothesis. The only thing that changes with\nthe t-distribution is the underlying probabilities for evaluating the observed outcomes. The\n“fatter” the tail in a particular probability distribution (e.g., the t-distribution for eight degrees\nof freedom), the more dispersion we would expect in our observed data just as a matter of\nchance, and therefore the less confident we can be in rejecting our null hypothesis. For example, suppose we are running a regression equation, and the null hypothesis is that\nthe coefficient on a particular variable is zero. Once we get the regression results, we would", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nwe get for height will be distributed normally around the “true” coefficient for height in the\nadult population. They will still be distributed around the true coefficient for the whole\npopulation, but the shape of that distribution will not be our familiar bell-shaped normal\ncurve. Instead, we have to assume that repeated samples of just 25 will produce more\ndispersion around the true population coefficient---and therefore a distribution with “fatter\ntails.” And repeated samples of 10 will produce even more dispersion than that---and therefore\neven fatter tails. The t-distribution is actually a series, or “family,” of probability density\nfunctions that vary according to the size of our sample. Specifically, the more data we have in\nour sample, the more “degrees of freedom” we have when determining the appropriate\ndistribution against which to evaluate our results. In a more advanced class, you will learn\nexactly how to calculate degrees of freedom; for our purposes, they are roughly equal to the\nnumber of observations in our sample. For instance, a basic regression analysis with a sample\nof 10 and a single explanatory variable has 9 degrees of freedom. The more degrees of\nfreedom we have, the more confident we can be that our sample represents the true population,\nand the “tighter” our distribution will be, as the following diagram illustrates. When the number of degrees of freedom gets large, the t-distribution converges to the normal\ndistribution. That’s why when we are working with large data sets, we can use the normal\ndistribution for our assorted calculations. The t-distribution merely adds nuance to the same process of statistical inference that we\nhave been using throughout the book. We are still formulating a null hypothesis and then\ntesting it against some observed data. If the data we observe would be highly unlikely if the\nnull hypothesis were true, then we reject the null hypothesis. The only thing that changes with\nthe t-distribution is the underlying probabilities for evaluating the observed outcomes. The\n“fatter” the tail in a particular probability distribution (e.g., the t-distribution for eight degrees\nof freedom), the more dispersion we would expect in our observed data just as a matter of\nchance, and therefore the less confident we can be in rejecting our null hypothesis. For example, suppose we are running a regression equation, and the null hypothesis is that\nthe coefficient on a particular variable is zero. Once we get the regression results, we would", "tokens": 509, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 151, "segment_id": "00151", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000268"}
{"type": "chunk", "text": "calculate a t-statistic, which is the ratio of the observed coefficient to the standard error for that\ncoefficient.* This t-statistic is then evaluated against whatever t-distribution is appropriate for\nthe size of the data sample (since this is largely what determines the number of degrees of\nfreedom). When the t-statistic is sufficiently large, meaning that our observed coefficient is far\nfrom what the null hypothesis would predict, we can reject the null hypothesis at some level of\nstatistical significance. Again, this is the same basic process of statistical inference that we\nhave been employing throughout the book. The fewer the degrees of freedom (and therefore the “fatter” the tails of the relevant tdistribution), the higher the t-statistic will have to be in order for us to reject the null\nhypothesis at some given level of significance. In the hypothetical regression example\ndescribed above, if we had four degrees of freedom, we would need a t-statistic of at least\n2.13 to reject the null hypothesis at the .05 level (in a one-tailed test). However, if we have 20,000 degrees of freedom (which essentially allows us to use the\nnormal distribution), we would need only a t-statistic of 1.65 to reject the null hypothesis at the\n.05 level in the same one-tailed test. Regression Equation for Weight\n\n* You should consider this exercise “fun with data” rather than an authoritative exploration of any of the relationships\ndescribed in the subsequent regression equations. The purpose here is to provide an intuitive example of how regression\nanalysis works, not to do meaningful research on Americans’ weights. * “Parameter” is a fancy term for any statistic that describes a characteristic of some population; the mean weight for all\nadult men is a parameter of that population. So is the standard deviation. In the example here, the true association between\nheight and weight for the population is a parameter of that population. * When the null hypothesis is that a regression coefficient is zero (as is most often the case), the ratio of the observed\nregression coefficient to the standard error is known as the t-statistic. This will also be explained in the chapter appendix. * Broader discriminatory forces in society may affect the careers that women choose or the fact that they are more likely\nthan men to interrupt their careers to take care of children.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\ncalculate a t-statistic, which is the ratio of the observed coefficient to the standard error for that\ncoefficient.* This t-statistic is then evaluated against whatever t-distribution is appropriate for\nthe size of the data sample (since this is largely what determines the number of degrees of\nfreedom). When the t-statistic is sufficiently large, meaning that our observed coefficient is far\nfrom what the null hypothesis would predict, we can reject the null hypothesis at some level of\nstatistical significance. Again, this is the same basic process of statistical inference that we\nhave been employing throughout the book. The fewer the degrees of freedom (and therefore the “fatter” the tails of the relevant tdistribution), the higher the t-statistic will have to be in order for us to reject the null\nhypothesis at some given level of significance. In the hypothetical regression example\ndescribed above, if we had four degrees of freedom, we would need a t-statistic of at least\n2.13 to reject the null hypothesis at the .05 level (in a one-tailed test). However, if we have 20,000 degrees of freedom (which essentially allows us to use the\nnormal distribution), we would need only a t-statistic of 1.65 to reject the null hypothesis at the\n.05 level in the same one-tailed test. Regression Equation for Weight\n\n* You should consider this exercise “fun with data” rather than an authoritative exploration of any of the relationships\ndescribed in the subsequent regression equations. The purpose here is to provide an intuitive example of how regression\nanalysis works, not to do meaningful research on Americans’ weights. * “Parameter” is a fancy term for any statistic that describes a characteristic of some population; the mean weight for all\nadult men is a parameter of that population. So is the standard deviation. In the example here, the true association between\nheight and weight for the population is a parameter of that population. * When the null hypothesis is that a regression coefficient is zero (as is most often the case), the ratio of the observed\nregression coefficient to the standard error is known as the t-statistic. This will also be explained in the chapter appendix. * Broader discriminatory forces in society may affect the careers that women choose or the fact that they are more likely\nthan men to interrupt their careers to take care of children.", "tokens": 490, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 152, "segment_id": "00152", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000269"}
{"type": "chunk", "text": "So is the standard deviation. In the example here, the true association between\nheight and weight for the population is a parameter of that population. * When the null hypothesis is that a regression coefficient is zero (as is most often the case), the ratio of the observed\nregression coefficient to the standard error is known as the t-statistic. This will also be explained in the chapter appendix. * Broader discriminatory forces in society may affect the careers that women choose or the fact that they are more likely\nthan men to interrupt their careers to take care of children. However, these important issues are distinct from the narrower\nquestion of whether women are being paid less than men to do the same jobs. * These studies differ slightly from the regression equations introduced earlier in the chapter. The outcome of interest, or\ndependent variable, is binary in these studies. A participant either has some kind of heart-related health problem during the\nperiod of study or he does not. As a result, the researchers use a tool called multivariate logistic regression. The basic idea\nis the same as the ordinary least squares models described in this chapter. Each coefficient expresses the effect of a\nparticular explanatory variable on the dependent variable while holding the effects of other variables in the model constant. The key difference is that the variables in the equation all affect the likelihood that some event happens, such as having a\nheart attack during the period of study. In this study, for example, workers in the low control group are 1.99 times as likely", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nSo is the standard deviation. In the example here, the true association between\nheight and weight for the population is a parameter of that population. * When the null hypothesis is that a regression coefficient is zero (as is most often the case), the ratio of the observed\nregression coefficient to the standard error is known as the t-statistic. This will also be explained in the chapter appendix. * Broader discriminatory forces in society may affect the careers that women choose or the fact that they are more likely\nthan men to interrupt their careers to take care of children. However, these important issues are distinct from the narrower\nquestion of whether women are being paid less than men to do the same jobs. * These studies differ slightly from the regression equations introduced earlier in the chapter. The outcome of interest, or\ndependent variable, is binary in these studies. A participant either has some kind of heart-related health problem during the\nperiod of study or he does not. As a result, the researchers use a tool called multivariate logistic regression. The basic idea\nis the same as the ordinary least squares models described in this chapter. Each coefficient expresses the effect of a\nparticular explanatory variable on the dependent variable while holding the effects of other variables in the model constant. The key difference is that the variables in the equation all affect the likelihood that some event happens, such as having a\nheart attack during the period of study. In this study, for example, workers in the low control group are 1.99 times as likely", "tokens": 309, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 152, "segment_id": "00152", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000270"}
{"type": "chunk", "text": "CHAPTER 12\nCommon Regression Mistakes\nThe mandatory warning label\n\nHere is one of the most important things to remember when doing research that involves\n\nregression analysis: Try not to kill anyone. You can even put a little Post-it note on your\ncomputer monitor: “Do not kill people with your research.” Because some very smart people\nhave inadvertently violated that rule. Beginning in the 1990s, the medical establishment coalesced around the idea that older\nwomen should take estrogen supplements to protect against heart disease, osteoporosis, and\nother conditions associated with menopause.1 By 2001, some 15 million women were being\nprescribed estrogen in the belief that it would make them healthier. Why? Because research at\nthe time---using the basic methodology laid out in the last chapter---suggested this was a\nsensible medical strategy. In particular, a longitudinal study of 122,000 women (the Nurses’\nHealth Study) found a negative association between estrogen supplements and heart attacks. Women taking estrogen had one-third as many heart attacks as women who were not taking\nestrogen. This was not a couple of teenagers using dad’s computer to check out pornography\nand run regression equations. The Nurses’ Health Study is run by the Harvard Medical School\nand the Harvard School of Public Health. Meanwhile, scientists and physicians offered a medical theory for why hormone supplements\nmight be beneficial for female health. A woman’s ovaries produce less estrogen as she ages; if\nestrogen is important to the body, then making up for this deficit in old age could be protective\nof a woman’s long-term health. Hence the name of the treatment: hormone replacement therapy. Some researchers even began to suggest that older men should receive an estrogen boost.2\n\nAnd then, while millions of women were being prescribed hormone replacement therapy,\nestrogen was subjected to the most rigorous form of scientific scrutiny: clinical trials. Rather\nthan searching a large data set like the Nurses’ Health Study for statistical associations that\nmay or may not be causal, a clinical trial consists of a controlled experiment. One sample is\ngiven a treatment, such as hormone replacement; another sample is given a placebo. Clinical\ntrials showed that women taking estrogen had a higher incidence of heart disease, stroke, blood\nclots, breast cancer, and other adverse health outcomes. Estrogen supplements did have some\nbenefits, but those benefits were far outweighed by other risks.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nCHAPTER 12\nCommon Regression Mistakes\nThe mandatory warning label\n\nHere is one of the most important things to remember when doing research that involves\n\nregression analysis: Try not to kill anyone. You can even put a little Post-it note on your\ncomputer monitor: “Do not kill people with your research.” Because some very smart people\nhave inadvertently violated that rule. Beginning in the 1990s, the medical establishment coalesced around the idea that older\nwomen should take estrogen supplements to protect against heart disease, osteoporosis, and\nother conditions associated with menopause.1 By 2001, some 15 million women were being\nprescribed estrogen in the belief that it would make them healthier. Why? Because research at\nthe time---using the basic methodology laid out in the last chapter---suggested this was a\nsensible medical strategy. In particular, a longitudinal study of 122,000 women (the Nurses’\nHealth Study) found a negative association between estrogen supplements and heart attacks. Women taking estrogen had one-third as many heart attacks as women who were not taking\nestrogen. This was not a couple of teenagers using dad’s computer to check out pornography\nand run regression equations. The Nurses’ Health Study is run by the Harvard Medical School\nand the Harvard School of Public Health. Meanwhile, scientists and physicians offered a medical theory for why hormone supplements\nmight be beneficial for female health. A woman’s ovaries produce less estrogen as she ages; if\nestrogen is important to the body, then making up for this deficit in old age could be protective\nof a woman’s long-term health. Hence the name of the treatment: hormone replacement therapy. Some researchers even began to suggest that older men should receive an estrogen boost.2\n\nAnd then, while millions of women were being prescribed hormone replacement therapy,\nestrogen was subjected to the most rigorous form of scientific scrutiny: clinical trials. Rather\nthan searching a large data set like the Nurses’ Health Study for statistical associations that\nmay or may not be causal, a clinical trial consists of a controlled experiment. One sample is\ngiven a treatment, such as hormone replacement; another sample is given a placebo. Clinical\ntrials showed that women taking estrogen had a higher incidence of heart disease, stroke, blood\nclots, breast cancer, and other adverse health outcomes. Estrogen supplements did have some\nbenefits, but those benefits were far outweighed by other risks.", "tokens": 502, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 154, "segment_id": "00154", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000271"}
{"type": "chunk", "text": "Rather\nthan searching a large data set like the Nurses’ Health Study for statistical associations that\nmay or may not be causal, a clinical trial consists of a controlled experiment. One sample is\ngiven a treatment, such as hormone replacement; another sample is given a placebo. Clinical\ntrials showed that women taking estrogen had a higher incidence of heart disease, stroke, blood\nclots, breast cancer, and other adverse health outcomes. Estrogen supplements did have some\nbenefits, but those benefits were far outweighed by other risks. Beginning in 2002, doctors\nwere advised not to prescribe estrogen for their aging female patients. The New York Times\nMagazine asked a delicate but socially significant question: How many women died\nprematurely or suffered strokes or breast cancer because they were taking a pill that their\ndoctors had prescribed to keep them healthy? The answer: “A reasonable estimate would be tens of thousands.”3", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nRather\nthan searching a large data set like the Nurses’ Health Study for statistical associations that\nmay or may not be causal, a clinical trial consists of a controlled experiment. One sample is\ngiven a treatment, such as hormone replacement; another sample is given a placebo. Clinical\ntrials showed that women taking estrogen had a higher incidence of heart disease, stroke, blood\nclots, breast cancer, and other adverse health outcomes. Estrogen supplements did have some\nbenefits, but those benefits were far outweighed by other risks. Beginning in 2002, doctors\nwere advised not to prescribe estrogen for their aging female patients. The New York Times\nMagazine asked a delicate but socially significant question: How many women died\nprematurely or suffered strokes or breast cancer because they were taking a pill that their\ndoctors had prescribed to keep them healthy? The answer: “A reasonable estimate would be tens of thousands.”3", "tokens": 191, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 154, "segment_id": "00154", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000272"}
{"type": "chunk", "text": "Regression analysis is the hydrogen bomb of the statistics arsenal. Every person with a\npersonal computer and a large data set can be a researcher in his or her own home or cubicle. What could possibly go wrong? All kinds of things. Regression analysis provides precise\nanswers to complicated questions. These answers may or may not be accurate. In the wrong\nhands, regression analysis will yield results that are misleading or just plain wrong. And, as the\nestrogen example illustrates, even in the right hands this powerful statistical tool can send us\nspeeding dangerously in the wrong direction. The balance of this chapter will explain the most\ncommon regression “mistakes.” I put “mistakes” in quotation marks, because, as with all other\nkinds of statistical analysis, clever people can knowingly exploit these methodological points\nto nefarious ends. Here is a “Top Seven” list of the most common abuses of an otherwise extraordinary tool. Using regression to analyze a nonlinear relationship.* Have you ever read the warning label\non a hair dryer---the part that cautions, Do Not Use in the Bath Tub? And you think to\nyourself, “What kind of moron uses a hair dryer in the bath tub?” It’s an electrical appliance;\nyou don’t use electrical appliances around water. They’re not designed for that. If regression\nanalysis had a similar warning label, it would say, Do Not Use When There Is Not a Linear\nAssociation between the Variables That You Are Analyzing. Remember, a regression\ncoefficient describes the slope of the “line of best fit” for the data; a line that is not straight\nwill have a different slope in different places. As an example, consider the following\nhypothetical relationship between the number of golf lessons that I take during a month (an\nexplanatory variable) and my average score for an eighteen-hole round during that month (the\ndependent variable). As you can see from the scatter plot, there is no consistent linear\nrelationship. Effect of Golf Lessons on Score\n\nThere is a pattern, but it cannot be easily described with a single straight line. The first few\ngolf lessons appear to bring my score down rapidly. There is a negative association between\nlessons and my scores for this stretch; the slope is negative. More lessons yield lower scores\n(which is good in golf).", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nRegression analysis is the hydrogen bomb of the statistics arsenal. Every person with a\npersonal computer and a large data set can be a researcher in his or her own home or cubicle. What could possibly go wrong? All kinds of things. Regression analysis provides precise\nanswers to complicated questions. These answers may or may not be accurate. In the wrong\nhands, regression analysis will yield results that are misleading or just plain wrong. And, as the\nestrogen example illustrates, even in the right hands this powerful statistical tool can send us\nspeeding dangerously in the wrong direction. The balance of this chapter will explain the most\ncommon regression “mistakes.” I put “mistakes” in quotation marks, because, as with all other\nkinds of statistical analysis, clever people can knowingly exploit these methodological points\nto nefarious ends. Here is a “Top Seven” list of the most common abuses of an otherwise extraordinary tool. Using regression to analyze a nonlinear relationship.* Have you ever read the warning label\non a hair dryer---the part that cautions, Do Not Use in the Bath Tub? And you think to\nyourself, “What kind of moron uses a hair dryer in the bath tub?” It’s an electrical appliance;\nyou don’t use electrical appliances around water. They’re not designed for that. If regression\nanalysis had a similar warning label, it would say, Do Not Use When There Is Not a Linear\nAssociation between the Variables That You Are Analyzing. Remember, a regression\ncoefficient describes the slope of the “line of best fit” for the data; a line that is not straight\nwill have a different slope in different places. As an example, consider the following\nhypothetical relationship between the number of golf lessons that I take during a month (an\nexplanatory variable) and my average score for an eighteen-hole round during that month (the\ndependent variable). As you can see from the scatter plot, there is no consistent linear\nrelationship. Effect of Golf Lessons on Score\n\nThere is a pattern, but it cannot be easily described with a single straight line. The first few\ngolf lessons appear to bring my score down rapidly. There is a negative association between\nlessons and my scores for this stretch; the slope is negative. More lessons yield lower scores\n(which is good in golf).", "tokens": 480, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 155, "segment_id": "00155", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000273"}
{"type": "chunk", "text": "But then when I reach the point where I’m spending between $200 and $300 a month on\nlessons, the lessons do not seem to have much effect at all. There is no clear association over\nthis stretch between additional instruction and my golf scores; the slope is zero. And finally, the lessons appear to become counterproductive. Once I’m spending $300 a\nmonth on instruction, incremental lessons are associated with higher scores; the slope is\npositive over this stretch. (I’ll discuss the distinct possibility that the bad golf may be causing\nthe lessons, rather than the other way around, later in the chapter.)\n\nThe most important point here is that we cannot accurately summarize the relationship\nbetween lessons and scores with a single coefficient. The best interpretation of the pattern\ndescribed above is that golf lessons have several different linear relationships with my scores. You can see that; a statistics package will not. If you feed these data into a regression\nequation, the computer will give you a single coefficient. That coefficient will not accurately\nreflect the true relationship between the variables of interest. The results you get will be the\nstatistical equivalent of using a hair dryer in the bath tub. Regression analysis is meant to be used when the relationship between variables is linear. A\ntextbook or an advanced course in statistics will walk you through the other core assumptions\nunderlying regression analysis. As with any other tool, the further one deviates from its\nintended use, the less effective, or even potentially dangerous, it’s going to be. Correlation does not equal causation. Regression analysis can only demonstrate an\nassociation between two variables. As I have mentioned before, we cannot prove with statistics\nalone that a change in one variable is causing a change in the other. In fact, a sloppy regression\nequation can produce a large and statistically significant association between two variables\nthat have nothing to do with one another. Suppose we were searching for potential causes for\nthe rising rate of autism in the United States over the last two decades. Our dependent variable\n---the outcome we are seeking to explain---would be some measure of the incidence of the\nautism by year, such as the number of diagnosed cases for every 1,000 children of a certain\nage. If we were to include annual per capita income in China as an explanatory variable, we\nwould almost certainly find a positive and statistically significant association between rising\nincomes in China and rising autism rates in the U.S.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nBut then when I reach the point where I’m spending between $200 and $300 a month on\nlessons, the lessons do not seem to have much effect at all. There is no clear association over\nthis stretch between additional instruction and my golf scores; the slope is zero. And finally, the lessons appear to become counterproductive. Once I’m spending $300 a\nmonth on instruction, incremental lessons are associated with higher scores; the slope is\npositive over this stretch. (I’ll discuss the distinct possibility that the bad golf may be causing\nthe lessons, rather than the other way around, later in the chapter.)\n\nThe most important point here is that we cannot accurately summarize the relationship\nbetween lessons and scores with a single coefficient. The best interpretation of the pattern\ndescribed above is that golf lessons have several different linear relationships with my scores. You can see that; a statistics package will not. If you feed these data into a regression\nequation, the computer will give you a single coefficient. That coefficient will not accurately\nreflect the true relationship between the variables of interest. The results you get will be the\nstatistical equivalent of using a hair dryer in the bath tub. Regression analysis is meant to be used when the relationship between variables is linear. A\ntextbook or an advanced course in statistics will walk you through the other core assumptions\nunderlying regression analysis. As with any other tool, the further one deviates from its\nintended use, the less effective, or even potentially dangerous, it’s going to be. Correlation does not equal causation. Regression analysis can only demonstrate an\nassociation between two variables. As I have mentioned before, we cannot prove with statistics\nalone that a change in one variable is causing a change in the other. In fact, a sloppy regression\nequation can produce a large and statistically significant association between two variables\nthat have nothing to do with one another. Suppose we were searching for potential causes for\nthe rising rate of autism in the United States over the last two decades. Our dependent variable\n---the outcome we are seeking to explain---would be some measure of the incidence of the\nautism by year, such as the number of diagnosed cases for every 1,000 children of a certain\nage. If we were to include annual per capita income in China as an explanatory variable, we\nwould almost certainly find a positive and statistically significant association between rising\nincomes in China and rising autism rates in the U.S.", "tokens": 507, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 156, "segment_id": "00156", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000274"}
{"type": "chunk", "text": "Suppose we were searching for potential causes for\nthe rising rate of autism in the United States over the last two decades. Our dependent variable\n---the outcome we are seeking to explain---would be some measure of the incidence of the\nautism by year, such as the number of diagnosed cases for every 1,000 children of a certain\nage. If we were to include annual per capita income in China as an explanatory variable, we\nwould almost certainly find a positive and statistically significant association between rising\nincomes in China and rising autism rates in the U.S. over the past twenty years. Why? Because they both have been rising sharply over the same period. Yet I highly doubt\nthat a sharp recession in China would reduce the autism rate in the United States. (To be fair, if\nI observed a strong relationship between rapid economic growth in China and autism rates in\nChina alone, I might begin to search for some environmental factor related to economic growth,\nsuch as industrial pollution, that might explain the association.)\n\nThe kind of false association between two variables that I have just illustrated is just one\nexample of a more general phenomenon known as spurious causation. There are several other\nways in which an association between A and B can be wrongly interpreted. Reverse causality. A statistical association between A and B does not prove that A causes B. In fact, it’s entirely plausible that B is causing A. I alluded to this possibility earlier in the golf\nlesson example. Suppose that when I build a complex model to explain my golf scores, the", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nSuppose we were searching for potential causes for\nthe rising rate of autism in the United States over the last two decades. Our dependent variable\n---the outcome we are seeking to explain---would be some measure of the incidence of the\nautism by year, such as the number of diagnosed cases for every 1,000 children of a certain\nage. If we were to include annual per capita income in China as an explanatory variable, we\nwould almost certainly find a positive and statistically significant association between rising\nincomes in China and rising autism rates in the U.S. over the past twenty years. Why? Because they both have been rising sharply over the same period. Yet I highly doubt\nthat a sharp recession in China would reduce the autism rate in the United States. (To be fair, if\nI observed a strong relationship between rapid economic growth in China and autism rates in\nChina alone, I might begin to search for some environmental factor related to economic growth,\nsuch as industrial pollution, that might explain the association.)\n\nThe kind of false association between two variables that I have just illustrated is just one\nexample of a more general phenomenon known as spurious causation. There are several other\nways in which an association between A and B can be wrongly interpreted. Reverse causality. A statistical association between A and B does not prove that A causes B. In fact, it’s entirely plausible that B is causing A. I alluded to this possibility earlier in the golf\nlesson example. Suppose that when I build a complex model to explain my golf scores, the", "tokens": 319, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 156, "segment_id": "00156", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000275"}
{"type": "chunk", "text": "variable for golf lessons is consistently associated with worse scores. The more lessons I take,\nthe worse I shoot! One explanation is that I have a really, really bad golf instructor. A more\nplausible explanation is that I tend to take more lessons when I’m playing poorly; bad golf is\ncausing more lessons, not the other way around. (There are some simple methodological fixes\nto a problem of this nature. For example, I might include golf lessons in one month as an\nexplanatory variable for golf scores in the next month.)\n\nAs noted earlier in the chapter, causality may go in both directions. Suppose you do some\nresearch demonstrating that states that spend more money on K--12 education have higher rates\nof economic growth than states that spend less on K--12 education. A positive and significant\nassociation between these two variables does not provide any insight into which direction the\nrelationship happens to run. Investments in K--12 education could cause economic growth. On\nthe other hand, states that have strong economies can afford to spend more on K--12 education,\nso the strong economy could be causing the education spending. Or, education spending could\nboost economic growth, which makes possible additional education spending---the causality\ncould be going in both ways. The point is that we should not use explanatory variables that might be affected by the\noutcome that we are trying to explain, or else the results will become hopelessly tangled. For\nexample, it would be inappropriate to use the unemployment rate in a regression equation\nexplaining GDP growth, since unemployment is clearly affected by the rate of GDP growth. Or,\nto think of it another way, a regression analysis finding that lowering unemployment will boost\nGDP growth is a silly and meaningless finding, since boosting GDP growth is usually required\nin order to reduce unemployment. We should have reason to believe that our explanatory variables affect the dependent\n\nvariable, and not the other way around. Omitted variable bias. You should be skeptical the next time you see a huge headline\nproclaiming, “Golfers More Prone to Heart Disease, Cancer, and Arthritis!” I would not be\nsurprised if golfers have a higher incidence of all of those diseases than nongolfers; I also\nsuspect that golf is probably good for your health because it provides socialization and modest\nexercise. How can I reconcile those two statements? Very easily.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nvariable for golf lessons is consistently associated with worse scores. The more lessons I take,\nthe worse I shoot! One explanation is that I have a really, really bad golf instructor. A more\nplausible explanation is that I tend to take more lessons when I’m playing poorly; bad golf is\ncausing more lessons, not the other way around. (There are some simple methodological fixes\nto a problem of this nature. For example, I might include golf lessons in one month as an\nexplanatory variable for golf scores in the next month.)\n\nAs noted earlier in the chapter, causality may go in both directions. Suppose you do some\nresearch demonstrating that states that spend more money on K--12 education have higher rates\nof economic growth than states that spend less on K--12 education. A positive and significant\nassociation between these two variables does not provide any insight into which direction the\nrelationship happens to run. Investments in K--12 education could cause economic growth. On\nthe other hand, states that have strong economies can afford to spend more on K--12 education,\nso the strong economy could be causing the education spending. Or, education spending could\nboost economic growth, which makes possible additional education spending---the causality\ncould be going in both ways. The point is that we should not use explanatory variables that might be affected by the\noutcome that we are trying to explain, or else the results will become hopelessly tangled. For\nexample, it would be inappropriate to use the unemployment rate in a regression equation\nexplaining GDP growth, since unemployment is clearly affected by the rate of GDP growth. Or,\nto think of it another way, a regression analysis finding that lowering unemployment will boost\nGDP growth is a silly and meaningless finding, since boosting GDP growth is usually required\nin order to reduce unemployment. We should have reason to believe that our explanatory variables affect the dependent\n\nvariable, and not the other way around. Omitted variable bias. You should be skeptical the next time you see a huge headline\nproclaiming, “Golfers More Prone to Heart Disease, Cancer, and Arthritis!” I would not be\nsurprised if golfers have a higher incidence of all of those diseases than nongolfers; I also\nsuspect that golf is probably good for your health because it provides socialization and modest\nexercise. How can I reconcile those two statements? Very easily.", "tokens": 496, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 157, "segment_id": "00157", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000276"}
{"type": "chunk", "text": "We should have reason to believe that our explanatory variables affect the dependent\n\nvariable, and not the other way around. Omitted variable bias. You should be skeptical the next time you see a huge headline\nproclaiming, “Golfers More Prone to Heart Disease, Cancer, and Arthritis!” I would not be\nsurprised if golfers have a higher incidence of all of those diseases than nongolfers; I also\nsuspect that golf is probably good for your health because it provides socialization and modest\nexercise. How can I reconcile those two statements? Very easily. Any study that attempts to\nmeasure the effects of playing golf on health must control properly for age. In general, people\nplay more golf when they get older, particularly in retirement. Any analysis that leaves out age\nas an explanatory variable is going to miss the fact that golfers, on average, will be older than\nnongolfers. Golf isn’t killing people; old age is killing people, and they happen to enjoy playing\ngolf while it does. I suspect that when age is inserted into the regression analysis as a control\nvariable, we will get a different outcome. Among people who are the same age, golf may be\nmildly preventive of serious illnesses. That’s a pretty big difference. In this example, age is an important “omitted variable.” When we leave age out of a\nregression equation explaining heart disease or some other adverse health outcome, the\n“playing golf” variable takes on two explanatory roles rather than just one. It tells us the effect\nof playing golf on heart disease, and it tells us the effect of being old on heart disease (since\ngolfers tend to be older than the rest of the population). In the statistics lingo, we would say", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nWe should have reason to believe that our explanatory variables affect the dependent\n\nvariable, and not the other way around. Omitted variable bias. You should be skeptical the next time you see a huge headline\nproclaiming, “Golfers More Prone to Heart Disease, Cancer, and Arthritis!” I would not be\nsurprised if golfers have a higher incidence of all of those diseases than nongolfers; I also\nsuspect that golf is probably good for your health because it provides socialization and modest\nexercise. How can I reconcile those two statements? Very easily. Any study that attempts to\nmeasure the effects of playing golf on health must control properly for age. In general, people\nplay more golf when they get older, particularly in retirement. Any analysis that leaves out age\nas an explanatory variable is going to miss the fact that golfers, on average, will be older than\nnongolfers. Golf isn’t killing people; old age is killing people, and they happen to enjoy playing\ngolf while it does. I suspect that when age is inserted into the regression analysis as a control\nvariable, we will get a different outcome. Among people who are the same age, golf may be\nmildly preventive of serious illnesses. That’s a pretty big difference. In this example, age is an important “omitted variable.” When we leave age out of a\nregression equation explaining heart disease or some other adverse health outcome, the\n“playing golf” variable takes on two explanatory roles rather than just one. It tells us the effect\nof playing golf on heart disease, and it tells us the effect of being old on heart disease (since\ngolfers tend to be older than the rest of the population). In the statistics lingo, we would say", "tokens": 371, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 157, "segment_id": "00157", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000277"}
{"type": "chunk", "text": "that the golf variable is “picking up” the effect of age. The problem is that these two effects\nare comingled. At best, our results are a jumbled mess. At worst, we wrongly assume that golf\nis bad for your health, when in fact the opposite is likely to be true. Regression results will be misleading and inaccurate if the regression equation leaves out an\nimportant explanatory variable, particularly if other variables in the equation “pick up” that\neffect. Suppose we are trying to explain school quality. This is an important outcome to\nunderstand: What makes good schools? Our dependent variable---the quantifiable measure of\nquality---would most likely be test scores. We would almost certainly examine school\nspending as one explanatory variable in hopes of quantifying the relationship between spending\nand test scores. Do schools that spend more get better results? If school spending were the\nonly explanatory variable, I have no doubt that we would find a large and statistically\nsignificant relationship between spending and test scores. Yet that finding, and the implication\nthat we can spend our way to better schools, is deeply flawed. There are many potentially significant omitted variables here, but the crucial one is parental\neducation. Well-educated families tend to live in affluent areas that spend a lot of money on\ntheir schools; such families also tend to have children who score well on tests (and poor\nfamilies are more likely to have students who struggle). If we do not have some measure of the\nsocioeconomic status of the student body as a control variable, our regression results will\nprobably show a large positive association between school spending and test scores---when in\nfact, those results may be a function of the kind of students who are walking in the school door,\nnot the money that is being spent in the building. I remember a college professor’s pointing out that SAT scores are highly correlated with the\nnumber of cars that a family owns. He insinuated that the SAT was therefore an unfair and\ninappropriate tool for college admissions. The SAT has its flaws but the correlation between\nscores and family cars is not the one that concerns me most. I do not worry much that rich\nfamilies can get their kids into college by purchasing three extra automobiles. The number of\ncars in a family’s garage is a proxy for their income, education, and other measures of\nsocioeconomic status. The fact that wealthy kids do better on the SAT than poor kids is not\nnews.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nthat the golf variable is “picking up” the effect of age. The problem is that these two effects\nare comingled. At best, our results are a jumbled mess. At worst, we wrongly assume that golf\nis bad for your health, when in fact the opposite is likely to be true. Regression results will be misleading and inaccurate if the regression equation leaves out an\nimportant explanatory variable, particularly if other variables in the equation “pick up” that\neffect. Suppose we are trying to explain school quality. This is an important outcome to\nunderstand: What makes good schools? Our dependent variable---the quantifiable measure of\nquality---would most likely be test scores. We would almost certainly examine school\nspending as one explanatory variable in hopes of quantifying the relationship between spending\nand test scores. Do schools that spend more get better results? If school spending were the\nonly explanatory variable, I have no doubt that we would find a large and statistically\nsignificant relationship between spending and test scores. Yet that finding, and the implication\nthat we can spend our way to better schools, is deeply flawed. There are many potentially significant omitted variables here, but the crucial one is parental\neducation. Well-educated families tend to live in affluent areas that spend a lot of money on\ntheir schools; such families also tend to have children who score well on tests (and poor\nfamilies are more likely to have students who struggle). If we do not have some measure of the\nsocioeconomic status of the student body as a control variable, our regression results will\nprobably show a large positive association between school spending and test scores---when in\nfact, those results may be a function of the kind of students who are walking in the school door,\nnot the money that is being spent in the building. I remember a college professor’s pointing out that SAT scores are highly correlated with the\nnumber of cars that a family owns. He insinuated that the SAT was therefore an unfair and\ninappropriate tool for college admissions. The SAT has its flaws but the correlation between\nscores and family cars is not the one that concerns me most. I do not worry much that rich\nfamilies can get their kids into college by purchasing three extra automobiles. The number of\ncars in a family’s garage is a proxy for their income, education, and other measures of\nsocioeconomic status. The fact that wealthy kids do better on the SAT than poor kids is not\nnews.", "tokens": 508, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 158, "segment_id": "00158", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000278"}
{"type": "chunk", "text": "He insinuated that the SAT was therefore an unfair and\ninappropriate tool for college admissions. The SAT has its flaws but the correlation between\nscores and family cars is not the one that concerns me most. I do not worry much that rich\nfamilies can get their kids into college by purchasing three extra automobiles. The number of\ncars in a family’s garage is a proxy for their income, education, and other measures of\nsocioeconomic status. The fact that wealthy kids do better on the SAT than poor kids is not\nnews. (As noted earlier, the mean SAT critical reading score for students from families with a\nhousehold income over $200,000 is 134 points higher than the mean score for students in\nhouseholds with income below $20,000.)4 The bigger concern should be whether or not the\nSAT is “coachable.” How much can students improve their scores by taking private SAT prep\nclasses? Wealthy families clearly are better able to send their children to test prep classes. Any causal improvement between these classes and SAT scores would favor students from\nwealthy families relative to more disadvantaged students of equal abilities (who presumably\nalso could have raised their scores with a prep class but never had that opportunity). Highly correlated explanatory variables (multicollinearity). If a regression equation includes\ntwo or more explanatory variables that are highly correlated with each other, the analysis will\nnot necessarily be able to discern the true relationship between each of those variables and the\noutcome that we are trying to explain. An example will make this clearer. Assume we are\ntrying to gauge the effect of illegal drug use on SAT scores. Specifically, we have data on", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nHe insinuated that the SAT was therefore an unfair and\ninappropriate tool for college admissions. The SAT has its flaws but the correlation between\nscores and family cars is not the one that concerns me most. I do not worry much that rich\nfamilies can get their kids into college by purchasing three extra automobiles. The number of\ncars in a family’s garage is a proxy for their income, education, and other measures of\nsocioeconomic status. The fact that wealthy kids do better on the SAT than poor kids is not\nnews. (As noted earlier, the mean SAT critical reading score for students from families with a\nhousehold income over $200,000 is 134 points higher than the mean score for students in\nhouseholds with income below $20,000.)4 The bigger concern should be whether or not the\nSAT is “coachable.” How much can students improve their scores by taking private SAT prep\nclasses? Wealthy families clearly are better able to send their children to test prep classes. Any causal improvement between these classes and SAT scores would favor students from\nwealthy families relative to more disadvantaged students of equal abilities (who presumably\nalso could have raised their scores with a prep class but never had that opportunity). Highly correlated explanatory variables (multicollinearity). If a regression equation includes\ntwo or more explanatory variables that are highly correlated with each other, the analysis will\nnot necessarily be able to discern the true relationship between each of those variables and the\noutcome that we are trying to explain. An example will make this clearer. Assume we are\ntrying to gauge the effect of illegal drug use on SAT scores. Specifically, we have data on", "tokens": 343, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 158, "segment_id": "00158", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000279"}
{"type": "chunk", "text": "whether the participants in our study have ever used cocaine and also on whether they have\never used heroin. (We would presumably have many other control variables as well.) What is\nthe impact of cocaine use on SAT scores, holding other factors constant, including heroin use? And what is the impact of heroin use on SAT scores, controlling for cocaine use and other\nfactors? The coefficients on heroin and cocaine use might not actually be able to tell us that. The\nmethodological challenge is that people who have used heroin have likely also used cocaine. If\nwe put both variables in the equation, we will have very few individuals who have used one\ndrug but not the other, which leaves us very little variation in the data with which to calculate\ntheir independent effects. Think back for a moment to the mental imagery used to explain\nregression analysis in the last chapter. We divide our data sample into different “rooms” in\nwhich each observation is identical except for one variable, which then allows us to isolate the\neffect of that variable while controlling for other potential confounding factors. We may have\n692 individuals in our sample who have used both cocaine and heroin. However, we may have\nonly 3 individuals who have used cocaine but not heroin and 2 individuals who have used\nheroin and not cocaine. Any inference about the independent effect of just one drug or the other\nis going to be based on these tiny samples. We are unlikely to get meaningful coefficients on either the cocaine or the heroin variable;\nwe may also obscure the larger and more important relationship between SAT scores and using\neither one of these drugs. When two explanatory variables are highly correlated, researchers\nwill usually use one or the other in the regression equation, or they may create some kind of\ncomposite variable, such as “used cocaine or heroin.” For example, when researchers want to\ncontrol for a student’s overall socioeconomic background, they may include variables for both\n“mother’s education” and “father’s education,” since this inclusion provides important insight\ninto the educational background of the household. However, if the goal of the regression\nanalysis is to isolate the effect of either a mother’s or a father’s education, then putting both\nvariables into the equation is more likely to confuse the issue than to clarify it.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nwhether the participants in our study have ever used cocaine and also on whether they have\never used heroin. (We would presumably have many other control variables as well.) What is\nthe impact of cocaine use on SAT scores, holding other factors constant, including heroin use? And what is the impact of heroin use on SAT scores, controlling for cocaine use and other\nfactors? The coefficients on heroin and cocaine use might not actually be able to tell us that. The\nmethodological challenge is that people who have used heroin have likely also used cocaine. If\nwe put both variables in the equation, we will have very few individuals who have used one\ndrug but not the other, which leaves us very little variation in the data with which to calculate\ntheir independent effects. Think back for a moment to the mental imagery used to explain\nregression analysis in the last chapter. We divide our data sample into different “rooms” in\nwhich each observation is identical except for one variable, which then allows us to isolate the\neffect of that variable while controlling for other potential confounding factors. We may have\n692 individuals in our sample who have used both cocaine and heroin. However, we may have\nonly 3 individuals who have used cocaine but not heroin and 2 individuals who have used\nheroin and not cocaine. Any inference about the independent effect of just one drug or the other\nis going to be based on these tiny samples. We are unlikely to get meaningful coefficients on either the cocaine or the heroin variable;\nwe may also obscure the larger and more important relationship between SAT scores and using\neither one of these drugs. When two explanatory variables are highly correlated, researchers\nwill usually use one or the other in the regression equation, or they may create some kind of\ncomposite variable, such as “used cocaine or heroin.” For example, when researchers want to\ncontrol for a student’s overall socioeconomic background, they may include variables for both\n“mother’s education” and “father’s education,” since this inclusion provides important insight\ninto the educational background of the household. However, if the goal of the regression\nanalysis is to isolate the effect of either a mother’s or a father’s education, then putting both\nvariables into the equation is more likely to confuse the issue than to clarify it.", "tokens": 467, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 159, "segment_id": "00159", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000280"}
{"type": "chunk", "text": "However, if the goal of the regression\nanalysis is to isolate the effect of either a mother’s or a father’s education, then putting both\nvariables into the equation is more likely to confuse the issue than to clarify it. The correlation\nbetween a husband’s and a wife’s educational attainments is so high that we cannot depend on\nregression analysis to give us coefficients that meaningfully isolate the effect of either parent’s\neducation (just as it is hard to separate the impact of cocaine use from the impact of heroin\nuse). Extrapolating beyond the data. Regression analysis, like all forms of statistical inference, is\ndesigned to offer us insights into the world around us. We seek patterns that will hold true for\nthe larger population. However, our results are valid only for a population that is similar to\nthe sample on which the analysis has been done. In the last chapter, I created a regression\nequation to predict weight based on a number of independent variables. The R2 of my final\nmodel was .29, which means that it did a decent job of explaining the variation in weight for a\nlarge sample of individuals---all of whom happened to be adults. So what happens if we use our regression equation to predict the likely weight of a newborn? Let’s try it. My daughter was 21 inches when she was born. We’ll say that her age at birth was\nzero; she had no education and did not exercise. She was white and female. The regression", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nHowever, if the goal of the regression\nanalysis is to isolate the effect of either a mother’s or a father’s education, then putting both\nvariables into the equation is more likely to confuse the issue than to clarify it. The correlation\nbetween a husband’s and a wife’s educational attainments is so high that we cannot depend on\nregression analysis to give us coefficients that meaningfully isolate the effect of either parent’s\neducation (just as it is hard to separate the impact of cocaine use from the impact of heroin\nuse). Extrapolating beyond the data. Regression analysis, like all forms of statistical inference, is\ndesigned to offer us insights into the world around us. We seek patterns that will hold true for\nthe larger population. However, our results are valid only for a population that is similar to\nthe sample on which the analysis has been done. In the last chapter, I created a regression\nequation to predict weight based on a number of independent variables. The R2 of my final\nmodel was .29, which means that it did a decent job of explaining the variation in weight for a\nlarge sample of individuals---all of whom happened to be adults. So what happens if we use our regression equation to predict the likely weight of a newborn? Let’s try it. My daughter was 21 inches when she was born. We’ll say that her age at birth was\nzero; she had no education and did not exercise. She was white and female. The regression", "tokens": 306, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 159, "segment_id": "00159", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000281"}
{"type": "chunk", "text": "equation based on the Changing Lives data predicts that her weight at birth should have been\nnegative 19.6 pounds. (She weighed 81⁄2 pounds.)\n\nThe authors of one of the Whitehall studies referred to in the last chapter were strikingly\nexplicit in drawing their narrow conclusion: “Low control in the work environment is associated\nwith an increased risk of future coronary heart disease among men and women employed in\ngovernment offices”5 (italics added). Data mining (too many variables). If omitting important variables is a potential problem, then\npresumably adding as many explanatory variables as possible to a regression equation must be\nthe solution. Nope. Your results can be compromised if you include too many variables, particularly extraneous\nexplanatory variables with no theoretical justification. For example, one should not design a\nresearch strategy built around the following premise: Since we don’t know what causes autism,\nwe should put as many potential explanatory variables as possible in the regression equation\njust to see what might turn up as statistically significant; then maybe we’ll get some answers. If\nyou put enough junk variables in a regression equation, one of them is bound to meet the\nthreshold for statistical significance just by chance. The further danger is that junk variables\nare not always easily recognized as such. Clever researchers can always build a theory after\nthe fact for why some curious variable that is really just nonsense turns up as statistically\nsignificant. To make this point, I often do the same coin flipping exercise that I explained during the\nprobability discussion. In a class of forty students or so, I’ll have each student flip a coin. Any\nstudent who flips tails is eliminated; the rest flip again. In the second round, those who flip tails\nare once again eliminated. I continue the rounds of flipping until one student has flipped five or\nsix heads in a row. You may recall some of the silly follow-up questions: “What’s your secret? Is it in the wrist? Can you teach us to flip heads all the time? Maybe it’s that Harvard\nsweatshirt you’re wearing.”\n\nObviously the string of heads is just luck; the students have all watched it happen. However,\nthat is not necessarily how the result could or would be interpreted in a scientific context. The\nprobability of flipping five heads in a row is 1/32, or .03. This is comfortably below the .05\nthreshold we typically use to reject a null hypothesis.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nequation based on the Changing Lives data predicts that her weight at birth should have been\nnegative 19.6 pounds. (She weighed 81⁄2 pounds.)\n\nThe authors of one of the Whitehall studies referred to in the last chapter were strikingly\nexplicit in drawing their narrow conclusion: “Low control in the work environment is associated\nwith an increased risk of future coronary heart disease among men and women employed in\ngovernment offices”5 (italics added). Data mining (too many variables). If omitting important variables is a potential problem, then\npresumably adding as many explanatory variables as possible to a regression equation must be\nthe solution. Nope. Your results can be compromised if you include too many variables, particularly extraneous\nexplanatory variables with no theoretical justification. For example, one should not design a\nresearch strategy built around the following premise: Since we don’t know what causes autism,\nwe should put as many potential explanatory variables as possible in the regression equation\njust to see what might turn up as statistically significant; then maybe we’ll get some answers. If\nyou put enough junk variables in a regression equation, one of them is bound to meet the\nthreshold for statistical significance just by chance. The further danger is that junk variables\nare not always easily recognized as such. Clever researchers can always build a theory after\nthe fact for why some curious variable that is really just nonsense turns up as statistically\nsignificant. To make this point, I often do the same coin flipping exercise that I explained during the\nprobability discussion. In a class of forty students or so, I’ll have each student flip a coin. Any\nstudent who flips tails is eliminated; the rest flip again. In the second round, those who flip tails\nare once again eliminated. I continue the rounds of flipping until one student has flipped five or\nsix heads in a row. You may recall some of the silly follow-up questions: “What’s your secret? Is it in the wrist? Can you teach us to flip heads all the time? Maybe it’s that Harvard\nsweatshirt you’re wearing.”\n\nObviously the string of heads is just luck; the students have all watched it happen. However,\nthat is not necessarily how the result could or would be interpreted in a scientific context. The\nprobability of flipping five heads in a row is 1/32, or .03. This is comfortably below the .05\nthreshold we typically use to reject a null hypothesis.", "tokens": 506, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 160, "segment_id": "00160", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000282"}
{"type": "chunk", "text": "You may recall some of the silly follow-up questions: “What’s your secret? Is it in the wrist? Can you teach us to flip heads all the time? Maybe it’s that Harvard\nsweatshirt you’re wearing.”\n\nObviously the string of heads is just luck; the students have all watched it happen. However,\nthat is not necessarily how the result could or would be interpreted in a scientific context. The\nprobability of flipping five heads in a row is 1/32, or .03. This is comfortably below the .05\nthreshold we typically use to reject a null hypothesis. Our null hypothesis in this case is that\nthe student has no special talent for flipping heads; the lucky string of heads (which is bound to\nhappen for at least one student when I start with a large group) allows us to reject the null\nhypothesis and adopt the alternative hypothesis: This student has a special ability to flip heads. After he has achieved this impressive feat, we can study him for clues about his flipping\nsuccess---his flipping form, his athletic training, his extraordinary concentration while the coin\nis in the air, and so on. And it is all nonsense. This phenomenon can plague even legitimate research. The accepted convention is to reject\na null hypothesis when we observe something that would happen by chance only 1 in 20 times\nor less if the null hypothesis were true. Of course, if we conduct 20 studies, or if we include\n20 junk variables in a single regression equation, then on average we will get 1 bogus\nstatistically significant finding. The New York Times Magazine captured this tension", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nYou may recall some of the silly follow-up questions: “What’s your secret? Is it in the wrist? Can you teach us to flip heads all the time? Maybe it’s that Harvard\nsweatshirt you’re wearing.”\n\nObviously the string of heads is just luck; the students have all watched it happen. However,\nthat is not necessarily how the result could or would be interpreted in a scientific context. The\nprobability of flipping five heads in a row is 1/32, or .03. This is comfortably below the .05\nthreshold we typically use to reject a null hypothesis. Our null hypothesis in this case is that\nthe student has no special talent for flipping heads; the lucky string of heads (which is bound to\nhappen for at least one student when I start with a large group) allows us to reject the null\nhypothesis and adopt the alternative hypothesis: This student has a special ability to flip heads. After he has achieved this impressive feat, we can study him for clues about his flipping\nsuccess---his flipping form, his athletic training, his extraordinary concentration while the coin\nis in the air, and so on. And it is all nonsense. This phenomenon can plague even legitimate research. The accepted convention is to reject\na null hypothesis when we observe something that would happen by chance only 1 in 20 times\nor less if the null hypothesis were true. Of course, if we conduct 20 studies, or if we include\n20 junk variables in a single regression equation, then on average we will get 1 bogus\nstatistically significant finding. The New York Times Magazine captured this tension", "tokens": 334, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 160, "segment_id": "00160", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000283"}
{"type": "chunk", "text": "wonderfully in a quotation from Richard Peto, a medical statistician and epidemiologist:\n“Epidemiology is so beautiful and provides such an important perspective on human life and\ndeath, but an incredible amount of rubbish is published.”6\n\nEven the results of clinical trials, which are usually randomized experiments and therefore\nthe gold standard of medical research, should be viewed with some skepticism. In 2011, the\nWall Street Journal ran a front-page story on what it described as one of the “dirty little\nsecrets” of medical research: “Most results, including those that appear in top-flight peerreviewed journals, can’t be reproduced.”7 (A peer-reviewed journal is a publication in which\nstudies and articles are reviewed for methodological soundness by other experts in the same\nfield before being approved for publication; such publications are considered to be the\ngatekeepers for academic research.) One reason for this “dirty little secret” is the positive\npublication bias described in Chapter 7. If researchers and medical journals pay attention to\npositive findings and ignore negative findings, then they may well publish the one study that\nfinds a drug effective and ignore the nineteen in which it has no effect. Some clinical trials may\nalso have small samples (such as for a rare diseases), which magnifies the chances that random\nvariation in the data will get more attention than it deserves. On top of that, researchers may\nhave some conscious or unconscious bias, either because of a strongly held prior belief or\nbecause a positive finding would be better for their career. (No one ever gets rich or famous\nby proving what doesn’t cure cancer.)\n\nFor all of these reasons, a shocking amount of expert research turns out to be wrong. John\nIoannidis, a Greek doctor and epidemiologist, examined forty-nine studies published in three\nprominent medical journals.8 Each study had been cited in the medical literature at least a\nthousand times. Yet roughly one-third of the research was subsequently refuted by later work. (For example, some of the studies he examined promoted estrogen replacement therapy.) Dr. Ioannidis estimates that roughly half of the scientific papers published will eventually turn out\nto be wrong.9 His research was published in the Journal of the American Medical Association,\none of the journals in which the articles he studied had appeared. This does create a certain\nmind-bending irony: If Dr.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nwonderfully in a quotation from Richard Peto, a medical statistician and epidemiologist:\n“Epidemiology is so beautiful and provides such an important perspective on human life and\ndeath, but an incredible amount of rubbish is published.”6\n\nEven the results of clinical trials, which are usually randomized experiments and therefore\nthe gold standard of medical research, should be viewed with some skepticism. In 2011, the\nWall Street Journal ran a front-page story on what it described as one of the “dirty little\nsecrets” of medical research: “Most results, including those that appear in top-flight peerreviewed journals, can’t be reproduced.”7 (A peer-reviewed journal is a publication in which\nstudies and articles are reviewed for methodological soundness by other experts in the same\nfield before being approved for publication; such publications are considered to be the\ngatekeepers for academic research.) One reason for this “dirty little secret” is the positive\npublication bias described in Chapter 7. If researchers and medical journals pay attention to\npositive findings and ignore negative findings, then they may well publish the one study that\nfinds a drug effective and ignore the nineteen in which it has no effect. Some clinical trials may\nalso have small samples (such as for a rare diseases), which magnifies the chances that random\nvariation in the data will get more attention than it deserves. On top of that, researchers may\nhave some conscious or unconscious bias, either because of a strongly held prior belief or\nbecause a positive finding would be better for their career. (No one ever gets rich or famous\nby proving what doesn’t cure cancer.)\n\nFor all of these reasons, a shocking amount of expert research turns out to be wrong. John\nIoannidis, a Greek doctor and epidemiologist, examined forty-nine studies published in three\nprominent medical journals.8 Each study had been cited in the medical literature at least a\nthousand times. Yet roughly one-third of the research was subsequently refuted by later work. (For example, some of the studies he examined promoted estrogen replacement therapy.) Dr. Ioannidis estimates that roughly half of the scientific papers published will eventually turn out\nto be wrong.9 His research was published in the Journal of the American Medical Association,\none of the journals in which the articles he studied had appeared. This does create a certain\nmind-bending irony: If Dr.", "tokens": 495, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 161, "segment_id": "00161", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000284"}
{"type": "chunk", "text": "Yet roughly one-third of the research was subsequently refuted by later work. (For example, some of the studies he examined promoted estrogen replacement therapy.) Dr. Ioannidis estimates that roughly half of the scientific papers published will eventually turn out\nto be wrong.9 His research was published in the Journal of the American Medical Association,\none of the journals in which the articles he studied had appeared. This does create a certain\nmind-bending irony: If Dr. Ioannidis’s research is correct, then there is a good chance that his\nresearch is wrong. Regression analysis is still an awesome statistical tool. (Okay, perhaps my description of it as\na “miracle elixir” in the last chapter was a little hyperbolic.) Regression analysis enables us to\nfind key patterns in large data sets, and those patterns are often the key to important research in\nmedicine and the social sciences. Statistics gives us objective standards for evaluating these\npatterns. When used properly, regression analysis is an important part of the scientific method. Consider this chapter to be the mandatory warning label. All of the assorted specific warnings on that label can be boiled down to two key lessons. First, designing a good regression equation---figuring out what variables should be examined\nand where the data should come from---is more important than the underlying statistical\ncalculations. This process is referred to as estimating the equation, or specifying a good\nregression equation. The best researchers are the ones who can think logically about what", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nYet roughly one-third of the research was subsequently refuted by later work. (For example, some of the studies he examined promoted estrogen replacement therapy.) Dr. Ioannidis estimates that roughly half of the scientific papers published will eventually turn out\nto be wrong.9 His research was published in the Journal of the American Medical Association,\none of the journals in which the articles he studied had appeared. This does create a certain\nmind-bending irony: If Dr. Ioannidis’s research is correct, then there is a good chance that his\nresearch is wrong. Regression analysis is still an awesome statistical tool. (Okay, perhaps my description of it as\na “miracle elixir” in the last chapter was a little hyperbolic.) Regression analysis enables us to\nfind key patterns in large data sets, and those patterns are often the key to important research in\nmedicine and the social sciences. Statistics gives us objective standards for evaluating these\npatterns. When used properly, regression analysis is an important part of the scientific method. Consider this chapter to be the mandatory warning label. All of the assorted specific warnings on that label can be boiled down to two key lessons. First, designing a good regression equation---figuring out what variables should be examined\nand where the data should come from---is more important than the underlying statistical\ncalculations. This process is referred to as estimating the equation, or specifying a good\nregression equation. The best researchers are the ones who can think logically about what", "tokens": 306, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 161, "segment_id": "00161", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000285"}
{"type": "chunk", "text": "variables ought to be included in a regression equation, what might be missing, and how the\neventual results can and should be interpreted. Second, like most other statistical inference, regression analysis builds only a circumstantial\ncase. An association between two variables is like a fingerprint at the scene of the crime. It\npoints us in the right direction, but it’s rarely enough to convict. (And sometimes a fingerprint\nat the scene of a crime doesn’t belong to the perpetrator.) Any regression analysis needs a\ntheoretical underpinning: Why are the explanatory variables in the equation? What phenomena\nfrom other disciplines can explain the observed results? For instance, why do we think that\nwearing purple shoes would boost performance on the math portion of the SAT or that eating\npopcorn can help prevent prostate cancer? The results need to be replicated, or at least\nconsistent with other findings. Even a miracle elixir won’t work when not taken as directed. * There are more sophisticated methods that can be used to adapt regression analysis for use with nonlinear data. Before\nusing those tools, however, you need to appreciate why using the standard ordinary least squares approach with nonlinear\ndata will give you a meaningless result.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nvariables ought to be included in a regression equation, what might be missing, and how the\neventual results can and should be interpreted. Second, like most other statistical inference, regression analysis builds only a circumstantial\ncase. An association between two variables is like a fingerprint at the scene of the crime. It\npoints us in the right direction, but it’s rarely enough to convict. (And sometimes a fingerprint\nat the scene of a crime doesn’t belong to the perpetrator.) Any regression analysis needs a\ntheoretical underpinning: Why are the explanatory variables in the equation? What phenomena\nfrom other disciplines can explain the observed results? For instance, why do we think that\nwearing purple shoes would boost performance on the math portion of the SAT or that eating\npopcorn can help prevent prostate cancer? The results need to be replicated, or at least\nconsistent with other findings. Even a miracle elixir won’t work when not taken as directed. * There are more sophisticated methods that can be used to adapt regression analysis for use with nonlinear data. Before\nusing those tools, however, you need to appreciate why using the standard ordinary least squares approach with nonlinear\ndata will give you a meaningless result.", "tokens": 248, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 162, "segment_id": "00162", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000286"}
{"type": "chunk", "text": "CHAPTER 13\nProgram Evaluation\nWill going to Harvard change your life? Brilliant researchers in the social sciences are not brilliant because they can do complex\n\ncalculations in their heads, or because they win more money on Jeopardy than less brilliant\nresearchers do (though both these feats may be true). Brilliant researchers---those who\nappreciably change our knowledge of the world---are often individuals or teams who find\ncreative ways to do “controlled” experiments. To measure the effect of any treatment or\nintervention, we need something to measure it against. How would going to Harvard affect your\nlife? Well, to answer that question, we have to know what happens to you after you go to\nHarvard---and what happens to you after you don’t go to Harvard . Obviously we can’t have\ndata on both. Yet clever researchers find ways to compare some treatment (e.g., going to\nHarvard) with the counterfactual, which is what would have happened in the absence of that\ntreatment. To illustrate this point, let’s ponder a seemingly simple question: Does putting more police\nofficers on the street deter crime? This is a socially significant question, as crime imposes\nhuge costs on society. If a greater police presence lowers crime, either through deterrence or\nby catching and imprisoning bad guys, then investments in additional police officers could have\nlarge returns. On the other hand, police officers are relatively expensive; if they have little or\nno impact on crime reduction, then society could make better use of its resources elsewhere\n(perhaps with investments in crime-fighting technology, such as surveillance cameras). The challenge is that our seemingly simple question---what is the causal effect of more\npolice officers on crime?---turns out to be very difficult to answer. By this point in the book,\nyou should recognize that we cannot answer this question simply by examining whether\njurisdictions with a high number of police officers per capita have lower rates of crime. Zurich\nis not Los Angeles. Even a comparison of large American cities will be deeply flawed; Los\nAngeles, New York, Houston, Miami, Detroit, and Chicago are all different places with\ndifferent demographics and crime challenges. Our usual approach would be to attempt to specify a regression equation that controls for\nthese differences. Alas, even multiple regression analysis is not going to save us here.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nCHAPTER 13\nProgram Evaluation\nWill going to Harvard change your life? Brilliant researchers in the social sciences are not brilliant because they can do complex\n\ncalculations in their heads, or because they win more money on Jeopardy than less brilliant\nresearchers do (though both these feats may be true). Brilliant researchers---those who\nappreciably change our knowledge of the world---are often individuals or teams who find\ncreative ways to do “controlled” experiments. To measure the effect of any treatment or\nintervention, we need something to measure it against. How would going to Harvard affect your\nlife? Well, to answer that question, we have to know what happens to you after you go to\nHarvard---and what happens to you after you don’t go to Harvard . Obviously we can’t have\ndata on both. Yet clever researchers find ways to compare some treatment (e.g., going to\nHarvard) with the counterfactual, which is what would have happened in the absence of that\ntreatment. To illustrate this point, let’s ponder a seemingly simple question: Does putting more police\nofficers on the street deter crime? This is a socially significant question, as crime imposes\nhuge costs on society. If a greater police presence lowers crime, either through deterrence or\nby catching and imprisoning bad guys, then investments in additional police officers could have\nlarge returns. On the other hand, police officers are relatively expensive; if they have little or\nno impact on crime reduction, then society could make better use of its resources elsewhere\n(perhaps with investments in crime-fighting technology, such as surveillance cameras). The challenge is that our seemingly simple question---what is the causal effect of more\npolice officers on crime?---turns out to be very difficult to answer. By this point in the book,\nyou should recognize that we cannot answer this question simply by examining whether\njurisdictions with a high number of police officers per capita have lower rates of crime. Zurich\nis not Los Angeles. Even a comparison of large American cities will be deeply flawed; Los\nAngeles, New York, Houston, Miami, Detroit, and Chicago are all different places with\ndifferent demographics and crime challenges. Our usual approach would be to attempt to specify a regression equation that controls for\nthese differences. Alas, even multiple regression analysis is not going to save us here.", "tokens": 492, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 163, "segment_id": "00163", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000287"}
{"type": "chunk", "text": "By this point in the book,\nyou should recognize that we cannot answer this question simply by examining whether\njurisdictions with a high number of police officers per capita have lower rates of crime. Zurich\nis not Los Angeles. Even a comparison of large American cities will be deeply flawed; Los\nAngeles, New York, Houston, Miami, Detroit, and Chicago are all different places with\ndifferent demographics and crime challenges. Our usual approach would be to attempt to specify a regression equation that controls for\nthese differences. Alas, even multiple regression analysis is not going to save us here. If we\nattempt to explain crime rates (our dependent variable) by using police officers per capita as\nan explanatory variable (along with other controls), we will have a serious reverse causality\nproblem. We have a solid theoretical reason to believe that putting more police officers on the\nstreet will reduce crime, but it’s also possible that crime could “cause” police officers, in the\nsense that cities experiencing crime waves will hire more police officers. We could easily find\na positive but misleading association between crime and police: the places with the most\npolice officers have the worst crime problems. Of course, the places with lots of doctors also", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nBy this point in the book,\nyou should recognize that we cannot answer this question simply by examining whether\njurisdictions with a high number of police officers per capita have lower rates of crime. Zurich\nis not Los Angeles. Even a comparison of large American cities will be deeply flawed; Los\nAngeles, New York, Houston, Miami, Detroit, and Chicago are all different places with\ndifferent demographics and crime challenges. Our usual approach would be to attempt to specify a regression equation that controls for\nthese differences. Alas, even multiple regression analysis is not going to save us here. If we\nattempt to explain crime rates (our dependent variable) by using police officers per capita as\nan explanatory variable (along with other controls), we will have a serious reverse causality\nproblem. We have a solid theoretical reason to believe that putting more police officers on the\nstreet will reduce crime, but it’s also possible that crime could “cause” police officers, in the\nsense that cities experiencing crime waves will hire more police officers. We could easily find\na positive but misleading association between crime and police: the places with the most\npolice officers have the worst crime problems. Of course, the places with lots of doctors also", "tokens": 252, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 163, "segment_id": "00163", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000288"}
{"type": "chunk", "text": "tend to have the highest concentration of sick people. These doctors aren’t making people sick;\nthey are located in places where they are needed most (and at the same time sick people are\nmoving to places where they can get appropriate medical care). I suspect that there are\ndisproportionate numbers of oncologists and cardiologists in Florida; banishing them from the\nstate will not make the retiree population healthier. Welcome to program evaluation, which is the process by which we seek to measure the\ncausal effect of some intervention---anything from a new cancer drug to a job placement\nprogram for high school dropouts. Or putting more police officers on the street. The\nintervention that we care about is typically called the “treatment,” though that word is used\nmore expansively in a statistical context than in normal parlance. A treatment can be a literal\ntreatment, as in some kind of medical intervention, or it can be something like attending college\nor receiving job training upon release from prison. The point is that we are seeking to isolate\nthe effect of that single factor; ideally we would like to know how the group receiving that\ntreatment fares compared with some other group whose members are identical in all other\nrespects but for the treatment. Program evaluation offers a set of tools for isolating the treatment effect when cause and\neffect are otherwise elusive. Here is how Jonathan Klick and Alexander Tabarrok, researchers\nat the University of Pennsylvania and George Mason University, respectively, studied how\nputting more police officers on the street affects the crime rate. Their research strategy made\nuse of the terrorism alert system. Specifically, Washington, D.C., responds to “high alert” days\nfor terrorism by putting more officers in certain areas of the city, since the capital is a natural\nterrorism target. We can assume that there is no relationship between street crime and the\nterrorism threat, so this boost in the D.C. police presence is unrelated to the conventional\ncrime rate, or “exogenous.” The researchers’ most valuable insight was recognizing the natural\nexperiment here: What happens to ordinary crime on terrorism “high alert” days? The answer: The number of crimes committed when the terrorism threat was Orange (high\nalert and more police) was roughly 7 percent lower than when the terrorism threat level was\nYellow (elevated alert but no extra law enforcement precautions).", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\ntend to have the highest concentration of sick people. These doctors aren’t making people sick;\nthey are located in places where they are needed most (and at the same time sick people are\nmoving to places where they can get appropriate medical care). I suspect that there are\ndisproportionate numbers of oncologists and cardiologists in Florida; banishing them from the\nstate will not make the retiree population healthier. Welcome to program evaluation, which is the process by which we seek to measure the\ncausal effect of some intervention---anything from a new cancer drug to a job placement\nprogram for high school dropouts. Or putting more police officers on the street. The\nintervention that we care about is typically called the “treatment,” though that word is used\nmore expansively in a statistical context than in normal parlance. A treatment can be a literal\ntreatment, as in some kind of medical intervention, or it can be something like attending college\nor receiving job training upon release from prison. The point is that we are seeking to isolate\nthe effect of that single factor; ideally we would like to know how the group receiving that\ntreatment fares compared with some other group whose members are identical in all other\nrespects but for the treatment. Program evaluation offers a set of tools for isolating the treatment effect when cause and\neffect are otherwise elusive. Here is how Jonathan Klick and Alexander Tabarrok, researchers\nat the University of Pennsylvania and George Mason University, respectively, studied how\nputting more police officers on the street affects the crime rate. Their research strategy made\nuse of the terrorism alert system. Specifically, Washington, D.C., responds to “high alert” days\nfor terrorism by putting more officers in certain areas of the city, since the capital is a natural\nterrorism target. We can assume that there is no relationship between street crime and the\nterrorism threat, so this boost in the D.C. police presence is unrelated to the conventional\ncrime rate, or “exogenous.” The researchers’ most valuable insight was recognizing the natural\nexperiment here: What happens to ordinary crime on terrorism “high alert” days? The answer: The number of crimes committed when the terrorism threat was Orange (high\nalert and more police) was roughly 7 percent lower than when the terrorism threat level was\nYellow (elevated alert but no extra law enforcement precautions).", "tokens": 491, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 164, "segment_id": "00164", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000289"}
{"type": "chunk", "text": "We can assume that there is no relationship between street crime and the\nterrorism threat, so this boost in the D.C. police presence is unrelated to the conventional\ncrime rate, or “exogenous.” The researchers’ most valuable insight was recognizing the natural\nexperiment here: What happens to ordinary crime on terrorism “high alert” days? The answer: The number of crimes committed when the terrorism threat was Orange (high\nalert and more police) was roughly 7 percent lower than when the terrorism threat level was\nYellow (elevated alert but no extra law enforcement precautions). The authors also found that\nthe decrease in crime was sharpest in the police district that gets the most police attention on\nhigh-alert days (because it includes the White House, the Capitol, and the National Mall). The\nimportant takeaway is that we can answer tricky but socially meaningful questions---we just\nhave to be clever about it. Here are some of the most common approaches for isolating a\ntreatment effect. Randomized, controlled experiments. The most straightforward way to create a treatment and\ncontrol group is to---wait for it---create a treatment and control group. There are two big\nchallenges to this approach. First, there are many kinds of experiments that we cannot perform\non people. This constraint (I hope) is not going away anytime soon. As a result, we can do\ncontrolled experiments on human subjects only when there is reason to believe that the\ntreatment effect has a potentially positive outcome. This is often not the case (e.g.,\n“treatments” like experimenting with drugs or dropping out of high school), which is why we\nneed the strategies introduced in the balance of the chapter.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nWe can assume that there is no relationship between street crime and the\nterrorism threat, so this boost in the D.C. police presence is unrelated to the conventional\ncrime rate, or “exogenous.” The researchers’ most valuable insight was recognizing the natural\nexperiment here: What happens to ordinary crime on terrorism “high alert” days? The answer: The number of crimes committed when the terrorism threat was Orange (high\nalert and more police) was roughly 7 percent lower than when the terrorism threat level was\nYellow (elevated alert but no extra law enforcement precautions). The authors also found that\nthe decrease in crime was sharpest in the police district that gets the most police attention on\nhigh-alert days (because it includes the White House, the Capitol, and the National Mall). The\nimportant takeaway is that we can answer tricky but socially meaningful questions---we just\nhave to be clever about it. Here are some of the most common approaches for isolating a\ntreatment effect. Randomized, controlled experiments. The most straightforward way to create a treatment and\ncontrol group is to---wait for it---create a treatment and control group. There are two big\nchallenges to this approach. First, there are many kinds of experiments that we cannot perform\non people. This constraint (I hope) is not going away anytime soon. As a result, we can do\ncontrolled experiments on human subjects only when there is reason to believe that the\ntreatment effect has a potentially positive outcome. This is often not the case (e.g.,\n“treatments” like experimenting with drugs or dropping out of high school), which is why we\nneed the strategies introduced in the balance of the chapter.", "tokens": 349, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 164, "segment_id": "00164", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000290"}
{"type": "chunk", "text": "Second, there is a lot more variation among people than among laboratory rats. The\ntreatment effect that we are testing could easily be confounded by other variations in the\ntreatment and control groups; you are bound to have tall people, short people, sick people,\nhealthy people, males, females, criminals, alcoholics, investment bankers, and so on. How can\nwe ensure that differences across these other characteristics don’t mess up the results? I have\ngood news: This is one of those rare instances in life in which the best approach involves the\nleast work! The optimal way to create any treatment and control group is to distribute the study\nparticipants randomly across the two groups. The beauty of randomization is that it will\ngenerally distribute the non-treatment-related variables more or less evenly between the two\ngroups---both the characteristics that are obvious, such as sex, race, age, and education and the\nnonobservable characteristics that might otherwise mess up the results. Think about it: If we have 1,000 females in our prospective sample, then when we split the\nsample randomly into two groups, the most likely outcome is that 500 females will end up in\neach. Obviously we can’t expect that split exactly, but once again probability is our friend. The probability is low that one group will get a disproportionate number of women (or a\ndisproportionate number of individuals with any other characteristic). For example, if we have\na sample of 1,000 people, half of whom are women, there is less than a 1 percent chance of\ngetting fewer than 450 women in one group or the other. Obviously the bigger the samples, the\nmore effective randomization will be in creating two broadly similar groups. Medical trials typically aspire to do randomized, controlled experiments. Ideally these\nclinical trials are double-blind, meaning that neither the patient nor the physician knows who is\nreceiving the treatment and who is getting a placebo. This is obviously impossible with\ntreatments such as surgical procedures (the heart surgeon will, one hopes, know which patients\nare getting bypass surgery). Even with surgical procedures, however, it may still be possible to\nkeep patients from learning whether they are in the treatment or the control group. One of my\nfavorite studies involved an evaluation of a certain kind of knee surgery to alleviate pain. The\ntreatment group was given the surgery.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nSecond, there is a lot more variation among people than among laboratory rats. The\ntreatment effect that we are testing could easily be confounded by other variations in the\ntreatment and control groups; you are bound to have tall people, short people, sick people,\nhealthy people, males, females, criminals, alcoholics, investment bankers, and so on. How can\nwe ensure that differences across these other characteristics don’t mess up the results? I have\ngood news: This is one of those rare instances in life in which the best approach involves the\nleast work! The optimal way to create any treatment and control group is to distribute the study\nparticipants randomly across the two groups. The beauty of randomization is that it will\ngenerally distribute the non-treatment-related variables more or less evenly between the two\ngroups---both the characteristics that are obvious, such as sex, race, age, and education and the\nnonobservable characteristics that might otherwise mess up the results. Think about it: If we have 1,000 females in our prospective sample, then when we split the\nsample randomly into two groups, the most likely outcome is that 500 females will end up in\neach. Obviously we can’t expect that split exactly, but once again probability is our friend. The probability is low that one group will get a disproportionate number of women (or a\ndisproportionate number of individuals with any other characteristic). For example, if we have\na sample of 1,000 people, half of whom are women, there is less than a 1 percent chance of\ngetting fewer than 450 women in one group or the other. Obviously the bigger the samples, the\nmore effective randomization will be in creating two broadly similar groups. Medical trials typically aspire to do randomized, controlled experiments. Ideally these\nclinical trials are double-blind, meaning that neither the patient nor the physician knows who is\nreceiving the treatment and who is getting a placebo. This is obviously impossible with\ntreatments such as surgical procedures (the heart surgeon will, one hopes, know which patients\nare getting bypass surgery). Even with surgical procedures, however, it may still be possible to\nkeep patients from learning whether they are in the treatment or the control group. One of my\nfavorite studies involved an evaluation of a certain kind of knee surgery to alleviate pain. The\ntreatment group was given the surgery.", "tokens": 493, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 165, "segment_id": "00165", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000291"}
{"type": "chunk", "text": "Ideally these\nclinical trials are double-blind, meaning that neither the patient nor the physician knows who is\nreceiving the treatment and who is getting a placebo. This is obviously impossible with\ntreatments such as surgical procedures (the heart surgeon will, one hopes, know which patients\nare getting bypass surgery). Even with surgical procedures, however, it may still be possible to\nkeep patients from learning whether they are in the treatment or the control group. One of my\nfavorite studies involved an evaluation of a certain kind of knee surgery to alleviate pain. The\ntreatment group was given the surgery. The control group was given a “sham” surgery in which\nthe surgeon made three small incisions in the knee and “pretended to operate.”* It turned out\nthat the real surgery was no more effective than the sham surgery in relieving knee pain.1\n\nRandomized trials can be used to test some interesting phenomena. For example, do prayers\noffered by strangers improve postsurgical outcomes? Reasonable people have widely varying\nviews on religion, but a study published in the American Heart Journal conducted a controlled\nstudy that examined whether patients recovering from heart bypass surgery would have fewer\npostoperative complications if a large group of strangers prayed for their safe and speedy\nrecovery.2 The study involved 1,800 patients and members of three religious congregations\nfrom across the country. The patients, all of whom received coronary bypass surgery, were\ndivided into three groups: one group was not prayed for; one group was prayed for and was\ntold so; the third group was prayed for, but the participants in that group were told that they\nmight or might not receive prayers (thereby controlling for a prayer placebo effect). Meanwhile, the members of the religious congregations were told to offer prayers for specific\npatients by first name and the first initial of their last name (e.g., Charlie W.). The congregants", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nIdeally these\nclinical trials are double-blind, meaning that neither the patient nor the physician knows who is\nreceiving the treatment and who is getting a placebo. This is obviously impossible with\ntreatments such as surgical procedures (the heart surgeon will, one hopes, know which patients\nare getting bypass surgery). Even with surgical procedures, however, it may still be possible to\nkeep patients from learning whether they are in the treatment or the control group. One of my\nfavorite studies involved an evaluation of a certain kind of knee surgery to alleviate pain. The\ntreatment group was given the surgery. The control group was given a “sham” surgery in which\nthe surgeon made three small incisions in the knee and “pretended to operate.”* It turned out\nthat the real surgery was no more effective than the sham surgery in relieving knee pain.1\n\nRandomized trials can be used to test some interesting phenomena. For example, do prayers\noffered by strangers improve postsurgical outcomes? Reasonable people have widely varying\nviews on religion, but a study published in the American Heart Journal conducted a controlled\nstudy that examined whether patients recovering from heart bypass surgery would have fewer\npostoperative complications if a large group of strangers prayed for their safe and speedy\nrecovery.2 The study involved 1,800 patients and members of three religious congregations\nfrom across the country. The patients, all of whom received coronary bypass surgery, were\ndivided into three groups: one group was not prayed for; one group was prayed for and was\ntold so; the third group was prayed for, but the participants in that group were told that they\nmight or might not receive prayers (thereby controlling for a prayer placebo effect). Meanwhile, the members of the religious congregations were told to offer prayers for specific\npatients by first name and the first initial of their last name (e.g., Charlie W.). The congregants", "tokens": 396, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 165, "segment_id": "00165", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000292"}
{"type": "chunk", "text": "were given latitude in how they prayed, so long as the prayer included the phrase “for a\nsuccessful surgery with a quick, healthy recovery and no complications.”\n\nAnd? Will prayer be the cost-effective solution to America’s health care challenges? Probably not. The researchers did not find any difference in the rate of complications within\nthirty days of surgery for those who were offered prayers compared with those who were not. Critics of the study pointed out a potential omitted variable: prayers coming from other\nsources. As the New York Times summarized, “Experts said the study could not overcome\nperhaps the largest obstacle to prayer study: the unknown amount of prayer each person\nreceived from friends, families, and congregations around the world who pray daily for the sick\nand dying.”\n\nExperimenting on humans can get you arrested, or perhaps hauled off to appear before some\ninternational criminal tribunal. You should be aware of this. However, there is still room in the\nsocial sciences for randomized, controlled experiments involving “human subjects.” One\nfamous and influential experiment is the Tennessee Project STAR experiment, which tested the\neffect of smaller class sizes on student learning. The relationship between class size and\nlearning is hugely important. Nations around the world are struggling to improve educational\noutcomes. If smaller classes promote more effective learning, ceteris paribus, then society\nought to invest in hiring more teachers to bring class sizes down. At the same time, hiring\nteachers is expensive; if students in smaller classes are doing better for reasons unrelated to\nthe size of the class, then we could end up wasting an enormous amount of money. The relationship between class size and student achievement is surprisingly hard to study. Schools with small classes generally have greater resources, meaning that both the students and\nthe teachers are likely to be different from students and teachers in schools with larger classes. And within schools, smaller classes tend to be smaller for a reason. A principal may assign\ndifficult students to a small class, in which case we might find a spurious negative association\nbetween smaller classes and student achievement. Or veteran teachers may choose to teach\nsmall classes, in which case the benefit of small classes may come from the teachers who\nchoose to teach them rather than from the lower pupil-teacher ratio.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nwere given latitude in how they prayed, so long as the prayer included the phrase “for a\nsuccessful surgery with a quick, healthy recovery and no complications.”\n\nAnd? Will prayer be the cost-effective solution to America’s health care challenges? Probably not. The researchers did not find any difference in the rate of complications within\nthirty days of surgery for those who were offered prayers compared with those who were not. Critics of the study pointed out a potential omitted variable: prayers coming from other\nsources. As the New York Times summarized, “Experts said the study could not overcome\nperhaps the largest obstacle to prayer study: the unknown amount of prayer each person\nreceived from friends, families, and congregations around the world who pray daily for the sick\nand dying.”\n\nExperimenting on humans can get you arrested, or perhaps hauled off to appear before some\ninternational criminal tribunal. You should be aware of this. However, there is still room in the\nsocial sciences for randomized, controlled experiments involving “human subjects.” One\nfamous and influential experiment is the Tennessee Project STAR experiment, which tested the\neffect of smaller class sizes on student learning. The relationship between class size and\nlearning is hugely important. Nations around the world are struggling to improve educational\noutcomes. If smaller classes promote more effective learning, ceteris paribus, then society\nought to invest in hiring more teachers to bring class sizes down. At the same time, hiring\nteachers is expensive; if students in smaller classes are doing better for reasons unrelated to\nthe size of the class, then we could end up wasting an enormous amount of money. The relationship between class size and student achievement is surprisingly hard to study. Schools with small classes generally have greater resources, meaning that both the students and\nthe teachers are likely to be different from students and teachers in schools with larger classes. And within schools, smaller classes tend to be smaller for a reason. A principal may assign\ndifficult students to a small class, in which case we might find a spurious negative association\nbetween smaller classes and student achievement. Or veteran teachers may choose to teach\nsmall classes, in which case the benefit of small classes may come from the teachers who\nchoose to teach them rather than from the lower pupil-teacher ratio.", "tokens": 463, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 166, "segment_id": "00166", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000293"}
{"type": "chunk", "text": "Schools with small classes generally have greater resources, meaning that both the students and\nthe teachers are likely to be different from students and teachers in schools with larger classes. And within schools, smaller classes tend to be smaller for a reason. A principal may assign\ndifficult students to a small class, in which case we might find a spurious negative association\nbetween smaller classes and student achievement. Or veteran teachers may choose to teach\nsmall classes, in which case the benefit of small classes may come from the teachers who\nchoose to teach them rather than from the lower pupil-teacher ratio. Beginning in 1985, Tennessee’s Project STAR did a controlled experiment to test the effects\nof smaller classes.3 (Lamar Alexander was governor of Tennessee at the time; he later went on\nto become secretary of education under President George H. W. Bush.) In kindergarten,\nstudents in seventy-nine different schools were randomly assigned to either a small class (13--\n17 students), a regular class (22--25 students), or a regular class with both a regular teacher and\na teacher’s aide. Teachers were also randomly assigned to the different classrooms. Students\nstayed in the class type to which they were randomly assigned through third grade. Assorted\nlife realities chipped away at the randomization. Some students entered the system in the\nmiddle of the experiment; others left. Some students were moved from class to class for\ndisciplinary reasons; some parents lobbied successfully to have students moved to smaller\nclasses. And so on. Still, Project STAR remains the only randomized test of the effects of smaller classes. The\nresults turned out to be statistically and socially significant. Overall, students in the small", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nSchools with small classes generally have greater resources, meaning that both the students and\nthe teachers are likely to be different from students and teachers in schools with larger classes. And within schools, smaller classes tend to be smaller for a reason. A principal may assign\ndifficult students to a small class, in which case we might find a spurious negative association\nbetween smaller classes and student achievement. Or veteran teachers may choose to teach\nsmall classes, in which case the benefit of small classes may come from the teachers who\nchoose to teach them rather than from the lower pupil-teacher ratio. Beginning in 1985, Tennessee’s Project STAR did a controlled experiment to test the effects\nof smaller classes.3 (Lamar Alexander was governor of Tennessee at the time; he later went on\nto become secretary of education under President George H. W. Bush.) In kindergarten,\nstudents in seventy-nine different schools were randomly assigned to either a small class (13--\n17 students), a regular class (22--25 students), or a regular class with both a regular teacher and\na teacher’s aide. Teachers were also randomly assigned to the different classrooms. Students\nstayed in the class type to which they were randomly assigned through third grade. Assorted\nlife realities chipped away at the randomization. Some students entered the system in the\nmiddle of the experiment; others left. Some students were moved from class to class for\ndisciplinary reasons; some parents lobbied successfully to have students moved to smaller\nclasses. And so on. Still, Project STAR remains the only randomized test of the effects of smaller classes. The\nresults turned out to be statistically and socially significant. Overall, students in the small", "tokens": 346, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 166, "segment_id": "00166", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000294"}
{"type": "chunk", "text": "classes performed .15 standard deviations better on standardized tests than students in the\nregular-size classes; black students in small classes had gains that were twice as large. Now\nthe bad news. The Project STAR experiment cost roughly $12 million. The study on the effect\nof prayer on postsurgical complications cost $2.4 million. The finest studies are like the finest\nof anything else: They cost big bucks. Natural experiment. Not everybody has millions of dollars lying around to create a large,\nrandomized trial. A more economical alternative is to exploit a natural experiment, which\nhappens when random circumstances somehow create something approximating a randomized,\ncontrolled experiment. This was the case with our Washington, D.C., police example at the\nbeginning of the chapter. Life sometimes creates a treatment and control group by accident;\nwhen that occurs, researchers are eager to leap on the results. Consider the striking but\ncomplicated link between education and longevity. People who get more education tend to live\nlonger, even after controlling for things like income and access to health care. As the New York\nTimes has noted, “The one social factor that researchers agree is consistently linked to longer\nlives in every country where it has been studied is education. It is more important than race; it\nobliterates any effects of income.”4 But so far, that’s just a correlation. Does more education,\nceteris paribus, cause better health? If you think of the education itself as the “treatment,” will\ngetting more education make you live longer? This would appear to be a nearly impossible question to study, since people who choose to\nget more education are different from people who don’t. The difference between high school\ngraduates and college graduates is not just four years of schooling. There could easily be some\nunobservable characteristics shared by people who pursue education that also explain their\nlonger life expectancy. If that is the case, offering more education to those who would have\nchosen less education won’t actually improve their health. The improved health would not be a\nfunction of the incremental education; it would be a function of the kind of people who pursue\nthat incremental education. We cannot conduct a randomized experiment to solve this conundrum, because that would\ninvolve making some participants leave school earlier than they would like.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nclasses performed .15 standard deviations better on standardized tests than students in the\nregular-size classes; black students in small classes had gains that were twice as large. Now\nthe bad news. The Project STAR experiment cost roughly $12 million. The study on the effect\nof prayer on postsurgical complications cost $2.4 million. The finest studies are like the finest\nof anything else: They cost big bucks. Natural experiment. Not everybody has millions of dollars lying around to create a large,\nrandomized trial. A more economical alternative is to exploit a natural experiment, which\nhappens when random circumstances somehow create something approximating a randomized,\ncontrolled experiment. This was the case with our Washington, D.C., police example at the\nbeginning of the chapter. Life sometimes creates a treatment and control group by accident;\nwhen that occurs, researchers are eager to leap on the results. Consider the striking but\ncomplicated link between education and longevity. People who get more education tend to live\nlonger, even after controlling for things like income and access to health care. As the New York\nTimes has noted, “The one social factor that researchers agree is consistently linked to longer\nlives in every country where it has been studied is education. It is more important than race; it\nobliterates any effects of income.”4 But so far, that’s just a correlation. Does more education,\nceteris paribus, cause better health? If you think of the education itself as the “treatment,” will\ngetting more education make you live longer? This would appear to be a nearly impossible question to study, since people who choose to\nget more education are different from people who don’t. The difference between high school\ngraduates and college graduates is not just four years of schooling. There could easily be some\nunobservable characteristics shared by people who pursue education that also explain their\nlonger life expectancy. If that is the case, offering more education to those who would have\nchosen less education won’t actually improve their health. The improved health would not be a\nfunction of the incremental education; it would be a function of the kind of people who pursue\nthat incremental education. We cannot conduct a randomized experiment to solve this conundrum, because that would\ninvolve making some participants leave school earlier than they would like.", "tokens": 478, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 167, "segment_id": "00167", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000295"}
{"type": "chunk", "text": "There could easily be some\nunobservable characteristics shared by people who pursue education that also explain their\nlonger life expectancy. If that is the case, offering more education to those who would have\nchosen less education won’t actually improve their health. The improved health would not be a\nfunction of the incremental education; it would be a function of the kind of people who pursue\nthat incremental education. We cannot conduct a randomized experiment to solve this conundrum, because that would\ninvolve making some participants leave school earlier than they would like. (You try\nexplaining to someone that he can’t go to college---ever---because he is in the control group.)\nThe only possible test of the causal effect of education on longevity would be some kind of\nexperiment that forced a large segment of the population to stay in school longer than its\nmembers might otherwise choose. That’s at least morally acceptable since we expect a\npositive treatment effect. Still, we can’t force kids to stay in school; that’s not the American\nway. Oh, but it is. Every state has some kind of minimum schooling law, and at different points in\nhistory those laws have changed. That kind of exogenous change in schooling attainment---\nmeaning that it’s not caused by the individuals being studied---is exactly the kind of thing that\nmakes researchers swoon with excitement. Adriana Lleras-Muney, a graduate student at\nColumbia, saw the research potential in the fact that different states have changed their\nminimum schooling laws at different points in time. She went back in history and studied the\nrelationship between when states changed their minimum schooling laws and later changes in", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThere could easily be some\nunobservable characteristics shared by people who pursue education that also explain their\nlonger life expectancy. If that is the case, offering more education to those who would have\nchosen less education won’t actually improve their health. The improved health would not be a\nfunction of the incremental education; it would be a function of the kind of people who pursue\nthat incremental education. We cannot conduct a randomized experiment to solve this conundrum, because that would\ninvolve making some participants leave school earlier than they would like. (You try\nexplaining to someone that he can’t go to college---ever---because he is in the control group.)\nThe only possible test of the causal effect of education on longevity would be some kind of\nexperiment that forced a large segment of the population to stay in school longer than its\nmembers might otherwise choose. That’s at least morally acceptable since we expect a\npositive treatment effect. Still, we can’t force kids to stay in school; that’s not the American\nway. Oh, but it is. Every state has some kind of minimum schooling law, and at different points in\nhistory those laws have changed. That kind of exogenous change in schooling attainment---\nmeaning that it’s not caused by the individuals being studied---is exactly the kind of thing that\nmakes researchers swoon with excitement. Adriana Lleras-Muney, a graduate student at\nColumbia, saw the research potential in the fact that different states have changed their\nminimum schooling laws at different points in time. She went back in history and studied the\nrelationship between when states changed their minimum schooling laws and later changes in", "tokens": 339, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 167, "segment_id": "00167", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000296"}
{"type": "chunk", "text": "life expectancy in those states (by trolling through lots and lots of census data). She still had a\nmethodological challenge; if the residents of a state live longer after the state raises its\nminimum schooling law, we cannot attribute the longevity to the extra schooling. Life\nexpectancy is generally going up over time. People lived longer in 1900 than in 1850, no matter\nwhat the states did. However, Lleras-Muney had a natural control: states that did not change their minimum\nschooling laws. Her work approximates a giant laboratory experiment in which the residents of\nIllinois are forced to stay in school for seven years while their neighbors in Indiana can leave\nschool after six years. The difference is that this controlled experiment was made possible by a\nhistorical accident---hence the term “natural experiment.”\n\nWhat happened? Life expectancy of those adults who reached age thirty-five was extended\nby an extra one and a half years just by their attending one additional year of school.5 LlerasMuney’s results have been replicated in other countries where variations in mandatory\nschooling laws have created similar natural experiments. Some skepticism is in order. We still\ndo not understand the mechanism by which additional schooling leads to longer lives. Nonequivalent control. Sometimes the best available option for studying a treatment effect is\nto create nonrandomized treatment and control groups. Our hope/expectation is that the two\ngroups are broadly similar even though circumstances have not allowed us the statistical\nluxury of randomizing. The good news is that we have a treatment and a control group. The\nbad news is that any nonrandom assignment creates at least the potential for bias. There may\nbe unobserved differences between the treatment and control groups related to how participants\nare assigned to one group or the other. Hence the name “nonequivalent control.”\n\nA nonequivalent control group can still be a very helpful tool. Let’s ponder the question\nposed in the title of this chapter: Is there a significant life advantage to attending a highly\nselective college or university? Obviously the Harvard, Princeton, and Dartmouth graduates of\nthe world do very well. On average, they earn more money and have more expansive life\nopportunities than students who attend less selective institutions.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nlife expectancy in those states (by trolling through lots and lots of census data). She still had a\nmethodological challenge; if the residents of a state live longer after the state raises its\nminimum schooling law, we cannot attribute the longevity to the extra schooling. Life\nexpectancy is generally going up over time. People lived longer in 1900 than in 1850, no matter\nwhat the states did. However, Lleras-Muney had a natural control: states that did not change their minimum\nschooling laws. Her work approximates a giant laboratory experiment in which the residents of\nIllinois are forced to stay in school for seven years while their neighbors in Indiana can leave\nschool after six years. The difference is that this controlled experiment was made possible by a\nhistorical accident---hence the term “natural experiment.”\n\nWhat happened? Life expectancy of those adults who reached age thirty-five was extended\nby an extra one and a half years just by their attending one additional year of school.5 LlerasMuney’s results have been replicated in other countries where variations in mandatory\nschooling laws have created similar natural experiments. Some skepticism is in order. We still\ndo not understand the mechanism by which additional schooling leads to longer lives. Nonequivalent control. Sometimes the best available option for studying a treatment effect is\nto create nonrandomized treatment and control groups. Our hope/expectation is that the two\ngroups are broadly similar even though circumstances have not allowed us the statistical\nluxury of randomizing. The good news is that we have a treatment and a control group. The\nbad news is that any nonrandom assignment creates at least the potential for bias. There may\nbe unobserved differences between the treatment and control groups related to how participants\nare assigned to one group or the other. Hence the name “nonequivalent control.”\n\nA nonequivalent control group can still be a very helpful tool. Let’s ponder the question\nposed in the title of this chapter: Is there a significant life advantage to attending a highly\nselective college or university? Obviously the Harvard, Princeton, and Dartmouth graduates of\nthe world do very well. On average, they earn more money and have more expansive life\nopportunities than students who attend less selective institutions.", "tokens": 472, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 168, "segment_id": "00168", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000297"}
{"type": "chunk", "text": "There may\nbe unobserved differences between the treatment and control groups related to how participants\nare assigned to one group or the other. Hence the name “nonequivalent control.”\n\nA nonequivalent control group can still be a very helpful tool. Let’s ponder the question\nposed in the title of this chapter: Is there a significant life advantage to attending a highly\nselective college or university? Obviously the Harvard, Princeton, and Dartmouth graduates of\nthe world do very well. On average, they earn more money and have more expansive life\nopportunities than students who attend less selective institutions. (A 2008 study by\nPayScale.com found that the median pay for Dartmouth graduates with ten to twenty years of\nwork experience was $134,000, the highest of any undergraduate institution; Princeton was\nsecond with a median of $131,000.)6 As I hope you realize by this point, these impressive\nnumbers tell us absolutely nothing about the value of a Dartmouth or Princeton education. Students who attend Dartmouth and Princeton are talented when they apply; that’s why they get\naccepted. They would probably do well in life no matter where they went to college. What we don’t know is the treatment effect of attending a place like Harvard or Yale. Do the\ngraduates of these elite institutions do well in life because they were hyper-talented when they\nwalked onto the campus? Or do these colleges and universities add value by taking talented\nindividuals and making them even more productive? Or both? We cannot conduct a randomized experiment to answer this question. Few high school\nstudents would agree to be randomly assigned to a college; nor would Harvard and Dartmouth\nbe particularly keen about taking the students randomly assigned to them. We appear to be left", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThere may\nbe unobserved differences between the treatment and control groups related to how participants\nare assigned to one group or the other. Hence the name “nonequivalent control.”\n\nA nonequivalent control group can still be a very helpful tool. Let’s ponder the question\nposed in the title of this chapter: Is there a significant life advantage to attending a highly\nselective college or university? Obviously the Harvard, Princeton, and Dartmouth graduates of\nthe world do very well. On average, they earn more money and have more expansive life\nopportunities than students who attend less selective institutions. (A 2008 study by\nPayScale.com found that the median pay for Dartmouth graduates with ten to twenty years of\nwork experience was $134,000, the highest of any undergraduate institution; Princeton was\nsecond with a median of $131,000.)6 As I hope you realize by this point, these impressive\nnumbers tell us absolutely nothing about the value of a Dartmouth or Princeton education. Students who attend Dartmouth and Princeton are talented when they apply; that’s why they get\naccepted. They would probably do well in life no matter where they went to college. What we don’t know is the treatment effect of attending a place like Harvard or Yale. Do the\ngraduates of these elite institutions do well in life because they were hyper-talented when they\nwalked onto the campus? Or do these colleges and universities add value by taking talented\nindividuals and making them even more productive? Or both? We cannot conduct a randomized experiment to answer this question. Few high school\nstudents would agree to be randomly assigned to a college; nor would Harvard and Dartmouth\nbe particularly keen about taking the students randomly assigned to them. We appear to be left", "tokens": 365, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 168, "segment_id": "00168", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000298"}
{"type": "chunk", "text": "without any mechanism for testing the value of the treatment effect. Cleverness to the rescue! Economists Stacy Dale and Alan Krueger found a way to answer this question by exploiting*\nthe fact that many students apply to multiple colleges.7 Some of those students are accepted at\na highly selective school and choose to attend that school; others are accepted at a highly\nselective school but choose to attend a less selective college or university instead. Bingo! Now\nwe have a treatment group (those students who attended highly selective colleges and\nuniversities) and a nonequivalent control group (those students who were talented enough to\nbe accepted by such a school but opted to attend a less selective institution instead).†\n\nDale and Krueger studied longitudinal data on the earnings of both groups. This is not a\nperfect apples-and-apples comparison, and earnings are clearly not the only life outcome that\nmatters, but their findings should assuage the anxieties of overwrought high school students and\ntheir parents. Students who attended more selective colleges earned roughly the same as\nstudents of seemingly similar ability who attended less selective schools. The one exception\nwas students from low-income families, who earned more if they attended a selective college\nor university. The Dale and Krueger approach is an elegant way to sort out the treatment effect\n(spending four years at an elite institution) from the selection effect (the most talented students\nare admitted to those institutions). In a summary of the research for the New York Times , Alan\nKrueger indirectly answered the question posed in the title of this chapter, “Recognize that your\nown motivation, ambition, and talents will determine your success more than the college name\non your diploma.”8\n\nDifference in differences. One of the best ways to observe cause and effect is to do something\nand then see what happens. This is, after all, how infants and toddlers (and sometimes adults)\nlearn about the world. My children were very quick to learn that if they hurled pieces of food\nacross the kitchen (cause), the dog would race eagerly after them (effect). Presumably the\nsame power of observation can help inform the rest of life. If we cut taxes and the economy\nimproves, then the tax cuts must have been responsible. Maybe. The enormous potential pitfall with this approach is that life tends to be more\ncomplex than throwing chicken nuggets across the kitchen.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nwithout any mechanism for testing the value of the treatment effect. Cleverness to the rescue! Economists Stacy Dale and Alan Krueger found a way to answer this question by exploiting*\nthe fact that many students apply to multiple colleges.7 Some of those students are accepted at\na highly selective school and choose to attend that school; others are accepted at a highly\nselective school but choose to attend a less selective college or university instead. Bingo! Now\nwe have a treatment group (those students who attended highly selective colleges and\nuniversities) and a nonequivalent control group (those students who were talented enough to\nbe accepted by such a school but opted to attend a less selective institution instead).†\n\nDale and Krueger studied longitudinal data on the earnings of both groups. This is not a\nperfect apples-and-apples comparison, and earnings are clearly not the only life outcome that\nmatters, but their findings should assuage the anxieties of overwrought high school students and\ntheir parents. Students who attended more selective colleges earned roughly the same as\nstudents of seemingly similar ability who attended less selective schools. The one exception\nwas students from low-income families, who earned more if they attended a selective college\nor university. The Dale and Krueger approach is an elegant way to sort out the treatment effect\n(spending four years at an elite institution) from the selection effect (the most talented students\nare admitted to those institutions). In a summary of the research for the New York Times , Alan\nKrueger indirectly answered the question posed in the title of this chapter, “Recognize that your\nown motivation, ambition, and talents will determine your success more than the college name\non your diploma.”8\n\nDifference in differences. One of the best ways to observe cause and effect is to do something\nand then see what happens. This is, after all, how infants and toddlers (and sometimes adults)\nlearn about the world. My children were very quick to learn that if they hurled pieces of food\nacross the kitchen (cause), the dog would race eagerly after them (effect). Presumably the\nsame power of observation can help inform the rest of life. If we cut taxes and the economy\nimproves, then the tax cuts must have been responsible. Maybe. The enormous potential pitfall with this approach is that life tends to be more\ncomplex than throwing chicken nuggets across the kitchen.", "tokens": 502, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 169, "segment_id": "00169", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000299"}
{"type": "chunk", "text": "This is, after all, how infants and toddlers (and sometimes adults)\nlearn about the world. My children were very quick to learn that if they hurled pieces of food\nacross the kitchen (cause), the dog would race eagerly after them (effect). Presumably the\nsame power of observation can help inform the rest of life. If we cut taxes and the economy\nimproves, then the tax cuts must have been responsible. Maybe. The enormous potential pitfall with this approach is that life tends to be more\ncomplex than throwing chicken nuggets across the kitchen. Yes, we may have cut taxes at a\nspecific point in time, but there were other “interventions” unfolding during roughly the same\nstretch: More women were going to college, the Internet and other technological innovations\nwere raising the productivity of American workers, the Chinese currency was undervalued, the\nChicago Cubs fired their general manager, and so on. Whatever happened after the tax cut\ncannot be attributed solely to the tax cut. The challenge with any “before and after” kind of\nanalysis is that just because one thing follows another does not mean that there is a causal\nrelationship between the two. A “difference in differences” approach can help us identify the effects of some intervention\nby doing two things. First, we examine the “before” and “after” data for whatever group or\njurisdiction has received the treatment, such as the unemployment figures for a county that has\nimplemented a job training program. Second, we compare those data with the unemployment\nfigures over the same time period for a similar county that did not implement any such", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThis is, after all, how infants and toddlers (and sometimes adults)\nlearn about the world. My children were very quick to learn that if they hurled pieces of food\nacross the kitchen (cause), the dog would race eagerly after them (effect). Presumably the\nsame power of observation can help inform the rest of life. If we cut taxes and the economy\nimproves, then the tax cuts must have been responsible. Maybe. The enormous potential pitfall with this approach is that life tends to be more\ncomplex than throwing chicken nuggets across the kitchen. Yes, we may have cut taxes at a\nspecific point in time, but there were other “interventions” unfolding during roughly the same\nstretch: More women were going to college, the Internet and other technological innovations\nwere raising the productivity of American workers, the Chinese currency was undervalued, the\nChicago Cubs fired their general manager, and so on. Whatever happened after the tax cut\ncannot be attributed solely to the tax cut. The challenge with any “before and after” kind of\nanalysis is that just because one thing follows another does not mean that there is a causal\nrelationship between the two. A “difference in differences” approach can help us identify the effects of some intervention\nby doing two things. First, we examine the “before” and “after” data for whatever group or\njurisdiction has received the treatment, such as the unemployment figures for a county that has\nimplemented a job training program. Second, we compare those data with the unemployment\nfigures over the same time period for a similar county that did not implement any such", "tokens": 334, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 169, "segment_id": "00169", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000300"}
{"type": "chunk", "text": "program. The important assumption is that the two groups used for the analysis are largely\ncomparable except for the treatment; as a result, any significant difference in outcomes\nbetween the two groups can be attributed to the program or policy being evaluated. For\nexample, suppose that one county in Illinois implements a job training program to combat high\nunemployment. Over the ensuing two years, the unemployment rate continues to rise. Does that\nmake the program a failure? Who knows? Effect of Job Training on Unemployment in County A\n\nOther broad economic forces may be at work, including the possibility of a prolonged\neconomic slump. A difference-in-differences approach would compare the change in the\nunemployment rate over time in the county we are evaluating with the unemployment rate for a\nneighboring county with no job training program; the two counties must be similar in all other\nimportant ways: industry mix, demographics, and so on. How does the unemployment rate in\nthe county with the new job training program change over time relative to the county that did\nnot implement such a program? We can reasonably infer the treatment effect of the program\nby comparing the changes in the two counties over the period of study---the “difference in\ndifferences.” The other county in this study is effectively acting as a control group, which\nallows us to take advantage of the data collected before and after the intervention. If the\ncontrol group is good, it will be exposed to the same broader forces as our treatment group. The difference-in-differences approach can be particularly enlightening when the treatment\ninitially appears ineffective (unemployment is higher after the program is implemented than\nbefore), yet the control group shows us that the trend would have been even worse in the\nabsence of the intervention. Effect of Job Training on Unemployment in County A, with County B as a\nComparison", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nprogram. The important assumption is that the two groups used for the analysis are largely\ncomparable except for the treatment; as a result, any significant difference in outcomes\nbetween the two groups can be attributed to the program or policy being evaluated. For\nexample, suppose that one county in Illinois implements a job training program to combat high\nunemployment. Over the ensuing two years, the unemployment rate continues to rise. Does that\nmake the program a failure? Who knows? Effect of Job Training on Unemployment in County A\n\nOther broad economic forces may be at work, including the possibility of a prolonged\neconomic slump. A difference-in-differences approach would compare the change in the\nunemployment rate over time in the county we are evaluating with the unemployment rate for a\nneighboring county with no job training program; the two counties must be similar in all other\nimportant ways: industry mix, demographics, and so on. How does the unemployment rate in\nthe county with the new job training program change over time relative to the county that did\nnot implement such a program? We can reasonably infer the treatment effect of the program\nby comparing the changes in the two counties over the period of study---the “difference in\ndifferences.” The other county in this study is effectively acting as a control group, which\nallows us to take advantage of the data collected before and after the intervention. If the\ncontrol group is good, it will be exposed to the same broader forces as our treatment group. The difference-in-differences approach can be particularly enlightening when the treatment\ninitially appears ineffective (unemployment is higher after the program is implemented than\nbefore), yet the control group shows us that the trend would have been even worse in the\nabsence of the intervention. Effect of Job Training on Unemployment in County A, with County B as a\nComparison", "tokens": 380, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 170, "segment_id": "00170", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000301"}
{"type": "chunk", "text": "Discontinuity analysis. One way to create a treatment and control group is to compare the\noutcomes for some group that barely qualified for an intervention or treatment with the\noutcomes for a group that just missed the cutoff for eligibility and did not receive the\ntreatment. Those individuals who fall just above and just below some arbitrary cutoff, such as\nan exam score or a minimum household income, will be nearly identical in many important\nrespects; the fact that one group received the treatment and the other didn’t is essentially\narbitrary. As a result, we can compare their outcomes in ways that provide meaningful results\nabout the effectiveness of the relevant intervention. Suppose a school district requires summer school for struggling students. The district would\nlike to know whether the summer program has any long-term academic value. As usual, a\nsimple comparison between the students who attend summer school and those who do not\nwould be worse than useless. The students who attend summer school are there because they\nare struggling. Even if the summer school program is highly effective, the participating\nstudents will probably still do worse in the long run than the students who were not required to\ntake summer school. What we want to know is how the struggling students perform after taking\nsummer school compared with how they would have done if they had not taken summer\nschool. Yes, we could do some kind of controlled experiment in which struggling students are\nrandomly selected to attend summer school or not, but that would involve denying the control\ngroup access to a program that we think would be helpful. Instead, the treatment and control groups are created by comparing those students who just\nbarely fell below the threshold for summer school with those who just barely escaped it. Think\nabout it: the students who fail a midterm are appreciably different from students who do not\nfail the midterm. But students who get a 59 percent (a failing grade) are not appreciably\ndifferent from those students who get a 60 percent (a passing grade). If those who fail the\nmidterm are enrolled in some treatment, such as mandatory tutoring for the final exam, then we\nwould have a reasonable treatment and control group if we compared the final exam scores of\nthose who barely failed the midterm (and received tutoring) with the scores of those who barely\npassed the midterm (and did not get tutoring). This approach was used to determine the effectiveness of incarceration for juvenile\noffenders as a deterrent to future crime.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nDiscontinuity analysis. One way to create a treatment and control group is to compare the\noutcomes for some group that barely qualified for an intervention or treatment with the\noutcomes for a group that just missed the cutoff for eligibility and did not receive the\ntreatment. Those individuals who fall just above and just below some arbitrary cutoff, such as\nan exam score or a minimum household income, will be nearly identical in many important\nrespects; the fact that one group received the treatment and the other didn’t is essentially\narbitrary. As a result, we can compare their outcomes in ways that provide meaningful results\nabout the effectiveness of the relevant intervention. Suppose a school district requires summer school for struggling students. The district would\nlike to know whether the summer program has any long-term academic value. As usual, a\nsimple comparison between the students who attend summer school and those who do not\nwould be worse than useless. The students who attend summer school are there because they\nare struggling. Even if the summer school program is highly effective, the participating\nstudents will probably still do worse in the long run than the students who were not required to\ntake summer school. What we want to know is how the struggling students perform after taking\nsummer school compared with how they would have done if they had not taken summer\nschool. Yes, we could do some kind of controlled experiment in which struggling students are\nrandomly selected to attend summer school or not, but that would involve denying the control\ngroup access to a program that we think would be helpful. Instead, the treatment and control groups are created by comparing those students who just\nbarely fell below the threshold for summer school with those who just barely escaped it. Think\nabout it: the students who fail a midterm are appreciably different from students who do not\nfail the midterm. But students who get a 59 percent (a failing grade) are not appreciably\ndifferent from those students who get a 60 percent (a passing grade). If those who fail the\nmidterm are enrolled in some treatment, such as mandatory tutoring for the final exam, then we\nwould have a reasonable treatment and control group if we compared the final exam scores of\nthose who barely failed the midterm (and received tutoring) with the scores of those who barely\npassed the midterm (and did not get tutoring). This approach was used to determine the effectiveness of incarceration for juvenile\noffenders as a deterrent to future crime.", "tokens": 505, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 171, "segment_id": "00171", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000302"}
{"type": "chunk", "text": "But students who get a 59 percent (a failing grade) are not appreciably\ndifferent from those students who get a 60 percent (a passing grade). If those who fail the\nmidterm are enrolled in some treatment, such as mandatory tutoring for the final exam, then we\nwould have a reasonable treatment and control group if we compared the final exam scores of\nthose who barely failed the midterm (and received tutoring) with the scores of those who barely\npassed the midterm (and did not get tutoring). This approach was used to determine the effectiveness of incarceration for juvenile\noffenders as a deterrent to future crime. Obviously this kind of analysis cannot simply", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nBut students who get a 59 percent (a failing grade) are not appreciably\ndifferent from those students who get a 60 percent (a passing grade). If those who fail the\nmidterm are enrolled in some treatment, such as mandatory tutoring for the final exam, then we\nwould have a reasonable treatment and control group if we compared the final exam scores of\nthose who barely failed the midterm (and received tutoring) with the scores of those who barely\npassed the midterm (and did not get tutoring). This approach was used to determine the effectiveness of incarceration for juvenile\noffenders as a deterrent to future crime. Obviously this kind of analysis cannot simply", "tokens": 135, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 171, "segment_id": "00171", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000303"}
{"type": "chunk", "text": "compare the recidivism rates of juvenile offenders who are imprisoned with the recidivism\nrates for juvenile offenders who received lighter sentences. The juvenile offenders who are\nsent to prison typically commit more serious crimes than the juvenile offenders who receive\nlighter sentences; that’s why they go to prison. Nor can we create a treatment and control\ngroup by distributing prison sentences randomly (unless you want to risk twenty-five years in\nthe big house the next time you make an illegal right turn on red). Randi Hjalmarsson, a\nresearcher now at the University of London, exploited rigid sentencing guidelines for juvenile\noffenders in the state of Washington to gain insight into the causal effect of a prison sentence\non future criminal behavior. Specifically, she compared the recidivism rate for those juvenile\noffenders who were “just barely” sentenced to prison with the recidivism rate for those\njuveniles who “just barely” got a pass (which usually involved a fine or probation).9\n\nThe Washington criminal justice system creates a grid for each convicted offender that is\nused to administer a sentence. The x-axis measures the offender’s prior adjudicated offenses. For example, each prior felony counts as one point; each prior misdemeanor counts as onequarter point. The point total is rounded down to a whole number (which will matter in a\nmoment). Meanwhile, the y-axis measures the severity of the current offense on a scale from E\n(least serious) to A+ (most serious). A convicted juvenile’s sentence is literally calculated by\nfinding the appropriate box on the grid: An offender with two points’ worth of prior offenses\nwho commits a Class B felony will receive fifteen to thirty-six months in a juvenile jail. A\nconvicted offender with only one point worth of prior offenses who commits the same crime\nwill not be sent to jail. That discontinuity is what motivated the research strategy. Hjalmarsson\ncompared the outcomes for convicted offenders who fell just above and below the threshold\nfor a jail sentence. As she explains in the paper, “If there are two individuals with a current\noffense class of C+ and [prior] adjudication scores of 23⁄4 and 3, then only the latter individual\nwill be sentenced to state incarceration.”\n\nFor research purposes, those two individuals are essentially the same---until one of them\ngoes to jail. And at that point, their behavior does appear to diverge sharply.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\ncompare the recidivism rates of juvenile offenders who are imprisoned with the recidivism\nrates for juvenile offenders who received lighter sentences. The juvenile offenders who are\nsent to prison typically commit more serious crimes than the juvenile offenders who receive\nlighter sentences; that’s why they go to prison. Nor can we create a treatment and control\ngroup by distributing prison sentences randomly (unless you want to risk twenty-five years in\nthe big house the next time you make an illegal right turn on red). Randi Hjalmarsson, a\nresearcher now at the University of London, exploited rigid sentencing guidelines for juvenile\noffenders in the state of Washington to gain insight into the causal effect of a prison sentence\non future criminal behavior. Specifically, she compared the recidivism rate for those juvenile\noffenders who were “just barely” sentenced to prison with the recidivism rate for those\njuveniles who “just barely” got a pass (which usually involved a fine or probation).9\n\nThe Washington criminal justice system creates a grid for each convicted offender that is\nused to administer a sentence. The x-axis measures the offender’s prior adjudicated offenses. For example, each prior felony counts as one point; each prior misdemeanor counts as onequarter point. The point total is rounded down to a whole number (which will matter in a\nmoment). Meanwhile, the y-axis measures the severity of the current offense on a scale from E\n(least serious) to A+ (most serious). A convicted juvenile’s sentence is literally calculated by\nfinding the appropriate box on the grid: An offender with two points’ worth of prior offenses\nwho commits a Class B felony will receive fifteen to thirty-six months in a juvenile jail. A\nconvicted offender with only one point worth of prior offenses who commits the same crime\nwill not be sent to jail. That discontinuity is what motivated the research strategy. Hjalmarsson\ncompared the outcomes for convicted offenders who fell just above and below the threshold\nfor a jail sentence. As she explains in the paper, “If there are two individuals with a current\noffense class of C+ and [prior] adjudication scores of 23⁄4 and 3, then only the latter individual\nwill be sentenced to state incarceration.”\n\nFor research purposes, those two individuals are essentially the same---until one of them\ngoes to jail. And at that point, their behavior does appear to diverge sharply.", "tokens": 503, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 172, "segment_id": "00172", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000304"}
{"type": "chunk", "text": "That discontinuity is what motivated the research strategy. Hjalmarsson\ncompared the outcomes for convicted offenders who fell just above and below the threshold\nfor a jail sentence. As she explains in the paper, “If there are two individuals with a current\noffense class of C+ and [prior] adjudication scores of 23⁄4 and 3, then only the latter individual\nwill be sentenced to state incarceration.”\n\nFor research purposes, those two individuals are essentially the same---until one of them\ngoes to jail. And at that point, their behavior does appear to diverge sharply. The juvenile\noffenders who go to jail are significantly less likely to be convicted of another crime (after they\nare released from jail). We care about what works. This is true in medicine, in economics, in business, in criminal\njustice---in everything. Yet causality is a tough nut to crack, even in cases where cause and\neffect seems stunningly obvious. To understand the true impact of a treatment, we need to\nknow the “counterfactual,” which is what would have happened in the absence of that treatment\nor intervention. Often the counterfactual is difficult or impossible to observe. Consider a\nnonstatistics example: Did the U.S. invasion of Iraq make America safer? There is only one intellectually honest answer: We will never know. The reason we will\nnever know is that we do not know---and cannot know---what would have happened if the\nUnited States had not invaded Iraq. True, the United States did not find weapons of mass\ndestruction. But it is possible that on the day after the United States did not invade Iraq\nSaddam Hussein could have climbed into the shower and said to himself, “I could really use a\nhydrogen bomb. I wonder if the North Koreans will sell me one?” After that, who knows?", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThat discontinuity is what motivated the research strategy. Hjalmarsson\ncompared the outcomes for convicted offenders who fell just above and below the threshold\nfor a jail sentence. As she explains in the paper, “If there are two individuals with a current\noffense class of C+ and [prior] adjudication scores of 23⁄4 and 3, then only the latter individual\nwill be sentenced to state incarceration.”\n\nFor research purposes, those two individuals are essentially the same---until one of them\ngoes to jail. And at that point, their behavior does appear to diverge sharply. The juvenile\noffenders who go to jail are significantly less likely to be convicted of another crime (after they\nare released from jail). We care about what works. This is true in medicine, in economics, in business, in criminal\njustice---in everything. Yet causality is a tough nut to crack, even in cases where cause and\neffect seems stunningly obvious. To understand the true impact of a treatment, we need to\nknow the “counterfactual,” which is what would have happened in the absence of that treatment\nor intervention. Often the counterfactual is difficult or impossible to observe. Consider a\nnonstatistics example: Did the U.S. invasion of Iraq make America safer? There is only one intellectually honest answer: We will never know. The reason we will\nnever know is that we do not know---and cannot know---what would have happened if the\nUnited States had not invaded Iraq. True, the United States did not find weapons of mass\ndestruction. But it is possible that on the day after the United States did not invade Iraq\nSaddam Hussein could have climbed into the shower and said to himself, “I could really use a\nhydrogen bomb. I wonder if the North Koreans will sell me one?” After that, who knows?", "tokens": 389, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 172, "segment_id": "00172", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000305"}
{"type": "chunk", "text": "Of course, it’s also possible that Saddam Hussein could have climbed into that same shower\non the day after the United States did not invade Iraq and said to himself, “I could really use\n---” at which point he slipped on a bar of soap, hit his head on an ornate marble fixture, and\ndied. In that case, the world would have been rid of Saddam Hussein without the enormous\ncosts associated with the U.S. invasion. Who knows what would have happened? The purpose of any program evaluation is to provide some kind of counterfactual against\nwhich a treatment or intervention can be measured. In the case of a randomized, controlled\nexperiment, the control group is the counterfactual. In cases where a controlled experiment is\nimpractical or immoral, we need to find some other way of approximating the counterfactual. Our understanding of the world depends on finding clever ways to do that. * The participants did know that they were participating in a clinical trial and might receive the sham surgery. * Researchers love to use the word “exploit.” It has a specific meaning in terms of taking advantage of some data-related\nopportunity. For example, when researchers find some natural experiment that creates a treatment and control group, they\nwill describe how they plan to “exploit the variation in the data.”\n† There is potential for bias here. Both groups of students are talented enough to get into a highly selective school. However, one group of students chose to go to such a school, and the other group did not. The group of students who chose\nto attend a less selective school may be less motivated, less hardworking, or different in some other ways that we cannot\nobserve. If Dale and Krueger had found that students who attend a highly selective school had higher lifetime earnings than\nstudents who were accepted at such a school but went to a less selective college instead, we still could not be certain\nwhether the difference was due to the selective school or to the kind of student who opted to attend such a school when\ngiven a choice. This potential bias turns out to be unimportant in the Dale and Krueger study, however, because of its\ndirection.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nOf course, it’s also possible that Saddam Hussein could have climbed into that same shower\non the day after the United States did not invade Iraq and said to himself, “I could really use\n---” at which point he slipped on a bar of soap, hit his head on an ornate marble fixture, and\ndied. In that case, the world would have been rid of Saddam Hussein without the enormous\ncosts associated with the U.S. invasion. Who knows what would have happened? The purpose of any program evaluation is to provide some kind of counterfactual against\nwhich a treatment or intervention can be measured. In the case of a randomized, controlled\nexperiment, the control group is the counterfactual. In cases where a controlled experiment is\nimpractical or immoral, we need to find some other way of approximating the counterfactual. Our understanding of the world depends on finding clever ways to do that. * The participants did know that they were participating in a clinical trial and might receive the sham surgery. * Researchers love to use the word “exploit.” It has a specific meaning in terms of taking advantage of some data-related\nopportunity. For example, when researchers find some natural experiment that creates a treatment and control group, they\nwill describe how they plan to “exploit the variation in the data.”\n† There is potential for bias here. Both groups of students are talented enough to get into a highly selective school. However, one group of students chose to go to such a school, and the other group did not. The group of students who chose\nto attend a less selective school may be less motivated, less hardworking, or different in some other ways that we cannot\nobserve. If Dale and Krueger had found that students who attend a highly selective school had higher lifetime earnings than\nstudents who were accepted at such a school but went to a less selective college instead, we still could not be certain\nwhether the difference was due to the selective school or to the kind of student who opted to attend such a school when\ngiven a choice. This potential bias turns out to be unimportant in the Dale and Krueger study, however, because of its\ndirection.", "tokens": 452, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 173, "segment_id": "00173", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000306"}
{"type": "chunk", "text": "If Dale and Krueger had found that students who attend a highly selective school had higher lifetime earnings than\nstudents who were accepted at such a school but went to a less selective college instead, we still could not be certain\nwhether the difference was due to the selective school or to the kind of student who opted to attend such a school when\ngiven a choice. This potential bias turns out to be unimportant in the Dale and Krueger study, however, because of its\ndirection. Dale and Krueger find that the students who attended highly selective schools did not earn significantly more in\nlife than students who were accepted but went elsewhere despite the fact that the students who declined to attend a\nhighly selective school may have had attributes that caused them to earn less in life apart from their education. If\nanything, the bias here causes the findings to overstate the pecuniary benefits of attending a highly selective college---\nwhich turn out to be insubstantial anyway.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nIf Dale and Krueger had found that students who attend a highly selective school had higher lifetime earnings than\nstudents who were accepted at such a school but went to a less selective college instead, we still could not be certain\nwhether the difference was due to the selective school or to the kind of student who opted to attend such a school when\ngiven a choice. This potential bias turns out to be unimportant in the Dale and Krueger study, however, because of its\ndirection. Dale and Krueger find that the students who attended highly selective schools did not earn significantly more in\nlife than students who were accepted but went elsewhere despite the fact that the students who declined to attend a\nhighly selective school may have had attributes that caused them to earn less in life apart from their education. If\nanything, the bias here causes the findings to overstate the pecuniary benefits of attending a highly selective college---\nwhich turn out to be insubstantial anyway.", "tokens": 198, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 173, "segment_id": "00173", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000307"}
{"type": "chunk", "text": "Conclusion\nFive questions that statistics\ncan help answer\n\nNot that long ago, information was much harder to gather and far more expensive to analyze. Imagine studying the information from one million credit card transactions in the era---only a\nfew decades back---when there were merely paper receipts and no personal computers for\nanalyzing the accumulated data. During the Great Depression, there were no official statistics\nwith which to gauge the depth of the economic problems. Government did not collect official\ninformation on either gross domestic product (GDP) or unemployment, meaning that politicians\nwere attempting to do the economic equivalent of navigating through a forest without a\ncompass. Herbert Hoover declared that the Great Depression was over in 1930, on the basis of\nthe inaccurate and outdated data that were available. He told the country in his State of Union\naddress that two and a half million Americans were out of work. In fact, five million\nAmericans were jobless, and unemployment was climbing by one hundred thousand every\nweek. As James Surowiecki recently observed in The New Yorker , “Washington was making\npolicy in the dark.”1\n\nWe are now awash in data. For the most part, that is a good thing. The statistical tools\nintroduced in this book can be used to address some of our most significant social challenges. In that vein, I thought it fitting to finish the book with questions, not answers. As we try to\ndigest and analyze staggering quantities of information, here are five important (and admittedly\nrandom) questions whose socially significant answers will involve many of the tools introduced\nin this book. WHAT IS THE FUTURE OF FOOTBALL? In 2009, Malcolm Gladwell posed a question in a New Yorker article that first struck me as\nneedlessly sensationalist and provocative: How different are dog fighting and football?2 The\nconnection between the two activities stemmed from the fact that quarterback Michael Vick,\nwho had served time in prison for his involvement in a dog-fighting ring, had been reinstated in\nthe National Football League just as information was beginning to emerge that football-related\nhead trauma may be associated with depression, memory loss, dementia, and other\nneurological problems later in life. Gladwell’s central premise was that both professional\nfootball and dog fighting are inherently devastating to the participants. By the end of the article,\nI was convinced that he had raised an intriguing point. Here is what we know.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nConclusion\nFive questions that statistics\ncan help answer\n\nNot that long ago, information was much harder to gather and far more expensive to analyze. Imagine studying the information from one million credit card transactions in the era---only a\nfew decades back---when there were merely paper receipts and no personal computers for\nanalyzing the accumulated data. During the Great Depression, there were no official statistics\nwith which to gauge the depth of the economic problems. Government did not collect official\ninformation on either gross domestic product (GDP) or unemployment, meaning that politicians\nwere attempting to do the economic equivalent of navigating through a forest without a\ncompass. Herbert Hoover declared that the Great Depression was over in 1930, on the basis of\nthe inaccurate and outdated data that were available. He told the country in his State of Union\naddress that two and a half million Americans were out of work. In fact, five million\nAmericans were jobless, and unemployment was climbing by one hundred thousand every\nweek. As James Surowiecki recently observed in The New Yorker , “Washington was making\npolicy in the dark.”1\n\nWe are now awash in data. For the most part, that is a good thing. The statistical tools\nintroduced in this book can be used to address some of our most significant social challenges. In that vein, I thought it fitting to finish the book with questions, not answers. As we try to\ndigest and analyze staggering quantities of information, here are five important (and admittedly\nrandom) questions whose socially significant answers will involve many of the tools introduced\nin this book. WHAT IS THE FUTURE OF FOOTBALL? In 2009, Malcolm Gladwell posed a question in a New Yorker article that first struck me as\nneedlessly sensationalist and provocative: How different are dog fighting and football?2 The\nconnection between the two activities stemmed from the fact that quarterback Michael Vick,\nwho had served time in prison for his involvement in a dog-fighting ring, had been reinstated in\nthe National Football League just as information was beginning to emerge that football-related\nhead trauma may be associated with depression, memory loss, dementia, and other\nneurological problems later in life. Gladwell’s central premise was that both professional\nfootball and dog fighting are inherently devastating to the participants. By the end of the article,\nI was convinced that he had raised an intriguing point. Here is what we know.", "tokens": 497, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 174, "segment_id": "00174", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000308"}
{"type": "chunk", "text": "former NFL players have shared publicly their post-football battles with depression, memory\nloss, and dementia. Perhaps the most poignant was Dave Duerson, a former safety and Super\nBowl winner for the Chicago Bears, who committed suicide by shooting himself in the chest; he\nleft explicit instructions for his family to have his brain studied after his death. In a phone survey of a thousand randomly selected former NFL players who had played at\nleast three years in the league, 6.1 percent of the former players over fifty reported that they\nhad received a diagnosis of “dementia, Alzheimer’s disease, or other memory-related disease.”\nThat’s five times the national average for that age group. For younger players, the rate of\ndiagnosis was nineteen times the national average. Hundreds of former NFL players have now\nsued both the league and the makers of football helmets for allegedly hiding information about\nthe dangers of head trauma.3\n\nlaboratory at\n\nOne of the researchers studying the impacts of brain trauma is Ann McKee, who runs the\nneuropathology\nin Bedford, Massachusetts. the Veterans Hospital\n(Coincidentally, McKee also does the neuropathology work for the Framingham Heart Study.)\nDr. McKee has documented the buildup of abnormal proteins called tau in the brains of\nathletes who have suffered brain trauma, such as boxers and football players. This leads to a\ncondition known as chronic traumatic encephalopathy, or CTE, which is a progressive\nneurological disorder that has many of the same manifestations as Alzheimer’s. Meanwhile, other researchers have been documenting the connection between football and\nbrain trauma. Kevin Guskiewicz, who runs the Sports Concussion Research Program at the\nUniversity of North Carolina, has installed sensors on the inside of the helmets of North\nCarolina football players to record the force and nature of blows to the head. According to his\ndata, players routinely receive blows to the head with a force equivalent to hitting the\nwindshield in a car crash at twenty-five miles per hour. Here is what we don’t know. Is the brain injury evidence uncovered so far representative of\nthe long-term neurological risks that all professional football players face? Or might this just\nbe a “cluster” of adverse outcomes that is a statistical aberration? Even if it turns out that\nfootball players do face significantly higher risks of neurological disorder later in life, we\nwould still have to probe the causality.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nformer NFL players have shared publicly their post-football battles with depression, memory\nloss, and dementia. Perhaps the most poignant was Dave Duerson, a former safety and Super\nBowl winner for the Chicago Bears, who committed suicide by shooting himself in the chest; he\nleft explicit instructions for his family to have his brain studied after his death. In a phone survey of a thousand randomly selected former NFL players who had played at\nleast three years in the league, 6.1 percent of the former players over fifty reported that they\nhad received a diagnosis of “dementia, Alzheimer’s disease, or other memory-related disease.”\nThat’s five times the national average for that age group. For younger players, the rate of\ndiagnosis was nineteen times the national average. Hundreds of former NFL players have now\nsued both the league and the makers of football helmets for allegedly hiding information about\nthe dangers of head trauma.3\n\nlaboratory at\n\nOne of the researchers studying the impacts of brain trauma is Ann McKee, who runs the\nneuropathology\nin Bedford, Massachusetts. the Veterans Hospital\n(Coincidentally, McKee also does the neuropathology work for the Framingham Heart Study.)\nDr. McKee has documented the buildup of abnormal proteins called tau in the brains of\nathletes who have suffered brain trauma, such as boxers and football players. This leads to a\ncondition known as chronic traumatic encephalopathy, or CTE, which is a progressive\nneurological disorder that has many of the same manifestations as Alzheimer’s. Meanwhile, other researchers have been documenting the connection between football and\nbrain trauma. Kevin Guskiewicz, who runs the Sports Concussion Research Program at the\nUniversity of North Carolina, has installed sensors on the inside of the helmets of North\nCarolina football players to record the force and nature of blows to the head. According to his\ndata, players routinely receive blows to the head with a force equivalent to hitting the\nwindshield in a car crash at twenty-five miles per hour. Here is what we don’t know. Is the brain injury evidence uncovered so far representative of\nthe long-term neurological risks that all professional football players face? Or might this just\nbe a “cluster” of adverse outcomes that is a statistical aberration? Even if it turns out that\nfootball players do face significantly higher risks of neurological disorder later in life, we\nwould still have to probe the causality.", "tokens": 506, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 175, "segment_id": "00175", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000309"}
{"type": "chunk", "text": "According to his\ndata, players routinely receive blows to the head with a force equivalent to hitting the\nwindshield in a car crash at twenty-five miles per hour. Here is what we don’t know. Is the brain injury evidence uncovered so far representative of\nthe long-term neurological risks that all professional football players face? Or might this just\nbe a “cluster” of adverse outcomes that is a statistical aberration? Even if it turns out that\nfootball players do face significantly higher risks of neurological disorder later in life, we\nwould still have to probe the causality. Might the kind of men who play football (and boxing\nand hockey) be prone to this kind of problem? Is it possible that some other factors, such as\nsteroid use, are contributing to the neurological problems later in life? If the accumulating evidence does suggest a clear, causal link between playing football and\nlong-term brain injury, one overriding question will have to be addressed by players (and the\nparents of younger players), coaches, lawyers, NFL officials, and perhaps even government\nregulators: Is there a way to play the game of football that reduces most or all of the head\ntrauma risk? If not, then what? This is the point behind Malcolm Gladwell’s comparison of\nfootball and dog fighting. He explains that dog fighting is abhorrent to the public because the\ndog owner willingly submits his dog to a contest that culminates in suffering and destruction. “And why?” he asks. “For the entertainment of an audience and the chance of a payday. In the\nnineteenth century, dog fighting was widely accepted by the American public. But we no longer\nfind that kind of transaction morally acceptable in a sport.”", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nAccording to his\ndata, players routinely receive blows to the head with a force equivalent to hitting the\nwindshield in a car crash at twenty-five miles per hour. Here is what we don’t know. Is the brain injury evidence uncovered so far representative of\nthe long-term neurological risks that all professional football players face? Or might this just\nbe a “cluster” of adverse outcomes that is a statistical aberration? Even if it turns out that\nfootball players do face significantly higher risks of neurological disorder later in life, we\nwould still have to probe the causality. Might the kind of men who play football (and boxing\nand hockey) be prone to this kind of problem? Is it possible that some other factors, such as\nsteroid use, are contributing to the neurological problems later in life? If the accumulating evidence does suggest a clear, causal link between playing football and\nlong-term brain injury, one overriding question will have to be addressed by players (and the\nparents of younger players), coaches, lawyers, NFL officials, and perhaps even government\nregulators: Is there a way to play the game of football that reduces most or all of the head\ntrauma risk? If not, then what? This is the point behind Malcolm Gladwell’s comparison of\nfootball and dog fighting. He explains that dog fighting is abhorrent to the public because the\ndog owner willingly submits his dog to a contest that culminates in suffering and destruction. “And why?” he asks. “For the entertainment of an audience and the chance of a payday. In the\nnineteenth century, dog fighting was widely accepted by the American public. But we no longer\nfind that kind of transaction morally acceptable in a sport.”", "tokens": 354, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 175, "segment_id": "00175", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000310"}
{"type": "chunk", "text": "Nearly every kind of statistical analysis described in this book is currently being used to\n\nfigure out whether or not professional football as we know it now has a future. WHAT (IF ANYTHING) IS CAUSING THE\nDRAMATIC RISE IN THE INCIDENCE OF AUTISM? In 2012, the Centers for Disease Control reported that 1 in 88 American children has been\ndiagnosed with an autism spectrum disorder (on the basis of data from 2008).4 The rate of\ndiagnosis had climbed from 1 in 110 in 2006, and 1 in 150 in 2002---or nearly a doubling in\nless than a decade. Autism spectrum disorders (ASDs) are a group of developmental\ndisabilities characterized by atypical development in socialization, communication, and\nbehavior. The “spectrum” indicates that autism encompasses a broad range of behaviorally\ndefined conditions.5 Boys are five times as likely to be diagnosed with an ASD as girls\n(meaning that the incidence for boys is even higher than 1 in 88). The first intriguing statistical question is whether we are experiencing an epidemic of autism,\nan “epidemic of diagnosis,” or some combination of the two?6 In previous decades, children\nwith an autism spectrum disorder had symptoms that might have gone undiagnosed, or their\ndevelopmental challenges might have been described more generally as a “learning disability.”\nDoctors, parents, and teachers are now much more aware of the symptoms of ASDs, which\nnaturally leads to more diagnoses regardless of whether or not the incidence of autism is on the\nrise. In any case, the shockingly high incidence of ASDs represents a serious challenge for\nfamilies, for schools, and for the rest of society. The average lifetime cost of managing an\nautism spectrum disorder for a single individual is $3.5 million.7 Despite what is clearly an\nepidemic, we know amazingly little about what causes the condition. Thomas Insel, director of\nthe National Institute of Mental Health, has said, “Is it cell phones? Ultrasound? Diet sodas? Every parent has a theory. At this point, we just don’t know.”8\n\nWhat is different or unique about the lives and backgrounds of children with ASDs? What\nare the most significant physiological differences between children with and without an ASD? Is the incidence of ASDs different across countries? If so, why?", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nNearly every kind of statistical analysis described in this book is currently being used to\n\nfigure out whether or not professional football as we know it now has a future. WHAT (IF ANYTHING) IS CAUSING THE\nDRAMATIC RISE IN THE INCIDENCE OF AUTISM? In 2012, the Centers for Disease Control reported that 1 in 88 American children has been\ndiagnosed with an autism spectrum disorder (on the basis of data from 2008).4 The rate of\ndiagnosis had climbed from 1 in 110 in 2006, and 1 in 150 in 2002---or nearly a doubling in\nless than a decade. Autism spectrum disorders (ASDs) are a group of developmental\ndisabilities characterized by atypical development in socialization, communication, and\nbehavior. The “spectrum” indicates that autism encompasses a broad range of behaviorally\ndefined conditions.5 Boys are five times as likely to be diagnosed with an ASD as girls\n(meaning that the incidence for boys is even higher than 1 in 88). The first intriguing statistical question is whether we are experiencing an epidemic of autism,\nan “epidemic of diagnosis,” or some combination of the two?6 In previous decades, children\nwith an autism spectrum disorder had symptoms that might have gone undiagnosed, or their\ndevelopmental challenges might have been described more generally as a “learning disability.”\nDoctors, parents, and teachers are now much more aware of the symptoms of ASDs, which\nnaturally leads to more diagnoses regardless of whether or not the incidence of autism is on the\nrise. In any case, the shockingly high incidence of ASDs represents a serious challenge for\nfamilies, for schools, and for the rest of society. The average lifetime cost of managing an\nautism spectrum disorder for a single individual is $3.5 million.7 Despite what is clearly an\nepidemic, we know amazingly little about what causes the condition. Thomas Insel, director of\nthe National Institute of Mental Health, has said, “Is it cell phones? Ultrasound? Diet sodas? Every parent has a theory. At this point, we just don’t know.”8\n\nWhat is different or unique about the lives and backgrounds of children with ASDs? What\nare the most significant physiological differences between children with and without an ASD? Is the incidence of ASDs different across countries? If so, why?", "tokens": 504, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 176, "segment_id": "00176", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000311"}
{"type": "chunk", "text": "Thomas Insel, director of\nthe National Institute of Mental Health, has said, “Is it cell phones? Ultrasound? Diet sodas? Every parent has a theory. At this point, we just don’t know.”8\n\nWhat is different or unique about the lives and backgrounds of children with ASDs? What\nare the most significant physiological differences between children with and without an ASD? Is the incidence of ASDs different across countries? If so, why? Traditional statistical\ndetective work is turning up clues. One recent study by researchers at the University of California at Davis identified ten\nlocations in California with autism rates that are double the rates of surrounding areas; each of\nthe autism clusters is a neighborhood with a concentration of white, highly educated parents.9\nIs that a clue, or a coincidence? Or might it reflect that relatively privileged families are more\nlikely to have an autism spectrum disorder diagnosed? The same researchers are also\nconducting a study in which they will collect dust samples from the homes of 1,300 families\nwith an autistic child to test for chemicals or other environmental contaminants than may play a\ncausal role. Meanwhile, other researchers have identified what appears to be a genetic component to\nautism by studying ASDs among identical and fraternal twins.10 The likelihood that two", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThomas Insel, director of\nthe National Institute of Mental Health, has said, “Is it cell phones? Ultrasound? Diet sodas? Every parent has a theory. At this point, we just don’t know.”8\n\nWhat is different or unique about the lives and backgrounds of children with ASDs? What\nare the most significant physiological differences between children with and without an ASD? Is the incidence of ASDs different across countries? If so, why? Traditional statistical\ndetective work is turning up clues. One recent study by researchers at the University of California at Davis identified ten\nlocations in California with autism rates that are double the rates of surrounding areas; each of\nthe autism clusters is a neighborhood with a concentration of white, highly educated parents.9\nIs that a clue, or a coincidence? Or might it reflect that relatively privileged families are more\nlikely to have an autism spectrum disorder diagnosed? The same researchers are also\nconducting a study in which they will collect dust samples from the homes of 1,300 families\nwith an autistic child to test for chemicals or other environmental contaminants than may play a\ncausal role. Meanwhile, other researchers have identified what appears to be a genetic component to\nautism by studying ASDs among identical and fraternal twins.10 The likelihood that two", "tokens": 267, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 176, "segment_id": "00176", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000312"}
{"type": "chunk", "text": "children in the same family have an ASD is higher among identical twins (who share the same\ngenetic makeup) than among fraternal twins (whose genetic similarity is the same as for regular\nsiblings). This finding does not rule out significant environmental factors, or perhaps the\ninteraction between environmental and genetic factors. After all, heart disease has a significant\ngenetic component, but clearly smoking, diet, exercise, and many other behavioral and\nenvironmental factors all matter, too. One of the most important contributions of statistical analysis so far has been to debunk\nfalse causes, many of which have arisen because of a confusion between correlation and\ncausation. An autism spectrum disorder often appears suddenly between a child’s first and\nsecond birthdays. This has led to a widespread belief that childhood vaccinations, particularly\nthe triple vaccine for measles, mumps, and rubella (MMR), are causing the rising incidence of\nautism. Dan Burton, a member of Congress from Indiana, told the New York Times , “My\ngrandson received nine shots in one day, seven of which contained thimerosal, which is 50\npercent mercury as you know, and he became autistic a short time later.”11\n\nScientists have soundly refuted the false association between thimerosal and ASDs. Autism\nrates did not decline when thimerosal was removed from the MMR vaccine, nor are autism\nrates lower in countries that never used this vaccine. Nonetheless, the false connection persists,\nwhich has caused some parents to refuse to vaccinate their children. Ironically, this offers no\nprotection against autism while putting children at risk of contracting other serious diseases\n(and contributing to the spread of those diseases in the population). Autism poses one of the greatest medical and social challenge of our day. We understand so\nlittle about the disorder relative to its huge (and possibly growing) impact on our collective\nwell-being. Researchers are using every tool in this book (and lots more) to change that. HOW CAN WE IDENTIFY AND REWARD\nGOOD TEACHERS AND SCHOOLS? We need good schools. And we need good teachers in order to have good schools. Thus, it\nfollows logically that we ought to reward good teachers and good schools while firing bad\nteachers and closing bad schools. How exactly do we do that? Test scores give us an objective measure of student performance.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nchildren in the same family have an ASD is higher among identical twins (who share the same\ngenetic makeup) than among fraternal twins (whose genetic similarity is the same as for regular\nsiblings). This finding does not rule out significant environmental factors, or perhaps the\ninteraction between environmental and genetic factors. After all, heart disease has a significant\ngenetic component, but clearly smoking, diet, exercise, and many other behavioral and\nenvironmental factors all matter, too. One of the most important contributions of statistical analysis so far has been to debunk\nfalse causes, many of which have arisen because of a confusion between correlation and\ncausation. An autism spectrum disorder often appears suddenly between a child’s first and\nsecond birthdays. This has led to a widespread belief that childhood vaccinations, particularly\nthe triple vaccine for measles, mumps, and rubella (MMR), are causing the rising incidence of\nautism. Dan Burton, a member of Congress from Indiana, told the New York Times , “My\ngrandson received nine shots in one day, seven of which contained thimerosal, which is 50\npercent mercury as you know, and he became autistic a short time later.”11\n\nScientists have soundly refuted the false association between thimerosal and ASDs. Autism\nrates did not decline when thimerosal was removed from the MMR vaccine, nor are autism\nrates lower in countries that never used this vaccine. Nonetheless, the false connection persists,\nwhich has caused some parents to refuse to vaccinate their children. Ironically, this offers no\nprotection against autism while putting children at risk of contracting other serious diseases\n(and contributing to the spread of those diseases in the population). Autism poses one of the greatest medical and social challenge of our day. We understand so\nlittle about the disorder relative to its huge (and possibly growing) impact on our collective\nwell-being. Researchers are using every tool in this book (and lots more) to change that. HOW CAN WE IDENTIFY AND REWARD\nGOOD TEACHERS AND SCHOOLS? We need good schools. And we need good teachers in order to have good schools. Thus, it\nfollows logically that we ought to reward good teachers and good schools while firing bad\nteachers and closing bad schools. How exactly do we do that? Test scores give us an objective measure of student performance.", "tokens": 485, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 177, "segment_id": "00177", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000313"}
{"type": "chunk", "text": "We understand so\nlittle about the disorder relative to its huge (and possibly growing) impact on our collective\nwell-being. Researchers are using every tool in this book (and lots more) to change that. HOW CAN WE IDENTIFY AND REWARD\nGOOD TEACHERS AND SCHOOLS? We need good schools. And we need good teachers in order to have good schools. Thus, it\nfollows logically that we ought to reward good teachers and good schools while firing bad\nteachers and closing bad schools. How exactly do we do that? Test scores give us an objective measure of student performance. Yet we know that some\nstudents will do much better on standardized tests than others for reasons that have nothing to\ndo with what is going on inside a classroom or a school. The seemingly simple solution is to\nevaluate schools and teachers on the basis of the progress that their students make over some\nperiod of time. What did students know when they started in a certain classroom with a\nparticular teacher? What did they know a year later? The difference is the “value added” in\nthat classroom. We can even use statistics to get a more refined sense of this value added by taking into\naccount the demographic characteristics of the students in a given classroom, such as race,\nincome, and performance on other tests (which can be a measure of aptitude). If a teacher\nmakes significant gains with students who have typically struggled in the past, then he or she", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nWe understand so\nlittle about the disorder relative to its huge (and possibly growing) impact on our collective\nwell-being. Researchers are using every tool in this book (and lots more) to change that. HOW CAN WE IDENTIFY AND REWARD\nGOOD TEACHERS AND SCHOOLS? We need good schools. And we need good teachers in order to have good schools. Thus, it\nfollows logically that we ought to reward good teachers and good schools while firing bad\nteachers and closing bad schools. How exactly do we do that? Test scores give us an objective measure of student performance. Yet we know that some\nstudents will do much better on standardized tests than others for reasons that have nothing to\ndo with what is going on inside a classroom or a school. The seemingly simple solution is to\nevaluate schools and teachers on the basis of the progress that their students make over some\nperiod of time. What did students know when they started in a certain classroom with a\nparticular teacher? What did they know a year later? The difference is the “value added” in\nthat classroom. We can even use statistics to get a more refined sense of this value added by taking into\naccount the demographic characteristics of the students in a given classroom, such as race,\nincome, and performance on other tests (which can be a measure of aptitude). If a teacher\nmakes significant gains with students who have typically struggled in the past, then he or she", "tokens": 298, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 177, "segment_id": "00177", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000314"}
{"type": "chunk", "text": "can be deemed as highly effective. Voilà! We can now evaluate teacher quality with statistical precision. And the good schools,\n\nof course, are just the ones full of effective teachers. How do these handy statistical evaluations work in practice? In 2012, New York City took\nthe plunge and published ratings of all 18,000 public school teachers on the basis of a “valueadded assessment” that measured gains in their students’ test scores while taking into account\nvarious student characteristics.12 The Los Angeles Times published a similar set of rankings\nfor Los Angeles teachers in 2010. In both New York and LA, the reaction has been loud and mixed. Arne Duncan, the U.S. secretary of education, has generally been supportive of these kinds of value-added\nassessments. They provide information where none previously existed. After the Los Angeles\ndata were published, Secretary Duncan told the New York Times , “Silence is not an option.”\nThe Obama administration has provided financial incentives for states to develop value-added\nindicators for paying and promoting teachers. Proponents of these evaluation measures\nrightfully point out that they are a huge potential improvement over systems in which all\nteachers are paid according to a uniform salary schedule that gives zero weight to any measure\nof performance in the classroom. On the other hand, many experts have warned that these kinds of teacher assessment data\nhave large margins of error and can deliver misleading results. The union representing New\nYork City teachers spent more than $100,000 on a newspaper advertising campaign built\naround the headline “This Is No Way to Rate a Teacher.” 13 Opponents argue that the valueadded assessments create false precision that will be abused by parents and public officials\nwho do not understand the limitations of this kind of assessment. This appears to be a case where everybody is right---up to a point. Doug Staiger, an\neconomist at Dartmouth College who works extensively with value-added data for teachers,\nwarns that these data are inherently “noisy.” The results for a given teacher are often based on\na single test taken on a single day by a single group of students. All kinds of factors can lead\nto random fluctuations---anything from a particularly difficult group of students to a broken\nair-conditioning unit clanking away in the classroom on test day. The correlation in\nperformance from year to year for a single teacher that uses these indicators is only about .35.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\ncan be deemed as highly effective. Voilà! We can now evaluate teacher quality with statistical precision. And the good schools,\n\nof course, are just the ones full of effective teachers. How do these handy statistical evaluations work in practice? In 2012, New York City took\nthe plunge and published ratings of all 18,000 public school teachers on the basis of a “valueadded assessment” that measured gains in their students’ test scores while taking into account\nvarious student characteristics.12 The Los Angeles Times published a similar set of rankings\nfor Los Angeles teachers in 2010. In both New York and LA, the reaction has been loud and mixed. Arne Duncan, the U.S. secretary of education, has generally been supportive of these kinds of value-added\nassessments. They provide information where none previously existed. After the Los Angeles\ndata were published, Secretary Duncan told the New York Times , “Silence is not an option.”\nThe Obama administration has provided financial incentives for states to develop value-added\nindicators for paying and promoting teachers. Proponents of these evaluation measures\nrightfully point out that they are a huge potential improvement over systems in which all\nteachers are paid according to a uniform salary schedule that gives zero weight to any measure\nof performance in the classroom. On the other hand, many experts have warned that these kinds of teacher assessment data\nhave large margins of error and can deliver misleading results. The union representing New\nYork City teachers spent more than $100,000 on a newspaper advertising campaign built\naround the headline “This Is No Way to Rate a Teacher.” 13 Opponents argue that the valueadded assessments create false precision that will be abused by parents and public officials\nwho do not understand the limitations of this kind of assessment. This appears to be a case where everybody is right---up to a point. Doug Staiger, an\neconomist at Dartmouth College who works extensively with value-added data for teachers,\nwarns that these data are inherently “noisy.” The results for a given teacher are often based on\na single test taken on a single day by a single group of students. All kinds of factors can lead\nto random fluctuations---anything from a particularly difficult group of students to a broken\nair-conditioning unit clanking away in the classroom on test day. The correlation in\nperformance from year to year for a single teacher that uses these indicators is only about .35.", "tokens": 500, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 178, "segment_id": "00178", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000315"}
{"type": "chunk", "text": "Doug Staiger, an\neconomist at Dartmouth College who works extensively with value-added data for teachers,\nwarns that these data are inherently “noisy.” The results for a given teacher are often based on\na single test taken on a single day by a single group of students. All kinds of factors can lead\nto random fluctuations---anything from a particularly difficult group of students to a broken\nair-conditioning unit clanking away in the classroom on test day. The correlation in\nperformance from year to year for a single teacher that uses these indicators is only about .35. (Interestingly, the correlation in year-to-year performance for Major League baseball players is\nalso around .35, as measured by batting average for hitters and earned run average for\npitchers.)14\n\nThe teacher effectiveness data are useful, says Staiger, but they are just one tool in the\nprocess for evaluating teacher performance. The data get “less noisy” when authorities have\nmore years of data for a particular teacher with different classrooms of students (just as we can\ntell more about an athlete when we have data for more games and more seasons). In the case of\nthe New York City teacher ratings, principals in the system had been prepped on the\nappropriate use of the value-added data and the inherent limitations. The public did not get that\nbriefing. As a result, the teacher assessments are too often viewed as a definitive guide to the\n“good” and “bad” teachers. We like rankings---just think U.S. News & World Report college", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nDoug Staiger, an\neconomist at Dartmouth College who works extensively with value-added data for teachers,\nwarns that these data are inherently “noisy.” The results for a given teacher are often based on\na single test taken on a single day by a single group of students. All kinds of factors can lead\nto random fluctuations---anything from a particularly difficult group of students to a broken\nair-conditioning unit clanking away in the classroom on test day. The correlation in\nperformance from year to year for a single teacher that uses these indicators is only about .35. (Interestingly, the correlation in year-to-year performance for Major League baseball players is\nalso around .35, as measured by batting average for hitters and earned run average for\npitchers.)14\n\nThe teacher effectiveness data are useful, says Staiger, but they are just one tool in the\nprocess for evaluating teacher performance. The data get “less noisy” when authorities have\nmore years of data for a particular teacher with different classrooms of students (just as we can\ntell more about an athlete when we have data for more games and more seasons). In the case of\nthe New York City teacher ratings, principals in the system had been prepped on the\nappropriate use of the value-added data and the inherent limitations. The public did not get that\nbriefing. As a result, the teacher assessments are too often viewed as a definitive guide to the\n“good” and “bad” teachers. We like rankings---just think U.S. News & World Report college", "tokens": 318, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 178, "segment_id": "00178", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000316"}
{"type": "chunk", "text": "rankings---even when the data do not support such precision. Staiger offers a final warning of different sort: We had better be certain that the outcomes we\nare measuring, such as the results of a given standardized test, truly track with what we care\nabout in the long run. Some unique data from the Air Force Academy suggest, not surprisingly,\nthat the test scores that glimmer now may not be gold in the future. The Air Force Academy,\nlike the other military academies, randomly assigns its cadets to different sections of\nstandardized core courses, such as introductory calculus. This randomization eliminates any\npotential selection effect when comparing the effectiveness of professors; over time, we can\nassume that all professors get students with similar aptitudes (unlike most universities, where\nstudents of different abilities can select into or out of different courses). The Air Force\nAcademy also uses the same syllabi and exams in every section of a particular course. Scott\nCarrell and James West, professors at the University of California at Davis and the Air Force\nAcademy, exploited this elegant arrangement to answer one of the most important questions in\nhigher education: Which professors are most effective?15\n\nThe answer: The professors with less experience and fewer degrees from fancy\nuniversities. These professors have students who typically do better on the standardized exams\nfor the introductory courses. They also get better student evaluations for their courses. Clearly\nthese young, motivated instructors are more committed to their teaching than the old, crusty\nprofessors with PhDs from places like Harvard. The old guys must be using the same\nyellowing teaching notes that they used in 1978; they probably think PowerPoint is an energy\ndrink---except that they don’t know what an energy drink is either. Obviously the data tell us\nthat we should fire these old codgers, or at least let them retire gracefully. But hold on. Let’s not fire anybody yet. The Air Force Academy study had another relevant\nfinding---about student performance over a longer horizon. Carrell and West found that in\nmath and science the students who had more experienced (and more highly credentialed)\ninstructors in the introductory courses do better in their mandatory follow-on courses than\nstudents who had less experienced professors in the introductory courses. One logical\ninterpretation is that less experienced instructors are more likely to “teach to the test” in the\nintroductory course.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nrankings---even when the data do not support such precision. Staiger offers a final warning of different sort: We had better be certain that the outcomes we\nare measuring, such as the results of a given standardized test, truly track with what we care\nabout in the long run. Some unique data from the Air Force Academy suggest, not surprisingly,\nthat the test scores that glimmer now may not be gold in the future. The Air Force Academy,\nlike the other military academies, randomly assigns its cadets to different sections of\nstandardized core courses, such as introductory calculus. This randomization eliminates any\npotential selection effect when comparing the effectiveness of professors; over time, we can\nassume that all professors get students with similar aptitudes (unlike most universities, where\nstudents of different abilities can select into or out of different courses). The Air Force\nAcademy also uses the same syllabi and exams in every section of a particular course. Scott\nCarrell and James West, professors at the University of California at Davis and the Air Force\nAcademy, exploited this elegant arrangement to answer one of the most important questions in\nhigher education: Which professors are most effective?15\n\nThe answer: The professors with less experience and fewer degrees from fancy\nuniversities. These professors have students who typically do better on the standardized exams\nfor the introductory courses. They also get better student evaluations for their courses. Clearly\nthese young, motivated instructors are more committed to their teaching than the old, crusty\nprofessors with PhDs from places like Harvard. The old guys must be using the same\nyellowing teaching notes that they used in 1978; they probably think PowerPoint is an energy\ndrink---except that they don’t know what an energy drink is either. Obviously the data tell us\nthat we should fire these old codgers, or at least let them retire gracefully. But hold on. Let’s not fire anybody yet. The Air Force Academy study had another relevant\nfinding---about student performance over a longer horizon. Carrell and West found that in\nmath and science the students who had more experienced (and more highly credentialed)\ninstructors in the introductory courses do better in their mandatory follow-on courses than\nstudents who had less experienced professors in the introductory courses. One logical\ninterpretation is that less experienced instructors are more likely to “teach to the test” in the\nintroductory course.", "tokens": 501, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 179, "segment_id": "00179", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000317"}
{"type": "chunk", "text": "But hold on. Let’s not fire anybody yet. The Air Force Academy study had another relevant\nfinding---about student performance over a longer horizon. Carrell and West found that in\nmath and science the students who had more experienced (and more highly credentialed)\ninstructors in the introductory courses do better in their mandatory follow-on courses than\nstudents who had less experienced professors in the introductory courses. One logical\ninterpretation is that less experienced instructors are more likely to “teach to the test” in the\nintroductory course. This produces impressive exam scores and happy students when it comes\nto filling out the instructor evaluation. Meanwhile, the old, crusty professors (whom we nearly fired just one paragraph ago) focus\nless on the exam and more on the important concepts, which are what matter most in follow-on\ncourses and in life after the Air Force Academy. Clearly we need to evaluate teachers and professors. We just have to make sure that we do\nit right. The long-term policy challenge, rooted in statistics, is to develop a system that rewards\na teacher’s real value added in the classroom. WHAT ARE THE BEST TOOLS\nFOR FIGHTING GLOBAL POVERTY? We know strikingly little about how to make poor countries less poor. True, we understand the\nthings that distinguish rich countries from poor countries, such as their education levels and the", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nBut hold on. Let’s not fire anybody yet. The Air Force Academy study had another relevant\nfinding---about student performance over a longer horizon. Carrell and West found that in\nmath and science the students who had more experienced (and more highly credentialed)\ninstructors in the introductory courses do better in their mandatory follow-on courses than\nstudents who had less experienced professors in the introductory courses. One logical\ninterpretation is that less experienced instructors are more likely to “teach to the test” in the\nintroductory course. This produces impressive exam scores and happy students when it comes\nto filling out the instructor evaluation. Meanwhile, the old, crusty professors (whom we nearly fired just one paragraph ago) focus\nless on the exam and more on the important concepts, which are what matter most in follow-on\ncourses and in life after the Air Force Academy. Clearly we need to evaluate teachers and professors. We just have to make sure that we do\nit right. The long-term policy challenge, rooted in statistics, is to develop a system that rewards\na teacher’s real value added in the classroom. WHAT ARE THE BEST TOOLS\nFOR FIGHTING GLOBAL POVERTY? We know strikingly little about how to make poor countries less poor. True, we understand the\nthings that distinguish rich countries from poor countries, such as their education levels and the", "tokens": 283, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 179, "segment_id": "00179", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000318"}
{"type": "chunk", "text": "quality of their governments. And it is also true that we have watched countries like India and\nChina transform themselves economically over the last several decades. But even with this\nknowledge, it is not obvious what steps we can take to make places like Mali or Burkina Faso,\nless poor. Where should we begin? French economist Esther Duflo is transforming our knowledge of global poverty by\nretrofitting an old tool for new purposes: the randomized, controlled experiment. Duflo, who\nteaches at MIT, literally conducts experiments on different interventions to improve the lives of\nthe poor in developing countries. For example, one of the longstanding problems with schools\nin India is absenteeism among teachers, particularly in small, rural schools with only a single\nteacher. Duflo and her coauthor Rema Hanna tested a clever, technology-driven solution on a\nrandom sample of 60 one-teacher schools in the Indian state of Rajasthan.16 Teachers in these\n60 experimental schools were offered a bonus for good attendance. Here is the creative part:\nThe teachers were given cameras with tamperproof date and time stamps. They proved that\nthey had showed up each day by having their picture taken with their students.17\n\nAbsenteeism dropped by half among teachers in the experimental schools compared with\nteachers in a randomly selected control group of 60 schools. Student test scores went up, and\nmore students graduated into the next level of education. (I bet the photos are adorable, too!)\n\nOne of Duflo’s experiments in Kenya involved giving a randomly selected group of farmers\na small subsidy to buy fertilizer right after the harvest. Prior evidence suggested that fertilizer\nraises crop yields appreciably. Farmers were aware of this benefit, but when it came time to\nput a new crop into the ground, they often did not have enough money left over from the last\ncrop to buy fertilizer. This perpetuates what is known as a “poverty trap” since the subsistence\nfarmers are too poor to make themselves less poor. Duflo and her coauthors found that a tiny\nsubsidy---free fertilizer delivery---offered to farmers when they still had cash after the harvest\nincreased fertilizer use by 10 to 20 percentage points compared with use in a control group.18\n\nEsther Duflo has even waded into the gender war. Who is more responsible when it comes to\nhandling the family’s finances, men or women?", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nquality of their governments. And it is also true that we have watched countries like India and\nChina transform themselves economically over the last several decades. But even with this\nknowledge, it is not obvious what steps we can take to make places like Mali or Burkina Faso,\nless poor. Where should we begin? French economist Esther Duflo is transforming our knowledge of global poverty by\nretrofitting an old tool for new purposes: the randomized, controlled experiment. Duflo, who\nteaches at MIT, literally conducts experiments on different interventions to improve the lives of\nthe poor in developing countries. For example, one of the longstanding problems with schools\nin India is absenteeism among teachers, particularly in small, rural schools with only a single\nteacher. Duflo and her coauthor Rema Hanna tested a clever, technology-driven solution on a\nrandom sample of 60 one-teacher schools in the Indian state of Rajasthan.16 Teachers in these\n60 experimental schools were offered a bonus for good attendance. Here is the creative part:\nThe teachers were given cameras with tamperproof date and time stamps. They proved that\nthey had showed up each day by having their picture taken with their students.17\n\nAbsenteeism dropped by half among teachers in the experimental schools compared with\nteachers in a randomly selected control group of 60 schools. Student test scores went up, and\nmore students graduated into the next level of education. (I bet the photos are adorable, too!)\n\nOne of Duflo’s experiments in Kenya involved giving a randomly selected group of farmers\na small subsidy to buy fertilizer right after the harvest. Prior evidence suggested that fertilizer\nraises crop yields appreciably. Farmers were aware of this benefit, but when it came time to\nput a new crop into the ground, they often did not have enough money left over from the last\ncrop to buy fertilizer. This perpetuates what is known as a “poverty trap” since the subsistence\nfarmers are too poor to make themselves less poor. Duflo and her coauthors found that a tiny\nsubsidy---free fertilizer delivery---offered to farmers when they still had cash after the harvest\nincreased fertilizer use by 10 to 20 percentage points compared with use in a control group.18\n\nEsther Duflo has even waded into the gender war. Who is more responsible when it comes to\nhandling the family’s finances, men or women?", "tokens": 504, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 180, "segment_id": "00180", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000319"}
{"type": "chunk", "text": "This perpetuates what is known as a “poverty trap” since the subsistence\nfarmers are too poor to make themselves less poor. Duflo and her coauthors found that a tiny\nsubsidy---free fertilizer delivery---offered to farmers when they still had cash after the harvest\nincreased fertilizer use by 10 to 20 percentage points compared with use in a control group.18\n\nEsther Duflo has even waded into the gender war. Who is more responsible when it comes to\nhandling the family’s finances, men or women? In rich countries, this is the kind of thing that\ncouples can squabble over in marriage counseling. In poor countries, it can literally determine\nwhether the children get enough to eat. Anecdotal evidence going back to the dawn of\ncivilization suggests that women place a high priority on the health and welfare of their\nchildren, while men are more inclined to drink up their wages at the local pub (or whatever the\ncaveman equivalent was). At worst, this anecdotal evidence merely reinforces age-old\nstereotypes. At best, it is a hard thing to prove, because a family’s finances are comingled to\nsome extent. How can we separate out how husbands and wives choose to spend communal\nresources? Duflo did not shy away from this delicate question.19 To the contrary, she found a\nfascinating natural experiment. In Côte d’Ivoire, women and men in a family typically share\nresponsibility for some crops. For longstanding cultural reasons, men and women also cultivate\ndifferent cash crops of their own. (Men grow cocoa, coffee, and some other things; women\ngrow plantains, coconuts, and a few other crops.) The beauty of this arrangement from a\nresearch standpoint is that the men’s crops and the women’s crops respond to rainfall patterns", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThis perpetuates what is known as a “poverty trap” since the subsistence\nfarmers are too poor to make themselves less poor. Duflo and her coauthors found that a tiny\nsubsidy---free fertilizer delivery---offered to farmers when they still had cash after the harvest\nincreased fertilizer use by 10 to 20 percentage points compared with use in a control group.18\n\nEsther Duflo has even waded into the gender war. Who is more responsible when it comes to\nhandling the family’s finances, men or women? In rich countries, this is the kind of thing that\ncouples can squabble over in marriage counseling. In poor countries, it can literally determine\nwhether the children get enough to eat. Anecdotal evidence going back to the dawn of\ncivilization suggests that women place a high priority on the health and welfare of their\nchildren, while men are more inclined to drink up their wages at the local pub (or whatever the\ncaveman equivalent was). At worst, this anecdotal evidence merely reinforces age-old\nstereotypes. At best, it is a hard thing to prove, because a family’s finances are comingled to\nsome extent. How can we separate out how husbands and wives choose to spend communal\nresources? Duflo did not shy away from this delicate question.19 To the contrary, she found a\nfascinating natural experiment. In Côte d’Ivoire, women and men in a family typically share\nresponsibility for some crops. For longstanding cultural reasons, men and women also cultivate\ndifferent cash crops of their own. (Men grow cocoa, coffee, and some other things; women\ngrow plantains, coconuts, and a few other crops.) The beauty of this arrangement from a\nresearch standpoint is that the men’s crops and the women’s crops respond to rainfall patterns", "tokens": 389, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 180, "segment_id": "00180", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000320"}
{"type": "chunk", "text": "in different ways. In years in which cocoa and coffee do well, men have more disposable\nincome to spend. In years in which plantains and coconuts do well, the women have more extra\ncash. Now we need merely broach a delicate question: Are the children in these families better-off\nin years in which the men’s crops do well, or in the years when the women have a particularly\nbountiful harvest? The answer: When the women do well, they spend some of their extra cash on more food for\n\nthe family. The men don’t. Sorry guys. In 2010, Duflo was awarded the John Bates Clark Medal. This prize is presented by the\nAmerican Economic Association to the best economist under the age of forty. * Among\neconomist geeks, this prize is considered to be more prestigious than the Nobel Prize in\nEconomics because it was historically awarded only every two years. (Beginning with Duflo’s\naward in 2010, the medal is now presented annually.) In any case, the Clark Medal is the MVP\naward for people with thick glasses (metaphorically speaking). Duflo is doing program evaluation. Her work, and the work of others now using her methods,\nis literally changing the lives of the poor. From a statistical standpoint, Duflo’s work has\nencouraged us to think more broadly about how randomized, controlled experiments---long\nthought to be the province of the laboratory sciences---can be used more widely to tease out\ncausal relationships in many other areas of life. WHO GETS TO KNOW WHAT ABOUT YOU? Last summer, we hired a new babysitter. When she arrived at the house, I began to explain our\nfamily background: “I am a professor, my wife is a teacher . . .”\n\n“Oh, I know,” the sitter said with a wave of the hand. “I Googled you.”\nI was simultaneously relieved that I did not have to finish my spiel and mildly alarmed by\nhow much of my life could be cobbled together from a short Internet search. Our capacity to\ngather and analyze huge quantities of data---the marriage of digital information with cheap\ncomputing power and the Internet---is unique in human history. We are going to need some new\nrules for this new era. Let’s put the power of data in perspective with just one example from the retailer Target. Like most companies, Target strives to increase profits by understanding its customers.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nin different ways. In years in which cocoa and coffee do well, men have more disposable\nincome to spend. In years in which plantains and coconuts do well, the women have more extra\ncash. Now we need merely broach a delicate question: Are the children in these families better-off\nin years in which the men’s crops do well, or in the years when the women have a particularly\nbountiful harvest? The answer: When the women do well, they spend some of their extra cash on more food for\n\nthe family. The men don’t. Sorry guys. In 2010, Duflo was awarded the John Bates Clark Medal. This prize is presented by the\nAmerican Economic Association to the best economist under the age of forty. * Among\neconomist geeks, this prize is considered to be more prestigious than the Nobel Prize in\nEconomics because it was historically awarded only every two years. (Beginning with Duflo’s\naward in 2010, the medal is now presented annually.) In any case, the Clark Medal is the MVP\naward for people with thick glasses (metaphorically speaking). Duflo is doing program evaluation. Her work, and the work of others now using her methods,\nis literally changing the lives of the poor. From a statistical standpoint, Duflo’s work has\nencouraged us to think more broadly about how randomized, controlled experiments---long\nthought to be the province of the laboratory sciences---can be used more widely to tease out\ncausal relationships in many other areas of life. WHO GETS TO KNOW WHAT ABOUT YOU? Last summer, we hired a new babysitter. When she arrived at the house, I began to explain our\nfamily background: “I am a professor, my wife is a teacher . . .”\n\n“Oh, I know,” the sitter said with a wave of the hand. “I Googled you.”\nI was simultaneously relieved that I did not have to finish my spiel and mildly alarmed by\nhow much of my life could be cobbled together from a short Internet search. Our capacity to\ngather and analyze huge quantities of data---the marriage of digital information with cheap\ncomputing power and the Internet---is unique in human history. We are going to need some new\nrules for this new era. Let’s put the power of data in perspective with just one example from the retailer Target. Like most companies, Target strives to increase profits by understanding its customers.", "tokens": 512, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 181, "segment_id": "00181", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000321"}
{"type": "chunk", "text": "“I Googled you.”\nI was simultaneously relieved that I did not have to finish my spiel and mildly alarmed by\nhow much of my life could be cobbled together from a short Internet search. Our capacity to\ngather and analyze huge quantities of data---the marriage of digital information with cheap\ncomputing power and the Internet---is unique in human history. We are going to need some new\nrules for this new era. Let’s put the power of data in perspective with just one example from the retailer Target. Like most companies, Target strives to increase profits by understanding its customers. To do\nthat, the company hires statisticians to do the kind of “predictive analytics” described earlier in\nthe book; they use sales data combined with other information on consumers to figure out who\nbuys what and why. Nothing about this is inherently bad, for it means that the Target near you\nis likely to have exactly what you want. But let’s drill down for a moment on just one example of the kinds of things that the\nstatisticians working in the windowless basement at corporate headquarters can figure out. Target has learned that pregnancy is a particularly important time in terms of developing\nshopping patterns. Pregnant women develop “retail relationships” that can last for decades. As\na result, Target wants to identify pregnant women, particularly those in their second trimester,\nand get them into their stores more often. A writer for the New York Times Magazine followed", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\n“I Googled you.”\nI was simultaneously relieved that I did not have to finish my spiel and mildly alarmed by\nhow much of my life could be cobbled together from a short Internet search. Our capacity to\ngather and analyze huge quantities of data---the marriage of digital information with cheap\ncomputing power and the Internet---is unique in human history. We are going to need some new\nrules for this new era. Let’s put the power of data in perspective with just one example from the retailer Target. Like most companies, Target strives to increase profits by understanding its customers. To do\nthat, the company hires statisticians to do the kind of “predictive analytics” described earlier in\nthe book; they use sales data combined with other information on consumers to figure out who\nbuys what and why. Nothing about this is inherently bad, for it means that the Target near you\nis likely to have exactly what you want. But let’s drill down for a moment on just one example of the kinds of things that the\nstatisticians working in the windowless basement at corporate headquarters can figure out. Target has learned that pregnancy is a particularly important time in terms of developing\nshopping patterns. Pregnant women develop “retail relationships” that can last for decades. As\na result, Target wants to identify pregnant women, particularly those in their second trimester,\nand get them into their stores more often. A writer for the New York Times Magazine followed", "tokens": 299, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 181, "segment_id": "00181", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000322"}
{"type": "chunk", "text": "the predictive analytics team at Target as it sought to find and attract pregnant shoppers.20\n\nThe first part is easy. Target has a baby shower registry in which pregnant women register\nfor baby gifts in advance of the birth of their children. These women are already Target\nshoppers, and they’ve effectively told the store that they are pregnant. But here is the statistical\ntwist: Target figured out that other women who demonstrate the same shopping patterns are\nprobably pregnant, too. For example, pregnant women often switch to unscented lotions. They\nbegin to buy vitamin supplements. They start buying extrabig bags of cotton balls. The Target\npredictive analytics gurus identified twenty-five products that together made possible a\n“pregnancy prediction score.” The whole point of this analysis was to send pregnant women\npregnancy-related coupons in hopes of hooking them as long-term Target shoppers. How good was the model? The New York Times Magazine reported a story about a man\nfrom Minneapolis who walked into a Target store and demanded to see a manager. The man\nwas irate that his high school daughter was being bombarded with pregnancy-related coupons\nfrom Target. “She’s still in high school and you’re sending her coupons for baby clothes and\ncribs? Are you trying to encourage her to get pregnant?” the man asked. The store manager apologized profusely. He even called the father several days later to\napologize again. Only this time, the man was less irate; it was his turn to be apologetic. “It\nturns out there’s been some activities in my house I haven’t been completely aware of,” the\nfather said. “She’s due in August.”\n\nThe Target statisticians had figured out that his daughter was pregnant before he did. That is their business . . . and also not their business. It can feel more than a little intrusive. For that reason, some companies now mask how much they know about you. For example, if\nyou are a pregnant woman in your second trimester, you may get some coupons in the mail for\ncribs and diapers---along with a discount on a riding lawn mower and a coupon for free\nbowling socks with the purchase of any pair of bowling shoes. To you, it just seems fortuitous\nthat the pregnancy-related coupons came in the mail along with the other junk.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nthe predictive analytics team at Target as it sought to find and attract pregnant shoppers.20\n\nThe first part is easy. Target has a baby shower registry in which pregnant women register\nfor baby gifts in advance of the birth of their children. These women are already Target\nshoppers, and they’ve effectively told the store that they are pregnant. But here is the statistical\ntwist: Target figured out that other women who demonstrate the same shopping patterns are\nprobably pregnant, too. For example, pregnant women often switch to unscented lotions. They\nbegin to buy vitamin supplements. They start buying extrabig bags of cotton balls. The Target\npredictive analytics gurus identified twenty-five products that together made possible a\n“pregnancy prediction score.” The whole point of this analysis was to send pregnant women\npregnancy-related coupons in hopes of hooking them as long-term Target shoppers. How good was the model? The New York Times Magazine reported a story about a man\nfrom Minneapolis who walked into a Target store and demanded to see a manager. The man\nwas irate that his high school daughter was being bombarded with pregnancy-related coupons\nfrom Target. “She’s still in high school and you’re sending her coupons for baby clothes and\ncribs? Are you trying to encourage her to get pregnant?” the man asked. The store manager apologized profusely. He even called the father several days later to\napologize again. Only this time, the man was less irate; it was his turn to be apologetic. “It\nturns out there’s been some activities in my house I haven’t been completely aware of,” the\nfather said. “She’s due in August.”\n\nThe Target statisticians had figured out that his daughter was pregnant before he did. That is their business . . . and also not their business. It can feel more than a little intrusive. For that reason, some companies now mask how much they know about you. For example, if\nyou are a pregnant woman in your second trimester, you may get some coupons in the mail for\ncribs and diapers---along with a discount on a riding lawn mower and a coupon for free\nbowling socks with the purchase of any pair of bowling shoes. To you, it just seems fortuitous\nthat the pregnancy-related coupons came in the mail along with the other junk.", "tokens": 490, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 182, "segment_id": "00182", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000323"}
{"type": "chunk", "text": "That is their business . . . and also not their business. It can feel more than a little intrusive. For that reason, some companies now mask how much they know about you. For example, if\nyou are a pregnant woman in your second trimester, you may get some coupons in the mail for\ncribs and diapers---along with a discount on a riding lawn mower and a coupon for free\nbowling socks with the purchase of any pair of bowling shoes. To you, it just seems fortuitous\nthat the pregnancy-related coupons came in the mail along with the other junk. In fact, the\ncompany knows that you don’t bowl or cut your own lawn; it’s merely covering its tracks so\nthat what it knows about you doesn’t seem so spooky. Facebook, a company with virtually no physical assets, has become one of the most\nvaluable companies in the world. To investors (as opposed to users), Facebook has one\nenormous asset: data. Investors don’t love Facebook because it allows them to reconnect with\ntheir prom dates. They love Facebook because every click of the mouse yields data about\nwhere users live, where they shop, what they buy, who they know, and how they spend their\ntime. To users, who are hoping to reconnect with their prom dates, the corporate data gathering\ncan overstep the boundaries of privacy. Chris Cox, Facebook’s vice president of product, told the New York Times , “The challenge\n\nof the information age is what to do with it.”21\n\nYep. And in the public arena, the marriage of data and technology gets even trickier. Cities\naround the world have installed thousands of security cameras in public places, some of which\nwill soon have facial recognition technology. Law enforcement authorities can follow any car", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThat is their business . . . and also not their business. It can feel more than a little intrusive. For that reason, some companies now mask how much they know about you. For example, if\nyou are a pregnant woman in your second trimester, you may get some coupons in the mail for\ncribs and diapers---along with a discount on a riding lawn mower and a coupon for free\nbowling socks with the purchase of any pair of bowling shoes. To you, it just seems fortuitous\nthat the pregnancy-related coupons came in the mail along with the other junk. In fact, the\ncompany knows that you don’t bowl or cut your own lawn; it’s merely covering its tracks so\nthat what it knows about you doesn’t seem so spooky. Facebook, a company with virtually no physical assets, has become one of the most\nvaluable companies in the world. To investors (as opposed to users), Facebook has one\nenormous asset: data. Investors don’t love Facebook because it allows them to reconnect with\ntheir prom dates. They love Facebook because every click of the mouse yields data about\nwhere users live, where they shop, what they buy, who they know, and how they spend their\ntime. To users, who are hoping to reconnect with their prom dates, the corporate data gathering\ncan overstep the boundaries of privacy. Chris Cox, Facebook’s vice president of product, told the New York Times , “The challenge\n\nof the information age is what to do with it.”21\n\nYep. And in the public arena, the marriage of data and technology gets even trickier. Cities\naround the world have installed thousands of security cameras in public places, some of which\nwill soon have facial recognition technology. Law enforcement authorities can follow any car", "tokens": 367, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 182, "segment_id": "00182", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000324"}
{"type": "chunk", "text": "anywhere it may go (and keep extensive records of where it has been) by attaching a global\npositioning device to the vehicle and then tracking it by satellite. Is this a cheap and efficient\nway to monitor potential criminal activity? Or is this the government using technology to\ntrample on our personal liberty? In 2012, the U.S. Supreme Court decided unanimously that it\nwas the latter, ruling that law enforcement officials can no longer attach tracking devices to\nprivate vehicles without a warrant.*\n\nMeanwhile, governments around the world maintain huge DNA databases that are a powerful\ntool for solving crimes. Whose DNA should be in the database? That of all convicted\ncriminals? That of every person who is arrested (whether or not eventually convicted)? Or a\nsample from every one of us? We are just beginning to wrestle with the issues that lie at the intersection of technology and\npersonal data---none of which were terribly relevant when government information was stored\nin dusty basement filing cabinets rather than in digital databases that are potentially searchable\nby anyone from anywhere. Statistics is more important than ever before because we have more\nmeaningful opportunities to make use of data. Yet the formulas will not tell us which uses of\ndata are appropriate and which are not. Math cannot supplant judgment. In that vein, let’s finish the book with some word association: fire, knives, automobiles, hair\nremoval cream. Each one of these things serves an important purpose. Each one makes our\nlives better. And each one can cause some serious problems when abused. Now you can add statistics to that list. Go forth and use data wisely and well! * I was ineligible for the 2010 prize since I was over forty. Also, I’d done nothing to deserve it. * The United States v. Jones.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nanywhere it may go (and keep extensive records of where it has been) by attaching a global\npositioning device to the vehicle and then tracking it by satellite. Is this a cheap and efficient\nway to monitor potential criminal activity? Or is this the government using technology to\ntrample on our personal liberty? In 2012, the U.S. Supreme Court decided unanimously that it\nwas the latter, ruling that law enforcement officials can no longer attach tracking devices to\nprivate vehicles without a warrant.*\n\nMeanwhile, governments around the world maintain huge DNA databases that are a powerful\ntool for solving crimes. Whose DNA should be in the database? That of all convicted\ncriminals? That of every person who is arrested (whether or not eventually convicted)? Or a\nsample from every one of us? We are just beginning to wrestle with the issues that lie at the intersection of technology and\npersonal data---none of which were terribly relevant when government information was stored\nin dusty basement filing cabinets rather than in digital databases that are potentially searchable\nby anyone from anywhere. Statistics is more important than ever before because we have more\nmeaningful opportunities to make use of data. Yet the formulas will not tell us which uses of\ndata are appropriate and which are not. Math cannot supplant judgment. In that vein, let’s finish the book with some word association: fire, knives, automobiles, hair\nremoval cream. Each one of these things serves an important purpose. Each one makes our\nlives better. And each one can cause some serious problems when abused. Now you can add statistics to that list. Go forth and use data wisely and well! * I was ineligible for the 2010 prize since I was over forty. Also, I’d done nothing to deserve it. * The United States v. Jones.", "tokens": 372, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 183, "segment_id": "00183", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000325"}
{"type": "chunk", "text": "Appendix\nStatistical software\n\nI suspect that you won’t be doing your statistical analysis with a pencil, paper, and calculator. Here is a quick tour of the software packages most commonly used for the kinds of tasks\ndescribed in this book. Microsoft Excel\nMicrosoft Excel is probably the most widely used program to compute simple statistics such\nas mean and standard deviation. Excel can also do basic regression analysis. Most computers\ncome loaded with Microsoft Office, so Excel is probably sitting on your desk right now. Excel\nis user-friendly compared with more sophisticated statistical software packages. The basic\nstatistical calculations can be done by means of the formula bar. Excel cannot perform some of the advanced tasks that more specialized programs can do. However, there are Excel extensions that you can buy (and some that you can download for\nfree) that will expand the program’s statistical capabilities. One huge advantage to Excel is that\nit offers simple ways to display two-dimensional data with visually appealing graphics. These\ngraphics can be easily dropped into Microsoft PowerPoint and Microsoft Word. Stata*\nStata is a statistical package used worldwide by research professionals; its interface has a\nserious, academic feel. Stata has a wide range of capabilities to do basic tasks, such as\ncreating data tables and calculating descriptive statistics. Of course, that is not why university\nprofessors and other serious researchers choose Stata. The software is designed to handle\nsophisticated statistical tests and data modeling that are far beyond the kinds of things\ndescribed in this book. Stata is a great fit for those who have a solid understanding of statistics (a basic\nunderstanding of programming also helps) and those who do not need fancy formatting---just\nthe answers to their statistical queries. Stata is not the best choice if your goal is to produce\nquick graphics from the data. Expert users say that Stata can produce nice graphics but that\nExcel is easier to use for that purpose. Stata offers several different stand-alone software packages. You can either license the\nproduct for a year (after a year, the software no longer works on your computer) or license it\nforever. One of the cheapest options is Stata/IC, which is designed for “students and\nresearchers with moderate-sized datasets.” There is a discount for users who are in the\neducation sector. Even then, a single-user annual license for Stata/IC is $295 and a perpetual\nlicense is $595.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nAppendix\nStatistical software\n\nI suspect that you won’t be doing your statistical analysis with a pencil, paper, and calculator. Here is a quick tour of the software packages most commonly used for the kinds of tasks\ndescribed in this book. Microsoft Excel\nMicrosoft Excel is probably the most widely used program to compute simple statistics such\nas mean and standard deviation. Excel can also do basic regression analysis. Most computers\ncome loaded with Microsoft Office, so Excel is probably sitting on your desk right now. Excel\nis user-friendly compared with more sophisticated statistical software packages. The basic\nstatistical calculations can be done by means of the formula bar. Excel cannot perform some of the advanced tasks that more specialized programs can do. However, there are Excel extensions that you can buy (and some that you can download for\nfree) that will expand the program’s statistical capabilities. One huge advantage to Excel is that\nit offers simple ways to display two-dimensional data with visually appealing graphics. These\ngraphics can be easily dropped into Microsoft PowerPoint and Microsoft Word. Stata*\nStata is a statistical package used worldwide by research professionals; its interface has a\nserious, academic feel. Stata has a wide range of capabilities to do basic tasks, such as\ncreating data tables and calculating descriptive statistics. Of course, that is not why university\nprofessors and other serious researchers choose Stata. The software is designed to handle\nsophisticated statistical tests and data modeling that are far beyond the kinds of things\ndescribed in this book. Stata is a great fit for those who have a solid understanding of statistics (a basic\nunderstanding of programming also helps) and those who do not need fancy formatting---just\nthe answers to their statistical queries. Stata is not the best choice if your goal is to produce\nquick graphics from the data. Expert users say that Stata can produce nice graphics but that\nExcel is easier to use for that purpose. Stata offers several different stand-alone software packages. You can either license the\nproduct for a year (after a year, the software no longer works on your computer) or license it\nforever. One of the cheapest options is Stata/IC, which is designed for “students and\nresearchers with moderate-sized datasets.” There is a discount for users who are in the\neducation sector. Even then, a single-user annual license for Stata/IC is $295 and a perpetual\nlicense is $595.", "tokens": 505, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 184, "segment_id": "00184", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000326"}
{"type": "chunk", "text": "Expert users say that Stata can produce nice graphics but that\nExcel is easier to use for that purpose. Stata offers several different stand-alone software packages. You can either license the\nproduct for a year (after a year, the software no longer works on your computer) or license it\nforever. One of the cheapest options is Stata/IC, which is designed for “students and\nresearchers with moderate-sized datasets.” There is a discount for users who are in the\neducation sector. Even then, a single-user annual license for Stata/IC is $295 and a perpetual\nlicense is $595. If you plan to launch a satellite to Mars and need to do some really serious", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nExpert users say that Stata can produce nice graphics but that\nExcel is easier to use for that purpose. Stata offers several different stand-alone software packages. You can either license the\nproduct for a year (after a year, the software no longer works on your computer) or license it\nforever. One of the cheapest options is Stata/IC, which is designed for “students and\nresearchers with moderate-sized datasets.” There is a discount for users who are in the\neducation sector. Even then, a single-user annual license for Stata/IC is $295 and a perpetual\nlicense is $595. If you plan to launch a satellite to Mars and need to do some really serious", "tokens": 144, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 184, "segment_id": "00184", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000327"}
{"type": "chunk", "text": "number crunching, you can look into more advanced Stata packages, which can cost thousands\nof dollars. SAS†\nSAS has a broad appeal not only to professional researchers but also to business analysts and\nengineers because of its broad range of analytical capabilities. SAS sells two different\nstatistical packages. The first is called SAS Analytics Pro, which can read data in virtually any\nformat and perform advanced data analysis. The software also has good data visualization\ntools, such as advanced mapping capabilities. It’s not cheap. Even for those in the education\nand government sectors, a single commercial or individual license for this package is $8,500,\nplus an annual license fee. The second SAS statistical package is SAS Visual Data Discovery. It has an easy-to-use\ninterface that requires no knowledge of coding or programming, while still providing advanced\ndata analysis capabilities. As its name suggests, this package is meant to allow the user to\neasily explore data with interactive visualization. You can also export the data animations into\npresentations, Web pages, and other documents. This one is not cheap either. A single\ncommercial or individual license for this package is $9,810, plus an annual license fee. SAS sells some specialized management tools, such as a product that uses statistics to\n\ndetect fraud and financial crimes. R\nThis may sound like a character in a James Bond movie. In fact, R is a popular statistical\npackage that is free or “open source.” It can be downloaded and easily installed on your\ncomputer in a matter of minutes. There is also an active “R community” that shares code and\ncan offer help and guidance when needed. Not only is R the cheapest option, but it is also one of the most malleable of all of the\npackages described here. Depending on your perspective, this flexibility is either frustrating or\none of R’s great assets. If you are new to statistical software, the program offers almost no\nstructure. The interface will not help you along much. On the other hand, programmers (and\neven people who have just a basic familiarity with coding principles) can find the lack of\nstructure liberating. Users are free to tell the program to do exactly what they want it to do,\nincluding having it work with outside programs. IBM SPSS*\nIBM SPSS has something for everyone, from hard-core statisticians to less statistically rugged\nbusiness analysts. IBM SPSS is good for beginners because it offers a menu-driven interface.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nnumber crunching, you can look into more advanced Stata packages, which can cost thousands\nof dollars. SAS†\nSAS has a broad appeal not only to professional researchers but also to business analysts and\nengineers because of its broad range of analytical capabilities. SAS sells two different\nstatistical packages. The first is called SAS Analytics Pro, which can read data in virtually any\nformat and perform advanced data analysis. The software also has good data visualization\ntools, such as advanced mapping capabilities. It’s not cheap. Even for those in the education\nand government sectors, a single commercial or individual license for this package is $8,500,\nplus an annual license fee. The second SAS statistical package is SAS Visual Data Discovery. It has an easy-to-use\ninterface that requires no knowledge of coding or programming, while still providing advanced\ndata analysis capabilities. As its name suggests, this package is meant to allow the user to\neasily explore data with interactive visualization. You can also export the data animations into\npresentations, Web pages, and other documents. This one is not cheap either. A single\ncommercial or individual license for this package is $9,810, plus an annual license fee. SAS sells some specialized management tools, such as a product that uses statistics to\n\ndetect fraud and financial crimes. R\nThis may sound like a character in a James Bond movie. In fact, R is a popular statistical\npackage that is free or “open source.” It can be downloaded and easily installed on your\ncomputer in a matter of minutes. There is also an active “R community” that shares code and\ncan offer help and guidance when needed. Not only is R the cheapest option, but it is also one of the most malleable of all of the\npackages described here. Depending on your perspective, this flexibility is either frustrating or\none of R’s great assets. If you are new to statistical software, the program offers almost no\nstructure. The interface will not help you along much. On the other hand, programmers (and\neven people who have just a basic familiarity with coding principles) can find the lack of\nstructure liberating. Users are free to tell the program to do exactly what they want it to do,\nincluding having it work with outside programs. IBM SPSS*\nIBM SPSS has something for everyone, from hard-core statisticians to less statistically rugged\nbusiness analysts. IBM SPSS is good for beginners because it offers a menu-driven interface.", "tokens": 508, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 185, "segment_id": "00185", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000328"}
{"type": "chunk", "text": "If you are new to statistical software, the program offers almost no\nstructure. The interface will not help you along much. On the other hand, programmers (and\neven people who have just a basic familiarity with coding principles) can find the lack of\nstructure liberating. Users are free to tell the program to do exactly what they want it to do,\nincluding having it work with outside programs. IBM SPSS*\nIBM SPSS has something for everyone, from hard-core statisticians to less statistically rugged\nbusiness analysts. IBM SPSS is good for beginners because it offers a menu-driven interface. IBM SPSS also offers a range of tools or “modules” that are designed to perform specific\nfunctions, such as IBM SPSS Forecasting, IBM SPSS Advanced Statistics, IBM SPSS\nVisualization Designer, and IBM SPSS Regression. The modules can be purchased\nindividually or combined into packages.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nIf you are new to statistical software, the program offers almost no\nstructure. The interface will not help you along much. On the other hand, programmers (and\neven people who have just a basic familiarity with coding principles) can find the lack of\nstructure liberating. Users are free to tell the program to do exactly what they want it to do,\nincluding having it work with outside programs. IBM SPSS*\nIBM SPSS has something for everyone, from hard-core statisticians to less statistically rugged\nbusiness analysts. IBM SPSS is good for beginners because it offers a menu-driven interface. IBM SPSS also offers a range of tools or “modules” that are designed to perform specific\nfunctions, such as IBM SPSS Forecasting, IBM SPSS Advanced Statistics, IBM SPSS\nVisualization Designer, and IBM SPSS Regression. The modules can be purchased\nindividually or combined into packages.", "tokens": 184, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 185, "segment_id": "00185", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000329"}
{"type": "chunk", "text": "The most basic package offered is IBM SPSS Statistics Standard Edition, which allows you\nto calculate simple statistics and perform basic data analysis, such as identifying trends and\nbuilding predictive models. A single fixed-term commercial license is $2,250. The premium\npackage, which includes most of the modules, is $6,750. Discounts are available for those who\nwork in the education sector. * See http:///. † See http:///technologies/analytics/statistics/. * See http://www-01.ibm.com/software/analytics/spss/products/statistics/.", "full_text": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nThe most basic package offered is IBM SPSS Statistics Standard Edition, which allows you\nto calculate simple statistics and perform basic data analysis, such as identifying trends and\nbuilding predictive models. A single fixed-term commercial license is $2,250. The premium\npackage, which includes most of the modules, is $6,750. Discounts are available for those who\nwork in the education sector. * See http:///. † See http:///technologies/analytics/statistics/. * See http://www-01.ibm.com/software/analytics/spss/products/statistics/.", "tokens": 115, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 186, "segment_id": "00186", "chapter_num": null, "chapter_title": null, "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000330"}
{"type": "chunk", "text": "Notes\n\nChapter 1: What’s the Point? 1 Central Intelligence Agency, The World Factbook,\n\nhttps://www.cia.gov/library/publications/the-world-factbook/. 2 Steve Lohr, “For Today’s Graduate, Just One Word: Statistics,” New York Times , August\n\n6, 2009. 3 Ibid. 4 Baseball-Reference.com, http:///players/m/mantlmi01.shtml. 5 Trip Gabriel, “Cheats Find an Adversary in Technology,” New York Times , December 28,\n\n2010. 6 Eyder Peralta, “Atlanta Man Wins Lottery for Second Time in Three Years,” NPR News\n\n(blog), November 29, 2011. 7 Alan B. Krueger, What Makes a Terrorist: Economics and the Roots of Terrorism\n\n(Princeton: Princeton University Press, 2008). Chapter 2: Descriptive Statistics\n1 U.S. Census Bureau, Current Population Survey, Annual Social and Economic\nSupplements, http://www.census.gov/hhes/www/income/data/historical/people/. 2 Malcolm Gladwell, “The Order of Things,” The New Yorker, February 14, 2011. 3 CIA, World Factbook, and United Nations Development Program, 2011 Human\n\nDevelopment Report, http://hdr.undp.org/en/statistics/. 4 Baseball-Reference.com. Chapter 3: Deceptive Description\n1 Robert Griffith, The Politics of Fear: Joseph R. McCarthy and the Senate, 2nd ed. (Amherst: University of Massachusetts Press, 1987), p. 49. 2 “Catching Up,” Economist, August 23, 2003. 3 Carl Bialik, “When the Median Doesn’t Mean What It Seems,” Wall Street Journal , May\n\n21--22, 2011. 4 Stephen Jay Gould, “The Median Isn’t the Message,” with a prefatory note and postscript\n\nby Steve Dunn, http://cancerguide.org/median_not_msg.html. 5 See http:///box-office/alltime. 6 Box Office Mojo (boxofficemojo.com), June 29, 2011. 7 Steve Patterson, “527% Tax Hike May Shock Some, But It’s Only About $5,” Chicago\n\nSun-Times, December 5, 2005.", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 1: What’s the Point? | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nNotes\n\nChapter 1: What’s the Point? 1 Central Intelligence Agency, The World Factbook,\n\nhttps://www.cia.gov/library/publications/the-world-factbook/. 2 Steve Lohr, “For Today’s Graduate, Just One Word: Statistics,” New York Times , August\n\n6, 2009. 3 Ibid. 4 Baseball-Reference.com, http:///players/m/mantlmi01.shtml. 5 Trip Gabriel, “Cheats Find an Adversary in Technology,” New York Times , December 28,\n\n2010. 6 Eyder Peralta, “Atlanta Man Wins Lottery for Second Time in Three Years,” NPR News\n\n(blog), November 29, 2011. 7 Alan B. Krueger, What Makes a Terrorist: Economics and the Roots of Terrorism\n\n(Princeton: Princeton University Press, 2008). Chapter 2: Descriptive Statistics\n1 U.S. Census Bureau, Current Population Survey, Annual Social and Economic\nSupplements, http://www.census.gov/hhes/www/income/data/historical/people/. 2 Malcolm Gladwell, “The Order of Things,” The New Yorker, February 14, 2011. 3 CIA, World Factbook, and United Nations Development Program, 2011 Human\n\nDevelopment Report, http://hdr.undp.org/en/statistics/. 4 Baseball-Reference.com. Chapter 3: Deceptive Description\n1 Robert Griffith, The Politics of Fear: Joseph R. McCarthy and the Senate, 2nd ed. (Amherst: University of Massachusetts Press, 1987), p. 49. 2 “Catching Up,” Economist, August 23, 2003. 3 Carl Bialik, “When the Median Doesn’t Mean What It Seems,” Wall Street Journal , May\n\n21--22, 2011. 4 Stephen Jay Gould, “The Median Isn’t the Message,” with a prefatory note and postscript\n\nby Steve Dunn, http://cancerguide.org/median_not_msg.html. 5 See http:///box-office/alltime. 6 Box Office Mojo (boxofficemojo.com), June 29, 2011. 7 Steve Patterson, “527% Tax Hike May Shock Some, But It’s Only About $5,” Chicago\n\nSun-Times, December 5, 2005.", "tokens": 497, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 187, "segment_id": "00187", "chapter_num": "1", "chapter_title": "What’s the Point?", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 1: What’s the Point? | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000331"}
{"type": "chunk", "text": "4 Stephen Jay Gould, “The Median Isn’t the Message,” with a prefatory note and postscript\n\nby Steve Dunn, http://cancerguide.org/median_not_msg.html. 5 See http:///box-office/alltime. 6 Box Office Mojo (boxofficemojo.com), June 29, 2011. 7 Steve Patterson, “527% Tax Hike May Shock Some, But It’s Only About $5,” Chicago\n\nSun-Times, December 5, 2005. 8 Rebecca Leung, “‘The ‘Texas Miracle’: 60 Minutes II Investigates Claims That Houston\n\nSchools Falsified Dropout Rates,” CBSNews.com, August 25, 2004. 9 Marc Santora, “Cardiologists Say Rankings Sway Surgical Decisions,” New York Times ,", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 1: What’s the Point? | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\n4 Stephen Jay Gould, “The Median Isn’t the Message,” with a prefatory note and postscript\n\nby Steve Dunn, http://cancerguide.org/median_not_msg.html. 5 See http:///box-office/alltime. 6 Box Office Mojo (boxofficemojo.com), June 29, 2011. 7 Steve Patterson, “527% Tax Hike May Shock Some, But It’s Only About $5,” Chicago\n\nSun-Times, December 5, 2005. 8 Rebecca Leung, “‘The ‘Texas Miracle’: 60 Minutes II Investigates Claims That Houston\n\nSchools Falsified Dropout Rates,” CBSNews.com, August 25, 2004. 9 Marc Santora, “Cardiologists Say Rankings Sway Surgical Decisions,” New York Times ,", "tokens": 172, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 187, "segment_id": "00187", "chapter_num": "1", "chapter_title": "What’s the Point?", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 1: What’s the Point? | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000332"}
{"type": "chunk", "text": "January 11, 2005. 10 Interview with National Public Radio, August 20, 2006,\n\nhttp:///templates/story/story.php?storyId=5678463. 11 See http:///education/articles/2010/08/17/frequently-asked-questionscollege-rankings#4. 12 Gladwell, “Order of Things.”\n13 Interview with National Public Radio, February 22, 2007,\n\nhttp:///templates/story/story.php?storyId=7383744. Chapter 4: Correlation\n1 College Board, FAQs,\n\nhttp:///prod_downloads/about/news_info/cbsenior/yr2010/correlationsof-predictors-with-first-year-college-grade-point-average.pdf. 2 College Board, 2011 College-Bound Seniors Total Group Profile Report,\n\nhttp://professionals.collegeboard.com/profdownload/cbs2011_total_group_report.pdf. 3 See http:///rules. Chapter 5: Basic Probability\n1 David A. Aaker, Managing Brand Equity: Capitalizing on the Value of a Brand Name\n\n(New York: Free Press, 1991). 2 Victor J. Tremblay and Carol Horton Tremblay, The U.S. Brewing Industry: Data and\n\nEconomic Analysis (Cambridge: MIT Press, 2005). 3 Australian Transport Safety Bureau Discussion Paper, “Cross Modal Safety Comparisons,”\n\nJanuary 1, 2005. 4 Marcia Dunn, “1 in 21 Trillion Chance Satellite Will Hit You,” Chicago Sun-Times,\n\nSeptember 21, 2011. 5 Steven D. Levitt and Stephen J. Dubner, Freakonomics: A Rogue Economist Explores the\n\nHidden Side of Everything (New York: William Morrow Paperbacks, 2009). 6 Garrick Blalock, Vrinda Kadiyali, and Daniel Simon, “Driving Fatalities after 9/11: A\n\nHidden Cost of Terrorism” (unpublished manuscript, December 5, 2005). 7 The general genetic testing information comes from Human Genome Project Information,\nForensics,\n\nDNA\nhttp://www.ornl.gov/sci/techresources/Human_Genome/elsi/forensics.shtml. 8 Jason Felch and Maura Dolan, “FBI Resists Scrutiny of ‘Matches,’ ” Los Angeles Times,\n\nJuly 20, 2008.", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 1: What’s the Point? | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nJanuary 11, 2005. 10 Interview with National Public Radio, August 20, 2006,\n\nhttp:///templates/story/story.php?storyId=5678463. 11 See http:///education/articles/2010/08/17/frequently-asked-questionscollege-rankings#4. 12 Gladwell, “Order of Things.”\n13 Interview with National Public Radio, February 22, 2007,\n\nhttp:///templates/story/story.php?storyId=7383744. Chapter 4: Correlation\n1 College Board, FAQs,\n\nhttp:///prod_downloads/about/news_info/cbsenior/yr2010/correlationsof-predictors-with-first-year-college-grade-point-average.pdf. 2 College Board, 2011 College-Bound Seniors Total Group Profile Report,\n\nhttp://professionals.collegeboard.com/profdownload/cbs2011_total_group_report.pdf. 3 See http:///rules. Chapter 5: Basic Probability\n1 David A. Aaker, Managing Brand Equity: Capitalizing on the Value of a Brand Name\n\n(New York: Free Press, 1991). 2 Victor J. Tremblay and Carol Horton Tremblay, The U.S. Brewing Industry: Data and\n\nEconomic Analysis (Cambridge: MIT Press, 2005). 3 Australian Transport Safety Bureau Discussion Paper, “Cross Modal Safety Comparisons,”\n\nJanuary 1, 2005. 4 Marcia Dunn, “1 in 21 Trillion Chance Satellite Will Hit You,” Chicago Sun-Times,\n\nSeptember 21, 2011. 5 Steven D. Levitt and Stephen J. Dubner, Freakonomics: A Rogue Economist Explores the\n\nHidden Side of Everything (New York: William Morrow Paperbacks, 2009). 6 Garrick Blalock, Vrinda Kadiyali, and Daniel Simon, “Driving Fatalities after 9/11: A\n\nHidden Cost of Terrorism” (unpublished manuscript, December 5, 2005). 7 The general genetic testing information comes from Human Genome Project Information,\nForensics,\n\nDNA\nhttp://www.ornl.gov/sci/techresources/Human_Genome/elsi/forensics.shtml. 8 Jason Felch and Maura Dolan, “FBI Resists Scrutiny of ‘Matches,’ ” Los Angeles Times,\n\nJuly 20, 2008.", "tokens": 506, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 188, "segment_id": "00188", "chapter_num": "1", "chapter_title": "What’s the Point?", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 1: What’s the Point? | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000333"}
{"type": "chunk", "text": "7 The general genetic testing information comes from Human Genome Project Information,\nForensics,\n\nDNA\nhttp://www.ornl.gov/sci/techresources/Human_Genome/elsi/forensics.shtml. 8 Jason Felch and Maura Dolan, “FBI Resists Scrutiny of ‘Matches,’ ” Los Angeles Times,\n\nJuly 20, 2008. 9 David Leonhardt, “In Football, 6 + 2 Often Equals 6,” New York Times, January 16, 2000. 10 Roger Lowenstein, “The War on Insider Trading: Market Beaters Beware,” New York\n\nTimes Magazine, September 22, 2011. 11 Erica Goode, “Sending the Police before There’s a Crime,” New York Times , August 15,\n\n2011. 12 The insurance risk data come from all of the following: “Teen Drivers,” Insurance", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 1: What’s the Point? | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\n7 The general genetic testing information comes from Human Genome Project Information,\nForensics,\n\nDNA\nhttp://www.ornl.gov/sci/techresources/Human_Genome/elsi/forensics.shtml. 8 Jason Felch and Maura Dolan, “FBI Resists Scrutiny of ‘Matches,’ ” Los Angeles Times,\n\nJuly 20, 2008. 9 David Leonhardt, “In Football, 6 + 2 Often Equals 6,” New York Times, January 16, 2000. 10 Roger Lowenstein, “The War on Insider Trading: Market Beaters Beware,” New York\n\nTimes Magazine, September 22, 2011. 11 Erica Goode, “Sending the Police before There’s a Crime,” New York Times , August 15,\n\n2011. 12 The insurance risk data come from all of the following: “Teen Drivers,” Insurance", "tokens": 191, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 188, "segment_id": "00188", "chapter_num": "1", "chapter_title": "What’s the Point?", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 1: What’s the Point? | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000334"}
{"type": "chunk", "text": "Information Institute, March 2012; “Texting Laws and Collision Claim Frequencies,”\nInsurance Institute for Highway Safety, September 2010; “Hot Wheels,” National Insurance\nCrime Bureau, August 2, 2011. 13 Charles Duhigg, “What Does Your Credit Card Company Know about You?” New York\n\nTimes Magazine, May 12, 2009. Chapter 51⁄2 : The Monty Hall Problem\n1 John Tierney, “And behind Door No. 1, a Fatal Flaw,” New York Times, April 8, 2008. 2 Leonard Mlodinow, The Drunkard’s Walk: How Randomness Rules Our Lives (New York:\n\nVintage Books, 2009). Chapter 6: Problems with Probability\n1 Joe Nocera, “Risk Mismanagement,” New York Times Magazine, January 2, 2009. 2 Robert E. Hall, “The Long Slump,” American Economic Review 101, no. 2 (April 2011):\n\n431--69. 3 Alan Greenspan, Testimony before the House Committee on Government Oversight and\n\nReform, October 23, 2008. 4 Hank Paulson, Speech at Dartmouth College, Hanover, NH, August 11, 2011. 5 “The Probability of Injustice,” Economist, January 22, 2004. 6 Thomas Gilovich, Robert Vallone, and Amos Tversky, “The Hot Hand in Basketball: On\nthe Misperception of Random Sequences,” Cognitive Psychology 17, no. 3 (1985): 295--\n314. 7 Ulrike Malmendier and Geoffrey Tate, “Superstar CEOs,” Quarterly Journal of Economics\n\n124, no. 4 (November 2009): 1593--638. 8 “The Price of Equality,” Economist, November 15, 2003. Chapter 7: The Importance of Data\n1 Benedict Carey, “Learning from the Spurned and Tipsy Fruit Fly,” New York Times , March\n\n15, 2012. 2 Cynthia Crossen, “Fiasco in 1936 Survey Brought ‘Science’ to Election Polling,” Wall\n\nStreet Journal, October 2, 2006. 3 Tara Parker-Pope, “Chances of Sexual Recovery Vary Widely after Prostate Cancer,” New\n\nYork Times, September 21, 2011.", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 1: What’s the Point? | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nInformation Institute, March 2012; “Texting Laws and Collision Claim Frequencies,”\nInsurance Institute for Highway Safety, September 2010; “Hot Wheels,” National Insurance\nCrime Bureau, August 2, 2011. 13 Charles Duhigg, “What Does Your Credit Card Company Know about You?” New York\n\nTimes Magazine, May 12, 2009. Chapter 51⁄2 : The Monty Hall Problem\n1 John Tierney, “And behind Door No. 1, a Fatal Flaw,” New York Times, April 8, 2008. 2 Leonard Mlodinow, The Drunkard’s Walk: How Randomness Rules Our Lives (New York:\n\nVintage Books, 2009). Chapter 6: Problems with Probability\n1 Joe Nocera, “Risk Mismanagement,” New York Times Magazine, January 2, 2009. 2 Robert E. Hall, “The Long Slump,” American Economic Review 101, no. 2 (April 2011):\n\n431--69. 3 Alan Greenspan, Testimony before the House Committee on Government Oversight and\n\nReform, October 23, 2008. 4 Hank Paulson, Speech at Dartmouth College, Hanover, NH, August 11, 2011. 5 “The Probability of Injustice,” Economist, January 22, 2004. 6 Thomas Gilovich, Robert Vallone, and Amos Tversky, “The Hot Hand in Basketball: On\nthe Misperception of Random Sequences,” Cognitive Psychology 17, no. 3 (1985): 295--\n314. 7 Ulrike Malmendier and Geoffrey Tate, “Superstar CEOs,” Quarterly Journal of Economics\n\n124, no. 4 (November 2009): 1593--638. 8 “The Price of Equality,” Economist, November 15, 2003. Chapter 7: The Importance of Data\n1 Benedict Carey, “Learning from the Spurned and Tipsy Fruit Fly,” New York Times , March\n\n15, 2012. 2 Cynthia Crossen, “Fiasco in 1936 Survey Brought ‘Science’ to Election Polling,” Wall\n\nStreet Journal, October 2, 2006. 3 Tara Parker-Pope, “Chances of Sexual Recovery Vary Widely after Prostate Cancer,” New\n\nYork Times, September 21, 2011.", "tokens": 514, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 189, "segment_id": "00189", "chapter_num": "1", "chapter_title": "What’s the Point?", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 1: What’s the Point? | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000335"}
{"type": "chunk", "text": "8 “The Price of Equality,” Economist, November 15, 2003. Chapter 7: The Importance of Data\n1 Benedict Carey, “Learning from the Spurned and Tipsy Fruit Fly,” New York Times , March\n\n15, 2012. 2 Cynthia Crossen, “Fiasco in 1936 Survey Brought ‘Science’ to Election Polling,” Wall\n\nStreet Journal, October 2, 2006. 3 Tara Parker-Pope, “Chances of Sexual Recovery Vary Widely after Prostate Cancer,” New\n\nYork Times, September 21, 2011. 4 Benedict Carey, “Researchers Find Bias in Drug Trial Reporting,” New York Times ,\n\nJanuary 17, 2008. 5 Siddhartha Mukherjee, “Do Cellphones Cause Brain Cancer?” New York Times , April 17,\n\n2011. 6 Gary Taubes, “Do We Really Know What Makes Us Healthy?” New York Times ,\n\nSeptember 16, 2007. Chapter 8: The Central Limit Theorem", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 1: What’s the Point? | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\n8 “The Price of Equality,” Economist, November 15, 2003. Chapter 7: The Importance of Data\n1 Benedict Carey, “Learning from the Spurned and Tipsy Fruit Fly,” New York Times , March\n\n15, 2012. 2 Cynthia Crossen, “Fiasco in 1936 Survey Brought ‘Science’ to Election Polling,” Wall\n\nStreet Journal, October 2, 2006. 3 Tara Parker-Pope, “Chances of Sexual Recovery Vary Widely after Prostate Cancer,” New\n\nYork Times, September 21, 2011. 4 Benedict Carey, “Researchers Find Bias in Drug Trial Reporting,” New York Times ,\n\nJanuary 17, 2008. 5 Siddhartha Mukherjee, “Do Cellphones Cause Brain Cancer?” New York Times , April 17,\n\n2011. 6 Gary Taubes, “Do We Really Know What Makes Us Healthy?” New York Times ,\n\nSeptember 16, 2007. Chapter 8: The Central Limit Theorem", "tokens": 218, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 189, "segment_id": "00189", "chapter_num": "1", "chapter_title": "What’s the Point?", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 1: What’s the Point? | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000336"}
{"type": "chunk", "text": "1 U.S. Census Bureau. Chapter 9: Inference\n1 John Friedman, Out of the Blue: A History of Lightning: Science, Superstition, and\n\nAmazing Stories of Survival (New York: Delacorte Press, 2008). 2 “Low Marks All Round,” Economist, July 14, 2011. 3 Trip Gabriel and Matt Richtel, “Inflating the Software Report Card,” New York Times ,\n\nOctober 9, 2011. 4 Jennifer Corbett Dooren, “Link in Autism, Brain Size,” Wall Street Journal, May 3, 2011. 5 Heather Cody Hazlett et al., “Early Brain Overgrowth in Autism Associated with an\nIncrease in Cortical Surface Area before Age 2 Years,” Archives of General Psychiatry 68,\nno. 5 (May 2011): 467--76. 6 Benedict Carey, “Top Journal Plans to Publish a Paper on ESP, and Psychologists Sense\n\nOutrage,” New York Times, January 6, 2011. Chapter 10: Polling\n1 Jeff Zeleny and Megan Thee-Brenan, “New Poll Finds a Deep Distrust of Government,”\n\nNew York Times, October 26, 2011. 2 Lydia Saad, “Americans Hold Firm to Support for Death Penalty,” Gallup.com, November\n\n17, 2008. 3 Phone interview with Frank Newport, November 30, 2011. 4 Stanley Presser, “Sex, Samples, and Response Errors,” Contemporary Sociology 24, no. 4\n\n(July 1995): 296--98. 5 The results were published in two different formats, one more academic than the other. Edward O. Lauman, The Social Organization of Sexuality: Sexual Practices in the United\nStates (Chicago: University of Chicago Press, 1994); Robert T. Michael, John H. Gagnon,\nEdward O. Laumann, and Gina Kolata, Sex in America: A Definitive Survey (New York:\nGrand Central Publishing, 1995). 6 Kaye Wellings, book review in British Medical Journal 310, no. 6978 (February 25, 1995):\n\n540. 7 John DeLamater, “The NORC Sex Survey,” Science 270, no. 5235 (October 20, 1995):\n\n501.", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\n1 U.S. Census Bureau. Chapter 9: Inference\n1 John Friedman, Out of the Blue: A History of Lightning: Science, Superstition, and\n\nAmazing Stories of Survival (New York: Delacorte Press, 2008). 2 “Low Marks All Round,” Economist, July 14, 2011. 3 Trip Gabriel and Matt Richtel, “Inflating the Software Report Card,” New York Times ,\n\nOctober 9, 2011. 4 Jennifer Corbett Dooren, “Link in Autism, Brain Size,” Wall Street Journal, May 3, 2011. 5 Heather Cody Hazlett et al., “Early Brain Overgrowth in Autism Associated with an\nIncrease in Cortical Surface Area before Age 2 Years,” Archives of General Psychiatry 68,\nno. 5 (May 2011): 467--76. 6 Benedict Carey, “Top Journal Plans to Publish a Paper on ESP, and Psychologists Sense\n\nOutrage,” New York Times, January 6, 2011. Chapter 10: Polling\n1 Jeff Zeleny and Megan Thee-Brenan, “New Poll Finds a Deep Distrust of Government,”\n\nNew York Times, October 26, 2011. 2 Lydia Saad, “Americans Hold Firm to Support for Death Penalty,” Gallup.com, November\n\n17, 2008. 3 Phone interview with Frank Newport, November 30, 2011. 4 Stanley Presser, “Sex, Samples, and Response Errors,” Contemporary Sociology 24, no. 4\n\n(July 1995): 296--98. 5 The results were published in two different formats, one more academic than the other. Edward O. Lauman, The Social Organization of Sexuality: Sexual Practices in the United\nStates (Chicago: University of Chicago Press, 1994); Robert T. Michael, John H. Gagnon,\nEdward O. Laumann, and Gina Kolata, Sex in America: A Definitive Survey (New York:\nGrand Central Publishing, 1995). 6 Kaye Wellings, book review in British Medical Journal 310, no. 6978 (February 25, 1995):\n\n540. 7 John DeLamater, “The NORC Sex Survey,” Science 270, no. 5235 (October 20, 1995):\n\n501.", "tokens": 504, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 190, "segment_id": "00190", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000337"}
{"type": "chunk", "text": "Michael, John H. Gagnon,\nEdward O. Laumann, and Gina Kolata, Sex in America: A Definitive Survey (New York:\nGrand Central Publishing, 1995). 6 Kaye Wellings, book review in British Medical Journal 310, no. 6978 (February 25, 1995):\n\n540. 7 John DeLamater, “The NORC Sex Survey,” Science 270, no. 5235 (October 20, 1995):\n\n501. 8 Presser, “Sex, Samples, and Response Errors.”\n\nChapter 11: Regression Analysis\n1 Marianne Bertrand, Claudia Goldin, and Lawrence F. Katz, “Dynamics of the Gender Gap\nfor Young Professionals in the Corporate and Financial Sectors,” NBER Working Paper\n14681, January 2009. 2 M. G. Marmot, Geoffrey Rose, M. Shipley, and P. J. S. Hamilton, “Employment Grade and\nCoronary Heart Disease in British Civil Servants,” Journal of Epidemiology and\nCommunity Health 32, no. 4 (1978): 244--49.", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nMichael, John H. Gagnon,\nEdward O. Laumann, and Gina Kolata, Sex in America: A Definitive Survey (New York:\nGrand Central Publishing, 1995). 6 Kaye Wellings, book review in British Medical Journal 310, no. 6978 (February 25, 1995):\n\n540. 7 John DeLamater, “The NORC Sex Survey,” Science 270, no. 5235 (October 20, 1995):\n\n501. 8 Presser, “Sex, Samples, and Response Errors.”\n\nChapter 11: Regression Analysis\n1 Marianne Bertrand, Claudia Goldin, and Lawrence F. Katz, “Dynamics of the Gender Gap\nfor Young Professionals in the Corporate and Financial Sectors,” NBER Working Paper\n14681, January 2009. 2 M. G. Marmot, Geoffrey Rose, M. Shipley, and P. J. S. Hamilton, “Employment Grade and\nCoronary Heart Disease in British Civil Servants,” Journal of Epidemiology and\nCommunity Health 32, no. 4 (1978): 244--49.", "tokens": 245, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 190, "segment_id": "00190", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000338"}
{"type": "chunk", "text": "3 Hans Bosma, Michael G. Marmot, Harry Hemingway, Amanda C. Nicholson, Eric Brunner,\nand Stephen A. Stansfeld, “Low Job Control and Risk of Coronary Heart Disease in\nWhitehall II (Prospective Cohort) Study,” British Medical Journal 314, no. 7080 (February\n22, 1997): 558--65. 4 Peter L. Schnall, Paul A. Landesbergis, and Dean Baker, “Job Strain and Cardiovascular\n\nDisease,” Annual Review of Public Health 15 (1994): 381--411. 5 M. G. Marmot, H. Bosma, H. Hemingway, E. Brunner, and S. Stansfeld, “Contribution of\nJob Control and Other Risk Factors to Social Variations in Coronary Heart Disease\nIncidence,” Lancet 350 (July 26, 1997): 235--39. Chapter 12: Common Regression Mistakes\n1 Gary Taubes, “Do We Really Know What Makes Us Healthy?” New York Times\n\nMagazine, September 16, 2007. 2 “Vive la Difference,” Economist, October 20, 2001. 3 Taubes, “Do We Really Know?”\n4 College Board, 2011 College-Bound Seniors Total Group Profile Report,\n\nhttp://professionals.collegeboard.com/profdownload/cbs2011_total_group_report.pdf. 5 Hans Bosma et al., “Low Job Control and Risk of Coronary Heart Disease in Whitehall II\n(Prospective Cohort) Study,” British Medical Journal 314, no. 7080 (February 22, 1997):\n564. 6 Taubes, “Do We Really Know?”\n7 Gautam Naik, “Scientists’ Elusive Goal: Reproducing Study Results,” Wall Street Journal ,\n\nDecember 2, 2011. 8 John P. A. Ioannidis, “Contradicted and Initially Stronger Effects in Highly Cited Clinical\nResearch,” Journal of the American Medical Association 294, no. 2 (July 13, 2005): 218--\n28. 9 “Scientific Accuracy and Statistics,” Economist, September 1, 2005. Chapter 13: Program Evaluation\n1 Gina Kolata, “Arthritis Surgery in Ailing Knees Is Cited as Sham,” New York Times , July\n\n11, 2002.", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\n3 Hans Bosma, Michael G. Marmot, Harry Hemingway, Amanda C. Nicholson, Eric Brunner,\nand Stephen A. Stansfeld, “Low Job Control and Risk of Coronary Heart Disease in\nWhitehall II (Prospective Cohort) Study,” British Medical Journal 314, no. 7080 (February\n22, 1997): 558--65. 4 Peter L. Schnall, Paul A. Landesbergis, and Dean Baker, “Job Strain and Cardiovascular\n\nDisease,” Annual Review of Public Health 15 (1994): 381--411. 5 M. G. Marmot, H. Bosma, H. Hemingway, E. Brunner, and S. Stansfeld, “Contribution of\nJob Control and Other Risk Factors to Social Variations in Coronary Heart Disease\nIncidence,” Lancet 350 (July 26, 1997): 235--39. Chapter 12: Common Regression Mistakes\n1 Gary Taubes, “Do We Really Know What Makes Us Healthy?” New York Times\n\nMagazine, September 16, 2007. 2 “Vive la Difference,” Economist, October 20, 2001. 3 Taubes, “Do We Really Know?”\n4 College Board, 2011 College-Bound Seniors Total Group Profile Report,\n\nhttp://professionals.collegeboard.com/profdownload/cbs2011_total_group_report.pdf. 5 Hans Bosma et al., “Low Job Control and Risk of Coronary Heart Disease in Whitehall II\n(Prospective Cohort) Study,” British Medical Journal 314, no. 7080 (February 22, 1997):\n564. 6 Taubes, “Do We Really Know?”\n7 Gautam Naik, “Scientists’ Elusive Goal: Reproducing Study Results,” Wall Street Journal ,\n\nDecember 2, 2011. 8 John P. A. Ioannidis, “Contradicted and Initially Stronger Effects in Highly Cited Clinical\nResearch,” Journal of the American Medical Association 294, no. 2 (July 13, 2005): 218--\n28. 9 “Scientific Accuracy and Statistics,” Economist, September 1, 2005. Chapter 13: Program Evaluation\n1 Gina Kolata, “Arthritis Surgery in Ailing Knees Is Cited as Sham,” New York Times , July\n\n11, 2002.", "tokens": 520, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 191, "segment_id": "00191", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000339"}
{"type": "chunk", "text": "8 John P. A. Ioannidis, “Contradicted and Initially Stronger Effects in Highly Cited Clinical\nResearch,” Journal of the American Medical Association 294, no. 2 (July 13, 2005): 218--\n28. 9 “Scientific Accuracy and Statistics,” Economist, September 1, 2005. Chapter 13: Program Evaluation\n1 Gina Kolata, “Arthritis Surgery in Ailing Knees Is Cited as Sham,” New York Times , July\n\n11, 2002. 2 Benedict Carey, “Long-Awaited Medical Study Questions the Power of Prayer,” New York\n\nTimes, March 31, 2006. 3 Diane Whitmore Schanzenbach, “What Have Researchers Learned from Project STAR?”\n\nHarris School Working Paper, August 2006. 4 Gina Kolata, “A Surprising Secret to a Long Life: Stay in School,” New York Times ,\n\nJanuary 3, 2007. 5 Adriana Lleras-Muney, “The Relationship between Education and Adult Mortality in the\n\nUnited States,” Review of Economic Studies 72, no. 1 (2005): 189--221. 6 Kurt Badenhausen, “Top Colleges for Getting Rich,” Forbes.com, July 30, 2008. 7 Stacy Berg Dale and Alan Krueger, “Estimating the Payoff to Attending a More Selective", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\n8 John P. A. Ioannidis, “Contradicted and Initially Stronger Effects in Highly Cited Clinical\nResearch,” Journal of the American Medical Association 294, no. 2 (July 13, 2005): 218--\n28. 9 “Scientific Accuracy and Statistics,” Economist, September 1, 2005. Chapter 13: Program Evaluation\n1 Gina Kolata, “Arthritis Surgery in Ailing Knees Is Cited as Sham,” New York Times , July\n\n11, 2002. 2 Benedict Carey, “Long-Awaited Medical Study Questions the Power of Prayer,” New York\n\nTimes, March 31, 2006. 3 Diane Whitmore Schanzenbach, “What Have Researchers Learned from Project STAR?”\n\nHarris School Working Paper, August 2006. 4 Gina Kolata, “A Surprising Secret to a Long Life: Stay in School,” New York Times ,\n\nJanuary 3, 2007. 5 Adriana Lleras-Muney, “The Relationship between Education and Adult Mortality in the\n\nUnited States,” Review of Economic Studies 72, no. 1 (2005): 189--221. 6 Kurt Badenhausen, “Top Colleges for Getting Rich,” Forbes.com, July 30, 2008. 7 Stacy Berg Dale and Alan Krueger, “Estimating the Payoff to Attending a More Selective", "tokens": 300, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 191, "segment_id": "00191", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000340"}
{"type": "chunk", "text": "College: An Application of Selection on Observables and Unobservables,” Quarterly\nJournal of Economics 117, no. 4 (November 2002): 1491--527. 8 Alan B. Krueger, “Children Smart Enough to Get into Elite Schools May Not Need to\n\nBother,” New York Times, April 27, 2000. 9 Randi Hjalmarsson, “Juvenile Jails: A Path to the Straight and Narrow or to Hardened\n\nCriminality?” Journal of Law and Economics 52, no. 4 (November 2009): 779--809. Conclusion\n1 James Surowiecki, “A Billion Prices Now,” The New Yorker, May 30, 2011. 2 Malcolm Gladwell, “Offensive Play,” The New Yorker, October 19, 2009. 3 Ken Belson, “N.F.L. Roundup; Concussion Suits Joined,” New York Times , February 1,\n\n2012. 4 Shirley S. Wang, “Autism Diagnoses Up Sharply in U.S.,” Wall Street Journal , March 30,\n\n2012. 5 Catherine Rice, “Prevalence of Autism Spectrum Disorders,” Autism and Developmental\n\nDisabilities Monitoring Network, Centers for Disease Control and Prevention, 2006,\nhttp://www.cdc.gov/mmwr/preview/mmwrhtml/ss5810a1.htm. 6 Alan Zarembo, “Autism Boom: An Epidemic of Disease or of Discovery?” latimes.com,\n\nDecember 11, 2011. 7 Michael Ganz, “The Lifetime Distribution of the Incremental Societal Costs of Autism,”\n\nArchives of Pediatrics & Adolescent Medicine 161, no. 4 (April 2007): 343--49. 8 Gardiner Harris and Anahad O’Connor, “On Autism’s Cause, It’s Parents vs. Research,”\n\nNew York Times, June 25, 2005. 9 Julie Steenhuysen, “Study Turns Up 10 Autism Clusters in California,” Yahoo! News ,\n\nJanuary 5, 2012. 10 Joachim Hallmayer et al., “Genetic Heritability and Shared Environmental Factors among\nTwin Pairs with Autism,” Archives of General Psychiatry 68, no. 11 (November 2011):\n1095--102. 11 Gardiner Harris and Anahad O’Connor, “On Autism’s Cause, It’s Parents vs.", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nCollege: An Application of Selection on Observables and Unobservables,” Quarterly\nJournal of Economics 117, no. 4 (November 2002): 1491--527. 8 Alan B. Krueger, “Children Smart Enough to Get into Elite Schools May Not Need to\n\nBother,” New York Times, April 27, 2000. 9 Randi Hjalmarsson, “Juvenile Jails: A Path to the Straight and Narrow or to Hardened\n\nCriminality?” Journal of Law and Economics 52, no. 4 (November 2009): 779--809. Conclusion\n1 James Surowiecki, “A Billion Prices Now,” The New Yorker, May 30, 2011. 2 Malcolm Gladwell, “Offensive Play,” The New Yorker, October 19, 2009. 3 Ken Belson, “N.F.L. Roundup; Concussion Suits Joined,” New York Times , February 1,\n\n2012. 4 Shirley S. Wang, “Autism Diagnoses Up Sharply in U.S.,” Wall Street Journal , March 30,\n\n2012. 5 Catherine Rice, “Prevalence of Autism Spectrum Disorders,” Autism and Developmental\n\nDisabilities Monitoring Network, Centers for Disease Control and Prevention, 2006,\nhttp://www.cdc.gov/mmwr/preview/mmwrhtml/ss5810a1.htm. 6 Alan Zarembo, “Autism Boom: An Epidemic of Disease or of Discovery?” latimes.com,\n\nDecember 11, 2011. 7 Michael Ganz, “The Lifetime Distribution of the Incremental Societal Costs of Autism,”\n\nArchives of Pediatrics & Adolescent Medicine 161, no. 4 (April 2007): 343--49. 8 Gardiner Harris and Anahad O’Connor, “On Autism’s Cause, It’s Parents vs. Research,”\n\nNew York Times, June 25, 2005. 9 Julie Steenhuysen, “Study Turns Up 10 Autism Clusters in California,” Yahoo! News ,\n\nJanuary 5, 2012. 10 Joachim Hallmayer et al., “Genetic Heritability and Shared Environmental Factors among\nTwin Pairs with Autism,” Archives of General Psychiatry 68, no. 11 (November 2011):\n1095--102. 11 Gardiner Harris and Anahad O’Connor, “On Autism’s Cause, It’s Parents vs.", "tokens": 525, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 192, "segment_id": "00192", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000341"}
{"type": "chunk", "text": "Research,”\n\nNew York Times, June 25, 2005. 9 Julie Steenhuysen, “Study Turns Up 10 Autism Clusters in California,” Yahoo! News ,\n\nJanuary 5, 2012. 10 Joachim Hallmayer et al., “Genetic Heritability and Shared Environmental Factors among\nTwin Pairs with Autism,” Archives of General Psychiatry 68, no. 11 (November 2011):\n1095--102. 11 Gardiner Harris and Anahad O’Connor, “On Autism’s Cause, It’s Parents vs. Research,”\n\nNew York Times, June 25, 2005. 12 Fernanda Santos and Robert Gebeloff, “Teacher Quality Widely Diffused, Ratings\n\nIndicate,” New York Times, February 24, 2012. 13 Winnie Hu, “With Teacher Ratings Set to Be Released, Union Opens Campaign to\n\nDiscredit Them,” New York Times, February 23, 2012. 14 T. Schall and G. Smith, “Do Baseball Players Regress to the Mean?” American Statistician\n\n54 (2000): 231--35. 15 Scott E. Carrell and James E. West, “Does Professor Quality Matter? Evidence from\nRandom Assignment of Students to Professors,” National Bureau of Economic Research\nWorking Paper 14081, June 2008. 16 Esther Duflo and Rema Hanna, “Monitoring Works: Getting Teachers to Come to School,”\n\nNational Bureau of Economic Research Working Paper 11880, December 2005.", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nResearch,”\n\nNew York Times, June 25, 2005. 9 Julie Steenhuysen, “Study Turns Up 10 Autism Clusters in California,” Yahoo! News ,\n\nJanuary 5, 2012. 10 Joachim Hallmayer et al., “Genetic Heritability and Shared Environmental Factors among\nTwin Pairs with Autism,” Archives of General Psychiatry 68, no. 11 (November 2011):\n1095--102. 11 Gardiner Harris and Anahad O’Connor, “On Autism’s Cause, It’s Parents vs. Research,”\n\nNew York Times, June 25, 2005. 12 Fernanda Santos and Robert Gebeloff, “Teacher Quality Widely Diffused, Ratings\n\nIndicate,” New York Times, February 24, 2012. 13 Winnie Hu, “With Teacher Ratings Set to Be Released, Union Opens Campaign to\n\nDiscredit Them,” New York Times, February 23, 2012. 14 T. Schall and G. Smith, “Do Baseball Players Regress to the Mean?” American Statistician\n\n54 (2000): 231--35. 15 Scott E. Carrell and James E. West, “Does Professor Quality Matter? Evidence from\nRandom Assignment of Students to Professors,” National Bureau of Economic Research\nWorking Paper 14081, June 2008. 16 Esther Duflo and Rema Hanna, “Monitoring Works: Getting Teachers to Come to School,”\n\nNational Bureau of Economic Research Working Paper 11880, December 2005.", "tokens": 334, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 192, "segment_id": "00192", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000342"}
{"type": "chunk", "text": "17 Christopher Udry, “Esther Duflo: 2010 John Bates Clark Medalist,” Journal of Economic\n\nPerspectives 25, no. 3 (Summer 2011): 197--216. 18 Esther Duflo, Michael Kremer, and Jonathan Robinson, “Nudging Farmers to Use Fertilizer:\nTheory and Experimental Evidence from Kenya,” National Bureau of Economic Research\nWorking Paper 15131, July 2009. 19 Esther Duflo and Christopher Udry, “Intrahousehold Resource Allocation in CÔte d’Ivoire:\nSocial Norms, Separate Accounts and Consumption Choices,” Working Paper, December\n21, 2004. 20 Charles Duhigg, “How Companies Learn Your Secrets,” New York Times Magazine ,\n\nFebruary 16, 2012. 21 Somini Sengupta and Evelyn M. Rusli, “Personal Data’s Value? Facebook Set to Find\n\nOut,” New York Times, February 1, 2012.", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\n17 Christopher Udry, “Esther Duflo: 2010 John Bates Clark Medalist,” Journal of Economic\n\nPerspectives 25, no. 3 (Summer 2011): 197--216. 18 Esther Duflo, Michael Kremer, and Jonathan Robinson, “Nudging Farmers to Use Fertilizer:\nTheory and Experimental Evidence from Kenya,” National Bureau of Economic Research\nWorking Paper 15131, July 2009. 19 Esther Duflo and Christopher Udry, “Intrahousehold Resource Allocation in CÔte d’Ivoire:\nSocial Norms, Separate Accounts and Consumption Choices,” Working Paper, December\n21, 2004. 20 Charles Duhigg, “How Companies Learn Your Secrets,” New York Times Magazine ,\n\nFebruary 16, 2012. 21 Somini Sengupta and Evelyn M. Rusli, “Personal Data’s Value? Facebook Set to Find\n\nOut,” New York Times, February 1, 2012.", "tokens": 212, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 193, "segment_id": "00193", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000343"}
{"type": "chunk", "text": "Acknowledgments\n\nThis book was conceived as an homage to an earlier W. W. Norton classic, How to Lie with\n\nStatistics by Darrell Huff, which was written in the 1950s and has sold over a million copies. That book, like this one, was written to demystify statistics and persuade everyday readers that\nwhat they don’t understand about the numbers behind the headlines can hurt them. I hope that\nI’ve done justice to Mr. Huff’s classic. In any event, I would be delighted to have sold a\nmillion copies fifty years from now! I am continually grateful to W. W. Norton, and to Drake McFeely in particular, for enabling\nme to write books that address significant topics in a way that is understandable to lay readers. Drake has been a great friend and supporter for more than a decade now. Jeff Shreve is the guy at W. W. Norton who brought this book to fruition. Upon meeting Jeff,\none might think that he is far too nice to enforce the multiple deadlines involved with the\nproduction of a book like this. Not true. Yes, he really is that nice, but somehow his gentle\nprodding seems to get the job done. (For example, these acknowledgments are due tomorrow\nmorning.) I appreciate having a kind taskmaster to move things along. My biggest debt of gratitude goes to the many men and women who do the important\nresearch and analysis described in this book. I am not a statistician, nor am I a researcher. I am\nmerely a translator of other people’s interesting and significant work. I hope that I have\nconveyed throughout this book how important good research and sound analysis are to making\nus healthier, wealthier, safer, and better informed. In particular, I would like to acknowledge the wide-ranging work of Princeton economist\nAlan Krueger, who has made clever and significant research contributions on topics ranging\nfrom the roots of terrorism to the economic returns from higher education. (His findings on both\nof these topics are pleasantly counterintuitive.) More importantly (to me), Alan was one of my\ngraduate school statistics professors; I have always been impressed by his ability to\nsuccessfully balance research, teaching, and public service. Jim Sallee, Jeff Grogger, Patty Anderson, and Arthur Minetz all read earlier drafts of the\nmanuscript and made numerous helpful suggestions. Thank you for saving me from myself!", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nAcknowledgments\n\nThis book was conceived as an homage to an earlier W. W. Norton classic, How to Lie with\n\nStatistics by Darrell Huff, which was written in the 1950s and has sold over a million copies. That book, like this one, was written to demystify statistics and persuade everyday readers that\nwhat they don’t understand about the numbers behind the headlines can hurt them. I hope that\nI’ve done justice to Mr. Huff’s classic. In any event, I would be delighted to have sold a\nmillion copies fifty years from now! I am continually grateful to W. W. Norton, and to Drake McFeely in particular, for enabling\nme to write books that address significant topics in a way that is understandable to lay readers. Drake has been a great friend and supporter for more than a decade now. Jeff Shreve is the guy at W. W. Norton who brought this book to fruition. Upon meeting Jeff,\none might think that he is far too nice to enforce the multiple deadlines involved with the\nproduction of a book like this. Not true. Yes, he really is that nice, but somehow his gentle\nprodding seems to get the job done. (For example, these acknowledgments are due tomorrow\nmorning.) I appreciate having a kind taskmaster to move things along. My biggest debt of gratitude goes to the many men and women who do the important\nresearch and analysis described in this book. I am not a statistician, nor am I a researcher. I am\nmerely a translator of other people’s interesting and significant work. I hope that I have\nconveyed throughout this book how important good research and sound analysis are to making\nus healthier, wealthier, safer, and better informed. In particular, I would like to acknowledge the wide-ranging work of Princeton economist\nAlan Krueger, who has made clever and significant research contributions on topics ranging\nfrom the roots of terrorism to the economic returns from higher education. (His findings on both\nof these topics are pleasantly counterintuitive.) More importantly (to me), Alan was one of my\ngraduate school statistics professors; I have always been impressed by his ability to\nsuccessfully balance research, teaching, and public service. Jim Sallee, Jeff Grogger, Patty Anderson, and Arthur Minetz all read earlier drafts of the\nmanuscript and made numerous helpful suggestions. Thank you for saving me from myself!", "tokens": 501, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 194, "segment_id": "00194", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000344"}
{"type": "chunk", "text": "(His findings on both\nof these topics are pleasantly counterintuitive.) More importantly (to me), Alan was one of my\ngraduate school statistics professors; I have always been impressed by his ability to\nsuccessfully balance research, teaching, and public service. Jim Sallee, Jeff Grogger, Patty Anderson, and Arthur Minetz all read earlier drafts of the\nmanuscript and made numerous helpful suggestions. Thank you for saving me from myself! Frank Newport of Gallup and Mike Kagay from the New York Times were kind enough to\nspend time walking me through the methodological nuances of polling. Despite all of their\nefforts, the mistakes that remain are my own. Katie Wade was an indefatigable research assistant. (I have always wanted to use the word\n“indefatigable” and, finally, this is the perfect context.) Katie is the source of many of the\nanecdotes and examples that illuminate concepts throughout the book. No Katie, no fun\nexamples. I have wanted to write books since I was in elementary school. The person who enables me\nto do that, and to make a living at it, is my agent, Tina Bennett. Tina embodies the best of the\npublishing business. She delights in bringing meaningful work to fruition while tirelessly", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\n(His findings on both\nof these topics are pleasantly counterintuitive.) More importantly (to me), Alan was one of my\ngraduate school statistics professors; I have always been impressed by his ability to\nsuccessfully balance research, teaching, and public service. Jim Sallee, Jeff Grogger, Patty Anderson, and Arthur Minetz all read earlier drafts of the\nmanuscript and made numerous helpful suggestions. Thank you for saving me from myself! Frank Newport of Gallup and Mike Kagay from the New York Times were kind enough to\nspend time walking me through the methodological nuances of polling. Despite all of their\nefforts, the mistakes that remain are my own. Katie Wade was an indefatigable research assistant. (I have always wanted to use the word\n“indefatigable” and, finally, this is the perfect context.) Katie is the source of many of the\nanecdotes and examples that illuminate concepts throughout the book. No Katie, no fun\nexamples. I have wanted to write books since I was in elementary school. The person who enables me\nto do that, and to make a living at it, is my agent, Tina Bennett. Tina embodies the best of the\npublishing business. She delights in bringing meaningful work to fruition while tirelessly", "tokens": 265, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 194, "segment_id": "00194", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000345"}
{"type": "chunk", "text": "promoting the interests of her clients. And last, my family deserves credit for tolerating me as I produced this book. (The chapter\ndeadlines were posted on the refrigerator.) There is evidence that I become 31 percent crankier\nand 23 percent more exhausted when approaching (or missing) major book deadlines. My wife,\nLeah, is the first, best, and most important editor of everything that I write. Thank you for that,\nand for being such a smart, supportive, and fun partner in all other endeavors. The book is dedicated to my oldest daughter, Katrina. It is hard to believe that the child who\nwas in a crib when I wrote Naked Economics can now read chapters and provide meaningful\nfeedback. Katrina, you are a parent’s dream, as are Sophie and C. J., who will soon be reading\nchapters and manuscripts, too.", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\npromoting the interests of her clients. And last, my family deserves credit for tolerating me as I produced this book. (The chapter\ndeadlines were posted on the refrigerator.) There is evidence that I become 31 percent crankier\nand 23 percent more exhausted when approaching (or missing) major book deadlines. My wife,\nLeah, is the first, best, and most important editor of everything that I write. Thank you for that,\nand for being such a smart, supportive, and fun partner in all other endeavors. The book is dedicated to my oldest daughter, Katrina. It is hard to believe that the child who\nwas in a crib when I wrote Naked Economics can now read chapters and provide meaningful\nfeedback. Katrina, you are a parent’s dream, as are Sophie and C. J., who will soon be reading\nchapters and manuscripts, too.", "tokens": 179, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 195, "segment_id": "00195", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000346"}
{"type": "chunk", "text": "Index\n\nPage numbers in italics refer to figures. “absolute” score, 22, 23, 48\n\nin percentages, 28\n\nacademic reputation, 56\naccuracy, 37--38, 84, 99\nACT, 55\nAfrican Americans, 201\nage, 198, 199, 204\nAir Force Academy, 248--49\nairline passengers, 23--24, 25\nAlabama, 87\nalcohol, 110--11, 114\nAlexander, Lamar, 230\nalgorithms, 64--65\nAllstate, 81, 87\nAlzheimer’s disease, 242, 243\nAmerican Economic Association,\n\n251\n\nAmerican Heart Journal, 229\nAmericans’ Changing Lives, 135, 135, 136, 137, 138--41, 150--52, 166, 192, 193, 195, 196,\n\n199, 200, 201, 202, 204, 208, 221\n\nannuities, 107\nantidepressants, 121\nArbetter, Brian, x\nArchives of General Psychiatry, 155, 156--60\narsenic, 48\nassembly lines, 53\nAT&T, 42\nat bats, 32\nAustralian Transport Safety Board, 71--72\nAustria, 65\nautism, 4, 221, 244--46\n\nbrain size and, 155--60, 165\n\nAvatar (film), 47, 48\naverage, see mean\naverage income, 16--17, 18--19, 27, 55\naverage yards per pass attempt, 1", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nIndex\n\nPage numbers in italics refer to figures. “absolute” score, 22, 23, 48\n\nin percentages, 28\n\nacademic reputation, 56\naccuracy, 37--38, 84, 99\nACT, 55\nAfrican Americans, 201\nage, 198, 199, 204\nAir Force Academy, 248--49\nairline passengers, 23--24, 25\nAlabama, 87\nalcohol, 110--11, 114\nAlexander, Lamar, 230\nalgorithms, 64--65\nAllstate, 81, 87\nAlzheimer’s disease, 242, 243\nAmerican Economic Association,\n\n251\n\nAmerican Heart Journal, 229\nAmericans’ Changing Lives, 135, 135, 136, 137, 138--41, 150--52, 166, 192, 193, 195, 196,\n\n199, 200, 201, 202, 204, 208, 221\n\nannuities, 107\nantidepressants, 121\nArbetter, Brian, x\nArchives of General Psychiatry, 155, 156--60\narsenic, 48\nassembly lines, 53\nAT&T, 42\nat bats, 32\nAustralian Transport Safety Board, 71--72\nAustria, 65\nautism, 4, 221, 244--46\n\nbrain size and, 155--60, 165\n\nAvatar (film), 47, 48\naverage, see mean\naverage income, 16--17, 18--19, 27, 55\naverage yards per pass attempt, 1", "tokens": 364, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 196, "segment_id": "00196", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000347"}
{"type": "chunk", "text": "baboons, 207\nbanks, 170n\nBard College, 57\nBaseball Info Solutions, 16, 31--32\nbaseball players, 5, 248\n\nbest ever, 13, 15, 30, 31--32\n\nbasketball, streaks in, 103\nbatting averages, xii, 1, 4, 5, 15--16\nbell curve, 20, 25--26, 26, 133, 134, 136, 208, 209\nBernoulli trial, 70\nBertrand, Marianne, 203\nBhutto, Benazir, 58, 64\nBhutto (film), 58--60, 64\nbias, 113\nbinary variables, 200\nbinomial experiment, 70\nBlack Swan, The: The Impact of the Highly Improbable (Taleb), 98--99\nBlalock, Garrick, 72--73\nblind taste tests, 68--71, 79, 79, 80, 97, 99\nblood pressure, 115, 116\nblue-green algae, 116--17\nBoston Celtics, 103\nBoston Marathon, 23--24, 25\nBotstein, Leon, 57\nbowling scores, 4\nboxers, 242, 243\nbrain cancer, 145\nbrain size, autism and, 155--60, 165\nbran muffins, 11, 153--54\nBrazil, 3\nbreast cancer, 122, 163\nBrunei, 31\nBudweiser, 68, 69\nBuffett, Warren, 19, 81--82, 84\nBureau of Labor Statistics, U.S., 46\nBurton, Dan, 246\nBush, George H. W., 230\nBush, George W., 43, 53\nBusinessweek, 107\n\nCaddyshack (film), 44--45\ncalculus, ix--xi, xii", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nbaboons, 207\nbanks, 170n\nBard College, 57\nBaseball Info Solutions, 16, 31--32\nbaseball players, 5, 248\n\nbest ever, 13, 15, 30, 31--32\n\nbasketball, streaks in, 103\nbatting averages, xii, 1, 4, 5, 15--16\nbell curve, 20, 25--26, 26, 133, 134, 136, 208, 209\nBernoulli trial, 70\nBertrand, Marianne, 203\nBhutto, Benazir, 58, 64\nBhutto (film), 58--60, 64\nbias, 113\nbinary variables, 200\nbinomial experiment, 70\nBlack Swan, The: The Impact of the Highly Improbable (Taleb), 98--99\nBlalock, Garrick, 72--73\nblind taste tests, 68--71, 79, 79, 80, 97, 99\nblood pressure, 115, 116\nblue-green algae, 116--17\nBoston Celtics, 103\nBoston Marathon, 23--24, 25\nBotstein, Leon, 57\nbowling scores, 4\nboxers, 242, 243\nbrain cancer, 145\nbrain size, autism and, 155--60, 165\nbran muffins, 11, 153--54\nBrazil, 3\nbreast cancer, 122, 163\nBrunei, 31\nBudweiser, 68, 69\nBuffett, Warren, 19, 81--82, 84\nBureau of Labor Statistics, U.S., 46\nBurton, Dan, 246\nBush, George H. W., 230\nBush, George W., 43, 53\nBusinessweek, 107\n\nCaddyshack (film), 44--45\ncalculus, ix--xi, xii", "tokens": 448, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 197, "segment_id": "00197", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000348"}
{"type": "chunk", "text": "California, 41\nCalifornia, University of, 245\nCanada, 3, 65\nCanadian Tire, 88\ncancer, 44, 162, 226\n\nbrain, 145\nbran muffins and, 11, 153--54\nbreast, 122, 163\ncauses of, xv, 4\ncell phones and, 145\ncolon, 11, 153--54\ndiet and, 122\nprostate, 163, 224\nscreening for, 163\nsmoking and, xiv, 9--10, 11\n\ncancer clusters, 103--4\ncar crashes, 8, 76\ncardiology, 54--55\ncardiovascular disease, see heart disease\ncar insurance, 81, 107\nCarnegie Mellon University, 155\nCarrell, Scott, 248--49\ncars, 72\nCarter, Jimmy, 50\ncasinos, 7, 79, 84, 99, 102\ncausation, causality, 225--40, 243\n\nas not implied by correlation, 63, 154, 215--17, 245--46\nreverse, 216--17\n\nCaveon Test Security, 8\nCBS News, 170, 172, 177, 178\ncellular phone service, 42\nCenters for Disease Control, 244\ncentral limit theorem, 127--42, 146, 195\n\nin autism study, 156, 158, 159\nin polling, 130, 170--71, 174\nsampling and, 127--30, 139\n\ncentral tendency, 17--18, 19, 20--21, 20, 21, 34\n\nsee also mean; median\n\nCEOs, 19\nChanging Lives, see Americans’ Changing Lives\ncharter schools, 113", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nCalifornia, 41\nCalifornia, University of, 245\nCanada, 3, 65\nCanadian Tire, 88\ncancer, 44, 162, 226\n\nbrain, 145\nbran muffins and, 11, 153--54\nbreast, 122, 163\ncauses of, xv, 4\ncell phones and, 145\ncolon, 11, 153--54\ndiet and, 122\nprostate, 163, 224\nscreening for, 163\nsmoking and, xiv, 9--10, 11\n\ncancer clusters, 103--4\ncar crashes, 8, 76\ncardiology, 54--55\ncardiovascular disease, see heart disease\ncar insurance, 81, 107\nCarnegie Mellon University, 155\nCarrell, Scott, 248--49\ncars, 72\nCarter, Jimmy, 50\ncasinos, 7, 79, 84, 99, 102\ncausation, causality, 225--40, 243\n\nas not implied by correlation, 63, 154, 215--17, 245--46\nreverse, 216--17\n\nCaveon Test Security, 8\nCBS News, 170, 172, 177, 178\ncellular phone service, 42\nCenters for Disease Control, 244\ncentral limit theorem, 127--42, 146, 195\n\nin autism study, 156, 158, 159\nin polling, 130, 170--71, 174\nsampling and, 127--30, 139\n\ncentral tendency, 17--18, 19, 20--21, 20, 21, 34\n\nsee also mean; median\n\nCEOs, 19\nChanging Lives, see Americans’ Changing Lives\ncharter schools, 113", "tokens": 414, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 198, "segment_id": "00198", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000349"}
{"type": "chunk", "text": "Chase, Chevy, 44--45\ncheating, 89\n\nauthor accused of, 143--44, 149\non standardized tests, 4, 8--9, 86, 145, 148--49\n\nChevrolet, 87\nChevrolet Corvette, 30--31\nChicago, University of, 75, 182, 203\nChicago Bears, 1--2\nChicago Cubs, 105--6, 235--36\nChicago Police Department, 87\nChicago Sun-Times, 154\nchild care decisions, 187\nChina, 39\n\ncurrency of, 235\n\ncholesterol, 116\nchronic traumatic encephalopathy (CTE), 243\ncivil servants, 185--87, 195, 205--7, 221\nclarity, 38--39\nclimate change, 180\nclustering, clusters, 103--4\nof sample means, 138\n\ncocaine, 219--20\ncocoa, 251\ncoefficents, 196, 197, 199, 208, 220\n\non height, 193\nregression, 193, 195, 196\nsize of, 197--98\nsee also correlation coefficient (r)\n\ncoffee, 251\nCognitive Tutor, 155\ncoin flipping, 71, 75--76, 100, 104, 221--22\n\ngambler’s fallacy and, 106--7\n\nCold War, 49\nCollege Board, 62, 63, 64\ncollege degrees, 4\ncollege rankings, 30\ncolon cancer, 11, 153--54\ncommercial banks, 97\nCommunist Party, 37\ncompletion rate, 1\nconfidence, 149", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nChase, Chevy, 44--45\ncheating, 89\n\nauthor accused of, 143--44, 149\non standardized tests, 4, 8--9, 86, 145, 148--49\n\nChevrolet, 87\nChevrolet Corvette, 30--31\nChicago, University of, 75, 182, 203\nChicago Bears, 1--2\nChicago Cubs, 105--6, 235--36\nChicago Police Department, 87\nChicago Sun-Times, 154\nchild care decisions, 187\nChina, 39\n\ncurrency of, 235\n\ncholesterol, 116\nchronic traumatic encephalopathy (CTE), 243\ncivil servants, 185--87, 195, 205--7, 221\nclarity, 38--39\nclimate change, 180\nclustering, clusters, 103--4\nof sample means, 138\n\ncocaine, 219--20\ncocoa, 251\ncoefficents, 196, 197, 199, 208, 220\n\non height, 193\nregression, 193, 195, 196\nsize of, 197--98\nsee also correlation coefficient (r)\n\ncoffee, 251\nCognitive Tutor, 155\ncoin flipping, 71, 75--76, 100, 104, 221--22\n\ngambler’s fallacy and, 106--7\n\nCold War, 49\nCollege Board, 62, 63, 64\ncollege degrees, 4\ncollege rankings, 30\ncolon cancer, 11, 153--54\ncommercial banks, 97\nCommunist Party, 37\ncompletion rate, 1\nconfidence, 149", "tokens": 382, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 199, "segment_id": "00199", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000350"}
{"type": "chunk", "text": "confidence interval, 158, 171--75, 176--77\nCongress, U.S., 171\nconstant, 193\ncontrol, 11, 198\n\nnonequivalent, 233--40\nsee also regression analysis\n\ncontrol group, 114, 126, 227--28, 238--39\n\nas counterfactual, 240\n\ncontrolled experiments, ethics and, 9\ncontrol variables, see explanatory variables\nCook County, Ill., 48--49\nCooper, Linda, 144--45\nCornell University, 103\ncoronary angioplasty, 54--55\ncoronary bypass surgery, 229--30\ncorrelation, 58--67\n\nof exercise and weight, 60\nof height and weight, 59, 59, 60, 61, 63, 65--67, 189--204, 190, 191, 208\nnegative, 60, 62\nas not implying causation, 63, 154, 215--17, 245--46\nperfect, 60\nin sports streaks, 103\n\ncorrelation coefficient (r), 60--61\n\ncalculation of, 65--67\n\ncost of living adjustments, 47\ncot death, 101--2\nCôte d’Ivoire, 251\nCouncil of Economic Advisers, 32\nCox, Chris, 254\ncredit cards, xv, 88, 241\ncredit risks, 88\ncredit score, 87\ncrime, criminals, 11, 14, 89\n\npolice officers and deterrence of, 225--26, 227\npredicting, 86--87\n\ncross-sectional data, 115--16\n\nrecall bias and, 123\n\nCruise, Tom, 86\nCSI: Miami, 73\ncumulative probability, 165\ncurrencies, 96", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nconfidence interval, 158, 171--75, 176--77\nCongress, U.S., 171\nconstant, 193\ncontrol, 11, 198\n\nnonequivalent, 233--40\nsee also regression analysis\n\ncontrol group, 114, 126, 227--28, 238--39\n\nas counterfactual, 240\n\ncontrolled experiments, ethics and, 9\ncontrol variables, see explanatory variables\nCook County, Ill., 48--49\nCooper, Linda, 144--45\nCornell University, 103\ncoronary angioplasty, 54--55\ncoronary bypass surgery, 229--30\ncorrelation, 58--67\n\nof exercise and weight, 60\nof height and weight, 59, 59, 60, 61, 63, 65--67, 189--204, 190, 191, 208\nnegative, 60, 62\nas not implying causation, 63, 154, 215--17, 245--46\nperfect, 60\nin sports streaks, 103\n\ncorrelation coefficient (r), 60--61\n\ncalculation of, 65--67\n\ncost of living adjustments, 47\ncot death, 101--2\nCôte d’Ivoire, 251\nCouncil of Economic Advisers, 32\nCox, Chris, 254\ncredit cards, xv, 88, 241\ncredit risks, 88\ncredit score, 87\ncrime, criminals, 11, 14, 89\n\npolice officers and deterrence of, 225--26, 227\npredicting, 86--87\n\ncross-sectional data, 115--16\n\nrecall bias and, 123\n\nCruise, Tom, 86\nCSI: Miami, 73\ncumulative probability, 165\ncurrencies, 96", "tokens": 409, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 200, "segment_id": "00200", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000351"}
{"type": "chunk", "text": "Cutler, Jay, 2\ncyanobacteria, 117\n\nDale, Stacy, 234--35\nDark Knight, The (film), 47\nDartmouth College, 233, 234, 247\ndata, xv, 3, 115--17, 241--42, 252, 255\n\nfor comparison, 113--15\ncross-sectional, 115--16\ndisagreements over, 13--14\nnormal distribution of, 25--26\npoor, xiv\nas representative, 111--13\nsampling of, 6--7, 111--13\nstatistics and, 4, 111\nsummary of, 5, 14, 15--16, 17\n\ndata, problems with, 117--26\n\nhealthy user bias, 125--26, 154\npublication bias, 120--22, 223\nselection bias, 118--19, 178\nsurvivorship bias, 123--25\n\ndata mining, 221--23\nData Mining and Predictive Analysis: Intelligence Gathering and Crime Analysis, 87\ndata tables, 258\ndating, 36\ndeceptive description, 36--57\n\nfor coronary angioplasty, 54--55\nmean as, 42--43\nmedian as, 42--44\nstandardized tests as, 51--52, 53--54\n\ndeciles, 22\nDefense Department, U.S., 49--50, 50\ndementia, 242\nDemocratic Party, U.S.:\n\ndefense spending and, 49\ntax increase and, 29\n\ndependent variables, 192, 193--94, 197, 198, 199, 206n, 216, 217, 226\ndepression, 121, 242\ndescriptive statistics, 15--35\n\ncentral tendency discovered in, 18--22\ndispersion measured in, 23--25\nissues framed by, 33", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nCutler, Jay, 2\ncyanobacteria, 117\n\nDale, Stacy, 234--35\nDark Knight, The (film), 47\nDartmouth College, 233, 234, 247\ndata, xv, 3, 115--17, 241--42, 252, 255\n\nfor comparison, 113--15\ncross-sectional, 115--16\ndisagreements over, 13--14\nnormal distribution of, 25--26\npoor, xiv\nas representative, 111--13\nsampling of, 6--7, 111--13\nstatistics and, 4, 111\nsummary of, 5, 14, 15--16, 17\n\ndata, problems with, 117--26\n\nhealthy user bias, 125--26, 154\npublication bias, 120--22, 223\nselection bias, 118--19, 178\nsurvivorship bias, 123--25\n\ndata mining, 221--23\nData Mining and Predictive Analysis: Intelligence Gathering and Crime Analysis, 87\ndata tables, 258\ndating, 36\ndeceptive description, 36--57\n\nfor coronary angioplasty, 54--55\nmean as, 42--43\nmedian as, 42--44\nstandardized tests as, 51--52, 53--54\n\ndeciles, 22\nDefense Department, U.S., 49--50, 50\ndementia, 242\nDemocratic Party, U.S.:\n\ndefense spending and, 49\ntax increase and, 29\n\ndependent variables, 192, 193--94, 197, 198, 199, 206n, 216, 217, 226\ndepression, 121, 242\ndescriptive statistics, 15--35\n\ncentral tendency discovered in, 18--22\ndispersion measured in, 23--25\nissues framed by, 33", "tokens": 425, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 201, "segment_id": "00201", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000352"}
{"type": "chunk", "text": "middle-class economic health as measured by, 16--17\nin Stata, 258\nas summary, 15--16, 30\nDiamantopoulou, Anna, 107\ndiapers, 14\ndiet, 115, 198\n\ncancer and, 122\n\ndifference in differences, 235--37\ndiscontinuity analysis, 238--40\ndispersion, 23--24, 196, 210\n\nmean affected by, 44\nof sample means, 136\nDisraeli, Benjamin, 36\ndistrust, 169\nDNA databases, 254\nDNA testing, xi, 10\n\ncriminal evidence from, 74--75, 105\nloci in, 73--75\nprosecutor’s fallacy and, 105\n\ndogfighting, 242, 243--44\ndriving, 71--72, 73\ndropout rate, 53, 54\ndrugs, 9, 43--44, 115\ndrug smugglers, 108, 109\ndrug treatment, 120\nDrunkard’s Walk, The (Mlodinow), 92\nDubner, Stephen, 72\nDuerson, Dave, 242\nDuflo, Esther, 250--52\ndummy variables, 200\nDuncan, Arne, 247\nDunkels, Andrejs, xv\n\nEconomist, 39--40, 41, 42, 101--2\neducation, 115, 194, 200, 201, 204--5, 216--17, 218, 220, 249\n\nincome and, 233--35\nlongevity and, 231--33\neducational attainment, 31\nEducation Department, U.S., 155\nEgypt, 170n\nEinhorn, David, 98\nelection, U.S., of 1936, 118--19", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nmiddle-class economic health as measured by, 16--17\nin Stata, 258\nas summary, 15--16, 30\nDiamantopoulou, Anna, 107\ndiapers, 14\ndiet, 115, 198\n\ncancer and, 122\n\ndifference in differences, 235--37\ndiscontinuity analysis, 238--40\ndispersion, 23--24, 196, 210\n\nmean affected by, 44\nof sample means, 136\nDisraeli, Benjamin, 36\ndistrust, 169\nDNA databases, 254\nDNA testing, xi, 10\n\ncriminal evidence from, 74--75, 105\nloci in, 73--75\nprosecutor’s fallacy and, 105\n\ndogfighting, 242, 243--44\ndriving, 71--72, 73\ndropout rate, 53, 54\ndrugs, 9, 43--44, 115\ndrug smugglers, 108, 109\ndrug treatment, 120\nDrunkard’s Walk, The (Mlodinow), 92\nDubner, Stephen, 72\nDuerson, Dave, 242\nDuflo, Esther, 250--52\ndummy variables, 200\nDuncan, Arne, 247\nDunkels, Andrejs, xv\n\nEconomist, 39--40, 41, 42, 101--2\neducation, 115, 194, 200, 201, 204--5, 216--17, 218, 220, 249\n\nincome and, 233--35\nlongevity and, 231--33\neducational attainment, 31\nEducation Department, U.S., 155\nEgypt, 170n\nEinhorn, David, 98\nelection, U.S., of 1936, 118--19", "tokens": 417, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 202, "segment_id": "00202", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000353"}
{"type": "chunk", "text": "employment, 39--40\nEnron: The Smartest Guys in the Room (film), 59\nepidemiology, 222--23\nestimate, 187--88\nestrogen supplements, 211--12\nE.T. (film), 48\nEuropean Commission, 107--8, 109\neuros, 45\nex-convicts, 113, 147--48, 227\nexercise, 125--26, 198, 201\nheart disease and, 188--89\nweight and, 60, 62\n\nexit polls, 172--73\nexpected loss, 81\nexpected value, 77\n\nof football plays, 77--78\nof lottery tickets, 78--79\nof male pattern baldness drug investment, 82--84\n\nexplanatory variables, 192, 193--94, 197, 198, 199, 203, 217\n\nhighly correlated, 219--20\nextender warranties, 80--81, 82\nextramarital sexual activity, 183\nextra point, 71, 77--78\nextrapolation, 220--21\nextrasensory perception (ESP), 161\n\nFacebook, 254\nfalse negatives (Type II errors), 84, 162--64\nfalse positives (Type I errors), 84--85, 162--64\nfamily structure, 115\nfat tails, 208, 209--10\nFBI, 74--75\nFederal Emergency Management Administration, 144\nfilms, highest-grossing, 47--48\nfinancial crisis of 2008, 7--8, 38, 95--100, 109\nfinancial industry, 7--8, 38, 95--100, 109\nfires, 8\nFog of War (film), 59\nFood and Drug Administration, U.S., 83\nfood deserts, 201\nfood stamps, 200, 201\nfootball, 51", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nemployment, 39--40\nEnron: The Smartest Guys in the Room (film), 59\nepidemiology, 222--23\nestimate, 187--88\nestrogen supplements, 211--12\nE.T. (film), 48\nEuropean Commission, 107--8, 109\neuros, 45\nex-convicts, 113, 147--48, 227\nexercise, 125--26, 198, 201\nheart disease and, 188--89\nweight and, 60, 62\n\nexit polls, 172--73\nexpected loss, 81\nexpected value, 77\n\nof football plays, 77--78\nof lottery tickets, 78--79\nof male pattern baldness drug investment, 82--84\n\nexplanatory variables, 192, 193--94, 197, 198, 199, 203, 217\n\nhighly correlated, 219--20\nextender warranties, 80--81, 82\nextramarital sexual activity, 183\nextra point, 71, 77--78\nextrapolation, 220--21\nextrasensory perception (ESP), 161\n\nFacebook, 254\nfalse negatives (Type II errors), 84, 162--64\nfalse positives (Type I errors), 84--85, 162--64\nfamily structure, 115\nfat tails, 208, 209--10\nFBI, 74--75\nFederal Emergency Management Administration, 144\nfilms, highest-grossing, 47--48\nfinancial crisis of 2008, 7--8, 38, 95--100, 109\nfinancial industry, 7--8, 38, 95--100, 109\nfires, 8\nFog of War (film), 59\nFood and Drug Administration, U.S., 83\nfood deserts, 201\nfood stamps, 200, 201\nfootball, 51", "tokens": 431, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 203, "segment_id": "00203", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000354"}
{"type": "chunk", "text": "extra point vs. two-point conversion in, 71, 77--78\nhead trauma in, 114, 242--44\npasser rating in, 1--2, 3, 56\n\nforeign investment, 41\n“4:15 report,” 96, 97\nFramingham Heart Study, 115--16, 136, 243\nfraud, 86, 107\nFreakonomics (Levitt and Dubner), 72\nfrequency distribution, 20, 20, 25\nfreshman retention rate, 56--57\nfruit flies, 110--11, 113, 114\n\nGallup Organization, 7, 177, 180, 181\ngambler’s fallacy, 102, 106--7\ngambling, 7, 79, 99, 102\nGates, Bill, 18--19, 27, 134\ngay marriage, 171\nGDP, 217\ngender, as explanatory variable, 198, 199--200, 204, 205\ngender discrimination, 107--8, 202--4\nGeneral Electric, 95, 96\nGeneral Equivalency Diploma (GED), 53\ngenetic factors, 115\ngenetics, 245\nGermany, 39\nGilovich, Thomas, 103\nGini index, 2--3\nGladwell, Malcolm, 30--31, 56, 242, 243--44\nglobalization, 41\nglobal warming, 180\nGodfather, The (film), 47\nGoldin, Claudia, 203\ngolf, 217\ngolf lessons, 214--15, 214\ngolf range finder, 38, 99\nGone with the Wind (film), 47, 48\nGoogle, 4\nGould, Stephen Jay, 44\ngovernment debt, 99--100\ngrade point average, 1, 5--6, 63\ngraduation rate, 56--57\nGreat Britain, probabilistic fallacy in criminal justice system in, 100--102", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nextra point vs. two-point conversion in, 71, 77--78\nhead trauma in, 114, 242--44\npasser rating in, 1--2, 3, 56\n\nforeign investment, 41\n“4:15 report,” 96, 97\nFramingham Heart Study, 115--16, 136, 243\nfraud, 86, 107\nFreakonomics (Levitt and Dubner), 72\nfrequency distribution, 20, 20, 25\nfreshman retention rate, 56--57\nfruit flies, 110--11, 113, 114\n\nGallup Organization, 7, 177, 180, 181\ngambler’s fallacy, 102, 106--7\ngambling, 7, 79, 99, 102\nGates, Bill, 18--19, 27, 134\ngay marriage, 171\nGDP, 217\ngender, as explanatory variable, 198, 199--200, 204, 205\ngender discrimination, 107--8, 202--4\nGeneral Electric, 95, 96\nGeneral Equivalency Diploma (GED), 53\ngenetic factors, 115\ngenetics, 245\nGermany, 39\nGilovich, Thomas, 103\nGini index, 2--3\nGladwell, Malcolm, 30--31, 56, 242, 243--44\nglobalization, 41\nglobal warming, 180\nGodfather, The (film), 47\nGoldin, Claudia, 203\ngolf, 217\ngolf lessons, 214--15, 214\ngolf range finder, 38, 99\nGone with the Wind (film), 47, 48\nGoogle, 4\nGould, Stephen Jay, 44\ngovernment debt, 99--100\ngrade point average, 1, 5--6, 63\ngraduation rate, 56--57\nGreat Britain, probabilistic fallacy in criminal justice system in, 100--102", "tokens": 461, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 204, "segment_id": "00204", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000355"}
{"type": "chunk", "text": "Great Depression, 99, 241\nGreat Recession, 39\nGreen Bay Packers, 1--2\nGreenspan, Alan, 97, 99\nGrogger, Jeff, 32\ngross domestic product (GDP), 241\nGuantánamo Bay, 164\nguilty beyond a reasonable doubt, 148\nguns, 72\nGuskiewicz, Kevin, 243\n\nHall, Monty, xi--xii, 90--94\nHanna, Rema, 250\nHarvard University, 211, 225, 233, 234, 249\n“HCb2” count, 24--25\nHDL cholesterol, 116\nhead trauma, 114, 242--44\nhealth care, 189\n\ncost containment in, 85\n\nhealth insurance, 82\nhealthy user bias, 125--26, 154\nheart disease, 145, 148, 198, 217--18\n\nestrogen supplements and, 211\nexercise and, 188--89\nFramingham study on, 115--16, 136\ngenetic component of, 245\nstress and, 185--87, 205--7\n\nhedge fund managers, 19\nheight, 115, 204, 205\n\nmean, 25, 26, 35, 156--57, 159, 166--68\nweight correlated with, 59, 59, 60, 61, 63, 65--67, 189--204, 190, 191, 208\n\nheroin, 219--20\nhighly correlated explanatory variables, 219--20\nhigh school dropouts, 226--27\nHIV/AIDS, 84--85, 182\nHjalmarsson, Randi, 239\nhockey players, 242\nhome foreclosures, 99\nhomelessness, 6\nhome mortgages, 97\nhomeowner’s insurance, 82\nhomosexuality, 182", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nGreat Depression, 99, 241\nGreat Recession, 39\nGreen Bay Packers, 1--2\nGreenspan, Alan, 97, 99\nGrogger, Jeff, 32\ngross domestic product (GDP), 241\nGuantánamo Bay, 164\nguilty beyond a reasonable doubt, 148\nguns, 72\nGuskiewicz, Kevin, 243\n\nHall, Monty, xi--xii, 90--94\nHanna, Rema, 250\nHarvard University, 211, 225, 233, 234, 249\n“HCb2” count, 24--25\nHDL cholesterol, 116\nhead trauma, 114, 242--44\nhealth care, 189\n\ncost containment in, 85\n\nhealth insurance, 82\nhealthy user bias, 125--26, 154\nheart disease, 145, 148, 198, 217--18\n\nestrogen supplements and, 211\nexercise and, 188--89\nFramingham study on, 115--16, 136\ngenetic component of, 245\nstress and, 185--87, 205--7\n\nhedge fund managers, 19\nheight, 115, 204, 205\n\nmean, 25, 26, 35, 156--57, 159, 166--68\nweight correlated with, 59, 59, 60, 61, 63, 65--67, 189--204, 190, 191, 208\n\nheroin, 219--20\nhighly correlated explanatory variables, 219--20\nhigh school dropouts, 226--27\nHIV/AIDS, 84--85, 182\nHjalmarsson, Randi, 239\nhockey players, 242\nhome foreclosures, 99\nhomelessness, 6\nhome mortgages, 97\nhomeowner’s insurance, 82\nhomosexuality, 182", "tokens": 436, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 205, "segment_id": "00205", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000356"}
{"type": "chunk", "text": "Honda Civic, 87\nHoover, Herbert, 241\nhormones, 207\nhot hands, 102--3\nHouston, Tex., 53--54\nHow to Lie with Statistics (Huff), 14\nHussein, Saddam, 240\nhypothesis testing, 146--48, 197\n\nIBM SPSS, 260\nIllinois, 29, 41, 87, 236\nlottery in, 78--79, 81\nincarceration, 239--40\nincentives, 53\nincome, 32, 114, 194, 198, 204, 205\n\neducation and, 235\nper capita, 16--17, 18--19, 27, 55, 216\nright skew in, 133--34, 133\nincome inequality, 2--3, 41--42\nincome tax, 29, 114\nincome verification, 87\nindependent events:\n\ngambler’s fallacy and, 106--7\nmisunderstanding of, 102--3\nprobability of both happening, 75--76, 100\nprobability of either happening, 76--77\n\nindependent variables, see explanatory variables\nIndia, 39, 45, 64\nindicator, 95\ninfinite series, xii--xiii\ninflation, 16, 45--46, 47\ninformation, 96--97\nInsel, Thomas, 245\ninsurance, 8, 71, 81--82, 84, 89, 144--45\n\ngender equality and, 107--8\n\ninterception rate, 1\nInternet, 235\nInternet surveys, 178\nintervention, 225, 226--27\nintuition, xii--xiv\nIonnidis, John, 223\nIowa straw poll, 118", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nHonda Civic, 87\nHoover, Herbert, 241\nhormones, 207\nhot hands, 102--3\nHouston, Tex., 53--54\nHow to Lie with Statistics (Huff), 14\nHussein, Saddam, 240\nhypothesis testing, 146--48, 197\n\nIBM SPSS, 260\nIllinois, 29, 41, 87, 236\nlottery in, 78--79, 81\nincarceration, 239--40\nincentives, 53\nincome, 32, 114, 194, 198, 204, 205\n\neducation and, 235\nper capita, 16--17, 18--19, 27, 55, 216\nright skew in, 133--34, 133\nincome inequality, 2--3, 41--42\nincome tax, 29, 114\nincome verification, 87\nindependent events:\n\ngambler’s fallacy and, 106--7\nmisunderstanding of, 102--3\nprobability of both happening, 75--76, 100\nprobability of either happening, 76--77\n\nindependent variables, see explanatory variables\nIndia, 39, 45, 64\nindicator, 95\ninfinite series, xii--xiii\ninflation, 16, 45--46, 47\ninformation, 96--97\nInsel, Thomas, 245\ninsurance, 8, 71, 81--82, 84, 89, 144--45\n\ngender equality and, 107--8\n\ninterception rate, 1\nInternet, 235\nInternet surveys, 178\nintervention, 225, 226--27\nintuition, xii--xiv\nIonnidis, John, 223\nIowa straw poll, 118", "tokens": 409, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 206, "segment_id": "00206", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000357"}
{"type": "chunk", "text": "Iraq War, 240\n\nJaws (film), 47\njet engines, 100\nJeter, Derek, 15, 19\njob placement program, 226\njob training, 113, 227, 236--37, 236, 237\nJohn Bates Clark Medal, 251\nJournal of Personality and Social Psychology, 160--61\nJournal of the American Medical Association, 223\nJ. P. Morgan, 96\njudgment, 57\njuvenile offenders, 239--40\nKadiyali, Vrinda, 72--73\nKael, Pauline, 118\nKathmandu, Nepal, 116--17\nKatz, Lawrence, 203\nKenya, 250\nKinney, Delma, 9\nKlick, Jonathan, 227\nKnight, Ted, 44--45\nKrueger, Alan, 12--13, 32, 234--35\nKuwait, 31\n\nLandon, Alf, 118--19\nlaser printers, central tendency explained by, 17--18, 19, 20--21, 20, 21, 34\nlaw of large numbers, 78--79, 84, 107\nleast squares, 190--94\nlegal system, 148, 162\n“lemon” problem, 21\nLet’s Make A Deal, xi--xii, 90--94\nleukemia, 104\nleverage, 96\nLevitt, Steve, 72\nlife expectancy, 31, 43\nliquidity, 96\nliteracy rate, 55\nLiterary Digest, 118--19\nLleras-Muney, Adriana, 232--33\nlongevity, education and, 231--33\nlongitudinal studies:\n\nChanging Lives, 135, 135, 136, 137, 138--41, 150--52, 166, 192, 193, 195, 196, 199, 200, 201,", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nIraq War, 240\n\nJaws (film), 47\njet engines, 100\nJeter, Derek, 15, 19\njob placement program, 226\njob training, 113, 227, 236--37, 236, 237\nJohn Bates Clark Medal, 251\nJournal of Personality and Social Psychology, 160--61\nJournal of the American Medical Association, 223\nJ. P. Morgan, 96\njudgment, 57\njuvenile offenders, 239--40\nKadiyali, Vrinda, 72--73\nKael, Pauline, 118\nKathmandu, Nepal, 116--17\nKatz, Lawrence, 203\nKenya, 250\nKinney, Delma, 9\nKlick, Jonathan, 227\nKnight, Ted, 44--45\nKrueger, Alan, 12--13, 32, 234--35\nKuwait, 31\n\nLandon, Alf, 118--19\nlaser printers, central tendency explained by, 17--18, 19, 20--21, 20, 21, 34\nlaw of large numbers, 78--79, 84, 107\nleast squares, 190--94\nlegal system, 148, 162\n“lemon” problem, 21\nLet’s Make A Deal, xi--xii, 90--94\nleukemia, 104\nleverage, 96\nLevitt, Steve, 72\nlife expectancy, 31, 43\nliquidity, 96\nliteracy rate, 55\nLiterary Digest, 118--19\nLleras-Muney, Adriana, 232--33\nlongevity, education and, 231--33\nlongitudinal studies:\n\nChanging Lives, 135, 135, 136, 137, 138--41, 150--52, 166, 192, 193, 195, 196, 199, 200, 201,", "tokens": 445, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 207, "segment_id": "00207", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000358"}
{"type": "chunk", "text": "202, 204, 208, 221\n\non education, 116\non education and income, 231--33\nhealthy user bias and, 125\non heart disease, 115--16\nrecall bias and, 122--23\nLos Angeles, Calif., 247\nLos Angeles Times, 74, 247\nlottery:\n\ndouble winner of, 9\nirrationality of playing, xi, 78--79, 81, 89\n\nLotus Evora, 30--31\nluck, 106\n\nmalaria, 146--47, 148\nmale pattern baldness, 82--84, 83\nMalkiel, Burton, 125n\nMalmendier, Ulrike, 107\nmammograms, 163\nManning, Peyton, 30\nMantle, Mickey, 5\nmanufacturing, 39--40, 39\nmarathon runners, 23--24, 25\nmarathons, 127--29\nmarbles in urn, probability and, 112, 178--79\nmargin of error, see confidence\n\ninterval\n\nmarket research companies, 113\nMartin, J. P., 88\nMauritius, 41\nMcCarthy, Joseph, 37\nMcKee, Ann, 243\nMcPherson, Michael, 56--57\nMeadow, Roy, 101--2\nMeadow’s Law, 101--2\nmean, 18--19, 146, 196n\n\naffected by dispersion, 44\nin autism study, 156\ncentral limit theorem and, 129, 131, 132\nin correlation coefficient, 67\nformula for, 66\nof height of Americans, 25, 26", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\n202, 204, 208, 221\n\non education, 116\non education and income, 231--33\nhealthy user bias and, 125\non heart disease, 115--16\nrecall bias and, 122--23\nLos Angeles, Calif., 247\nLos Angeles Times, 74, 247\nlottery:\n\ndouble winner of, 9\nirrationality of playing, xi, 78--79, 81, 89\n\nLotus Evora, 30--31\nluck, 106\n\nmalaria, 146--47, 148\nmale pattern baldness, 82--84, 83\nMalkiel, Burton, 125n\nMalmendier, Ulrike, 107\nmammograms, 163\nManning, Peyton, 30\nMantle, Mickey, 5\nmanufacturing, 39--40, 39\nmarathon runners, 23--24, 25\nmarathons, 127--29\nmarbles in urn, probability and, 112, 178--79\nmargin of error, see confidence\n\ninterval\n\nmarket research companies, 113\nMartin, J. P., 88\nMauritius, 41\nMcCarthy, Joseph, 37\nMcKee, Ann, 243\nMcPherson, Michael, 56--57\nMeadow, Roy, 101--2\nMeadow’s Law, 101--2\nmean, 18--19, 146, 196n\n\naffected by dispersion, 44\nin autism study, 156\ncentral limit theorem and, 129, 131, 132\nin correlation coefficient, 67\nformula for, 66\nof height of Americans, 25, 26", "tokens": 381, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 208, "segment_id": "00208", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000359"}
{"type": "chunk", "text": "of income, 134\nmedian vs., 18, 19, 44\nin Microsoft Excel, 257\npossible deceptiveness of, 42--43\nstandard error for difference of, 164--65\n\nmeasles, mumps, and rubella (MMR), 245--46\nmedian, 21\n\nmean vs., 18, 19, 44\noutliers and, 43\npossible deceptiveness of, 42--44\n\n“Median Isn’t the Message, The” (Gould), 44\nmemory loss, 242\nmen, money handling by, 250--51\nMichelob, 68--71, 80\nMichigan Supreme Court, 56\nMicrosoft Excel, 61, 67, 257\nmiddle class, 13, 15, 16--17, 32\nas measured by median, 19\n\nMiller, 68, 69\nminimum wage, 46--47\nMinority Report (film), 86\nMiss America pageant, 30\nMlodinow, Leonard, 92\nmodels, financial, 7--8, 38, 95--100\nmonkeys, 207\nMonty Hall problem, xi--xii, 90--94\nmotorcycles, 72\nMoyer, Steve, 16, 31--32\nMRI scans, 163\nmulticollinearity, 219--20\nmultinationals, 170n\nmultiple regression analysis, 199--204, 226\nmultivariate logistic regression, 206n\nmutual funds, 123--25\n\nNASA, 72\nNational Football League, 1--2, 3, 51, 56, 242--44\nNational Institute of Mental Health, 245\nNational Opinion Research Center (NORC), 7, 181--83\nNative Americans, 184\nnatural experiments, 231--33\nNBA, 103", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nof income, 134\nmedian vs., 18, 19, 44\nin Microsoft Excel, 257\npossible deceptiveness of, 42--43\nstandard error for difference of, 164--65\n\nmeasles, mumps, and rubella (MMR), 245--46\nmedian, 21\n\nmean vs., 18, 19, 44\noutliers and, 43\npossible deceptiveness of, 42--44\n\n“Median Isn’t the Message, The” (Gould), 44\nmemory loss, 242\nmen, money handling by, 250--51\nMichelob, 68--71, 80\nMichigan Supreme Court, 56\nMicrosoft Excel, 61, 67, 257\nmiddle class, 13, 15, 16--17, 32\nas measured by median, 19\n\nMiller, 68, 69\nminimum wage, 46--47\nMinority Report (film), 86\nMiss America pageant, 30\nMlodinow, Leonard, 92\nmodels, financial, 7--8, 38, 95--100\nmonkeys, 207\nMonty Hall problem, xi--xii, 90--94\nmotorcycles, 72\nMoyer, Steve, 16, 31--32\nMRI scans, 163\nmulticollinearity, 219--20\nmultinationals, 170n\nmultiple regression analysis, 199--204, 226\nmultivariate logistic regression, 206n\nmutual funds, 123--25\n\nNASA, 72\nNational Football League, 1--2, 3, 51, 56, 242--44\nNational Institute of Mental Health, 245\nNational Opinion Research Center (NORC), 7, 181--83\nNative Americans, 184\nnatural experiments, 231--33\nNBA, 103", "tokens": 418, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 209, "segment_id": "00209", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000360"}
{"type": "chunk", "text": "negative correlation, 60, 62\nNetflix, 4, 58--59, 60, 62, 64--65\nNewport, Frank, 177\nNew York, 41, 54--55\nNew York, N.Y., 247\nNew Yorker, 30--31, 118, 241, 242\nNew York Times, 4, 43, 86--87, 91--92, 96, 98--99, 110, 125, 154--55, 161, 169, 170, 172, 177,\n\n178, 229--30, 231, 235, 246, 247, 254\n\nNew York Times Magazine, 88, 122, 222, 253\nNixon, Richard, 118\nNobel Prize in Economics, 251\nNocera, Joe, 96, 98--99\nnominal figures, 45--46, 47\nnonequivalent control, 233--40\nnonlinear relationships, 214--15\nnormal distribution, see bell curve\nNorth Carolina, University of, 155--60\nNorth Dakota, 41, 183--84\nnull hypothesis, 146--48, 149, 156--57, 166, 188\n\nthreshold for rejection of, 149--50, 152, 153, 161--64, 197, 222\n\nNurses’ Health Study, 211\n\nObama, Barack, 17, 32\n\njob rating of, 169, 170, 171, 177, 179\n\nobesity, heart disease and, 115--16\nobservations, 67, 192--93\nOccupy Wall Street, 16, 169--70\nomitted variable bias, 217--19\non-base percentage, 31\n“one-tailed” hypothesis test, 151, 166--68\nordinary least squares (OLS), 190--94\nosteoporosis, 211\noutliers, 18--20, 21\n\nmean-reversion of, 105\nmedian’s insensitivity to, 43\nsample mean and, 138\nin variance, 34\n\noutput, 39--40\noutsourcing, 41\n\nPaige, Rod, 53, 54\nPakistan, 64", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nnegative correlation, 60, 62\nNetflix, 4, 58--59, 60, 62, 64--65\nNewport, Frank, 177\nNew York, 41, 54--55\nNew York, N.Y., 247\nNew Yorker, 30--31, 118, 241, 242\nNew York Times, 4, 43, 86--87, 91--92, 96, 98--99, 110, 125, 154--55, 161, 169, 170, 172, 177,\n\n178, 229--30, 231, 235, 246, 247, 254\n\nNew York Times Magazine, 88, 122, 222, 253\nNixon, Richard, 118\nNobel Prize in Economics, 251\nNocera, Joe, 96, 98--99\nnominal figures, 45--46, 47\nnonequivalent control, 233--40\nnonlinear relationships, 214--15\nnormal distribution, see bell curve\nNorth Carolina, University of, 155--60\nNorth Dakota, 41, 183--84\nnull hypothesis, 146--48, 149, 156--57, 166, 188\n\nthreshold for rejection of, 149--50, 152, 153, 161--64, 197, 222\n\nNurses’ Health Study, 211\n\nObama, Barack, 17, 32\n\njob rating of, 169, 170, 171, 177, 179\n\nobesity, heart disease and, 115--16\nobservations, 67, 192--93\nOccupy Wall Street, 16, 169--70\nomitted variable bias, 217--19\non-base percentage, 31\n“one-tailed” hypothesis test, 151, 166--68\nordinary least squares (OLS), 190--94\nosteoporosis, 211\noutliers, 18--20, 21\n\nmean-reversion of, 105\nmedian’s insensitivity to, 43\nsample mean and, 138\nin variance, 34\n\noutput, 39--40\noutsourcing, 41\n\nPaige, Rod, 53, 54\nPakistan, 64", "tokens": 501, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 210, "segment_id": "00210", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000361"}
{"type": "chunk", "text": "parameters, 196, 197\npasser rating, 1--2, 3, 56\npatterns, 14\nPaulson, Hank, 98\nPaxil, 121\nPayScale.com, 233\npeer assessment survey, 56\nPenn State, 56\nper capita economic output, 31\nper capita income, 16--17, 18--19, 27, 55, 216\npercentage of touchdown passes per pass attempt, 1\npercentages, 27--28, 29\nexaggeration by, 48\nformula for, 28\npercentiles, 22, 23\nperfect correlation, 60\nperfect negative correlation, 60\nPerry Preschool Study, 116\nPeto, Richard, 222\nPhiladelphia 76ers, 103\nphysics, x\nplacebo, 148, 228\nplacebo effect, 229\npolice officers, and deterrence of crime, 225--26, 227\npolling firms, 113\npolls, xv, 169--84\n\naccuracy of responses in, 181--83\ncentral limit theorem and, 130, 170--71, 174\nconfidence interval in, 171--75\nexit, 172--73\nmargin of error in, 171\nmethodology of, 178--82\npoorly done, 178\npresidential, xii\nproportion used in, 171--72\nregression analysis vs., 188\nresponse rate from, 179--80\nsample size in, 172, 175\nsampling in, 6--7, 111--13\nselection bias in, 178\non sexual activity, 6, 7, 181--83", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nparameters, 196, 197\npasser rating, 1--2, 3, 56\npatterns, 14\nPaulson, Hank, 98\nPaxil, 121\nPayScale.com, 233\npeer assessment survey, 56\nPenn State, 56\nper capita economic output, 31\nper capita income, 16--17, 18--19, 27, 55, 216\npercentage of touchdown passes per pass attempt, 1\npercentages, 27--28, 29\nexaggeration by, 48\nformula for, 28\npercentiles, 22, 23\nperfect correlation, 60\nperfect negative correlation, 60\nPerry Preschool Study, 116\nPeto, Richard, 222\nPhiladelphia 76ers, 103\nphysics, x\nplacebo, 148, 228\nplacebo effect, 229\npolice officers, and deterrence of crime, 225--26, 227\npolling firms, 113\npolls, xv, 169--84\n\naccuracy of responses in, 181--83\ncentral limit theorem and, 130, 170--71, 174\nconfidence interval in, 171--75\nexit, 172--73\nmargin of error in, 171\nmethodology of, 178--82\npoorly done, 178\npresidential, xii\nproportion used in, 171--72\nregression analysis vs., 188\nresponse rate from, 179--80\nsample size in, 172, 175\nsampling in, 6--7, 111--13\nselection bias in, 178\non sexual activity, 6, 7, 181--83", "tokens": 376, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 211, "segment_id": "00211", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000362"}
{"type": "chunk", "text": "standard error in, 172--76, 195\ntelephone, 112\n\nPorsche Cayman, 30--31\npounds, 45\npoverty, 200--201, 249--52\npoverty trap, 250\nprayer, 4, 13, 229--30, 231\nprecision, 37--38, 99, 247\npredictive analytics, 252--54\npredictive policing, 86--87, 108\npregnancy, 252--54\nPrinceton University, 233\nprinters, warranties on, 80--81, 82\nprisoners, drug treatment for, 120\nprivate information, 86\nprobability, 68--89\ncumulative, 165\nin gambling, 7\nlack of determinism in, 89\nlimits of, 9\nand marbles in urn, 112, 178--79\nusefulness of, xi\n\nprobability, problems with, 95--109\nin British justice system, 100--102\nclustering, 104--5\nand financial crisis of 2008, 7--8, 38, 95--100, 109\nprosecutor’s fallacy, 104--5\nreversion to the mean, 105--7\nstatistical discrimination, 107--9\n\nprobability density function, 79\nproductivity, 235\nprofiling, 108\nprosecutor’s fallacy, 104--5\nprostate cancer, 163, 224\nProzac, 121\nPSA test, 163\npublication bias, 120--22, 223\np-value, 152, 157n, 159, 160, 197--98\n\nQatar, 31\nquants, 95, 99\nquarterbacks, 1--2", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nstandard error in, 172--76, 195\ntelephone, 112\n\nPorsche Cayman, 30--31\npounds, 45\npoverty, 200--201, 249--52\npoverty trap, 250\nprayer, 4, 13, 229--30, 231\nprecision, 37--38, 99, 247\npredictive analytics, 252--54\npredictive policing, 86--87, 108\npregnancy, 252--54\nPrinceton University, 233\nprinters, warranties on, 80--81, 82\nprisoners, drug treatment for, 120\nprivate information, 86\nprobability, 68--89\ncumulative, 165\nin gambling, 7\nlack of determinism in, 89\nlimits of, 9\nand marbles in urn, 112, 178--79\nusefulness of, xi\n\nprobability, problems with, 95--109\nin British justice system, 100--102\nclustering, 104--5\nand financial crisis of 2008, 7--8, 38, 95--100, 109\nprosecutor’s fallacy, 104--5\nreversion to the mean, 105--7\nstatistical discrimination, 107--9\n\nprobability density function, 79\nproductivity, 235\nprofiling, 108\nprosecutor’s fallacy, 104--5\nprostate cancer, 163, 224\nProzac, 121\nPSA test, 163\npublication bias, 120--22, 223\np-value, 152, 157n, 159, 160, 197--98\n\nQatar, 31\nquants, 95, 99\nquarterbacks, 1--2", "tokens": 398, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 212, "segment_id": "00212", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000363"}
{"type": "chunk", "text": "quartiles, 22\n\nR (computer program), 259\nr (correlation coefficient), 60--61\n\ncalculation of, 65--67\n\nrace, 114, 200--201\nradio call-in show, 178\nRajasthan, India, 250\nrandom error, 106\nrandomization, 114--15\nrandomized, controlled experiments, 227--29\n\ncontrol group as counterfactual in, 240\non curing poverty, 250--52\nethics and, 227--28, 240\non prayer and surgery, 229--30\non school size, 230--31\n\nRandom Walk Down Wall Street, A (Malkiel), 125n\nrankings, 30--31, 56, 248\nRather, Dan, 53\nrational discrimination, 108\nReagan, Ronald, 49, 50\nreal figures, 46\nrecall bias, 122--23\nregression analysis, 10--12, 185--211\n\ndifficulty of, 187\non gender discrimination, 202--4\nof height and weight, 189--204, 190, 191\nin Microsoft Excel, 257\nmultiple, 199--204, 226\npolls vs., 188\nstandard error in, 195--97\nWhitehall studies, 185--87, 195, 205--7\n\nregression analysis, mistakes in, 187, 189, 211--24\n\ncorrelation mistaken for causation, 215--16\ndata mining, 221--23\nextrapolation, 220--21\nhighly correlated explanatory variables, 219--20\nwith nonlinear relationships, 214--15\nomitted variable bias, 217--19\n\nregression coefficient, 193, 195, 196\nregression equation, 191--92, 198, 201, 222\nrelative statistics, 22--23, 27", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nquartiles, 22\n\nR (computer program), 259\nr (correlation coefficient), 60--61\n\ncalculation of, 65--67\n\nrace, 114, 200--201\nradio call-in show, 178\nRajasthan, India, 250\nrandom error, 106\nrandomization, 114--15\nrandomized, controlled experiments, 227--29\n\ncontrol group as counterfactual in, 240\non curing poverty, 250--52\nethics and, 227--28, 240\non prayer and surgery, 229--30\non school size, 230--31\n\nRandom Walk Down Wall Street, A (Malkiel), 125n\nrankings, 30--31, 56, 248\nRather, Dan, 53\nrational discrimination, 108\nReagan, Ronald, 49, 50\nreal figures, 46\nrecall bias, 122--23\nregression analysis, 10--12, 185--211\n\ndifficulty of, 187\non gender discrimination, 202--4\nof height and weight, 189--204, 190, 191\nin Microsoft Excel, 257\nmultiple, 199--204, 226\npolls vs., 188\nstandard error in, 195--97\nWhitehall studies, 185--87, 195, 205--7\n\nregression analysis, mistakes in, 187, 189, 211--24\n\ncorrelation mistaken for causation, 215--16\ndata mining, 221--23\nextrapolation, 220--21\nhighly correlated explanatory variables, 219--20\nwith nonlinear relationships, 214--15\nomitted variable bias, 217--19\n\nregression coefficient, 193, 195, 196\nregression equation, 191--92, 198, 201, 222\nrelative statistics, 22--23, 27", "tokens": 420, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 213, "segment_id": "00213", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000364"}
{"type": "chunk", "text": "percentages as, 28\n\nRepublican Party, U.S.:\n\ndefense spending and, 49\npolling of, 170\ntax increase and, 29\n\nresiduals, 190--94\nresponse rate, 179--80\nreverse causality, 216--17\nreversion (regression) to the mean, 105--7\nRhode Island, 41\n“right-skewed,” 44, 133\nrisk assessment, 7--8\nrisk management, 98\nRochester, University of, 55\nRodgers, Aaron, 2, 29\nRoyal Statistical Society, 102\nRumsfeld, Donald, 14\nrupees, 45, 47\nRuth, Babe, 32\n\nSallee, Jim, 141n\nsample means, 132--33, 139, 139, 142, 150, 151, 167, 168\n\nin autism study, 156\nclustering of, 138\ndispersion of, 136\noutliers and, 138\n\nsampling, 6--7, 111--13, 134\n\nbad, 113\ncentral limit theorem and, 127--30\nof homeless, 6\nsize of, 113, 172, 175, 196, 220\n\nSanta Cruz, Calif., 86--87\nSAS, 258--59\nSAT scores, 55, 60, 62--63, 220\n\nhousehold televisions and, 63--64\nincome and, 63--64, 218--19\non math test, 25, 224\nmean and standard deviation in, 25, 26\n\nsatellites, 72\nSchlitz beer, 68--71, 79, 79, 80, 97, 99\nschools, 187, 246--49\nquality of, 51--52", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\npercentages as, 28\n\nRepublican Party, U.S.:\n\ndefense spending and, 49\npolling of, 170\ntax increase and, 29\n\nresiduals, 190--94\nresponse rate, 179--80\nreverse causality, 216--17\nreversion (regression) to the mean, 105--7\nRhode Island, 41\n“right-skewed,” 44, 133\nrisk assessment, 7--8\nrisk management, 98\nRochester, University of, 55\nRodgers, Aaron, 2, 29\nRoyal Statistical Society, 102\nRumsfeld, Donald, 14\nrupees, 45, 47\nRuth, Babe, 32\n\nSallee, Jim, 141n\nsample means, 132--33, 139, 139, 142, 150, 151, 167, 168\n\nin autism study, 156\nclustering of, 138\ndispersion of, 136\noutliers and, 138\n\nsampling, 6--7, 111--13, 134\n\nbad, 113\ncentral limit theorem and, 127--30\nof homeless, 6\nsize of, 113, 172, 175, 196, 220\n\nSanta Cruz, Calif., 86--87\nSAS, 258--59\nSAT scores, 55, 60, 62--63, 220\n\nhousehold televisions and, 63--64\nincome and, 63--64, 218--19\non math test, 25, 224\nmean and standard deviation in, 25, 26\n\nsatellites, 72\nSchlitz beer, 68--71, 79, 79, 80, 97, 99\nschools, 187, 246--49\nquality of, 51--52", "tokens": 411, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 214, "segment_id": "00214", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000365"}
{"type": "chunk", "text": "size of, 230--31\nScience, 110--11\nscorecards, 54--55\nSecurity and Exchange Commission, 86, 145\nselection bias, 118--19, 178\nself-reported voting behavior, 181, 182\nself-selection, 178\nSeptember 11, 2001, terrorist attacks of, 72--73, 74\n“Sex Study,” 181--83\nsexual behavior:\n\nof fruit flies, 110--11, 113, 114\nself-reporting of, 6, 7, 181--83\n\nShrek 2 (film), 47, 48\nsigma, see standard deviation\nsign, 193\nsignificance, 193, 195--96\n\nsize vs., 154\n\nsignificance level, 149--50, 152, 153, 157n, 166, 199\nSimon, Daniel, 73\nsimple random sample, 112\nSix Sigma Man, 70n\n60 Minutes II, 53\nsize, 193--94\n\nsignificance vs., 154\n\nslugging percentage, 31--32\nSmith, Carol, ix--x, xii\nsmoking, 116\n\ncancer caused by, xiv, 9--10, 11\nheart disease and, 115--16, 186--87, 189--90\n\nsmoking behavior, 115\nsocially insignificant effects, 194\n“Social Organization of Sexuality, The: Sexual Practices in the United States,” 181--83\nsodium, 27\nSound of Music, The (film), 48\nSouth Africa, 3\nSoviet Union, 49, 143\nspam filters, 163\nsports, streaks in, 102--3\nSports Concussion Research Program, 243\nSports Illustrated, 107\nsquash, 188--89", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nsize of, 230--31\nScience, 110--11\nscorecards, 54--55\nSecurity and Exchange Commission, 86, 145\nselection bias, 118--19, 178\nself-reported voting behavior, 181, 182\nself-selection, 178\nSeptember 11, 2001, terrorist attacks of, 72--73, 74\n“Sex Study,” 181--83\nsexual behavior:\n\nof fruit flies, 110--11, 113, 114\nself-reporting of, 6, 7, 181--83\n\nShrek 2 (film), 47, 48\nsigma, see standard deviation\nsign, 193\nsignificance, 193, 195--96\n\nsize vs., 154\n\nsignificance level, 149--50, 152, 153, 157n, 166, 199\nSimon, Daniel, 73\nsimple random sample, 112\nSix Sigma Man, 70n\n60 Minutes II, 53\nsize, 193--94\n\nsignificance vs., 154\n\nslugging percentage, 31--32\nSmith, Carol, ix--x, xii\nsmoking, 116\n\ncancer caused by, xiv, 9--10, 11\nheart disease and, 115--16, 186--87, 189--90\n\nsmoking behavior, 115\nsocially insignificant effects, 194\n“Social Organization of Sexuality, The: Sexual Practices in the United States,” 181--83\nsodium, 27\nSound of Music, The (film), 48\nSouth Africa, 3\nSoviet Union, 49, 143\nspam filters, 163\nsports, streaks in, 102--3\nSports Concussion Research Program, 243\nSports Illustrated, 107\nsquash, 188--89", "tokens": 404, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 215, "segment_id": "00215", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000366"}
{"type": "chunk", "text": "Staiger, Doug, 247--48\nStandard & Poor’s 500, 123--24\nstandard deviation, 23--25, 146, 196n\n\nin Atlanta cheating scandal, 149\nin autism study, 156\nin bell curve, 133\ncentral limit theorem and, 129, 131\nin correlation coefficient, 67\nformula for, 35\nin Microsoft Excel, 257\nin normal distribution, 26\n\nstandard error, 136--42, 152, 184\n\nin autism study, 159--60\nfor difference of means, 164--65\nformula for, 138, 150, 172, 176--77\nin polling, 172--76\nin regression analysis, 195--97\n\nstandardized tests, 246\n\ncentral limit theorem and, 129--30\ncheating on, 4, 8--9, 86, 145, 148--49\nas misleading indicator, 51--52, 53--54\nrelative statistics produced by, 22--23\nsee also SAT scores\n\nstandard units, 65\nStanford University, 103\nStar Wars Episode IV (film), 47, 48\nStata, 258\nstatistical discrimination, 107--9\nstatistical examples:\n\nauthor accused of cheating, 143--44, 149\nauthor’s investment, 28--29\naverage income, 16--17, 18--19, 27\nof basketball players’ heights, 156--57, 159, 166--68\ncentral limit theorem and, 127--28, 130, 131, 132\ncentral tendency, 17--18, 19, 20--21, 20, 21\ncredit risks, 88\ncrime and, 86--87\non effectiveness of teachers, 248--49\nFramingham study, 115--16, 136\ngolf range finder, 38, 99\nmale pattern baldness drug, 82--84, 83", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nStaiger, Doug, 247--48\nStandard & Poor’s 500, 123--24\nstandard deviation, 23--25, 146, 196n\n\nin Atlanta cheating scandal, 149\nin autism study, 156\nin bell curve, 133\ncentral limit theorem and, 129, 131\nin correlation coefficient, 67\nformula for, 35\nin Microsoft Excel, 257\nin normal distribution, 26\n\nstandard error, 136--42, 152, 184\n\nin autism study, 159--60\nfor difference of means, 164--65\nformula for, 138, 150, 172, 176--77\nin polling, 172--76\nin regression analysis, 195--97\n\nstandardized tests, 246\n\ncentral limit theorem and, 129--30\ncheating on, 4, 8--9, 86, 145, 148--49\nas misleading indicator, 51--52, 53--54\nrelative statistics produced by, 22--23\nsee also SAT scores\n\nstandard units, 65\nStanford University, 103\nStar Wars Episode IV (film), 47, 48\nStata, 258\nstatistical discrimination, 107--9\nstatistical examples:\n\nauthor accused of cheating, 143--44, 149\nauthor’s investment, 28--29\naverage income, 16--17, 18--19, 27\nof basketball players’ heights, 156--57, 159, 166--68\ncentral limit theorem and, 127--28, 130, 131, 132\ncentral tendency, 17--18, 19, 20--21, 20, 21\ncredit risks, 88\ncrime and, 86--87\non effectiveness of teachers, 248--49\nFramingham study, 115--16, 136\ngolf range finder, 38, 99\nmale pattern baldness drug, 82--84, 83", "tokens": 438, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 216, "segment_id": "00216", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000367"}
{"type": "chunk", "text": "marbles from urn, 112, 178--79\nNetflix algorithm, 4, 58--59, 60, 62, 64--65\nPerry Preschool Study, 116\npredictive policing, 86--87, 108\nof rare disease, 84--85, 85\nSchlitz beer, 68--71, 79, 79, 80\nsodium, 27\nstandard deviation, 23--25\nvalue at risk, 38, 95--97, 98--100\nsee also longitudinal studies\n\nstatistically significant findings, 11, 153, 154--55, 194, 221\nstatistical software, 257--60\nstatistics:\n\ndata vs., 111\nas detective work, 10--11, 14\nerrors in, 14\nlack of certainty in, 144--46\nlying with, 14\nmisleading conclusions from, xiv, 6\nrelative vs. absolute, 22--23\nreputation of, xii, 1\nas summary, 5, 14, 15--16, 17\nubiquity of, xii\nundesirable behavior caused by, 6\nusefulness of, xi, xii, xv, 3, 14\nsee also deceptive description; descriptive statistics\n\nsteroids, 243\nstock market, 71, 89\nstreaks, 102--3\nstress, heart disease and, 185--87, 205--7\nstroke, 116\nstudent selectivity, 55\nsubstance abuse, 110--11\nSuburban Cook County Tuberculosis Sanitarium District, 48--49\nsudden infant death syndrome (SIDS), 101--2\nsummation sign, 66\nsummer school, 238\nSuper Bowl, 68--71, 79, 97\nSupreme Court, U.S., 254\nsurgery, prayer and, 4, 13, 229--30, 231\nSurowiecki, James, 241", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nmarbles from urn, 112, 178--79\nNetflix algorithm, 4, 58--59, 60, 62, 64--65\nPerry Preschool Study, 116\npredictive policing, 86--87, 108\nof rare disease, 84--85, 85\nSchlitz beer, 68--71, 79, 79, 80\nsodium, 27\nstandard deviation, 23--25\nvalue at risk, 38, 95--97, 98--100\nsee also longitudinal studies\n\nstatistically significant findings, 11, 153, 154--55, 194, 221\nstatistical software, 257--60\nstatistics:\n\ndata vs., 111\nas detective work, 10--11, 14\nerrors in, 14\nlack of certainty in, 144--46\nlying with, 14\nmisleading conclusions from, xiv, 6\nrelative vs. absolute, 22--23\nreputation of, xii, 1\nas summary, 5, 14, 15--16, 17\nubiquity of, xii\nundesirable behavior caused by, 6\nusefulness of, xi, xii, xv, 3, 14\nsee also deceptive description; descriptive statistics\n\nsteroids, 243\nstock market, 71, 89\nstreaks, 102--3\nstress, heart disease and, 185--87, 205--7\nstroke, 116\nstudent selectivity, 55\nsubstance abuse, 110--11\nSuburban Cook County Tuberculosis Sanitarium District, 48--49\nsudden infant death syndrome (SIDS), 101--2\nsummation sign, 66\nsummer school, 238\nSuper Bowl, 68--71, 79, 97\nSupreme Court, U.S., 254\nsurgery, prayer and, 4, 13, 229--30, 231\nSurowiecki, James, 241", "tokens": 445, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 217, "segment_id": "00217", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000368"}
{"type": "chunk", "text": "survivorship bias, 123--25\nSweden, 3\nswimming pools, 72\n\nTabarrok, Alexander, 227\ntail risk, 98, 99\ntails, fat, 208, 209--10\nTaleb, Nicholas, 98--99\nTarget, 52--54\nTate, Geoffrey, 107\ntau, 243\nTaubes, Gary, 125\ntax, income, 29, 114\ntax cuts, 43, 114, 180, 235\nt-distribution, 196, 208--11\nteacher quality, 51\nteachers, 246--49\n\nabsenteeism among, 250\npay of, 4\n\ntechnology companies, 154--55\ntelecommunications, 42\ntelephone polls, 112\ntelevisions, 63--64\nTen Commandments, The (film), 48\nTenessee Project STAR experiment, 230--31\nterrorism, terrorists, 163--64\n\nalert system for, 227\ncauses of, 11, 12--13\nof 9/11, 72--73, 74\nrisks of, 73\n\ntest scores, 53, 198\n\nmean-reversion and, 106\nsee also SAT scores; standardized tests\n\nTexas, 41, 54\ntexting while driving, 88\nthimerosal, 246\nTierney, John, 91--92\nTitanic (film), 47\ntouchdowns, 1\ntrade, 41\ntreatment, 9, 113--14, 225, 226--27\ntreatment group, 114, 126, 227--29, 238--39", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nsurvivorship bias, 123--25\nSweden, 3\nswimming pools, 72\n\nTabarrok, Alexander, 227\ntail risk, 98, 99\ntails, fat, 208, 209--10\nTaleb, Nicholas, 98--99\nTarget, 52--54\nTate, Geoffrey, 107\ntau, 243\nTaubes, Gary, 125\ntax, income, 29, 114\ntax cuts, 43, 114, 180, 235\nt-distribution, 196, 208--11\nteacher quality, 51\nteachers, 246--49\n\nabsenteeism among, 250\npay of, 4\n\ntechnology companies, 154--55\ntelecommunications, 42\ntelephone polls, 112\ntelevisions, 63--64\nTen Commandments, The (film), 48\nTenessee Project STAR experiment, 230--31\nterrorism, terrorists, 163--64\n\nalert system for, 227\ncauses of, 11, 12--13\nof 9/11, 72--73, 74\nrisks of, 73\n\ntest scores, 53, 198\n\nmean-reversion and, 106\nsee also SAT scores; standardized tests\n\nTexas, 41, 54\ntexting while driving, 88\nthimerosal, 246\nTierney, John, 91--92\nTitanic (film), 47\ntouchdowns, 1\ntrade, 41\ntreatment, 9, 113--14, 225, 226--27\ntreatment group, 114, 126, 227--29, 238--39", "tokens": 373, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 218, "segment_id": "00218", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000369"}
{"type": "chunk", "text": "true population coefficient, 208\ntrue population parameter, 196, 197\nt-statistic, 197n\nTunisia, 170n\nTversky, Amos, 103\nTwain, Mark, 36\ntwin studies, 245\ntwo-point conversions, 71, 77--78\n“two-tailed” hypothesis test, 151, 166--68\nType I errors (false positives), 84--85, 162--64\nType II errors (false negatives), 84, 162--64\n\nuncertainty, 71, 74\nunemployment, 217, 236--37, 236, 241\nunions, 47, 247\nUnited Nations Human Development Index, 31, 55\nUnited States, 65\nGini index of, 3\nmanufacturing in, 39--40, 39\nmean of height in, 25, 26\nmiddle class in, 13, 15, 16--17, 32\nper capita economic output of, 31\n\nunit of analysis, 40--42\nU.S. News & World Report, 55--57, 248\n\nvaccinations, 245--46\nVallone, Robert, 103\nvalue-added assessments, 247--48\nvalue at risk, 38, 95--97, 98--100\nvariables, 224\n\ndependent, 192, 193--94, 197, 198, 199, 206n, 216, 217, 226\nexplanatory (independent), 192, 193--94, 197, 198, 199, 203, 217\nhighly correlated, 219--20\n\nVarian, Hal, 4\nvariance, 24\n\nformula for, 34--35\noutliers in, 34\n\nVerizon, 42\nVermont, 41\nveterans’ housing, 45--46\nVick, Michael, 242\nvitamins, 125", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\ntrue population coefficient, 208\ntrue population parameter, 196, 197\nt-statistic, 197n\nTunisia, 170n\nTversky, Amos, 103\nTwain, Mark, 36\ntwin studies, 245\ntwo-point conversions, 71, 77--78\n“two-tailed” hypothesis test, 151, 166--68\nType I errors (false positives), 84--85, 162--64\nType II errors (false negatives), 84, 162--64\n\nuncertainty, 71, 74\nunemployment, 217, 236--37, 236, 241\nunions, 47, 247\nUnited Nations Human Development Index, 31, 55\nUnited States, 65\nGini index of, 3\nmanufacturing in, 39--40, 39\nmean of height in, 25, 26\nmiddle class in, 13, 15, 16--17, 32\nper capita economic output of, 31\n\nunit of analysis, 40--42\nU.S. News & World Report, 55--57, 248\n\nvaccinations, 245--46\nVallone, Robert, 103\nvalue-added assessments, 247--48\nvalue at risk, 38, 95--97, 98--100\nvariables, 224\n\ndependent, 192, 193--94, 197, 198, 199, 206n, 216, 217, 226\nexplanatory (independent), 192, 193--94, 197, 198, 199, 203, 217\nhighly correlated, 219--20\n\nVarian, Hal, 4\nvariance, 24\n\nformula for, 34--35\noutliers in, 34\n\nVerizon, 42\nVermont, 41\nveterans’ housing, 45--46\nVick, Michael, 242\nvitamins, 125", "tokens": 440, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 219, "segment_id": "00219", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000370"}
{"type": "chunk", "text": "Copyright\n\nCopyright © 2013 by Charles Wheelan\n\nAll rights reserved\nPrinted in the United States of America\nFirst Edition\n\nFor information about permission to reproduce selections from this book, write to Permissions, W. W. Norton &\nCompany, Inc.,\n500 Fifth Avenue, New York, NY 10110\n\nFor information about special discounts for bulk purchases,\nplease contact W. W. Norton Special Sales at\nspecialsales@wwnorton.com or 800-233-4830\n\nManufacturing by Courier Westford\nProduction manager: Anna Oler\n\nISBN 978-0-393-07195-5 (hardcover)\neISBN 978-0-393-08982-0\n\nW. W. Norton & Company, Inc. 500 Fifth Avenue, New York, N.Y. 10110W. W. Norton & Company Ltd. Castle House, 75/76 Wells Street, London W1T 3QT", "full_text": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in\n\nCopyright\n\nCopyright © 2013 by Charles Wheelan\n\nAll rights reserved\nPrinted in the United States of America\nFirst Edition\n\nFor information about permission to reproduce selections from this book, write to Permissions, W. W. Norton &\nCompany, Inc.,\n500 Fifth Avenue, New York, NY 10110\n\nFor information about special discounts for bulk purchases,\nplease contact W. W. Norton Special Sales at\nspecialsales@wwnorton.com or 800-233-4830\n\nManufacturing by Courier Westford\nProduction manager: Anna Oler\n\nISBN 978-0-393-07195-5 (hardcover)\neISBN 978-0-393-08982-0\n\nW. W. Norton & Company, Inc. 500 Fifth Avenue, New York, N.Y. 10110W. W. Norton & Company Ltd. Castle House, 75/76 Wells Street, London W1T 3QT", "tokens": 195, "metadata": {"source_file": "naked-statistics-pdf.pdf", "page_num": 221, "segment_id": "00221", "chapter_num": "9", "chapter_title": "Inference", "section_num": "2011.6", "section_title": "The probability of that happening to the same person is somewhere in the range of 1 in", "has_table": false}, "context": "Source: naked-statistics-pdf.pdf | Chapter 9: Inference | Section 2011.6: The probability of that happening to the same person is somewhere in the range of 1 in", "chunk_id": "00000371"}
{"type": "audit", "total_chunks": 372, "source_pages": 222, "chunks_per_page": 1.6756756756756757, "avg_tokens": 402.9, "min_tokens": 115, "max_tokens": 525, "chunks_with_context": 372, "context_coverage": 1.0, "stage": "chunk", "version": "2.0.0", "created_at": "2025-11-05T13:18:48Z"}

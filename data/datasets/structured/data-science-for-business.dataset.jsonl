===DATASET_BEGIN===
{"segment_id": "__header__", "source": {"title": "Data Science for Business", "author": "Foster Provost and Tom Fawcett", "file_name": "Data-Science-for-Business.pdf", "file_size": 6563148, "pages": 409}, "book": "data-science-for-business", "total_cards": 409, "segment_ids": ["00001", "00002", "00003", "00004", "00005", "00006", "00007", "00008", "00009", "00010", "00011", "00012", "00013", "00014", "00015", "00016", "00017", "00018", "00019", "00020", "00021", "00022", "00023", "00024", "00025", "00026", "00027", "00028", "00029", "00030", "00031", "00032", "00033", "00034", "00035", "00036", "00037", "00038", "00039", "00040", "00041", "00042", "00043", "00044", "00045", "00046", "00047", "00048", "00049", "00050", "00051", "00052", "00053", "00054", "00055", "00056", "00057", "00058", "00059", "00060", "00061", "00062", "00063", "00064", "00065", "00066", "00067", "00068", "00069", "00070", "00071", "00072", "00073", "00074", "00075", "00076", "00077", "00078", "00079", "00080", "00081", "00082", "00083", "00084", "00085", "00086", "00087", "00088", "00089", "00090", "00091", "00092", "00093", "00094", "00095", "00096", "00097", "00098", "00099", "00100", "00101", "00102", "00103", "00104", "00105", "00106", "00107", "00108", "00109", "00110", "00111", "00112", "00113", "00114", "00115", "00116", "00117", "00118", "00119", "00120", "00121", "00122", "00123", "00124", "00125", "00126", "00127", "00128", "00129", "00130", "00131", "00132", "00133", "00134", "00135", "00136", "00137", "00138", "00139", "00140", "00141", "00142", "00143", "00144", "00145", "00146", "00147", "00148", "00149", "00150", "00151", "00152", "00153", "00154", "00155", "00156", "00157", "00158", "00159", "00160", "00161", "00162", "00163", "00164", "00165", "00166", "00167", "00168", "00169", "00170", "00171", "00172", "00173", "00174", "00175", "00176", "00177", "00178", "00179", "00180", "00181", "00182", "00183", "00184", "00185", "00186", "00187", "00188", "00189", "00190", "00191", "00192", "00193", "00194", "00195", "00196", "00197", "00198", "00199", "00200", "00201", "00202", "00203", "00204", "00205", "00206", "00207", "00208", "00209", "00210", "00211", "00212", "00213", "00214", "00215", "00216", "00217", "00218", "00219", "00220", "00221", "00222", "00223", "00224", "00225", "00226", "00227", "00228", "00229", "00230", "00231", "00232", "00233", "00234", "00235", "00236", "00237", "00238", "00239", "00240", "00241", "00242", "00243", "00244", "00245", "00246", "00247", "00248", "00249", "00250", "00251", "00252", "00253", "00254", "00255", "00256", "00257", "00258", "00259", "00260", "00261", "00262", "00263", "00264", "00265", "00266", "00267", "00268", "00269", "00270", "00271", "00272", "00273", "00274", "00275", "00276", "00277", "00278", "00279", "00280", "00281", "00282", "00283", "00284", "00285", "00286", "00287", "00288", "00289", "00290", "00291", "00292", "00293", "00294", "00295", "00296", "00297", "00298", "00299", "00300", "00301", "00302", "00303", "00304", "00305", "00306", "00307", "00308", "00309", "00310", "00311", "00312", "00313", "00314", "00315", "00316", "00317", "00318", "00319", "00320", "00321", "00322", "00323", "00324", "00325", "00326", "00327", "00328", "00329", "00330", "00331", "00332", "00333", "00334", "00335", "00336", "00337", "00338", "00339", "00340", "00341", "00342", "00343", "00344", "00345", "00346", "00347", "00348", "00349", "00350", "00351", "00352", "00353", "00354", "00355", "00356", "00357", "00358", "00359", "00360", "00361", "00362", "00363", "00364", "00365", "00366", "00367", "00368", "00369", "00370", "00371", "00372", "00373", "00374", "00375", "00376", "00377", "00378", "00379", "00380", "00381", "00382", "00383", "00384", "00385", "00386", "00387", "00388", "00389", "00390", "00391", "00392", "00393", "00394", "00395", "00396", "00397", "00398", "00399", "00400", "00401", "00402", "00403", "00404", "00405", "00406", "00407", "00408", "00409"], "dataset_created_at": "2025-11-05T11:25:37Z", "pdf_sha256": "d75ce63b8831db857a455fc47d5a9076949a7028eba08ca5d021da12fd21b9e4", "version": "2.0.0", "product": "archivist magika 2.0.0", "created_at": "2025-11-05T11:25:37Z", "stage": "structured", "structure_detected_at": "2025-11-05T11:25:37Z", "chapters": 1, "sections": 1, "toc": {"found": true, "start_page": 9, "end_page": 15, "page_count": 7}}
{"segment_id": "00001", "page_num": 1, "segment": "", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "none", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00002", "page_num": 2, "segment": "", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "none", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00003", "page_num": 3, "segment": "Praise\n\n“A must-read resource for anyone who is serious\nabout embracing the opportunity of big data.”\n\n--- Craig Vaughan\nGlobal Vice President at SAP\n\n“This timely book says out loud what has finally become apparent: in the modern world,\nData is Business, and you can no longer think business without thinking data. Read this\nbook and you will understand the Science behind thinking data.”\n\n--- Ron Bekkerman\nChief Data Officer at Carmel Ventures\n\n“A great book for business managers who lead or interact with data scientists, who wish to\nbetter understand the principals and algorithms available without the technical details of\nsingle-disciplinary books.”\n\n--- Ronny Kohavi\nPartner Architect at Microsoft Online Services Division\n\n“Provost and Fawcett have distilled their mastery of both the art and science of real-world\ndata analysis into an unrivalled introduction to the field.”\n\n---Geoff Webb\nEditor-in-Chief of Data Mining and Knowledge\nDiscovery Journal\n\n“I would love it if everyone I had to work with had read this book.”\n\n--- Claudia Perlich\nChief Scientist of M6D (Media6Degrees) and Advertising\nResearch Foundation Innovation Award Grand Winner (2013)", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00004", "page_num": 4, "segment": "“A foundational piece in the fast developing world of Data Science.\nA must read for anyone interested in the Big Data revolution.\"\n\n---Justin Gapper\nBusiness Unit Analytics Manager\nat Teledyne Scientific and Imaging\n\n“The authors, both renowned experts in data science before it had a name, have taken a\ncomplex topic and made it accessible to all levels, but mostly helpful to the budding data\nscientist. As far as I know, this is the first book of its kind---with a focus on data science\nconcepts as applied to practical business problems. It is liberally sprinkled with compelling\nreal-world examples outlining familiar, accessible problems in the business world: customer\nchurn, targeted marking, even whiskey analytics!\n\nThe book is unique in that it does not give a cookbook of algorithms, rather it helps the\nreader understand the underlying concepts behind data science, and most importantly how\nto approach and be successful at problem solving. Whether you are looking for a good\ncomprehensive overview of data science or are a budding data scientist in need of the basics,\nthis is a must-read.”\n\n--- Chris Volinsky\nDirector of Statistics Research at AT&T Labs and Winning\nTeam Member for the $1 Million Netflix Challenge\n\n“This book goes beyond data analytics 101. It’s the essential guide for those of us (all of us?)\nwhose businesses are built on the ubiquity of data opportunities and the new mandate for\ndata-driven decision-making.”\n\n---Tom Phillips\nCEO of Media6Degrees and Former Head of\nGoogle Search and Analytics\n\n“Intelligent use of data has become a force powering business to new levels of\ncompetitiveness. To thrive in this data-driven ecosystem, engineers, analysts, and managers\nalike must understand the options, design choices, and tradeoffs before them. With\nmotivating examples, clear exposition, and a breadth of details covering not only the “hows”\nbut the “whys”, Data Science for Business is the perfect primer for those wishing to become\ninvolved in the development and application of data-driven systems.”\n\n---Josh Attenberg\nData Science Lead at Etsy", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00005", "page_num": 5, "segment": "“Data is the foundation of new waves of productivity growth, innovation, and richer\ncustomer insight. Only recently viewed broadly as a source of competitive advantage, dealing\nwell with data is rapidly becoming table stakes to stay in the game. The authors’ deep applied\nexperience makes this a must read---a window into your competitor’s strategy.”\n\n--- Alan Murray\nSerial Entrepreneur; Partner at Coriolis Ventures\n\n“One of the best data mining books, which helped me think through various ideas on\nliquidity analysis in the FX business. The examples are excellent and help you take a deep\ndive into the subject! This one is going to be on my shelf for lifetime!”\n\n--- Nidhi Kathuria\nVice President of FX at Royal Bank of Scotland", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00006", "page_num": 6, "segment": "", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "none", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00007", "page_num": 7, "segment": "Data Science for Business\n\nFoster Provost and Tom Fawcett", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00008", "page_num": 8, "segment": "Data Science for Business\nby Foster Provost and Tom Fawcett\n\nCopyright © 2013 Foster Provost and Tom Fawcett. All rights reserved.\n\nPrinted in the United States of America.\n\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles (http://my.safaribooksonline.com). For more information, contact our corporate/\ninstitutional sales department: 800-998-9938 or corporate@oreilly.com.\n\nEditors: Mike Loukides and Meghan Blanchette\nProduction Editor: Christopher Hearse\nProofreader: Kiel Van Horn\nIndexer: WordCo Indexing Services, Inc.\n\nCover Designer: Mark Paglietti\nInterior Designer: David Futato\nIllustrator: Rebecca Demarest\n\nJuly 2013:\n\nFirst Edition\n\nRevision History for the First Edition:\n\n2013-07-25: First release\n\nSee http://oreilly.com/catalog/errata.csp?isbn=9781449361327 for release details.\n\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks. Where those designations\nappear in this book, and O’Reilly Media, Inc., was aware of a trademark claim, the designations have been\nprinted in caps or initial caps. Data Science for Business is a trademark of Foster Provost and Tom Fawcett.\n\nWhile every precaution has been taken in the preparation of this book, the publisher and authors assume\nno responsibility for errors or omissions, or for damages resulting from the use of the information contained\nherein.\n\nISBN: 978-1-449-36132-7\n\n[LSI]", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00009", "page_num": 9, "segment": "Table of Contents\n\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi\n\n1.\n\nIntroduction: Data-Analytic Thinking. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nThe Ubiquity of Data Opportunities 1\nExample: Hurricane Frances 3\nExample: Predicting Customer Churn 4\nData Science, Engineering, and Data-Driven Decision Making 4\nData Processing and “Big Data” 7\nFrom Big Data 1.0 to Big Data 2.0 8\nData and Data Science Capability as a Strategic Asset 9\nData-Analytic Thinking 12\nThis Book 14\nData Mining and Data Science, Revisited 14\nChemistry Is Not About Test Tubes: Data Science Versus the Work of the Data\n\nScientist 15\nSummary 16\n\n2. Business Problems and Data Science Solutions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nFundamental concepts: A set of canonical data mining tasks; The data mining process;\nSupervised versus unsupervised data mining.\nFrom Business Problems to Data Mining Tasks 19\nSupervised Versus Unsupervised Methods 24\nData Mining and Its Results 25\nThe Data Mining Process 26\nBusiness Understanding 27\nData Understanding 28\nData Preparation 29\nModeling 31\nEvaluation 31\n\niii", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00010", "page_num": 10, "segment": "Deployment 32\nImplications for Managing the Data Science Team 34\nOther Analytics Techniques and Technologies 35\nStatistics 35\nDatabase Querying 37\nData Warehousing 38\nRegression Analysis 39\nMachine Learning and Data Mining 39\nAnswering Business Questions with These Techniques 40\nSummary 41\n\n3.\n\n 43\n\nIntroduction to Predictive Modeling: From Correlation to Supervised Segmentation.\nFundamental concepts: Identifying informative attributes; Segmenting data by\nprogressive attribute selection.\nExemplary techniques: Finding correlations; Attribute/variable selection; Tree\ninduction.\nModels, Induction, and Prediction 44\nSupervised Segmentation 48\nSelecting Informative Attributes 49\nExample: Attribute Selection with Information Gain 56\nSupervised Segmentation with Tree-Structured Models 62\nVisualizing Segmentations 67\nTrees as Sets of Rules 71\nProbability Estimation 71\nExample: Addressing the Churn Problem with Tree Induction 73\nSummary 78\n\n4. Fitting a Model to Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\nFundamental concepts: Finding “optimal” model parameters based on data; Choosing\nthe goal for data mining; Objective functions; Loss functions.\nExemplary techniques: Linear regression; Logistic regression; Support-vector machines.\nClassification via Mathematical Functions 83\nLinear Discriminant Functions 85\nOptimizing an Objective Function 87\nAn Example of Mining a Linear Discriminant from Data 88\nLinear Discriminant Functions for Scoring and Ranking Instances 90\nSupport Vector Machines, Briefly 91\nRegression via Mathematical Functions 94\nClass Probability Estimation and Logistic “Regression” 96\n* Logistic Regression: Some Technical Details 99\nExample: Logistic Regression versus Tree Induction 102\nNonlinear Functions, Support Vector Machines, and Neural Networks 105\n\niv\n\n|\n\nTable of Contents", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00011", "page_num": 11, "segment": "Summary 108\n\n5. Overfitting and Its Avoidance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\nFundamental concepts: Generalization; Fitting and overfitting; Complexity control.\nExemplary techniques: Cross-validation; Attribute selection; Tree pruning;\nRegularization.\nGeneralization 111\nOverfitting 113\nOverfitting Examined 113\nHoldout Data and Fitting Graphs 113\nOverfitting in Tree Induction 116\nOverfitting in Mathematical Functions 118\nExample: Overfitting Linear Functions 119\n* Example: Why Is Overfitting Bad? 124\nFrom Holdout Evaluation to Cross-Validation 126\nThe Churn Dataset Revisited 129\nLearning Curves 130\nOverfitting Avoidance and Complexity Control 133\nAvoiding Overfitting with Tree Induction 133\nA General Method for Avoiding Overfitting 134\n* Avoiding Overfitting for Parameter Optimization 136\nSummary 140\n\n6. Similarity, Neighbors, and Clusters. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\n\nFundamental concepts: Calculating similarity of objects described by data; Using\nsimilarity for prediction; Clustering as similarity-based segmentation.\nExemplary techniques: Searching for similar entities; Nearest neighbor methods;\nClustering methods; Distance metrics for calculating similarity.\nSimilarity and Distance 142\nNearest-Neighbor Reasoning 144\nExample: Whiskey Analytics 144\nNearest Neighbors for Predictive Modeling 146\nHow Many Neighbors and How Much Influence? 149\nGeometric Interpretation, Overfitting, and Complexity Control 151\nIssues with Nearest-Neighbor Methods 154\nSome Important Technical Details Relating to Similarities and Neighbors 157\nHeterogeneous Attributes 157\n* Other Distance Functions 158\n* Combining Functions: Calculating Scores from Neighbors 161\nClustering 163\nExample: Whiskey Analytics Revisited 163\nHierarchical Clustering 164\n\nTable of Contents\n\n|\n\nv", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00012", "page_num": 12, "segment": "Nearest Neighbors Revisited: Clustering Around Centroids 169\nExample: Clustering Business News Stories 174\nUnderstanding the Results of Clustering 177\n* Using Supervised Learning to Generate Cluster Descriptions 179\nStepping Back: Solving a Business Problem Versus Data Exploration 182\nSummary 184\n\n7. Decision Analytic Thinking I: What Is a Good Model?. . . . . . . . . . . . . . . . . . . . . . . . . . . . 187\nFundamental concepts: Careful consideration of what is desired from data science\nresults; Expected value as a key evaluation framework; Consideration of appropriate\ncomparative baselines.\nExemplary techniques: Various evaluation metrics; Estimating costs and benefits;\nCalculating expected profit; Creating baseline methods for comparison.\nEvaluating Classifiers 188\nPlain Accuracy and Its Problems 189\nThe Confusion Matrix 189\nProblems with Unbalanced Classes 190\nProblems with Unequal Costs and Benefits 193\nGeneralizing Beyond Classification 193\nA Key Analytical Framework: Expected Value 194\nUsing Expected Value to Frame Classifier Use 195\nUsing Expected Value to Frame Classifier Evaluation 196\nEvaluation, Baseline Performance, and Implications for Investments in Data 204\nSummary 207\n\n8. Visualizing Model Performance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209\nFundamental concepts: Visualization of model performance under various kinds of\nuncertainty; Further consideration of what is desired from data mining results.\nExemplary techniques: Profit curves; Cumulative response curves; Lift curves; ROC\ncurves.\nRanking Instead of Classifying 209\nProfit Curves 212\nROC Graphs and Curves 214\nThe Area Under the ROC Curve (AUC) 219\nCumulative Response and Lift Curves 219\nExample: Performance Analytics for Churn Modeling 223\nSummary 231\n\n9. Evidence and Probabilities. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233\nFundamental concepts: Explicit evidence combination with Bayes’ Rule; Probabilistic\nreasoning via assumptions of conditional independence.\nExemplary techniques: Naive Bayes classification; Evidence lift.\n\nvi\n\n|\n\nTable of Contents", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00013", "page_num": 13, "segment": "Example: Targeting Online Consumers With Advertisements 233\nCombining Evidence Probabilistically 235\nJoint Probability and Independence 236\nBayes’ Rule 237\nApplying Bayes’ Rule to Data Science 239\nConditional Independence and Naive Bayes 240\nAdvantages and Disadvantages of Naive Bayes 242\nA Model of Evidence “Lift” 244\nExample: Evidence Lifts from Facebook “Likes” 245\nEvidence in Action: Targeting Consumers with Ads 247\nSummary 247\n\n10. Representing and Mining Text. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249\n\nFundamental concepts: The importance of constructing mining-friendly data\nrepresentations; Representation of text for data mining.\nExemplary techniques: Bag of words representation; TFIDF calculation; N-grams;\nStemming; Named entity extraction; Topic models.\nWhy Text Is Important 250\nWhy Text Is Difficult 250\nRepresentation 251\nBag of Words 252\nTerm Frequency 252\nMeasuring Sparseness: Inverse Document Frequency 254\nCombining Them: TFIDF 256\nExample: Jazz Musicians 256\n* The Relationship of IDF to Entropy 261\nBeyond Bag of Words 263\nN-gram Sequences 263\nNamed Entity Extraction 264\nTopic Models 264\nExample: Mining News Stories to Predict Stock Price Movement 266\nThe Task 266\nThe Data 268\nData Preprocessing 270\nResults 271\nSummary 275\n\n11. Decision Analytic Thinking II: Toward Analytical Engineering. . . . . . . . . . . . . . . . . . . .\nFundamental concept: Solving business problems with data science starts with\nanalytical engineering: designing an analytical solution, based on the data, tools, and\ntechniques available.\nExemplary technique: Expected value as a framework for data science solution design.\n\n 277\n\nTable of Contents\n\n|\n\nvii", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00014", "page_num": 14, "segment": "Targeting the Best Prospects for a Charity Mailing 278\n\nThe Expected Value Framework: Decomposing the Business Problem and\n\nRecomposing the Solution Pieces 278\nA Brief Digression on Selection Bias 280\nOur Churn Example Revisited with Even More Sophistication 281\n\nThe Expected Value Framework: Structuring a More Complicated Business\n\nProblem 281\nAssessing the Influence of the Incentive 283\nFrom an Expected Value Decomposition to a Data Science Solution 284\nSummary 287\n\n12. Other Data Science Tasks and Techniques. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289\nFundamental concepts: Our fundamental concepts as the basis of many common data\nscience techniques; The importance of familiarity with the building blocks of data\nscience.\nExemplary techniques: Association and co-occurrences; Behavior profiling; Link\nprediction; Data reduction; Latent information mining; Movie recommendation; Biasvariance decomposition of error; Ensembles of models; Causal reasoning from data.\nCo-occurrences and Associations: Finding Items That Go Together 290\nMeasuring Surprise: Lift and Leverage 291\nExample: Beer and Lottery Tickets 292\nAssociations Among Facebook Likes 293\nProfiling: Finding Typical Behavior 296\nLink Prediction and Social Recommendation 301\nData Reduction, Latent Information, and Movie Recommendation 302\nBias, Variance, and Ensemble Methods 306\nData-Driven Causal Explanation and a Viral Marketing Example 309\nSummary 310\n\n13. Data Science and Business Strategy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313\n\nFundamental concepts: Our principles as the basis of success for a data-driven\nbusiness; Acquiring and sustaining competitive advantage via data science; The\nimportance of careful curation of data science capability.\nThinking Data-Analytically, Redux 313\nAchieving Competitive Advantage with Data Science 315\nSustaining Competitive Advantage with Data Science 316\nFormidable Historical Advantage 317\nUnique Intellectual Property 317\nUnique Intangible Collateral Assets 318\nSuperior Data Scientists 318\nSuperior Data Science Management 320\nAttracting and Nurturing Data Scientists and Their Teams 321\n\nviii\n\n|\n\nTable of Contents", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00015", "page_num": 15, "segment": "Examine Data Science Case Studies 323\nBe Ready to Accept Creative Ideas from Any Source 324\nBe Ready to Evaluate Proposals for Data Science Projects 324\nExample Data Mining Proposal 325\nFlaws in the Big Red Proposal 326\nA Firm’s Data Science Maturity 327\n\n14. Conclusion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n 331\nThe Fundamental Concepts of Data Science 331\n\nApplying Our Fundamental Concepts to a New Problem: Mining Mobile\n\nDevice Data 334\nChanging the Way We Think about Solutions to Business Problems 337\nWhat Data Can’t Do: Humans in the Loop, Revisited 338\nPrivacy, Ethics, and Mining Data About Individuals 341\nIs There More to Data Science? 342\nFinal Example: From Crowd-Sourcing to Cloud-Sourcing 343\nFinal Words 344\n\nA. Proposal Review Guide. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347\n\nB. Another Sample Proposal. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351\n\nGlossary. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355\n\nBibliography. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359\n\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367\n\nTable of Contents\n\n|\n\nix", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00016", "page_num": 16, "segment": "", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "none", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00017", "page_num": 17, "segment": "Preface\n\nData Science for Business is intended for several sorts of readers:\n\n• Business people who will be working with data scientists, managing data science--\n\noriented projects, or investing in data science ventures,\n\n• Developers who will be implementing data science solutions, and\n\n• Aspiring data scientists.\n\nThis is not a book about algorithms, nor is it a replacement for a book about algorithms.\nWe deliberately avoided an algorithm-centered approach. We believe there is a relatively\nsmall set of fundamental concepts or principles that underlie techniques for extracting\nuseful knowledge from data. These concepts serve as the foundation for many wellknown algorithms of data mining. Moreover, these concepts underlie the analysis of\ndata-centered business problems, the creation and evaluation of data science solutions,\nand the evaluation of general data science strategies and proposals. Accordingly, we\norganized the exposition around these general principles rather than around specific\nalgorithms. Where necessary to describe procedural details, we use a combination of\ntext and diagrams, which we think are more accessible than a listing of detailed algorithmic steps.\n\nThe book does not presume a sophisticated mathematical background. However, by its\nvery nature the material is somewhat technical---the goal is to impart a significant understanding of data science, not just to give a high-level overview. In general, we have\ntried to minimize the mathematics and make the exposition as “conceptual” as possible.\n\nColleagues in industry comment that the book is invaluable for helping to align the\nunderstanding of the business, technical/development, and data science teams. That\nobservation is based on a small sample, so we are curious to see how general it truly is\n(see Chapter 5!). Ideally, we envision a book that any data scientist would give to his\ncollaborators from the development or business teams, effectively saying: if you really\n\nxi", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00018", "page_num": 18, "segment": "want to design/implement top-notch data science solutions to business problems, we\nall need to have a common understanding of this material.\n\nColleagues also tell us that the book has been quite useful in an unforeseen way: for\npreparing to interview data science job candidates. The demand from business for hiring\ndata scientists is strong and increasing. In response, more and more job seekers are\npresenting themselves as data scientists. Every data science job candidate should understand the fundamentals presented in this book. (Our industry colleagues tell us that\nthey are surprised how many do not. We have half-seriously discussed a follow-up\npamphlet “Cliff ’s Notes to Interviewing for Data Science Jobs.”)\n\nOur Conceptual Approach to Data Science\nIn this book we introduce a collection of the most important fundamental concepts of\ndata science. Some of these concepts are “headliners” for chapters, and others are introduced more naturally through the discussions (and thus they are not necessarily\nlabeled as fundamental concepts). The concepts span the process from envisioning the\nproblem, to applying data science techniques, to deploying the results to improve\ndecision-making. The concepts also undergird a large array of business analytics methods and techniques.\n\nThe concepts fit into three general types:\n\n1. Concepts about how data science fits in the organization and the competitive landscape, including ways to attract, structure, and nurture data science teams; ways for\nthinking about how data science leads to competitive advantage; and tactical concepts for doing well with data science projects.\n\n2. General ways of thinking data-analytically. These help in identifying appropriate\ndata and consider appropriate methods. The concepts include the data mining process as well as the collection of different high-level data mining tasks.\n\n3. General concepts for actually extracting knowledge from data, which undergird the\n\nvast array of data science tasks and their algorithms.\n\nFor example, one fundamental concept is that of determining the similarity of two\nentities described by data. This ability forms the basis for various specific tasks. It may\nbe used directly to find customers similar to a given customer. It forms the core of several\nprediction algorithms that estimate a target value such as the expected resouce usage of\na client or the probability of a customer to respond to an offer. It is also the basis for\nclustering techniques, which group entities by their shared features without a focused\nobjective. Similarity forms the basis of information retrieval, in which documents or\nwebpages relevant to a search query are retrieved. Finally, it underlies several common\nalgorithms for recommendation. A traditional algorithm-oriented book might present\neach of these tasks in a different chapter, under different names, with common aspects\n\nxii\n\n| Preface", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00019", "page_num": 19, "segment": "buried in algorithm details or mathematical propositions. In this book we instead focus\non the unifying concepts, presenting specific tasks and algorithms as natural manifestations of them.\n\nAs another example, in evaluating the utility of a pattern, we see a notion of lift--- how\nmuch more prevalent a pattern is than would be expected by chance---recurring broadly\nacross data science. It is used to evaluate very different sorts of patterns in different\ncontexts. Algorithms for targeting advertisements are evaluated by computing the lift\none gets for the targeted population. Lift is used to judge the weight of evidence for or\nagainst a conclusion. Lift helps determine whether a co-occurrence (an association) in\ndata is interesting, as opposed to simply being a natural consequence of popularity.\n\nWe believe that explaining data science around such fundamental concepts not only\naids the reader, it also facilitates communication between business stakeholders and\ndata scientists. It provides a shared vocabulary and enables both parties to understand\neach other better. The shared concepts lead to deeper discussions that may uncover\ncritical issues otherwise missed.\n\nTo the Instructor\nThis book has been used successfully as a textbook for a very wide variety of data science\ncourses. Historically, the book arose from the development of Foster’s multidisciplinary\nData Science classes at the Stern School at NYU, starting in the fall of 2005.1 The original\nclass was nominally for MBA students and MSIS students, but drew students from\nschools across the university. The most interesting aspect of the class was not that it\nappealed to MBA and MSIS students, for whom it was designed. More interesting, it\nalso was found to be very valuable by students with strong backgrounds in machine\nlearning and other technical disciplines. Part of the reason seemed to be that the focus\non fundamental principles and other issues besides algorithms was missing from their\ncurricula.\n\nAt NYU we now use the book in support of a variety of data science--related programs:\nthe original MBA and MSIS programs, undergraduate business analytics, NYU/Stern’s\nnew MS in Business Analytics program, and as the Introduction to Data Science for\nNYU’s new MS in Data Science. In addition, (prior to publication) the book has been\nadopted by more than a dozen other universities for programs in seven countries (and\ncounting), in business schools, in computer science programs, and for more general\nintroductions to data science.\n\nStay tuned to the books’ websites (see below) for information on how to obtain helpful\ninstructional material, including lecture slides, sample homework questions and prob1. Of course, each author has the distinct impression that he did the majority of the work on the book.\n\nPreface\n\n|\n\nxiii", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00020", "page_num": 20, "segment": "lems, example project instructions based on the frameworks from the book, exam questions, and more to come.\n\nWe keep an up-to-date list of known adoptees on the book’s website.\nClick Who’s Using It at the top.\n\nOther Skills and Concepts\nThere are many other concepts and skills that a practical data scientist needs to know\nbesides the fundamental principles of data science. These skills and concepts will be\ndiscussed in Chapter 1 and Chapter 2. The interested reader is encouraged to visit the\nbook’s website for pointers to material for learning these additional skills and concepts\n(for example, scripting in Python, Unix command-line processing, datafiles, common\ndata formats, databases and querying, big data architectures and systems like MapReduce and Hadoop, data visualization, and other related topics).\n\nSections and Notation\nIn addition to occasional footnotes, the book contains boxed “sidebars.” These are essentially extended footnotes. We reserve these for material that we consider interesting\nand worthwhile, but too long for a footnote and too much of a digression for the main\ntext.\n\nA note on the starred, “curvy road” sections\nThe occasional mathematical details are relegated to optional “starred”\nsections. These section titles will have asterisk prefixes, and they will\ninclude the “curvy road” graphic you see to the left to indicate that the\nsection contains more detailed mathematics or technical details than\nelsewhere. The book is written so that these sections may be skipped\nwithout loss of continuity, although in a few places we remind readers\nthat details appear there.\n\nConstructions in the text like (Smith and Jones, 2003) indicate a reference to an entry\nin the bibliography (in this case, the 2003 article or book by Smith and Jones); “Smith\nand Jones (2003)” is a similar reference. A single bibliography for the entire book appears\nin the endmatter.\n\nIn this book we try to keep math to a minimum, and what math there is we have simplified as much as possible without introducing confusion. For our readers with technical backgrounds, a few comments may be in order regarding our simplifying choices.\n\nxiv\n\n| Preface", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00021", "page_num": 21, "segment": "1. We avoid Sigma (Σ) and Pi (Π) notation, commonly used in textbooks to indicate\nsums and products, respectively. Instead we simply use equations with ellipses like\nthis:\n\nf (x) = w1x1 + w2x2 + ⋯ + wnxn\n\n2. Statistics books are usually careful to distinguish between a value and its estimate\nby putting a “hat” on variables that are estimates, so in such books you’ll typically\nsee a true probability denoted p and its estimate denoted p^. In this book we are\nalmost always talking about estimates from data, and putting hats on everything\nmakes equations verbose and ugly. Everything should be assumed to be an estimate\nfrom data unless we say otherwise.\n\n3. We simplify notation and remove extraneous variables where we believe they are\nclear from context. For example, when we discuss classifiers mathematically, we are\ntechnically dealing with decision predicates over feature vectors. Expressing this\nformally would lead to equations like:\n\n^\nf R() = xAge × - 1 + 0.7 × xBalance + 60\n\nInstead we opt for the more readable:\n\nf () = Age × - 1 + 0.7 × Balance + 60\n\nwith the understanding that x is a vector and Age and Balance are components of\nit.\n\nWe have tried to be consistent with typography, reserving fixed-width typewriter fonts\nlike sepal_width to indicate attributes or keywords in data. For example, in the textmining chapter, a word like 'discussing' designates a word in a document while dis\ncuss might be the resulting token in the data.\n\nThe following typographical conventions are used in this book:\n\nItalic\n\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\n\nConstant width\n\nUsed for program listings, as well as within paragraphs to refer to program elements\nsuch as variable or function names, databases, data types, environment variables,\nstatements, and keywords.\n\nPreface\n\n|\n\nxv", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00022", "page_num": 22, "segment": "Constant width italic\n\nShows text that should be replaced with user-supplied values or by values determined by context.\n\nThis icon signifies a tip, suggestion, or general note.\n\nThis icon indicates a warning or caution.\n\nUsing Examples\nIn addition to being an introduction to data science, this book is intended to be useful\nin discussions of and day-to-day work in the field. Answering a question by citing this\nbook and quoting examples does not require permission. We appreciate, but do not\nrequire, attribution. Formal attribution usually includes the title, author, publisher, and\nISBN. For example: “Data Science for Business by Foster Provost and Tom Fawcett\n(O’Reilly). Copyright 2013 Foster Provost and Tom Fawcett, 978-1-449-36132-7.”\n\nIf you feel your use of examples falls outside fair use or the permission given above, feel\nfree to contact us at permissions@oreilly.com.\n\nSafari® Books Online\n\nSafari Books Online is an on-demand digital library that delivers\nexpert content in both book and video form from the world’s leading authors in technology and business.\n\nTechnology professionals, software developers, web designers, and business and creative professionals use Safari Books Online as their primary resource for research, problem solving, learning, and certification training.\n\nSafari Books Online offers a range of product mixes and pricing programs for organizations, government agencies, and individuals. Subscribers have access to thousands of\nbooks, training videos, and prepublication manuscripts in one fully searchable database\nfrom publishers like O’Reilly Media, Prentice Hall Professional, Addison-Wesley Professional, Microsoft Press, Sams, Que, Peachpit Press, Focal Press, Cisco Press, John\nWiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe Press, FT\nPress, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, Course\nTechnology, and dozens more. For more information about Safari Books Online, please\nvisit us online.\n\nxvi\n\n| Preface", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00023", "page_num": 23, "segment": "How to Contact Us\nPlease address comments and questions concerning this book to the publisher:\n\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-998-9938 (in the United States or Canada)\n707-829-0515 (international or local)\n707-829-0104 (fax)\n\nWe have two web pages for this book, where we list errata, examples, and any additional\ninformation. You can access the publisher’s page at http://oreil.ly/data-science and the\nauthors’ page at http://.\n\nTo comment or ask technical questions about this book, send email to bookques\ntions@oreilly.com.\n\nFor more information about O’Reilly Media’s books, courses, conferences, and news,\nsee their website at http://.\n\nFind us on Facebook: http://facebook.com/oreilly\n\nFollow us on Twitter: http://twitter.com/oreillymedia\n\nWatch us on YouTube: http:///oreillymedia\n\nAcknowledgments\nThanks to all the many colleagues and others who have provided invaluable feedback,\ncriticism, suggestions, and encouragement based on many prior draft manuscripts. At\nthe risk of missing someone, let us thank in particular: Panos Adamopoulos, Manuel\nArriaga, Josh Attenberg, Solon Barocas, Ron Bekkerman, Josh Blumenstock, Aaron\nBrick, Jessica Clark, Nitesh Chawla, Peter Devito, Vasant Dhar, Jan Ehmke, Theos Evgeniou, Justin Gapper, Tomer Geva, Daniel Gillick, Shawndra Hill, Nidhi Kathuria,\nRonny Kohavi, Marios Kokkodis, Tom Lee, David Martens, Sophie Mohin, Lauren\nMoores, Alan Murray, Nick Nishimura, Balaji Padmanabhan, Jason Pan, Claudia Perlich, Gregory Piatetsky-Shapiro, Tom Phillips, Kevin Reilly, Maytal Saar-Tsechansky,\nEvan Sadler, Galit Shmueli, Roger Stein, Nick Street, Kiril Tsemekhman, Craig Vaughan,\nChris Volinsky, Wally Wang, Geoff Webb, and Rong Zheng. We would also like to thank\nmore generally the students from Foster’s classes, Data Mining for Business Analytics,\nPractical Data Science, and the Data Science Research Seminar. Questions and issues\nthat arose when using prior drafts of this book provided substantive feedback for improving it.\n\nPreface\n\n|\n\nxvii", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00024", "page_num": 24, "segment": "Thanks to David Stillwell, Thore Graepel, and Michal Kosinski for providing the Facebook Like data for some of the examples. Thanks to Nick Street for providing the cell\nnuclei data and for letting us use the cell nuclei image in Chapter 4. Thanks to David\nMartens for his help with the mobile locations visualization. Thanks to Chris Volinsky\nfor providing data from his work on the Netflix Challenge. Thanks to Sonny Tambe for\nearly access to his results on big data technologies and productivity. Thanks to Patrick\nPerry for pointing us to the bank call center example used in Chapter 12. Thanks to\nGeoff Webb for the use of the Magnum Opus association mining system.\n\nMost of all we thank our families for their love, patience and encouragement.\n\nA great deal of open source software was used in the preparation of this book and its\nexamples. The authors wish to thank the developers and contributors of:\n\n• Python and Perl\n\n• Scipy, Numpy, Matplotlib, and Scikit-Learn\n\n• Weka\n\n• The Machine Learning Repository at the University of California at Irvine (Bache\n\n& Lichman, 2013)\n\nFinally, we encourage readers to check our website for updates to this material, new\nchapters, errata, addenda, and accompanying slide sets.\n\n---Foster Provost and Tom Fawcett\n\nxviii\n\n| Preface", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00025", "page_num": 25, "segment": "CHAPTER 1\nIntroduction: Data-Analytic Thinking\n\nDream no small dreams for they have no power to\nmove the hearts of men.\n\n---Johann Wolfgang von Goethe\n\nThe past fifteen years have seen extensive investments in business infrastructure, which\nhave improved the ability to collect data throughout the enterprise. Virtually every aspect of business is now open to data collection and often even instrumented for data\ncollection: operations, manufacturing, supply-chain management, customer behavior,\nmarketing campaign performance, workflow procedures, and so on. At the same time,\ninformation is now widely available on external events such as market trends, industry\nnews, and competitors’ movements. This broad availability of data has led to increasing\ninterest in methods for extracting useful information and knowledge from data---the\nrealm of data science.\n\nThe Ubiquity of Data Opportunities\nWith vast amounts of data now available, companies in almost every industry are focused on exploiting data for competitive advantage. In the past, firms could employ\nteams of statisticians, modelers, and analysts to explore datasets manually, but the volume and variety of data have far outstripped the capacity of manual analysis. At the\nsame time, computers have become far more powerful, networking has become ubiquitous, and algorithms have been developed that can connect datasets to enable broader\nand deeper analyses than previously possible. The convergence of these phenomena has\ngiven rise to the increasingly widespread business application of data science principles\nand data-mining techniques.\n\nProbably the widest applications of data-mining techniques are in marketing for tasks\nsuch as targeted marketing, online advertising, and recommendations for cross-selling.\n\n1", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00026", "page_num": 26, "segment": "Data mining is used for general customer relationship management to analyze customer\nbehavior in order to manage attrition and maximize expected customer value. The\nfinance industry uses data mining for credit scoring and trading, and in operations via\nfraud detection and workforce management. Major retailers from Walmart to Amazon\napply data mining throughout their businesses, from marketing to supply-chain management. Many firms have differentiated themselves strategically with data science,\nsometimes to the point of evolving into data mining companies.\n\nThe primary goals of this book are to help you view business problems from a data\nperspective and understand principles of extracting useful knowledge from data. There\nis a fundamental structure to data-analytic thinking, and basic principles that should\nbe understood. There are also particular areas where intuition, creativity, common\nsense, and domain knowledge must be brought to bear. A data perspective will provide\nyou with structure and principles, and this will give you a framework to systematically\nanalyze such problems. As you get better at data-analytic thinking you will develop\nintuition as to how and where to apply creativity and domain knowledge.\n\nThroughout the first two chapters of this book, we will discuss in detail various topics\nand techniques related to data science and data mining. The terms “data science” and\n“data mining” often are used interchangeably, and the former has taken a life of its own\nas various individuals and organizations try to capitalize on the current hype surrounding it. At a high level, data science is a set of fundamental principles that guide the\nextraction of knowledge from data. Data mining is the extraction of knowledge from\ndata, via technologies that incorporate these principles. As a term, “data science” often\nis applied more broadly than the traditional use of “data mining,” but data mining techniques provide some of the clearest illustrations of the principles of data science.\n\nIt is important to understand data science even if you never intend to\napply it yourself. Data-analytic thinking enables you to evaluate proposals for data mining projects. For example, if an employee, a consultant, or a potential investment target proposes to improve a particular business application by extracting knowledge from data, you\nshould be able to assess the proposal systematically and decide whether it is sound or flawed. This does not mean that you will be able to\ntell whether it will actually succeed---for data mining projects, that\noften requires trying---but you should be able to spot obvious flaws,\nunrealistic assumptions, and missing pieces.\n\nThroughout the book we will describe a number of fundamental data science principles,\nand will illustrate each with at least one data mining technique that embodies the principle. For each principle there are usually many specific techniques that embody it, so\nin this book we have chosen to emphasize the basic principles in preference to specific\ntechniques. That said, we will not make a big deal about the difference between data\n\n2\n\n|\n\nChapter 1: Introduction: Data-Analytic Thinking", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00027", "page_num": 27, "segment": "science and data mining, except where it will have a substantial effect on understanding\nthe actual concepts.\n\nLet’s examine two brief case studies of analyzing data to extract predictive patterns.\n\nExample: Hurricane Frances\nConsider an example from a New York Times story from 2004:\n\nHurricane Frances was on its way, barreling across the Caribbean, threatening a direct\nhit on Florida’s Atlantic coast. Residents made for higher ground, but far away, in Bentonville, Ark., executives at Wal-Mart Stores decided that the situation offered a great\nopportunity for one of their newest data-driven weapons ... predictive technology.\n\nA week ahead of the storm’s landfall, Linda M. Dillman, Wal-Mart’s chief information\nofficer, pressed her staff to come up with forecasts based on what had happened when\nHurricane Charley struck several weeks earlier. Backed by the trillions of bytes’ worth of\nshopper history that is stored in Wal-Mart’s data warehouse, she felt that the company\ncould ‘start predicting what’s going to happen, instead of waiting for it to happen,’ as she\nput it. (Hays, 2004)\n\nConsider why data-driven prediction might be useful in this scenario. It might be useful\nto predict that people in the path of the hurricane would buy more bottled water. Maybe,\nbut this point seems a bit obvious, and why would we need data science to discover it?\nIt might be useful to project the amount of increase in sales due to the hurricane, to\nensure that local Wal-Marts are properly stocked. Perhaps mining the data could reveal\nthat a particular DVD sold out in the hurricane’s path---but maybe it sold out that week\nat Wal-Marts across the country, not just where the hurricane landing was imminent.\nThe prediction could be somewhat useful, but is probably more general than Ms. Dillman was intending.\n\nIt would be more valuable to discover patterns due to the hurricane that were not obvious. To do this, analysts might examine the huge volume of Wal-Mart data from prior,\nsimilar situations (such as Hurricane Charley) to identify unusual local demand for\nproducts. From such patterns, the company might be able to anticipate unusual demand\nfor products and rush stock to the stores ahead of the hurricane’s landfall.\n\nIndeed, that is what happened. The New York Times (Hays, 2004) reported that: “... the\nexperts mined the data and found that the stores would indeed need certain products\n---and not just the usual flashlights. ‘We didn’t know in the past that strawberry PopTarts increase in sales, like seven times their normal sales rate, ahead of a hurricane,’\nMs. Dillman said in a recent interview. ‘And the pre-hurricane top-selling item was\nbeer.’”1\n\n1. Of course! What goes better with strawberry Pop-Tarts than a nice cold beer?\n\nExample: Hurricane Frances\n\n|\n\n3", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00028", "page_num": 28, "segment": "Example: Predicting Customer Churn\nHow are such data analyses performed? Consider a second, more typical business scenario and how it might be treated from a data perspective. This problem will serve as a\nrunning example that will illuminate many of the issues raised in this book and provide\na common frame of reference.\n\nAssume you just landed a great analytical job with MegaTelCo, one of the largest telecommunication firms in the United States. They are having a major problem with customer retention in their wireless business. In the mid-Atlantic region, 20% of cell phone\ncustomers leave when their contracts expire, and it is getting increasingly difficult to\nacquire new customers. Since the cell phone market is now saturated, the huge growth\nin the wireless market has tapered off. Communications companies are now engaged\nin battles to attract each other’s customers while retaining their own. Customers switching from one company to another is called churn, and it is expensive all around: one\ncompany must spend on incentives to attract a customer while another company loses\nrevenue when the customer departs.\n\nYou have been called in to help understand the problem and to devise a solution. Attracting new customers is much more expensive than retaining existing ones, so a good\ndeal of marketing budget is allocated to prevent churn. Marketing has already designed\na special retention offer. Your task is to devise a precise, step-by-step plan for how the\ndata science team should use MegaTelCo’s vast data resources to decide which customers\nshould be offered the special retention deal prior to the expiration of their contracts.\n\nThink carefully about what data you might use and how they would be used. Specifically,\nhow should MegaTelCo choose a set of customers to receive their offer in order to best\nreduce churn for a particular incentive budget? Answering this question is much more\ncomplicated than it may seem initially. We will return to this problem repeatedly through\nthe book, adding sophistication to our solution as we develop an understanding of the\nfundamental data science concepts.\n\nIn reality, customer retention has been a major use of data mining\ntechnologies---especially in telecommunications and finance businesses. These more generally were some of the earliest and widest adopters of data mining technologies, for reasons discussed later.\n\nData Science, Engineering, and Data-Driven Decision\nMaking\nData science involves principles, processes, and techniques for understanding phenomena via the (automated) analysis of data. In this book, we will view the ultimate goal\n\n4\n\n|\n\nChapter 1: Introduction: Data-Analytic Thinking", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00029", "page_num": 29, "segment": "Figure 1-1. Data science in the context of various data-related processes in the\norganization.\n\nof data science as improving decision making, as this generally is of direct interest to\nbusiness.\n\nFigure 1-1 places data science in the context of various other closely related and datarelated processes in the organization. It distinguishes data science from other aspects\nof data processing that are gaining increasing attention in business. Let’s start at the top.\n\nData-driven decision-making (DDD) refers to the practice of basing decisions on the\nanalysis of data, rather than purely on intuition. For example, a marketer could select\nadvertisements based purely on her long experience in the field and her eye for what\nwill work. Or, she could base her selection on the analysis of data regarding how consumers react to different ads. She could also use a combination of these approaches.\nDDD is not an all-or-nothing practice, and different firms engage in DDD to greater or\nlesser degrees.\n\nThe benefits of data-driven decision-making have been demonstrated conclusively.\nEconomist Erik Brynjolfsson and his colleagues from MIT and Penn’s Wharton School\nconducted a study of how DDD affects firm performance (Brynjolfsson, Hitt, & Kim,\n2011). They developed a measure of DDD that rates firms as to how strongly they use\n\nData Science, Engineering, and Data-Driven Decision Making\n\n|\n\n5", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00030", "page_num": 30, "segment": "data to make decisions across the company. They show that statistically, the more datadriven a firm is, the more productive it is---even controlling for a wide range of possible\nconfounding factors. And the differences are not small. One standard deviation higher\non the DDD scale is associated with a 4%--6% increase in productivity. DDD also is\ncorrelated with higher return on assets, return on equity, asset utilization, and market\nvalue, and the relationship seems to be causal.\n\nThe sort of decisions we will be interested in in this book mainly fall into two types: (1)\ndecisions for which “discoveries” need to be made within data, and (2) decisions that\nrepeat, especially at massive scale, and so decision-making can benefit from even small\nincreases in decision-making accuracy based on data analysis. The Walmart example\nabove illustrates a type 1 problem: Linda Dillman would like to discover knowledge that\nwill help Walmart prepare for Hurricane Frances’s imminent arrival.\n\nIn 2012, Walmart’s competitor Target was in the news for a data-driven decision-making\ncase of its own, also a type 1 problem (Duhigg, 2012). Like most retailers, Target cares\nabout consumers’ shopping habits, what drives them, and what can influence them.\nConsumers tend to have inertia in their habits and getting them to change is very difficult. Decision makers at Target knew, however, that the arrival of a new baby in a family\nis one point where people do change their shopping habits significantly. In the Target\nanalyst’s words, “As soon as we get them buying diapers from us, they’re going to start\nbuying everything else too.” Most retailers know this and so they compete with each\nother trying to sell baby-related products to new parents. Since most birth records are\npublic, retailers obtain information on births and send out special offers to the new\nparents.\n\nHowever, Target wanted to get a jump on their competition. They were interested in\nwhether they could predict that people are expecting a baby. If they could, they would\ngain an advantage by making offers before their competitors. Using techniques of data\nscience, Target analyzed historical data on customers who later were revealed to have\nbeen pregnant, and were able to extract information that could predict which consumers\nwere pregnant. For example, pregnant mothers often change their diets, their wardrobes, their vitamin regimens, and so on. These indicators could be extracted from\nhistorical data, assembled into predictive models, and then deployed in marketing\ncampaigns. We will discuss predictive models in much detail as we go through the book.\nFor the time being, it is sufficient to understand that a predictive model abstracts away\nmost of the complexity of the world, focusing in on a particular set of indicators that\ncorrelate in some way with a quantity of interest (who will churn, or who will purchase,\nwho is pregnant, etc.). Importantly, in both the Walmart and the Target examples, the\n\n6\n\n|\n\nChapter 1: Introduction: Data-Analytic Thinking", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00031", "page_num": 31, "segment": "data analysis was not testing a simple hypothesis. Instead, the data were explored with\nthe hope that something useful would be discovered.2\n\nOur churn example illustrates a type 2 DDD problem. MegaTelCo has hundreds of\nmillions of customers, each a candidate for defection. Tens of millions of customers\nhave contracts expiring each month, so each one of them has an increased likelihood\nof defection in the near future. If we can improve our ability to estimate, for a given\ncustomer, how profitable it would be for us to focus on her, we can potentially reap large\nbenefits by applying this ability to the millions of customers in the population. This\nsame logic applies to many of the areas where we have seen the most intense application\nof data science and data mining: direct marketing, online advertising, credit scoring,\nfinancial trading, help-desk management, fraud detection, search ranking, product recommendation, and so on.\n\nThe diagram in Figure 1-1 shows data science supporting data-driven decision-making,\nbut also overlapping with data-driven decision-making. This highlights the often overlooked fact that, increasingly, business decisions are being made automatically by computer systems. Different industries have adopted automatic decision-making at different\nrates. The finance and telecommunications industries were early adopters, largely because of their precocious development of data networks and implementation of massivescale computing, which allowed the aggregation and modeling of data at a large scale,\nas well as the application of the resultant models to decision-making.\n\nIn the 1990s, automated decision-making changed the banking and consumer credit\nindustries dramatically. In the 1990s, banks and telecommunications companies also\nimplemented massive-scale systems for managing data-driven fraud control decisions.\nAs retail systems were increasingly computerized, merchandising decisions were automated. Famous examples include Harrah’s casinos’ reward programs and the automated\nrecommendations of Amazon and Netflix. Currently we are seeing a revolution in advertising, due in large part to a huge increase in the amount of time consumers are\nspending online, and the ability online to make (literally) split-second advertising\ndecisions.\n\nData Processing and “Big Data”\nIt is important to digress here to address another point. There is a lot to data processing\nthat is not data science---despite the impression one might get from the media. Data\nengineering and processing are critical to support data science, but they are more general. For example, these days many data processing skills, systems, and technologies\noften are mistakenly cast as data science. To understand data science and data-driven\n\n2. Target was successful enough that this case raised ethical questions on the deployment of such techniques.\nConcerns of ethics and privacy are interesting and very important, but we leave their discussion for another\ntime and place.\n\nData Processing and “Big Data”\n\n|\n\n7", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00032", "page_num": 32, "segment": "businesses it is important to understand the differences. Data science needs access to\ndata and it often benefits from sophisticated data engineering that data processing\ntechnologies may facilitate, but these technologies are not data science technologies per\nse. They support data science, as shown in Figure 1-1, but they are useful for much more.\nData processing technologies are very important for many data-oriented business tasks\nthat do not involve extracting knowledge or data-driven decision-making, such as efficient transaction processing, modern web system processing, and online advertising\ncampaign management.\n\n“Big data” technologies (such as Hadoop, HBase, and MongoDB) have received considerable media attention recently. Big data essentially means datasets that are too large\nfor traditional data processing systems, and therefore require new processing technologies. As with the traditional technologies, big data technologies are used for many\ntasks, including data engineering. Occasionally, big data technologies are actually used\nfor implementing data mining techniques. However, much more often the well-known\nbig data technologies are used for data processing in support of the data mining techniques and other data science activities, as represented in Figure 1-1.\n\nPreviously, we discussed Brynjolfsson’s study demonstrating the benefits of data-driven\ndecision-making. A separate study, conducted by economist Prasanna Tambe of NYU’s\nStern School, examined the extent to which big data technologies seem to help firms\n(Tambe, 2012). He finds that, after controlling for various possible confounding factors,\nusing big data technologies is associated with significant additional productivity growth.\nSpecifically, one standard deviation higher utilization of big data technologies is associated with 1%--3% higher productivity than the average firm; one standard deviation\nlower in terms of big data utilization is associated with 1%--3% lower productivity. This\nleads to potentially very large productivity differences between the firms at the extremes.\n\nFrom Big Data 1.0 to Big Data 2.0\nOne way to think about the state of big data technologies is to draw an analogy with the\nbusiness adoption of Internet technologies. In Web 1.0, businesses busied themselves\nwith getting the basic internet technologies in place, so that they could establish a web\npresence, build electronic commerce capability, and improve the efficiency of their operations. We can think of ourselves as being in the era of Big Data 1.0. Firms are busying\nthemselves with building the capabilities to process large data, largely in support of their\ncurrent operations---for example, to improve efficiency.\n\nOnce firms had incorporated Web 1.0 technologies thoroughly (and in the process had\ndriven down prices of the underlying technology) they started to look further. They\nbegan to ask what the Web could do for them, and how it could improve things they’d\nalways done---and we entered the era of Web 2.0, where new systems and companies\nbegan taking advantage of the interactive nature of the Web. The changes brought on\nby this shift in thinking are pervasive; the most obvious are the incorporation of social8\n\n|\n\nChapter 1: Introduction: Data-Analytic Thinking", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00033", "page_num": 33, "segment": "networking components, and the rise of the “voice” of the individual consumer (and\ncitizen).\n\nWe should expect a Big Data 2.0 phase to follow Big Data 1.0. Once firms have become\ncapable of processing massive data in a flexible fashion, they should begin asking: “What\ncan I now do that I couldn’t do before, or do better than I could do before?” This is likely\nto be the golden era of data science. The principles and techniques we introduce in this\nbook will be applied far more broadly and deeply than they are today.\n\nIt is important to note that in the Web 1.0 era some precocious companies began applying Web 2.0 ideas far ahead of the mainstream.\nAmazon is a prime example, incorporating the consumer’s “voice”\nearly on, in the rating of products, in product reviews (and deeper, in\nthe rating of product reviews). Similarly, we see some companies already applying Big Data 2.0. Amazon again is a company at the forefront, providing data-driven recommendations from massive data.\nThere are other examples as well. Online advertisers must process\nextremely large volumes of data (billions of ad impressions per day is\nnot unusual) and maintain a very high throughput (real-time bidding systems make decisions in tens of milliseconds). We should look\nto these and similar industries for hints at advances in big data and\ndata science that subsequently will be adopted by other industries.\n\nData and Data Science Capability as a Strategic Asset\nThe prior sections suggest one of the fundamental principles of data science: data, and\nthe capability to extract useful knowledge from data, should be regarded as key strategic\nassets. Too many businesses regard data analytics as pertaining mainly to realizing value\nfrom some existing data, and often without careful regard to whether the business has\nthe appropriate analytical talent. Viewing these as assets allows us to think explicitly\nabout the extent to which one should invest in them. Often, we don’t have exactly the\nright data to best make decisions and/or the right talent to best support making decisions\nfrom the data. Further, thinking of these as assets should lead us to the realization that\nthey are complementary. The best data science team can yield little value without the\nappropriate data; the right data often cannot substantially improve decisions without\nsuitable data science talent. As with all assets, it is often necessary to make investments.\nBuilding a top-notch data science team is a nontrivial undertaking, but can make a huge\ndifference for decision-making. We will discuss strategic considerations involving data\nscience in detail in Chapter 13. Our next case study will introduce the idea that thinking\nexplicitly about how to invest in data assets very often pays off handsomely.\n\nThe classic story of little Signet Bank from the 1990s provides a case in point. Previously,\nin the 1980s, data science had transformed the business of consumer credit. Modeling\n\nData and Data Science Capability as a Strategic Asset\n\n|\n\n9", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00034", "page_num": 34, "segment": "the probability of default had changed the industry from personal assessment of the\nlikelihood of default to strategies of massive scale and market share, which brought\nalong concomitant economies of scale. It may seem strange now, but at the time, credit\ncards essentially had uniform pricing, for two reasons: (1) the companies did not have\nadequate information systems to deal with differential pricing at massive scale, and (2)\nbank management believed customers would not stand for price discrimination.\nAround 1990, two strategic visionaries (Richard Fairbanks and Nigel Morris) realized\nthat information technology was powerful enough that they could do more sophisticated predictive modeling---using the sort of techniques that we discuss throughout this\nbook---and offer different terms (nowadays: pricing, credit limits, low-initial-rate balance transfers, cash back, loyalty points, and so on). These two men had no success\npersuading the big banks to take them on as consultants and let them try. Finally, after\nrunning out of big banks, they succeeded in garnering the interest of a small regional\nVirginia bank: Signet Bank. Signet Bank’s management was convinced that modeling\nprofitability, not just default probability, was the right strategy. They knew that a small\nproportion of customers actually account for more than 100% of a bank’s profit from\ncredit card operations (because the rest are break-even or money-losing). If they could\nmodel profitability, they could make better offers to the best customers and “skim the\ncream” of the big banks’ clientele.\n\nBut Signet Bank had one really big problem in implementing this strategy. They did not\nhave the appropriate data to model profitability with the goal of offering different terms\nto different customers. No one did. Since banks were offering credit with a specific set\nof terms and a specific default model, they had the data to model profitability (1) for\nthe terms they actually have offered in the past, and (2) for the sort of customer who\nwas actually offered credit (that is, those who were deemed worthy of credit by the\nexisting model).\n\nWhat could Signet Bank do? They brought into play a fundamental strategy of data\nscience: acquire the necessary data at a cost. Once we view data as a business asset, we\nshould think about whether and how much we are willing to invest. In Signet’s case,\ndata could be generated on the profitability of customers given different credit terms\nby conducting experiments. Different terms were offered at random to different customers. This may seem foolish outside the context of data-analytic thinking: you’re likely\nto lose money! This is true. In this case, losses are the cost of data acquisition. The dataanalytic thinker needs to consider whether she expects the data to have sufficient value\nto justify the investment.\n\nSo what happened with Signet Bank? As you might expect, when Signet began randomly\noffering terms to customers for data acquisition, the number of bad accounts soared.\nSignet went from an industry-leading “charge-off ” rate (2.9% of balances went unpaid)\nto almost 6% charge-offs. Losses continued for a few years while the data scientists\nworked to build predictive models from the data, evaluate them, and deploy them to\nimprove profit. Because the firm viewed these losses as investments in data, they per10\n\n|\n\nChapter 1: Introduction: Data-Analytic Thinking", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00035", "page_num": 35, "segment": "sisted despite complaints from stakeholders. Eventually, Signet’s credit card operation\nturned around and became so profitable that it was spun off to separate it from the\nbank’s other operations, which now were overshadowing the consumer credit success.\n\nFairbanks and Morris became Chairman and CEO and President and COO, and proceeded to apply data science principles throughout the business---not just customer\nacquisition but retention as well. When a customer calls looking for a better offer, datadriven models calculate the potential profitability of various possible actions (different\noffers, including sticking with the status quo), and the customer service representative’s\ncomputer presents the best offers to make.\n\nYou may not have heard of little Signet Bank, but if you’re reading this book you’ve\nprobably heard of the spin-off: Capital One. Fairbanks and Morris’s new company grew\nto be one of the largest credit card issuers in the industry with one of the lowest chargeoff rates. In 2000, the bank was reported to be carrying out 45,000 of these “scientific\ntests” as they called them.3\n\nStudies giving clear quantitative demonstrations of the value of a data asset are hard to\nfind, primarily because firms are hesitant to divulge results of strategic value. One exception is a study by Martens and Provost (2011) assessing whether data on the specific\ntransactions of a bank’s consumers can improve models for deciding what product offers\nto make. The bank built models from data to decide whom to target with offers for\ndifferent products. The investigation examined a number of different types of data and\ntheir effects on predictive performance. Sociodemographic data provide a substantial\nability to model the sort of consumers that are more likely to purchase one product or\nanother. However, sociodemographic data only go so far; after a certain volume of data,\nno additional advantage is conferred. In contrast, detailed data on customers’ individual\n(anonymized) transactions improve performance substantially over just using sociodemographic data. The relationship is clear and striking and---significantly, for the point\nhere---the predictive performance continues to improve as more data are used, increasing throughout the range investigated by Martens and Provost with no sign of abating.\nThis has an important implication: banks with bigger data assets may have an important\nstrategic advantage over their smaller competitors. If these trends generalize, and the\nbanks are able to apply sophisticated analytics, banks with bigger data assets should be\nbetter able to identify the best customers for individual products. The net result will be\neither increased adoption of the bank’s products, decreased cost of customer acquisition,\nor both.\n\nThe idea of data as a strategic asset is certainly not limited to Capital One, nor even to\nthe banking industry. Amazon was able to gather data early on online customers, which\nhas created significant switching costs: consumers find value in the rankings and recommendations that Amazon provides. Amazon therefore can retain customers more\n\n3. You can read more about Capital One’s story (Clemons & Thatcher, 1998; McNamee 2001).\n\nData and Data Science Capability as a Strategic Asset\n\n|\n\n11", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00036", "page_num": 36, "segment": "easily, and can even charge a premium (Brynjolfsson & Smith, 2000). Harrah’s casinos\nfamously invested in gathering and mining data on gamblers, and moved itself from a\nsmall player in the casino business in the mid-1990s to the acquisition of Caesar’s\nEntertainment in 2005 to become the world’s largest gambling company. The huge valuation of Facebook has been credited to its vast and unique data assets (Sengupta, 2012),\nincluding both information about individuals and their likes, as well as information\nabout the structure of the social network. Information about network structure has been\nshown to be important to predicting and has been shown to be remarkably helpful in\nbuilding models of who will buy certain products (Hill, Provost, & Volinsky, 2006). It\nis clear that Facebook has a remarkable data asset; whether they have the right data\nscience strategies to take full advantage of it is an open question.\n\nIn the book we will discuss in more detail many of the fundamental concepts behind\nthese success stories, in exploring the principles of data mining and data-analytic\nthinking.\n\nData-Analytic Thinking\nAnalyzing case studies such as the churn problem improves our ability to approach\nproblems “data-analytically.” Promoting such a perspective is a primary goal of this\nbook. When faced with a business problem, you should be able to assess whether and\nhow data can improve performance. We will discuss a set of fundamental concepts and\nprinciples that facilitate careful thinking. We will develop frameworks to structure the\nanalysis so that it can be done systematically.\n\nAs mentioned above, it is important to understand data science even if you never intend\nto do it yourself, because data analysis is now so critical to business strategy. Businesses\nincreasingly are driven by data analytics, so there is great professional advantage in\nbeing able to interact competently with and within such businesses. Understanding the\nfundamental concepts, and having frameworks for organizing data-analytic thinking\nnot only will allow one to interact competently, but will help to envision opportunities\nfor improving data-driven decision-making, or to see data-oriented competitive threats.\n\nFirms in many traditional industries are exploiting new and existing data resources for\ncompetitive advantage. They employ data science teams to bring advanced technologies\nto bear to increase revenue and to decrease costs. In addition, many new companies are\nbeing developed with data mining as a key strategic component. Facebook and Twitter,\nalong with many other “Digital 100” companies (Business Insider, 2012), have high\nvaluations due primarily to data assets they are committed to capturing or creating.4\nIncreasingly, managers need to oversee analytics teams and analysis projects, marketers\n\n4. Of course, this is not a new phenomenon. Amazon and Google are well-established companies that get\n\ntremendous value from their data assets.\n\n12\n\n|\n\nChapter 1: Introduction: Data-Analytic Thinking", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00037", "page_num": 37, "segment": "have to organize and understand data-driven campaigns, venture capitalists must be\nable to invest wisely in businesses with substantial data assets, and business strategists\nmust be able to devise plans that exploit data.\n\nAs a few examples, if a consultant presents a proposal to mine a data asset to improve\nyour business, you should be able to assess whether the proposal makes sense. If a\ncompetitor announces a new data partnership, you should recognize when it may put\nyou at a strategic disadvantage. Or, let’s say you take a position with a venture firm and\nyour first project is to assess the potential for investing in an advertising company. The\nfounders present a convincing argument that they will realize significant value from a\nunique body of data they will collect, and on that basis are arguing for a substantially\nhigher valuation. Is this reasonable? With an understanding of the fundamentals of data\nscience you should be able to devise a few probing questions to determine whether their\nvaluation arguments are plausible.\n\nOn a scale less grand, but probably more common, data analytics projects reach into all\nbusiness units. Employees throughout these units must interact with the data science\nteam. If these employees do not have a fundamental grounding in the principles of dataanalytic thinking, they will not really understand what is happening in the business.\nThis lack of understanding is much more damaging in data science projects than in\nother technical projects, because the data science is supporting improved decisionmaking. As we will describe in the next chapter, this requires a close interaction between\nthe data scientists and the business people responsible for the decision-making. Firms\nwhere the business people do not understand what the data scientists are doing are at a\nsubstantial disadvantage, because they waste time and effort or, worse, because they\nultimately make wrong decisions.\n\nThe need for managers with data-analytic skills\nThe consulting firm McKinsey and Company estimates that “there will\nbe a shortage of talent necessary for organizations to take advantage\nof big data. By 2018, the United States alone could face a shortage of\n140,000 to 190,000 people with deep analytical skills as well as 1.5\nmillion managers and analysts with the know-how to use the analysis of big data to make effective decisions.” (Manyika, 2011). Why 10\ntimes as many managers and analysts than those with deep analytical\nskills? Surely data scientists aren’t so difficult to manage that they need\n10 managers! The reason is that a business can get leverage from a data\nscience team for making better decisions in multiple areas of the business. However, as McKinsey is pointing out, the managers in those\nareas need to understand the fundamentals of data science to effectively get that leverage.\n\nData-Analytic Thinking\n\n|\n\n13", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00038", "page_num": 38, "segment": "This Book\nThis book concentrates on the fundamentals of data science and data mining. These are\na set of principles, concepts, and techniques that structure thinking and analysis. They\nallow us to understand data science processes and methods surprisingly deeply, without\nneeding to focus in depth on the large number of specific data mining algorithms.\n\nThere are many good books covering data mining algorithms and techniques, from\npractical guides to mathematical and statistical treatments. This book instead focuses\non the fundamental concepts and how they help us to think about problems where data\nmining may be brought to bear. That doesn’t mean that we will ignore the data mining\ntechniques; many algorithms are exactly the embodiment of the basic concepts. But\nwith only a few exceptions we will not concentrate on the deep technical details of how\nthe techniques actually work; we will try to provide just enough detail so that you will\nunderstand what the techniques do, and how they are based on the fundamental\nprinciples.\n\nData Mining and Data Science, Revisited\nThis book devotes a good deal of attention to the extraction of useful (nontrivial, hopefully actionable) patterns or models from large bodies of data (Fayyad, PiatetskyShapiro, & Smyth, 1996), and to the fundamental data science principles underlying\nsuch data mining. In our churn-prediction example, we would like to take the data on\nprior churn and extract patterns, for example patterns of behavior, that are useful---that\ncan help us to predict those customers who are more likely to leave in the future, or that\ncan help us to design better services.\n\nThe fundamental concepts of data science are drawn from many fields that study data\nanalytics. We introduce these concepts throughout the book, but let’s briefly discuss a\nfew now to get the basic flavor. We will elaborate on all of these and more in later\nchapters.\n\nFundamental concept: Extracting useful knowledge from data to solve business problems\ncan be treated systematically by following a process with reasonably well-defined stages.\nThe Cross Industry Standard Process for Data Mining, abbreviated CRISP-DM (CRISPDM Project, 2000), is one codification of this process. Keeping such a process in mind\nprovides a framework to structure our thinking about data analytics problems. For\nexample, in actual practice one repeatedly sees analytical “solutions” that are not based\non careful analysis of the problem or are not carefully evaluated. Structured thinking\nabout analytics emphasizes these often under-appreciated aspects of supporting\ndecision-making with data. Such structured thinking also contrasts critical points where\nhuman creativity is necessary versus points where high-powered analytical tools can be\nbrought to bear.\n\n14\n\n|\n\nChapter 1: Introduction: Data-Analytic Thinking", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00039", "page_num": 39, "segment": "Fundamental concept: From a large mass of data, information technology can be used to\nfind informative descriptive attributes of entities of interest. In our churn example, a\ncustomer would be an entity of interest, and each customer might be described by a\nlarge number of attributes, such as usage, customer service history, and many other\nfactors. Which of these actually gives us information on the customer’s likelihood of\nleaving the company when her contract expires? How much information? Sometimes\nthis process is referred to roughly as finding variables that “correlate” with churn (we\nwill discuss this notion precisely). A business analyst may be able to hypothesize some\nand test them, and there are tools to help facilitate this experimentation (see “Other\nAnalytics Techniques and Technologies” on page 35). Alternatively, the analyst could\napply information technology to automatically discover informative attributes---essentially doing large-scale automated experimentation. Further, as we will see, this concept\ncan be applied recursively to build models to predict churn based on multiple attributes.\n\nFundamental concept: If you look too hard at a set of data, you will find something---but\nit might not generalize beyond the data you’re looking at. This is referred to as overfitting a dataset. Data mining techniques can be very powerful, and the need to detect and\navoid overfitting is one of the most important concepts to grasp when applying data\nmining to real problems. The concept of overfitting and its avoidance permeates data\nscience processes, algorithms, and evaluation methods.\n\nFundamental concept: Formulating data mining solutions and evaluating the results\ninvolves thinking carefully about the context in which they will be used. If our goal is the\nextraction of potentially useful knowledge, how can we formulate what is useful? It\ndepends critically on the application in question. For our churn-management example,\nhow exactly are we going to use the patterns extracted from historical data? Should the\nvalue of the customer be taken into account in addition to the likelihood of leaving?\nMore generally, does the pattern lead to better decisions than some reasonable alternative? How well would one have done by chance? How well would one do with a smart\n“default” alternative?\n\nThese are just four of the fundamental concepts of data science that we will explore. By\nthe end of the book, we will have discussed a dozen such fundamental concepts in detail,\nand will have illustrated how they help us to structure data-analytic thinking and to\nunderstand data mining techniques and algorithms, as well as data science applications,\nquite generally.\n\nChemistry Is Not About Test Tubes: Data Science Versus\nthe Work of the Data Scientist\nBefore proceeding, we should briefly revisit the engineering side of data science. At the\ntime of this writing, discussions of data science commonly mention not just analytical\nskills and techniques for understanding data but popular tools used. Definitions of data\n\nChemistry Is Not About Test Tubes: Data Science Versus the Work of the Data Scientist\n\n|\n\n15", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00040", "page_num": 40, "segment": "scientists (and advertisements for positions) specify not just areas of expertise but also\nspecific programming languages and tools. It is common to see job advertisements\nmentioning data mining techniques (e.g., random forests, support vector machines),\nspecific application areas (recommendation systems, ad placement optimization),\nalongside popular software tools for processing big data (Hadoop, MongoDB). There\nis often little distinction between the science and the technology for dealing with large\ndatasets.\n\nWe must point out that data science, like computer science, is a young field. The particular concerns of data science are fairly new and general principles are just beginning\nto emerge. The state of data science may be likened to that of chemistry in the mid-19th\ncentury, when theories and general principles were being formulated and the field was\nlargely experimental. Every good chemist had to be a competent lab technician. Similarly, it is hard to imagine a working data scientist who is not proficient with certain\nsorts of software tools.\n\nHaving said this, this book focuses on the science and not on the technology. You will\nnot find instructions here on how best to run massive data mining jobs on Hadoop\nclusters, or even what Hadoop is or why you might want to learn about it.5 We focus\nhere on the general principles of data science that have emerged. In 10 years’ time the\npredominant technologies will likely have changed or advanced enough that a discussion here would be obsolete, while the general principles are the same as they were 20\nyears ago, and likely will change little over the coming decades.\n\nSummary\nThis book is about the extraction of useful information and knowledge from large volumes of data, in order to improve business decision-making. As the massive collection\nof data has spread through just about every industry sector and business unit, so have\nthe opportunities for mining the data. Underlying the extensive body of techniques for\nmining data is a much smaller set of fundamental concepts comprising data science.\nThese concepts are general and encapsulate much of the essence of data mining and\nbusiness analytics.\n\nSuccess in today’s data-oriented business environment requires being able to think about\nhow these fundamental concepts apply to particular business problems---to think dataanalytically. For example, in this chapter we discussed the principle that data should be\nthought of as a business asset, and once we are thinking in this direction we start to ask\nwhether (and how much) we should invest in data. Thus, an understanding of these\nfundamental concepts is important not only for data scientists themselves, but for any5. OK: Hadoop is a widely used open source architecture for doing highly parallelizable computations. It is one\nof the current “big data” technologies for processing massive datasets that exceed the capacity of relational\ndatabase systems. Hadoop is based on the MapReduce parallel processing framework introduced by Google.\n\n16\n\n|\n\nChapter 1: Introduction: Data-Analytic Thinking", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00041", "page_num": 41, "segment": "one working with data scientists, employing data scientists, investing in data-heavy\nventures, or directing the application of analytics in an organization.\n\nThinking data-analytically is aided by conceptual frameworks discussed throughout the\nbook. For example, the automated extraction of patterns from data is a process with\nwell-defined stages, which are the subject of the next chapter. Understanding the process\nand the stages helps to structure our data-analytic thinking, and to make it more systematic and therefore less prone to errors and omissions.\n\nThere is convincing evidence that data-driven decision-making and big data technologies substantially improve business performance. Data science supports data-driven\ndecision-making---and sometimes conducts such decision-making automatically---and\ndepends upon technologies for “big data” storage and engineering, but its principles are\nseparate. The data science principles we discuss in this book also differ from, and are\ncomplementary to, other important technologies, such as statistical hypothesis testing\nand database querying (which have their own books and classes). The next chapter\ndescribes some of these differences in more detail.\n\nSummary\n\n|\n\n17", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00042", "page_num": 42, "segment": "", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "none", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00043", "page_num": 43, "segment": "CHAPTER 2\nBusiness Problems and Data Science\nSolutions\n\nFundamental concepts: A set of canonical data mining tasks; The data mining process;\nSupervised versus unsupervised data mining.\n\nAn important principle of data science is that data mining is a process with fairly wellunderstood stages. Some involve the application of information technology, such as the\nautomated discovery and evaluation of patterns from data, while others mostly require\nan analyst’s creativity, business knowledge, and common sense. Understanding the\nwhole process helps to structure data mining projects, so they are closer to systematic\nanalyses rather than heroic endeavors driven by chance and individual acumen.\n\nSince the data mining process breaks up the overall task of finding patterns from data\ninto a set of well-defined subtasks, it is also useful for structuring discussions about data\nscience. In this book, we will use the process as an overarching framework for our\ndiscussion. This chapter introduces the data mining process, but first we provide additional context by discussing common types of data mining tasks. Introducing these\nallows us to be more concrete when presenting the overall process, as well as when\nintroducing other concepts in subsequent chapters.\n\nWe close the chapter by discussing a set of important business analytics subjects that\nare not the focus of this book (but for which there are many other helpful books), such\nas databases, data warehousing, and basic statistics.\n\nFrom Business Problems to Data Mining Tasks\nEach data-driven business decision-making problem is unique, comprising its own\ncombination of goals, desires, constraints, and even personalities. As with much engineering, though, there are sets of common tasks that underlie the business problems.\nIn collaboration with business stakeholders, data scientists decompose a business prob19", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00044", "page_num": 44, "segment": "lem into subtasks. The solutions to the subtasks can then be composed to solve the\noverall problem. Some of these subtasks are unique to the particular business problem,\nbut others are common data mining tasks. For example, our telecommunications churn\nproblem is unique to MegaTelCo: there are specifics of the problem that are different\nfrom churn problems of any other telecommunications firm. However, a subtask that\nwill likely be part of the solution to any churn problem is to estimate from historical\ndata the probability of a customer terminating her contract shortly after it has expired.\nOnce the idiosyncratic MegaTelCo data have been assembled into a particular format\n(described in the next chapter), this probability estimation fits the mold of one very\ncommon data mining task. We know a lot about solving the common data mining tasks,\nboth scientifically and practically. In later chapters, we also will provide data science\nframeworks to help with the decomposition of business problems and with the recomposition of the solutions to the subtasks.\n\nA critical skill in data science is the ability to decompose a dataanalytics problem into pieces such that each piece matches a known\ntask for which tools are available. Recognizing familiar problems and\ntheir solutions avoids wasting time and resources reinventing the\nwheel. It also allows people to focus attention on more interesting parts\nof the process that require human involvement---parts that have not\nbeen automated, so human creativity and intelligence must come into play.\n\nDespite the large number of specific data mining algorithms developed over the years,\nthere are only a handful of fundamentally different types of tasks these algorithms address. It is worth defining these tasks clearly. The next several chapters will use the first\ntwo (classification and regression) to illustrate several fundamental concepts. In what\nfollows, the term “an individual” will refer to an entity about which we have data, such\nas a customer or a consumer, or it could be an inanimate entity such as a business. We\nwill make this notion more precise in Chapter 3. In many business analytics projects,\nwe want to find “correlations” between a particular variable describing an individual\nand other variables. For example, in historical data we may know which customers left\nthe company after their contracts expired. We may want to find out which other variables\ncorrelate with a customer leaving in the near future. Finding such correlations are the\nmost basic examples of classification and regression tasks.\n\n1. Classification and class probability estimation attempt to predict, for each individual\nin a population, which of a (small) set of classes this individual belongs to. Usually\nthe classes are mutually exclusive. An example classification question would be:\n“Among all the customers of MegaTelCo, which are likely to respond to a given\noffer?” In this example the two classes could be called will respond and will not\nrespond.\n\n20\n\n|\n\nChapter 2: Business Problems and Data Science Solutions", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00045", "page_num": 45, "segment": "For a classification task, a data mining procedure produces a model that, given a\nnew individual, determines which class that individual belongs to. A closely related\ntask is scoring or class probability estimation. A scoring model applied to an individual produces, instead of a class prediction, a score representing the probability\n(or some other quantification of likelihood) that that individual belongs to each\nclass. In our customer response scenario, a scoring model would be able to evaluate\neach individual customer and produce a score of how likely each is to respond to\nthe offer. Classification and scoring are very closely related; as we shall see, a model\nthat can do one can usually be modified to do the other.\n\n2. Regression (“value estimation”) attempts to estimate or predict, for each individual,\nthe numerical value of some variable for that individual. An example regression\nquestion would be: “How much will a given customer use the service?” The property\n(variable) to be predicted here is service usage, and a model could belooking at other, similar individuals in the population and their historical usage. A\nregression procedure produces a model that, given an individual, estimates the\nvalue of the particular variable specific to that individual.\n\nRegression is related to classification, but the two are different. Informally, classification predicts whether something will happen, whereas regression predicts how\nmuch something will happen. The difference will become clearer as the book\nprogresses.\n\n3. Similarity matching attempts to identify similar individuals based on data known\nabout them. Similarity matching can be used directly to find similar entities. For\nexample, IBM is interested in finding companies similar to their best business customers, in order to focus their sales force on the best opportunities. They use similarity matching based on “firmographic” data describing characteristics of the\ncompanies. Similarity matching is the basis for one of the most popular methods\nfor making product recommendations (finding people who are similar to you in\nterms of the products they have liked or have purchased). Similarity measures underlie certain solutions to other data mining tasks, such as classification, regression,\nand clustering. We discuss similarity and its uses at length in Chapter 6.\n\n4. Clustering attempts to group individuals in a population together by their similarity,\nbut not driven by any specific purpose. An example clustering question would be:\n“Do our customers form natural groups or segments?” Clustering is useful in preliminary domain exploration to see which natural groups exist because these groups\nin turn may suggest other data mining tasks or approaches. Clustering also is used\nas input to decision-making processes focusing on questions such as: What products\nshould we offer or develop? How should our customer care teams (or sales teams) be\nstructured? We discuss clustering in depth in Chapter 6.\n\n5. Co-occurrence grouping (also known as frequent itemset mining, association rule\ndiscovery, and market-basket analysis) attempts to find associations between entities based on transactions involving them. An example co-occurrence question\n\nFrom Business Problems to Data Mining Tasks\n\n|\n\n21", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00046", "page_num": 46, "segment": "would be: What items are commonly purchased together? While clustering looks at\nsimilarity between objects based on the objects’ attributes, co-occurrence grouping\nconsiders similarity of objects based on their appearing together in transactions.\nFor example, analyzing purchase records from a supermarket may uncover that\nground meat is purchased together with hot sauce much more frequently than we\nmight expect. Deciding how to act upon this discovery might require some creativity, but it could suggest a special promotion, product display, or combination\noffer. Co-occurrence of products in purchases is a common type of grouping known\nas market-basket analysis. Some recommendation systems also perform a type of\naffinity grouping by finding, for example, pairs of books that are purchased frequently by the same people (“people who bought X also bought Y”).\n\nThe result of co-occurrence grouping is a description of items that occur together.\nThese descriptions usually include statistics on the frequency of the co-occurrence\nand an estimate of how surprising it is.\n\n6. Profiling (also known as behavior description) attempts to characterize the typical\nbehavior of an individual, group, or population. An example profiling question\nwould be: “What is the typical cell phone usage of this customer segment?” Behavior\nmay not have a simple description; profiling cell phone usage might require a complex description of night and weekend airtime averages, international usage, roaming charges, text minutes, and so on. Behavior can be described generally over an\nentire population, or down to the level of small groups or even individuals.\n\nProfiling is often used to establish behavioral norms for anomaly detection applications such as fraud detection and monitoring for intrusions to computer systems\n(such as someone breaking into your iTunes account). For example, if we know\nwhat kind of purchases a person typically makes on a credit card, we can determine\nwhether a new charge on the card fits that profile or not. We can use the degree of\nmismatch as a suspicion score and issue an alarm if it is too high.\n\n7. Link prediction attempts to predict connections between data items, usually by\nsuggesting that a link should exist, and possibly also estimating the strength of the\nlink. Link prediction is common in social networking systems: “Since you and Karen share 10 friends, maybe you’d like to be Karen’s friend?” Link prediction can\nalso estimate the strength of a link. For example, for recommending movies to\ncustomers one can think of a graph between customers and the movies they’ve\nwatched or rated. Within the graph, we search for links that do not exist between\ncustomers and movies, but that we predict should exist and should be strong. These\nlinks form the basis for recommendations.\n\n8. Data reduction attempts to take a large set of data and replace it with a smaller set\nof data that contains much of the important information in the larger set. The\nsmaller dataset may be easier to deal with or to process. Moreover, the smaller\ndataset may better reveal the information. For example, a massive dataset on consumer movie-viewing preferences may be reduced to a much smaller dataset re22\n\n|\n\nChapter 2: Business Problems and Data Science Solutions", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00047", "page_num": 47, "segment": "vealing the consumer taste preferences that are latent in the viewing data (for example, viewer genre preferences). Data reduction usually involves loss of information. What is important is the trade-off for improved insight.\n\n9. Causal modeling attempts to help us understand what events or actions actually\ninfluence others. For example, consider that we use predictive modeling to target\nadvertisements to consumers, and we observe that indeed the targeted consumers\npurchase at a higher rate subsequent to having been targeted. Was this because the\nadvertisements influenced the consumers to purchase? Or did the predictive models simply do a good job of identifying those consumers who would have purchased\nanyway? Techniques for causal modeling include those involving a substantial investment in data, such as randomized controlled experiments (e.g., so-called “A/B\ntests”), as well as sophisticated methods for drawing causal conclusions from observational data. Both experimental and observational methods for causal modeling\ngenerally can be viewed as “counterfactual” analysis: they attempt to understand\nwhat would be the difference between the situations---which cannot both happen\n---where the “treatment” event (e.g., showing an advertisement to a particular individual) were to happen, and were not to happen.\n\nIn all cases, a careful data scientist should always include with a causal conclusion\nthe exact assumptions that must be made in order for the causal conclusion to hold\n(there always are such assumptions---always ask). When undertaking causal modeling, a business needs to weigh the trade-off of increasing investment to reduce\nthe assumptions made, versus deciding that the conclusions are good enough given\nthe assumptions. Even in the most careful randomized, controlled experimentation,\nassumptions are made that could render the causal conclusions invalid. The discovery of the “placebo effect” in medicine illustrates a notorious situation where an\nassumption was overlooked in carefully designed randomized experimentation.\n\nDiscussing all of these tasks in detail would fill multiple books. In this book, we present\na collection of the most fundamental data science principles---principles that together\nunderlie all of these types of tasks. We will illustrate the principles mainly using classification, regression, similarity matching, and clustering, and will discuss others when\nthey provide important illustrations of the fundamental principles (toward the end of\nthe book).\n\nConsider which of these types of tasks might fit our churn-prediction problem. Often,\npractitioners formulate churn prediction as a problem of finding segments of customers\nwho are more or less likely to leave. This segmentation problem sounds like a classification problem, or possibly clustering, or even regression. To decide the best formulation, we first need to introduce some important distinctions.\n\nFrom Business Problems to Data Mining Tasks\n\n|\n\n23", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00048", "page_num": 48, "segment": "Supervised Versus Unsupervised Methods\nConsider two similar questions we might ask about a customer population. The first is:\n“Do our customers naturally fall into different groups?” Here no specific purpose or\ntarget has been specified for the grouping. When there is no such target, the data mining\nproblem is referred to as unsupervised. Contrast this with a slightly different question:\n“Can we find groups of customers who have particularly high likelihoods of canceling\ntheir service soon after their contracts expire?” Here there is a specific target defined:\nwill a customer leave when her contract expires? In this case, segmentation is being done\nfor a specific reason: to take action based on likelihood of churn. This is called a supervised data mining problem.\n\nA note on the terms: Supervised and unsupervised learning\nThe terms supervised and unsupervised were inherited from the field\nof machine learning. Metaphorically, a teacher “supervises” the learner by carefully providing target information along with a set of examples. An unsupervised learning task might involve the same set of\nexamples but would not include the target information. The learner\nwould be given no information about the purpose of the learning, but\nwould be left to form its own conclusions about what the examples\nhave in common.\n\nThe difference between these questions is subtle but important. If a specific target can\nbe provided, the problem can be phrased as a supervised one. Supervised tasks require\ndifferent techniques than unsupervised tasks do, and the results often are much more\nuseful. A supervised technique is given a specific purpose for the grouping---predicting\nthe target. Clustering, an unsupervised task, produces groupings based on similarities,\nbut there is no guarantee that these similarities are meaningful or will be useful for any\nparticular purpose.\n\nTechnically, another condition must be met for supervised data mining: there must be\ndata on the target. It is not enough that the target information exist in principle; it must\nalso exist in the data. For example, it might be useful to know whether a given customer\nwill stay for at least six months, but if in historical data this retention information is\nmissing or incomplete (if, say, the data are only retained for two months) the target\nvalues cannot be provided. Acquiring data on the target often is a key data science\ninvestment. The value for the target variable for an individual is often called the individual’s label, emphasizing that often (not always) one must incur expense to actively\nlabel the data.\n\nClassification, regression, and causal modeling generally are solved with supervised\nmethods. Similarity matching, link prediction, and data reduction could be either.\nClustering, co-occurrence grouping, and profiling generally are unsupervised. The\n\n24\n\n|\n\nChapter 2: Business Problems and Data Science Solutions", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00049", "page_num": 49, "segment": "fundamental principles of data mining that we will present underlie all these types of\ntechnique.\n\nTwo main subclasses of supervised data mining, classification and regression, are distinguished by the type of target. Regression involves a numeric target while classification\ninvolves a categorical (often binary) target. Consider these similar questions we might\naddress with supervised data mining:\n\n“Will this customer purchase service S1 if given incentive I?”\n\nThis is a classification problem because it has a binary target (the customer either\npurchases or does not).\n\n“Which service package (S1, S2, or none) will a customer likely purchase if given incentive I?”\n\nThis is also a classification problem, with a three-valued target.\n\n“How much will this customer use the service?”\n\nThis is a regression problem because it has a numeric target. The target variable is\nthe amount of usage (actual or predicted) per customer.\n\nThere are subtleties among these questions that should be brought out. For business\napplications we often want a numerical prediction over a categorical target. In the churn\nexample, a basic yes/no prediction of whether a customer is likely to continue to subscribe to the service may not be sufficient; we want to model the probability that the\ncustomer will continue. This is still considered classification modeling rather than regression because the underlying target is categorical. Where necessary for clarity, this\nis called “class probability estimation.”\n\nA vital part in the early stages of the data mining process is (i) to decide whether the\nline of attack will be supervised or unsupervised, and (ii) if supervised, to produce a\nprecise definition of a target variable. This variable must be a specific quantity that will\nbe the focus of the data mining (and for which we can obtain values for some example\ndata). We will return to this in Chapter 3.\n\nData Mining and Its Results\nThere is another important distinction pertaining to mining data: the difference between (1) mining the data to find patterns and build models, and (2) using the results\nof data mining. Students often confuse these two processes when studying data science,\nand managers sometimes confuse them when discussing business analytics. The use of\ndata mining results should influence and inform the data mining process itself, but the\ntwo should be kept distinct.\n\nIn our churn example, consider the deployment scenario in which the results will be\nused. We want to use the model to predict which of our customers will leave. Specifically,\nassume that data mining has created a class probability estimation model M. Given each\n\nData Mining and Its Results\n\n|\n\n25", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00050", "page_num": 50, "segment": "Figure 2-1. Data mining versus the use of data mining results. The upper half of the\nfigure illustrates the mining of historical data to produce a model. Importantly, the historical data have the target (“class”) value specified. The bottom half shows the result of\nthe data mining in use, where the model is applied to new data for which we do not\nknow the class value. The model predicts both the class value and the probability that\nthe class variable will take on that value.\n\nexisting customer, described using a set of characteristics, M takes these characteristics\nas input and produces a score or probability estimate of attrition. This is the use of the\nresults of data mining. The data mining produces the model M from some other, often\nhistorical, data.\n\nFigure 2-1 illustrates these two phases. Data mining produces the probability estimation\nmodel, as shown in the top half of the figure. In the use phase (bottom half), the model\nis applied to a new, unseen case and it generates a probability estimate for it.\n\nThe Data Mining Process\nData mining is a craft. It involves the application of a substantial amount of science and\ntechnology, but the proper application still involves art as well. But as with many mature\ncrafts, there is a well-understood process that places a structure on the problem, allowing\nreasonable consistency, repeatability, and objectiveness. A useful codification of the data\n\n26\n\n|\n\nChapter 2: Business Problems and Data Science Solutions", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00051", "page_num": 51, "segment": "mining process is given by the Cross Industry Standard Process for Data Mining\n(CRISP-DM; Shearer, 2000), illustrated in Figure 2-2.1\n\nFigure 2-2. The CRISP data mining process.\n\nThis process diagram makes explicit the fact that iteration is the rule rather than the\nexception. Going through the process once without having solved the problem is, generally speaking, not a failure. Often the entire process is an exploration of the data, and\nafter the first iteration the data science team knows much more. The next iteration can\nbe much more well-informed. Let’s now discuss the steps in detail.\n\nBusiness Understanding\nInitially, it is vital to understand the problem to be solved. This may seem obvious, but\nbusiness projects seldom come pre-packaged as clear and unambiguous data mining\n\n1. See also the Wikipedia page on the CRISP-DM process model.\n\nThe Data Mining Process\n\n|\n\n27", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00052", "page_num": 52, "segment": "problems. Often recasting the problem and designing a solution is an iterative process\nof discovery. The diagram shown in Figure 2-2 represents this as cycles within a cycle,\nrather than as a simple linear process. The initial formulation may not be complete or\noptimal so multiple iterations may be necessary for an acceptable solution formulation\nto appear.\n\nThe Business Understanding stage represents a part of the craft where the analysts’\ncreativity plays a large role. Data science has some things to say, as we will describe, but\noften the key to a great success is a creative problem formulation by some analyst regarding how to cast the business problem as one or more data science problems. Highlevel knowledge of the fundamentals helps creative business analysts see novel formulations.\n\nWe have a set of powerful tools to solve particular data mining problems: the basic data\nmining tasks discussed in “From Business Problems to Data Mining Tasks” on page\n19. Typically, the early stages of the endeavor involve designing a solution that takes\nadvantage of these tools. This can mean structuring (engineering) the problem such\nthat one or more subproblems involve building models for classification, regression,\nprobability estimation, and so on.\n\nIn this first stage, the design team should think carefully about the use scenario. This itself\nis one of the most important concepts of data science, to which we have devoted two\nentire chapters (Chapter 7 and Chapter 11). What exactly do we want to do? How exactly\nwould we do it? What parts of this use scenario constitute possible data mining models?\nIn discussing this in more detail, we will begin with a simplified view of the use scenario,\nbut as we go forward we will loop back and realize that often the use scenario must be\nadjusted to better reflect the actual business need. We will present conceptual tools to\nhelp our thinking here, for example framing a business problem in terms of expected\nvalue can allow us to systematically decompose it into data mining tasks.\n\nData Understanding\nIf solving the business problem is the goal, the data comprise the available raw material\nfrom which the solution will be built. It is important to understand the strengths and\nlimitations of the data because rarely is there an exact match with the problem. Historical\ndata often are collected for purposes unrelated to the current business problem, or for\nno explicit purpose at all. A customer database, a transaction database, and a marketing\nresponse database contain different information, may cover different intersecting populations, and may have varying degrees of reliability.\n\nIt is also common for the costs of data to vary. Some data will be available virtually for\nfree while others will require effort to obtain. Some data may be purchased. Still other\ndata simply won’t exist and will require entire ancillary projects to arrange their collection. A critical part of the data understanding phase is estimating the costs and benefits\nof each data source and deciding whether further investment is merited. Even after all\n\n28\n\n|\n\nChapter 2: Business Problems and Data Science Solutions", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00053", "page_num": 53, "segment": "datasets are acquired, collating them may require additional effort. For example, customer records and product identifiers are notoriously variable and noisy. Cleaning and\nmatching customer records to ensure only one record per customer is itself a complicated analytics problem (Hernández & Stolfo, 1995; Elmagarmid, Ipeirotis, & Verykios,\n2007).\n\nAs data understanding progresses, solution paths may change direction in response,\nand team efforts may even fork. Fraud detection provides an illustration of this. Data\nmining has been used extensively for fraud detection, and many fraud detection\nproblems involve classic supervised data mining tasks. Consider the task of catching\ncredit card fraud. Charges show up on each customer’s account, so fraudulent charges\nare usually caught---if not initially by the company, then later by the customer when\naccount activity is reviewed. We can assume that nearly all fraud is identified and reliably\nlabeled, since the legitimate customer and the person perpetrating the fraud are different\npeople and have opposite goals. Thus credit card transactions have reliable labels\n(fraud and legitimate) that may serve as targets for a supervised technique.\n\nNow consider the related problem of catching Medicare fraud. This is a huge problem\nin the United States costing billions of dollars annually. Though this may seem like a\nconventional fraud detection problem, as we consider the relationship of the business\nproblem to the data, we realize that the problem is significantly different. The perpetrators of fraud---medical providers who submit false claims, and sometimes their patients---are also legitimate service providers and users of the billing system. Those who\ncommit fraud are a subset of the legitimate users; there is no separate disinterested party\nwho will declare exactly what the “correct” charges should be. Consequently the Medicare billing data have no reliable target variable indicating fraud, and a supervised\nlearning approach that could work for credit card fraud is not applicable. Such a problem\nusually requires unsupervised approaches such as profiling, clustering, anomaly detection, and co-occurrence grouping.\n\nThe fact that both of these are fraud detection problems is a superficial similarity that\nis actually misleading. In data understanding we need to dig beneath the surface to\nuncover the structure of the business problem and the data that are available, and then\nmatch them to one or more data mining tasks for which we may have substantial science\nand technology to apply. It is not unusual for a business problem to contain several data\nmining tasks, often of different types, and combining their solutions will be necessary\n(see Chapter 11).\n\nData Preparation\nThe analytic technologies that we can bring to bear are powerful but they impose certain\nrequirements on the data they use. They often require data to be in a form different\nfrom how the data are provided naturally, and some conversion will be necessary.\n\nThe Data Mining Process\n\n|\n\n29", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00054", "page_num": 54, "segment": "Therefore a data preparation phase often proceeds along with data understanding, in\nwhich the data are manipulated and converted into forms that yield better results.\n\nTypical examples of data preparation are converting data to tabular format, removing\nor inferring missing values, and converting data to different types. Some data mining\ntechniques are designed for symbolic and categorical data, while others handle only\nnumeric values. In addition, numerical values must often be normalized or scaled so\nthat they are comparable. Standard techniques and rules of thumb are available for doing\nsuch conversions. Chapter 3 discusses the most typical format for mining data in some\ndetail.\n\nIn general, though, this book will not focus on data preparation techniques, which could\nbe the topic of a book by themselves (Pyle, 1999). We will define basic data formats in\nfollowing chapters, and will only be concerned with data preparation details when they\nshed light on some fundamental principle of data science or are necessary to present a\nconcrete example.\n\nMore generally, data scientists may spend considerable time early in\nthe process defining the variables used later in the process. This is one\nof the main points at which human creativity, common sense, and\nbusiness knowledge come into play. Often the quality of the data mining solution rests on how well the analysts structure the problems and\ncraft the variables (and sometimes it can be surprisingly hard for them\nto admit it).\n\nOne very general and important concern during data preparation is to beware of “leaks”\n(Kaufman et al. 2012). A leak is a situation where a variable collected in historical data\ngives information on the target variable---information that appears in historical data\nbut is not actually available when the decision has to be made. As an example, when\npredicting whether at a particular point in time a website visitor would end her session\nor continue surfing to another page, the variable “total number of webpages visited in\nthe session” is predictive. However, the total number of webpages visited in the session\nwould not be known until after the session was over (Kohavi et al., 2000)---at which\npoint one would know the value for the target variable! As another illustrative example,\nconsider predicting whether a customer will be a “big spender”; knowing the categories\nof the items purchased (or worse, the amount of tax paid) are very predictive, but are\nnot known at decision-making time (Kohavi & Parekh, 2003). Leakage must be considered carefully during data preparation, because data preparation typically is performed after the fact---from historical data. We present a more detailed example of a\nreal leak that was challenging to find in Chapter 14.\n\n30\n\n|\n\nChapter 2: Business Problems and Data Science Solutions", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00055", "page_num": 55, "segment": "Modeling\nModeling is the subject of the next several chapters and we will not dwell on it here,\nexcept to say that the output of modeling is some sort of model or pattern capturing\nregularities in the data.\n\nThe modeling stage is the primary place where data mining techniques are applied to\nthe data. It is important to have some understanding of the fundamental ideas of data\nmining, including the sorts of techniques and algorithms that exist, because this is the\npart of the craft where the most science and technology can be brought to bear.\n\nEvaluation\nThe purpose of the evaluation stage is to assess the data mining results rigorously and\nto gain confidence that they are valid and reliable before moving on. If we look hard\nenough at any dataset we will find patterns, but they may not survive careful scrutiny.\nWe would like to have confidence that the models and patterns extracted from the data\nare true regularities and not just idiosyncrasies or sample anomalies. It is possible to\ndeploy results immediately after data mining but this is inadvisable; it is usually far\neasier, cheaper, quicker, and safer to test a model first in a controlled laboratory setting.\n\nEqually important, the evaluation stage also serves to help ensure that the model satisfies\nthe original business goals. Recall that the primary goal of data science for business is\nto support decision making, and that we started the process by focusing on the business\nproblem we would like to solve. Usually a data mining solution is only a piece of the\nlarger solution, and it needs to be evaluated as such. Further, even if a model passes\nstrict evaluation tests in “in the lab,” there may be external considerations that make it\nimpractical. For example, a common flaw with detection solutions (such as fraud detection, spam detection, and intrusion monitoring) is that they produce too many false\nalarms. A model may be extremely accurate (> 99%) by laboratory standards, but evaluation in the actual business context may reveal that it still produces too many false\nalarms to be economically feasible. (How much would it cost to provide the staff to deal\nwith all those false alarms? What would be the cost in customer dissatisfaction?)\n\nEvaluating the results of data mining includes both quantitative and qualitative assessments. Various stakeholders have interests in the business decision-making that will be\naccomplished or supported by the resultant models. In many cases, these stakeholders\nneed to “sign off ” on the deployment of the models, and in order to do so need to be\nsatisfied by the quality of the model’s decisions. What that means varies from application\nto application, but often stakeholders are looking to see whether the model is going to\ndo more good than harm, and especially that the model is unlikely to make catastrophic\n\nThe Data Mining Process\n\n|\n\n31", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00056", "page_num": 56, "segment": "mistakes.2 To facilitate such qualitative assessment, the data scientist must think about\nthe comprehensibility of the model to stakeholders (not just to the data scientists). And\nif the model itself is not comprehensible (e.g., maybe the model is a very complex mathematical formula), how can the data scientists work to make the behavior of the model\nbe comprehensible.\n\nFinally, a comprehensive evaluation framework is important because getting detailed\ninformation on the performance of a deployed model may be difficult or impossible.\nOften there is only limited access to the deployment environment so making a comprehensive evaluation “in production” is difficult. Deployed systems typically contain\nmany “moving parts,” and assessing the contribution of a single part is difficult. Firms\nwith sophisticated data science teams wisely build testbed environments that mirror\nproduction data as closely as possible, in order to get the most realistic evaluations before\ntaking the risk of deployment.\n\nNonetheless, in some cases we may want to extend evaluation into the development\nenvironment, for example by instrumenting a live system to be able to conduct randomized experiments. In our churn example, if we have decided from laboratory tests that\na data mined model will give us better churn reduction, we may want to move on to an\n“in vivo” evaluation, in which a live system randomly applies the model to some customers while keeping other customers as a control group (recall our discussion of causal\nmodeling from Chapter 1). Such experiments must be designed carefully, and the technical details are beyond the scope of this book. The interested reader could start with\nthe lessons-learned articles by Ron Kohavi and his coauthors (Kohavi et al., 2007, 2009,\n2012). We may also want to instrument deployed systems for evaluations to make sure\nthat the world is not changing to the detriment of the model’s decision-making. For\nexample, behavior can change---in some cases, like fraud or spam, in direct response to\nthe deployment of models. Additionally, the output of the model is critically dependent\non the input data; input data can change in format and in substance, often without any\nalerting of the data science team. Raeder et al. (2012) present a detailed discussion of\nsystem design to help deal with these and other related evaluation-in-deployment issues.\n\nDeployment\nIn deployment the results of data mining---and increasingly the data mining techniques\nthemselves---are put into real use in order to realize some return on investment. The\nclearest cases of deployment involve implementing a predictive model in some information system or business process. In our churn example, a model for predicting the\nlikelihood of churn could be integrated with the business process for churn management\n\n2. For example, in one data mining project a model was created to diagnose problems in local phone networks,\nand to dispatch technicians to the likely site of the problem. Before deployment, a team of phone company\nstakeholders requested that the model be tweaked so that exceptions were made for hospitals.\n\n32\n\n|\n\nChapter 2: Business Problems and Data Science Solutions", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00057", "page_num": 57, "segment": "---for example, by sending special offers to customers who are predicted to be particularly at risk. (We will discuss this in increasing detail as the book proceeds.) A new fraud\ndetection model may be built into a workforce management information system, to\nmonitor accounts and create “cases” for fraud analysts to examine.\n\nIncreasingly, the data mining techniques themselves are deployed. For example, for\ntargeting online advertisements, systems are deployed that automatically build (and\ntest) models in production when a new advertising campaign is presented. Two main\nreasons for deploying the data mining system itself rather than the models produced\nby a data mining system are (i) the world may change faster than the data science team\ncan adapt, as with fraud and intrusion detection, and (ii) a business has too many modeling tasks for their data science team to manually curate each model individually. In\nthese cases, it may be best to deploy the data mining phase into production. In doing\nso, it is critical to instrument the process to alert the data science team of any seeming\nanomalies and to provide fail-safe operation (Raeder et al., 2012).\n\nDeployment can also be much less “technical.” In a celebrated case,\ndata mining discovered a set of rules that could help to quickly diagnose and fix a common error in industrial printing. The deployment\nsucceeded simply by taping a sheet of paper containing the rules to the\nside of the printers (Evans & Fisher, 2002). Deployment can also be\nmuch more subtle, such as a change to data acquisition procedures, or\na change to strategy, marketing, or operations resulting from insight\ngained from mining the data.\n\nDeploying a model into a production system typically requires that the model be recoded for the production environment, usually for greater speed or compatibility with\nan existing system. This may incur substantial expense and investment. In many cases,\nthe data science team is responsible for producing a working prototype, along with its\nevaluation. These are passed to a development team.\n\nPractically speaking, there are risks with “over the wall” transfers from\ndata science to development. It may be helpful to remember the maxim: “Your model is not what the data scientists design, it’s what the\nengineers build.” From a management perspective, it is advisable to\nhave members of the development team involved early on in the data\nscience project. They can begin as advisors, providing critical insight\nto the data science team. Increasingly in practice, these particular developers are “data science engineers”---software engineers who have\nparticular expertise both in the production systems and in data science. These developers gradually assume more responsibility as the\nproject matures. At some point the developers will take the lead and\n\nThe Data Mining Process\n\n|\n\n33", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00058", "page_num": 58, "segment": "assume ownership of the product. Generally, the data scientists should\nstill remain involved in the project into final deployment, as advisors\nor as developers depending on their skills.\n\nRegardless of whether deployment is successful, the process often returns to the Business\nUnderstanding phase. The process of mining data produces a great deal of insight into\nthe business problem and the difficulties of its solution. A second iteration can yield an\nimproved solution. Just the experience of thinking about the business, the data, and the\nperformance goals often leads to new ideas for improving business performance, and\neven new lines of business or new ventures.\n\nNote that it is not necessary to fail in deployment to start the cycle again. The Evaluation\nstage may reveal that results are not good enough to deploy, and we need to adjust the\nproblem definition or get different data. This is represented by the “shortcut” link from\nEvaluation back to Business Understanding in the process diagram. In practice, there\nshould be shortcuts back from each stage to each prior one because the process always\nretains some exploratory aspects, and a project should be flexible enough to revisit prior\nsteps based on discoveries made.3\n\nImplications for Managing the Data Science Team\nIt is tempting---but usually a mistake---to view the data mining process as a software\ndevelopment cycle. Indeed, data mining projects are often treated and managed as engineering projects, which is understandable when they are initiated by software departments, with dataa large software system and analytics results fed back\ninto it. Managers are usually familiar with software technologies and are comfortable\nmanaging software projects. Milestones can be agreed upon and success is usually unambiguous. Software managers might look at the CRISP data mining cycle (Figure 2-2)\nand think it looks comfortably similar to a software development cycle, so they should\nbe right at home managing an analytics project the same way.\n\nThis can be a mistake because data mining is an exploratory undertaking closer to\nresearch and development than it is to engineering. The CRISP cycle is based around\nexploration; it iterates on approaches and strategy rather than on software designs. Outcomes are far less certain, and the results of a given step may change the fundamental\nunderstanding of the problem. Engineering a data mining solution directly for deployment can be an expensive premature commitment. Instead, analytics projects should\nprepare to invest in information to reduce uncertainty in various ways. Small invest3. Software professionals may recognize the similarity to the philosophy of “Fail faster to succeed sooner”\n\n(Muoio, 1997).\n\n34\n\n|\n\nChapter 2: Business Problems and Data Science Solutions", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00059", "page_num": 59, "segment": "ments can be made via pilot studies and throwaway prototypes. Data scientists should\nreview the literature to see what else has been done and how it has worked. On a larger\nscale, a team can invest substantially in building experimental testbeds to allow extensive\nagile experimentation. If you’re a software manager, this will look more like research\nand exploration than you’re used to, and maybe more than you’re comfortable with.\n\nSoftware skills versus analytics skills\nAlthough data mining involves software, it also requires skills that may\nnot be common among programmers. In software engineering, the\nability to write efficient, high-quality code from requirements may be\nparamount. Team members may be evaluated using software metrics\nsuch as the amount of code written or number of bug tickets closed.\nIn analytics, it’s more important for individuals to be able to formulate problems well, to prototype solutions quickly, to make reasonable assumptions in the face of ill-structured problems, to design experiments that represent good investments, and to analyze results. In\nbuilding a data science team, these qualities, rather than traditional\nsoftware engineering expertise, are skills that should be sought.\n\nOther Analytics Techniques and Technologies\nBusiness analytics involves the application of various technologies to the analysis of\ndata. Many of these go beyond this book’s focus on data-analytic thinking and the principles of extracting useful patterns from data. Nonetheless, it is important to be acquainted with these related techniques, to understand what their goals are, what role\nthey play, and when it may be beneficial to consult experts in them.\n\nTo this end, we present six groups of related analytic techniques. Where appropriate we\ndraw comparisons and contrasts with data mining. The main difference is that data\nmining focuses on the automated search for knowledge, patterns, or regularities from\ndata.4 An important skill for a business analyst is to be able to recognize what sort of\nanalytic technique is appropriate for addressing a particular problem.\n\nStatistics\nThe term “statistics” has two different uses in business analytics. First, it is used as a\ncatchall term for the computation of particular numeric values of interest from data\n(e.g., “We need to gather some statistics on our customers’ usage to determine what’s\ngoing wrong here.”) These values often include sums, averages, rates, and so on. Let’s\n\n4. It is important to keep in mind that it is rare for the discovery to be completely automated. The important\nfactor is that data mining automates at least partially the search and discovery process, rather than providing\ntechnical support for manual search and discovery.\n\nOther Analytics Techniques and Technologies\n\n|\n\n35", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00060", "page_num": 60, "segment": "call these “summary statistics.” Often we want to dig deeper, and calculate summary\nstatistics conditionally on one or more subsets of the population (e.g., “Does the churn\nrate differ between male and female customers?” and “What about high-income customers in the Northeast (denotes a region of the USA)?”) Summary statistics are the\nbasic building blocks of much data science theory and practice.\n\nSummary statistics should be chosen with close attention to the business problem to be\nsolved (one of the fundamental principles we will present later), and also with attention\nto the distribution of the data they are summarizing. For example, the average (mean)\nincome in the United States according to the 2004 Census Bureau Economic Survey was\nover $60,000. If we were to use that as a measure of the average income in order to make\npolicy decisions, we would be misleading ourselves. The distribution of incomes in the\nU.S. is highly skewed, with many people making relatively little and some people making\nfantastically much. In such cases, the arithmetic mean tells us relatively little about how\nmuch people are making. Instead, we should use a different measure of “average” income, such as the median. The median income---that amount where half the population\nmakes more and half makes less---in the U.S. in the 2004 Census study was only $44,389\n---considerably less than the mean. This example may seem obvious because we are so\naccustomed to hearing about the “median income,” but the same reasoning applies to\nany computation of summary statistics: have you thought about the problem you would\nlike to solve or the question you would like to answer? Have you considered the distribution of the data, and whether the chosen statistic is appropriate?\n\nThe other use of the term “statistics” is to denote the field of study that goes by that\nname, for which we might differentiate by using the proper name, Statistics. The field\nof Statistics provides us with a huge amount of knowledge that underlies analytics, and\ncan be thought of as a component of the larger field of Data Science. For example,\nStatistics helps us to understand different data distributions and what statistics are appropriate to summarize each. Statistics helps us understand how to use data to test\nhypotheses and to estimate the uncertainty of conclusions. In relation to data mining,\nhypothesis testing can help determine whether an observed pattern is likely to be a valid,\ngeneral regularity as opposed to a chance occurrence in some particular dataset. Most\nrelevant to this book, many of the techniques for extracting models or patterns from\ndata have their roots in Statistics.\n\nFor example, a preliminary study may suggest that customers in the Northeast have a\nchurn rate of 22.5%, whereas the nationwide average churn rate is only 15%. This may\nbe just a chance fluctuation since the churn rate is not constant; it varies over regions\nand over time, so differences are to be expected. But the Northeast rate is one and a half\ntimes the U.S. average, which seems unusually high. What is the chance that this is due\nto random variation? Statistical hypothesis testing is used to answer such questions.\n\n36\n\n|\n\nChapter 2: Business Problems and Data Science Solutions", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00061", "page_num": 61, "segment": "Closely related is the quantification of uncertainty into confidence intervals. The overall\nchurn rate is 15%, but there is some variation; traditional statistical analysis may reveal\nthat 95% of the time the churn rate is expected to fall between 13% and 17%.\n\nThis contrasts with the (complementary) process of data mining, which may be seen as\nhypothesis generation. Can we find patterns in data in the first place? Hypothesis generation should then be followed by careful hypothesis testing (generally on different\ndata; see Chapter 5). In addition, data mining procedures may produce numerical estimates, and we often also want to provide confidence intervals on these estimates. We\nwill return to this when we discuss the evaluation of the results of data mining.\n\nIn this book we are not going to spend more time discussing these basic statistical\nconcepts. There are plenty of introductory books on statistics and statistics for business,\nand any treatment we would try to squeeze in would be either very narrow or superficial.\n\nThat said, one statistical term that is often heard in the context of business analytics is\n“correlation.” For example, “Are there any indicators that correlate with a customer’s\nlater defection?” As with the term statistics, “correlation” has both a general-purpose\nmeaning (variations in one quantity tell us something about variations in the other),\nand a specific technical meaning (e.g., linear correlation based on a particular mathematical formula). The notion of correlation will be the jumping off point for the rest of\nour discussion of data science for business, starting in the next chapter.\n\nDatabase Querying\nA query is a specific request for a subset of data or for statistics about data, formulated\nin a technical language and posed to a database system. Many tools are available to\nanswer one-off or repeating queries about data posed by an analyst. These tools are\nusually frontends to database systems, based on Structured Query Language (SQL) or\na tool with a graphical user interface (GUI) to help formulate queries (e.g., query-byexample, or QBE). For example, if the analyst can define “profitable” in operational\nterms computable from items in the database, then a query tool could answer: “Who\nare the most profitable customers in the Northeast?” The analyst may then run the query\nto retrieve a list of the most profitable customers, possibly ranked by profitability. This\nactivity differs fundamentally from data mining in that there is no discovery of patterns\nor models.\n\nDatabase queries are appropriate when an analyst already has an idea of what might be\nan interesting subpopulation of the data, and wants to investigate this population or\nconfirm a hypothesis about it. For example, if an analyst suspects that middle-aged men\nliving in the Northeast have some particularly interesting churning behavior, she could\ncompose a SQL query:\n\nSELECT * FROM CUSTOMERS WHERE AGE > 45 and SEX='M' and DOMICILE = 'NE'\n\nOther Analytics Techniques and Technologies\n\n|\n\n37", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00062", "page_num": 62, "segment": "If those are the people to be targeted with an offer, a query tool can be used to retrieve\nall of the information about them (“*”) from the CUSTOMERS table in the database.\n\nIn contrast, data mining could be used to come up with this query in the first place---\nas a pattern or regularity in the data. A data mining procedure might examine prior\ncustomers who did and did not defect, and determine that this segment (characterized\nas “AGE is greater than 45 and SEX is male and DOMICILE is Northeast-USA”) is\npredictive with respect to churn rate. After translating this into a SQL query, a query\ntool could then be used to find the matching records in the database.\n\nQuery tools generally have the ability to execute sophisticated logic, including computing summary statistics over subpopulations, sorting, joining together multiple tables\nwith related data, and more. Data scientists often become quite adept at writing queries\nto extract the data they need.\n\nOn-line Analytical Processing (OLAP) provides an easy-to-use GUI to query large data\ncollections, for the purpose of facilitating data exploration. The idea of “on-line” processing is that it is done in realtime, so analysts and decision makers can find answers\nto their queries quickly and efficiently. Unlike the “ad hoc” querying enabled by tools\nlike SQL, for OLAP the dimensions of analysis must be pre-programmed into the OLAP\nsystem. If we’ve foreseen that we would want to explore sales volume by region and\ntime, we could have these three dimensions programmed into the system, and drill down\ninto populations, often simply by clicking and dragging and manipulating dynamic\ncharts.\n\nOLAP systems are designed to facilitate manual or visual exploration of the data by\nanalysts. OLAP performs no modeling or automatic pattern finding. As an additional\ncontrast, unlike with OLAP, data mining tools generally can incorporate new dimensions of analysis easily as part of the exploration. OLAP tools can be a useful complement\nto data mining tools for discovery from business data.\n\nData Warehousing\nData warehouses collect and coalesce data from across an enterprise, often from multiple\ntransaction-processing systems, each with its own database. Analytical systems can access data warehouses. Data warehousing may be seen as a facilitating technology of data\nmining. It is not always necessary, as most data mining does not access a data warehouse,\nbut firms that decide to invest in data warehouses often can apply data mining more\nbroadly and more deeply in the organization. For example, if a data warehouse integrates\nrecords from sales and billing as well as from human resources, it can be used to find\ncharacteristic patterns of effective salespeople.\n\n38\n\n|\n\nChapter 2: Business Problems and Data Science Solutions", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00063", "page_num": 63, "segment": "Regression Analysis\nSome of the same methods we discuss in this book are at the core of a different set of\nanalytic methods, which often are collected under the rubric regression analysis, and\nare widely applied in the field of statistics and also in other fields founded on econometric analysis. This book will focus on different issues than usually encountered in a\nregression analysis book or class. Here we are less interested in explaining a particular\ndataset as we are in extracting patterns that will generalize to other data, and for the\npurpose of improving some business process. Typically, this will involve estimating or\npredicting values for cases that are not in the analyzed data set. So, as an example, in\nthis book we are less interested in digging into the reasons for churn (important as they\nmay be) in a particular historical set of data, and more interested in predicting which\ncustomers who have not yet left would be the best to target to reduce future churn.\nTherefore, we will spend some time talking about testing patterns on new data to evaluate their generality, and about techniques for reducing the tendency to find patterns\nspecific to a particular set of data, but that do not generalize to the population from\nwhich the data come.\n\nThe topic of explanatory modeling versus predictive modeling can elicit deep-felt debate,5 which goes well beyond our focus. What is important is to realize that there is\nconsiderable overlap in the techniques used, but that the lessons learned from explanatory modeling do not all apply to predictive modeling. So a reader with some background in regression analysis may encounter new and even seemingly contradictory\nlessons.6\n\nMachine Learning and Data Mining\nThe collection of methods for extracting (predictive) models from data, now known as\nmachine learning methods, were developed in several fields contemporaneously, most\nnotably Machine Learning, Applied Statistics, and Pattern Recognition. Machine Learning as a field of study arose as a subfield of Artificial Intelligence, which was concerned\nwith methods for improving the knowledge or performance of an intelligent agent over\ntime, in response to the agent’s experience in the world. Such improvement often involves analyzing data from the environment and making predictions about unknown\nquantities, and over the years this data analysis aspect of machine learning has come to\nplay a very large role in the field. As machine learning methods were deployed broadly,\nthe scientific disciplines of Machine Learning, Applied Statistics, and Pattern Recognition developed close ties, and the separation between the fields has blurred.\n\n5. The interested reader is urged to read the discussion by Shmueli (2010).\n\n6. Those who pursue the study in depth will have the seeming contradictions worked out. Such deep study is\n\nnot necessary to understand the fundamental principles.\n\nOther Analytics Techniques and Technologies\n\n|\n\n39", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00064", "page_num": 64, "segment": "The field of Data Mining (or KDD: Knowledge Discovery and Data Mining) started as\nan offshoot of Machine Learning, and they remain closely linked. Both fields are concerned with the analysis of data to find useful or informative patterns. Techniques and\nalgorithms are shared between the two; indeed, the areas are so closely related that\nresearchers commonly participate in both communities and transition between them\nseamlessly. Nevertheless, it is worth pointing out some of the differences to give\nperspective.\n\nSpeaking generally, because Machine Learning is concerned with many types of performance improvement, it includes subfields such as robotics and computer vision that\nare not part of KDD. It also is concerned with issues of agency and cognition---how will\nan intelligent agent use learned knowledge to reason and act in its environment---which\nare not concerns of Data Mining.\n\nHistorically, KDD spun off from Machine Learning as a research field focused on concerns raised by examining real-world applications, and a decade and a half later the\nKDD community remains more concerned with applications than Machine Learning\nis. As such, research focused on commercial applications and business issues of data\nanalysis tends to gravitate toward the KDD community rather than to Machine Learning. KDD also tends to be more concerned with the entire process of data analytics: data\npreparation, model learning, evaluation, and so on.\n\nAnswering Business Questions with These Techniques\nTo illustrate how these techniques apply to business analytics, consider a set of questions\nthat may arise and the technologies that would be appropriate for answering them. These\nquestions are all related but each is subtly different. It is important to understand these\ndifferences in order to understand what technologies one needs to employ and what\npeople may be necessary to consult.\n\n1. Who are the most profitable customers?\n\nIf “profitable” can be defined clearly based on existing data, this is a straightforward\ndatabase query. A standard query tool could be used to retrieve a set of customer\nrecords from a database. The results could be sorted by cumulative transaction\namount, or some other operational indicator of profitability.\n\n2. Is there really a difference between the profitable customers and the average customer?\n\nThis is a question about a conjecture or hypothesis (in this case, “There is a difference in value to the company between the profitable customers and the average\ncustomer”), and statistical hypothesis testing would be used to confirm or\ndisconfirm it. Statistical analysis could also derive a probability or confidence\nbound that the difference was real. Typically, the result would be like: “The value\nof these profitable customers is significantly different from that of the average customer, with probability < 5% that this is due to random chance.”\n\n40\n\n|\n\nChapter 2: Business Problems and Data Science Solutions", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00065", "page_num": 65, "segment": "3. But who really are these customers? Can I characterize them?\n\nWe often would like to do more than just list out the profitable customers. We would\nlike to describe common characteristics of profitable customers. The characteristics\nof individual customers can be extracted from a database using techniques such as\ndatabase querying, which also can be used to generate summary statistics. A deeper\nanalysis should involve determining what characteristics differentiate profitable\ncustomers from unprofitable ones. This is the realm of data science, using data\nmining techniques for automated pattern finding---which we discuss in depth in\nthe subsequent chapters.\n\n4. Will some particular new customer be profitable? How much revenue should I expect\n\nthis customer to generate?\n\nThese questions could be addressed by data mining techniques that examine historical customer records and produce predictive models of profitability. Such techniques would generate models from historical data that could then be applied to\nnew customers to generate predictions. Again, this is the subject of the following\nchapters.\n\nNote that this last pair of questions are subtly different data mining questions. The first,\na classification question, may be phrased as a prediction of whether a given new customer will be profitable (yes/no or the probability thereof). The second may be phrased\nas a prediction of the value (numerical) that the customer will bring to the company.\nMore on that as we proceed.\n\nSummary\nData mining is a craft. As with many crafts, there is a well-defined process that can help\nto increase the likelihood of a successful result. This process is a crucial conceptual tool\nfor thinking about data science projects. We will refer back to the data mining process\nrepeatedly throughout the book, showing how each fundamental concept fits in. In turn,\nunderstanding the fundamentals of data science substantially improves the chances of\nsuccess as an enterprise invokes the data mining process.\n\nThe various fields of study related to data science have developed a set of canonical task\ntypes, such as classification, regression, and clustering. Each task type serves a different\npurpose and has an associated set of solution techniques. A data scientist typically attacks a new project by decomposing it such that one or more of these canonical tasks is\nrevealed, choosing a solution technique for each, then composing the solutions. Doing\nthis expertly may take considerable experience and skill. A successful data mining\nproject involves an intelligent compromise between what the data can do (i.e., what they\ncan predict, and how well) and the project goals. For this reason it is important to keep\nin mind how data mining results will be used, and use this to inform the data mining\nprocess itself.\n\nSummary\n\n|\n\n41", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00066", "page_num": 66, "segment": "Data mining differs from, and is complementary to, important supporting technologies\nsuch as statistical hypothesis testing and database querying (which have their own books\nand classes). Though the boundaries between data mining and related techniques are\nnot always sharp, it is important to know about other techniques’ capabilities and\nstrengths to know when they should be used.\n\nTo a business manager, the data mining process is useful as a framework for analyzing\na data mining project or proposal. The process provides a systematic organization, including a set of questions that can be asked about a project or a proposed project to help\nunderstand whether the project is well conceived or is fundamentally flawed. We will\nreturn to this after we have discussed in detail some more of the fundamental principles\nthemselves---to which we turn now.\n\n42\n\n|\n\nChapter 2: Business Problems and Data Science Solutions", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00067", "page_num": 67, "segment": "CHAPTER 3\nIntroduction to Predictive Modeling: From\nCorrelation to Supervised Segmentation\n\nFundamental concepts: Identifying informative attributes; Segmenting data by progressive attribute selection.\n\nExemplary techniques: Finding correlations; Attribute/variable selection; Tree induction.\n\nThe previous chapters discussed models and modeling at a high level. This chapter\ndelves into one of the main topics of data mining: predictive modeling. Following our\nexample of data mining for churn prediction from the first section, we will begin by\nthinking of predictive modeling as supervised segmentation---how can we segment the\npopulation into groups that differ from each other with respect to some quantity of\ninterest. In particular, how can we segment the population with respect to something\nthat we would like to predict or estimate. The target of this prediction can be something\nwe would like to avoid, such as which customers are likely to leave the company when\ntheir contracts expire, which accounts have been defrauded, which potential customers\nare likely not to pay off their account balances (write-offs, such as defaulting on one’s\nphone bill or credit card balance), or which web pages contain objectionable content.\nThe target might instead be cast in a positive light, such as which consumers are most\nlikely to respond to an advertisement or special offer, or which web pages are most\nappropriate for a search query.\n\nIn the process of discussing supervised segmentation, we introduce one of the fundamental ideas of data mining: finding or selecting important, informative variables or\n“attributes” of the entities described by the data. What exactly it means to be “informative” varies among applications, but generally, information is a quantity that reduces\nuncertainty about something. So, if an old pirate gives me information about where his\ntreasure is hidden that does not mean that I know for certain where it is, it only means\nthat my uncertainty about where the treasure is hidden is reduced. The better the information, the more my uncertainty is reduced.\n\n43", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00068", "page_num": 68, "segment": "Now, recall the notion of “supervised” data mining from the previous chapter. A key to\nsupervised data mining is that we have some target quantity we would like to predict\nor to otherwise understand better. Often this quantity is unknown or unknowable at\nthe time we would like to make a business decision, such as whether a customer will\nchurn soon after her contract expires, or which accounts have been defrauded. Having\na target variable crystalizes our notion of finding informative attributes: is there one or\nmore other variables that reduces our uncertainty about the value of the target? This\nalso gives a common analytics application of the general notion of correlation discussed\nabove: we would like to find knowable attributes that correlate with the target of interest\n---that reduce our uncertainty in it. Just finding these correlated variables may provide\nimportant insight into the business problem.\n\nFinding informative attributes also is useful to help us deal with increasingly larger\ndatabases and data streams. Datasets that are too large pose computational problems\nfor analytic techniques, especially when the analyst does not have access to highperformance computers. One tried-and-true method for analyzing very large datasets\nis first to select a subset of the data to analyze. Selecting informative attributes provides\nan “intelligent” method for selecting an informative subset of the data. In addition,\nattribute selection prior to data-driven modeling can increase the accuracy of the modeling, for reasons we will discuss in Chapter 5.\n\nFinding informative attributes also is the basis for a widely used predictive modeling\ntechnique called tree induction, which we will introduce toward the end of this chapter\nas an application of this fundamental concept. Tree induction incorporates the idea of\nsupervised segmentation in an elegant manner, repeatedly selecting informative attributes. By the end of this chapter we will have achieved an understanding of: the basic\nconcepts of predictive modeling; the fundamental notion of finding informative\nattributes, along with one particular, illustrative technique for doing so; the notion of\ntree-structured models; and a basic understanding of the process for extracting treestructured models from a dataset---performing supervised segmentation.\n\nModels, Induction, and Prediction\nGenerally speaking, a model is a simplified representation of reality created to serve a\npurpose. It is simplified based on some assumptions about what is and is not important\nfor the specific purpose, or sometimes based on constraints on information or tractability. For example, a map is a model of the physical world. It abstracts away a tremendous amount of information that the mapmaker deemed irrelevant for its purpose. It\npreserves, and sometimes further simplifies, the relevant information. For example, a\nroad map keeps and highlights the roads, their basic topology, their relationships to\nplaces one would want to travel, and other relevant information. Various professions\nhave well-known model types: an architectural blueprint, an engineering prototype, the\n\n44\n\n|\n\nChapter 3: Introduction to Predictive Modeling: From Correlation to Supervised Segmentation", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00069", "page_num": 69, "segment": "Figure 3-1. Data mining terminology for a supervised classification problem. The problem is supervised because it has a target attribute and some “training” data where we\nknow the value for the target attribute. It is a classification (rather than regression)\nproblem because the target is a category (yes or no) rather than a number.\n\nBlack-Scholes model of option pricing, and so on. Each of these abstracts away details\nthat are not relevant to their main purpose and keeps those that are.\n\nIn data science, a predictive model is a formula for estimating the unknown value of\ninterest: the target. The formula could be mathematical, or it could be a logical statement\nsuch as a rule. Often it is a hybrid of the two. Given our division of supervised data\nmining into classification and regression, we will consider classification models (and\nclass-probability estimation models) and regression models.\n\nTerminology: Prediction\nIn common usage, prediction means to forecast a future event. In data\nscience, prediction more generally means to estimate an unknown\nvalue. This value could be something in the future (in common usage, true prediction), but it could also be something in the present or\nin the past. Indeed, since data mining usually deals with historical\ndata, models very often are built and tested using events from the past.\nPredictive models for credit scoring estimate the likelihood that a\npotential customer will default (become a write-off). Predictive models for spam filtering estimate whether a given piece of email is spam.\nPredictive models for fraud detection judge whether an account has\n\nModels, Induction, and Prediction\n\n|\n\n45", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00070", "page_num": 70, "segment": "been defrauded. The key is that the model is intended to be used to\nestimate an unknown value.\n\nThis is in contrast to descriptive modeling, where the primary purpose of the model is\nnot to estimate a value but instead to gain insight into the underlying phenomenon or\nprocess. A descriptive model of churn behavior would tell us what customers who churn\ntypically look like.1 A descriptive model must be judged in part on its intelligibility, and\na less accurate model may be preferred if it is easier to understand. A predictive model\nmay be judged solely on its predictive performance, although we will discuss why intelligibility is nonetheless important. The difference between these model types is not\nas strict as this may imply; some of the same techniques can be used for both, and usually\none model can serve both purposes (though sometimes poorly). Sometimes much of\nthe value of a predictive model is in the understanding gained from looking at it rather\nthan in the predictions it makes.\n\nBefore we discuss predictive modeling further, we must introduce some terminology.\nSupervised learning is model creation where the model describes a relationship between\na set of selected variables (attributes or features) and a predefined variable called the\ntarget variable. The model estimates the value of the target variable as a function (possibly a probabilistic function) of the features. So, for our churn-prediction problem we\nwould like to build a model of the propensity to churn as a function of customer account\nattributes, such as age, income, length with the company, number of calls to customer\nservice, overage charges, customer demographics, data usage, and others.\n\nFigure 3-1 illustrates some of the terminology we introduce here, in an oversimplified\nexample problem of credit write-off prediction. An instance or example represents a\nfact or a data point---in this case a historical customer who had been given credit. This\nis also called a row in database or spreadsheet terminology. An instance is described by\na set of attributes (fields, columns, variables, or features). An instance is also sometimes\ncalled a feature vector, because it can be represented as a fixed-length ordered collection\n(vector) of feature values. Unless stated otherwise, we will assume that the values of all\nthe attributes (but not the target) are present in the data.\n\n1. Descriptive modeling often is used to work toward a causal understanding of the data generating process\n\n(why do people churn?).\n\n46\n\n|\n\nChapter 3: Introduction to Predictive Modeling: From Correlation to Supervised Segmentation", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00071", "page_num": 71, "segment": "Many Names for the Same Things\nThe principles and techniques of data science historically have been studied in several\ndifferent fields, including machine learning, pattern recognition, statistics, databases,\nand others. As a result there often are several different names for the same things. We\ntypically will refer to a dataset, whose form usually is the same as a table of a database\nor a worksheet of a spreadsheet. A dataset contains a set of examples or instances. An\ninstance also is referred to as a row of a database table or sometimes a case in statistics.\n\nThe features (table columns) have many different names as well. Statisticians speak of\nindependent variables or predictors as the attributes supplied as input. In operations\nresearch you may also hear explanatory variable. The target variable, whose values are\nto be predicted, is commonly called the dependent variable in statistics. This terminology\nmay be somewhat confusing; the independent variables may not be independent of each\nother (or anything else), and the dependent variable doesn’t always depend on all the\nindependent variables. For this reason we have avoided the dependent/independent\nterminology in this book. Some experts consider the target variable to be included in\nthe set of features, some do not. The important thing is rather obvious: the target variable\nis not used to predict itself. However, it may be that prior values for the target variable\nare quite helpful to predict future values---so such prior values may be included as\nfeatures.\n\nThe creation of models from data is known as model induction. Induction is a term\nfrom philosophy that refers to generalizing from specific cases to general rules (or laws,\nor truths). Our models are general rules in a statistical sense (they usually do not hold\n100% of the time; often not nearly), and the procedure that creates the model from the\ndata is called the induction algorithm or learner. Most inductive procedures have variants that induce models both for classification and for regression. We will discuss\nmainly classification models because they tend to receive less attention in other treatments of statistics, and because they are relevant to many business problems (and thus\nmuch work in data science focuses on classification).\n\nTerminology: Induction and deduction\nInduction can be contrasted with deduction. Deduction starts with\ngeneral rules and specific facts, and creates other specific facts from\nthem. The use of our models can be considered a procedure of (probabilistic) deduction. We will get to this shortly.\n\nModels, Induction, and Prediction\n\n|\n\n47", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00072", "page_num": 72, "segment": "The input data for the induction algorithm, used for inducing the model, are called the\ntraining data. As mentioned in Chapter 2, they are called labeled data because the value\nfor the target variable (the label) is known.\n\nLet’s return to our example churn problem. Based on what we learned in Chapter 1 and\nChapter 2, we might decide that in the modeling stage we should build a “supervised\nsegmentation” model, which divides the sample into segments having (on average)\nhigher or lower tendency to leave the company after contract expiration. To think about\nhow this might be done, let’s now turn to one of our fundamental concepts: How can\nwe select one or more attributes/features/variables that will best divide the sample with\nrespect to our target variable of interest?\n\nSupervised Segmentation\nRecall that a predictive model focuses on estimating the value of some particular target\nvariable of interest. An intuitive way of thinking about extracting patterns from data in\na supervised manner is to try to segment the population into subgroups that have different values for the target variable (and within the subgroup the instances have similar\nvalues for the target variable). If the segmentation is done using values of variables that\nwill be known when the target is not, then these segments can be used to predict the\nvalue of the target variable. Moreover, the segmentation may at the same time provide\na human-understandable set of segmentation patterns. One such segment expressed in\nEnglish might be: “Middle-aged professionals who reside in New York City on average\nhave a churn rate of 5%.” Specifically, the term “middle-aged professionals who reside\nin New York City” is the definition of the segment (which references some particular\nattributes) and “a churn rate of 5%” describes the predicted value of the target variable\nfor the segment.2\n\nOften we are interested in applying data mining when we have many attributes, and are\nnot sure exactly what the segments should be. In our churn-prediction problem, who\nis to say what are the best segments for predicting the propensity to churn? If there exist\nin the data segments with significantly different (average) values for the target variable,\nwe would like to be able to extract them automatically.\n\nThis brings us to our fundamental concept: how can we judge whether a variable contains important information about the target variable? How much? We would like\nautomatically to get a selection of the more informative variables with respect to the\nparticular task at hand (namely, predicting the value of the target variable). Even better,\nwe might like to rank the variables by how good they are at predicting the value of the\ntarget.\n\n2. The predicted value can be estimated from the data in different ways, which we will get to. At this point we\n\ncan think of it roughly as an average of some sort from the training data that fall into the segment.\n\n48\n\n|\n\nChapter 3: Introduction to Predictive Modeling: From Correlation to Supervised Segmentation", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00073", "page_num": 73, "segment": "Figure 3-2. A set of people to be classified. The label over each head represents the value\nof the target variable (write-off or not). Colors and shapes represent different predictor\nattributes.\n\nConsider just the selection of the single most informative attribute. Solving this problem\nwill introduce our first concrete data mining technique---simple, but easily extendable\nto be very useful. In our example, what variable gives us the most information about\nthe future churn rate of the population? Being a professional? Age? Place of residence?\nIncome? Number of complaints to customer service? Amount of overage charges?\n\nWe now will look carefully into one useful way to select informative variables, and then\nlater will show how this technique can be used repeatedly to build a supervised segmentation. While very useful and illustrative, please keep in mind that direct, multivariate supervised segmentation is just one application of this fundamental idea of\nselecting informative variables. This notion should become one of your conceptual tools\nwhen thinking about data science problems more generally. For example, as we go forward we will delve into other modeling approaches, ones that do not incorporate\nvariable selection directly. When the world presents you with very large sets of attributes,\nit may be (extremely) useful to harken back to this early idea and to select a subset of\ninformative attributes. Doing so can substantially reduce the size of an unwieldy dataset,\nand as we will see, often will improve the accuracy of the resultant model.\n\nSelecting Informative Attributes\nGiven a large set of examples, how do we select an attribute to partition them in an\ninformative way? Let’s consider a binary (two class) classification problem, and think\nabout what we would like to get out of it. To be concrete, Figure 3-2 shows a simple\nsegmentation problem: twelve people represented as stick figures. There are two types\nof heads: square and circular; and two types of bodies: rectangular and oval; and two of\nthe people have gray bodies while the rest are white.\n\nThese are the attributes we will use to describe the people. Above each person is the\nbinary target label, Yes or No, indicating (for example) whether the person becomes a\nloan write-off. We could describe the data on these people as:\n\nSupervised Segmentation\n\n|\n\n49", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00074", "page_num": 74, "segment": "• Attributes:\n\n--- head-shape: square, circular\n\n--- body-shape: rectangular, oval\n\n--- body-color: gray, white\n\n• Target variable:\n\n--- write-off: Yes, No\n\nSo let’s ask ourselves: which of the attributes would be best to segment these people into\ngroups, in a way that will distinguish write-offs from non-write-offs? Technically, we\nwould like the resulting groups to be as pure as possible. By pure we mean homogeneous\nwith respect to the target variable. If every member of a group has the same value for\nthe target, then the group is pure. If there is at least one member of the group that has\na different value for the target variable than the rest of the group, then the group is\nimpure.\n\nUnfortunately, in real data we seldom expect to find a variable that will make the segments pure. However, if we can reduce the impurity substantially, then we can both\nlearn something about the data (and the corresponding population), and importantly\nfor this chapter, we can use the attribute in a predictive model---in our example, predicting that members of one segment will have higher or lower write-off rates than those\nin another segment. If we can do that, then we can for example offer credit to those with\nthe lower predicted write-off rates, or can offer different credit terms based on the\ndifferent predicted write-off rates.\n\nTechnically, there are several complications:\n\n1. Attributes rarely split a group perfectly. Even if one subgroup happens to be pure,\nthe other may not. For example, in Figure 3-2, consider if the second person were\nnot there. Then body-color=gray would create a pure segment (write-off=no). However, the other associated segment, body-color=white, still is not pure.\n\n2. In the prior example, the condition body-color=gray only splits off one single data\npoint into the pure subset. Is this better than another split that does not produce\nany pure subset, but reduces the impurity more broadly?\n\n3. Not all attributes are binary; many attributes have three or more distinct values. We\nmust take into account that one attribute can split into two groups while another\nmight split into three groups, or seven. How do we compare these?\n\n4. Some attributes take on numeric values (continuous or integer). Does it make sense\nto make a segment for every numeric value? (No.) How should we think about\ncreating supervised segmentations using numeric attributes?\n\n50\n\n|\n\nChapter 3: Introduction to Predictive Modeling: From Correlation to Supervised Segmentation", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00075", "page_num": 75, "segment": "Fortunately, for classification problems we can address all the issues by creating a formula that evaluates how well each attribute splits a set of examples into segments, with\nrespect to a chosen target variable. Such a formula is based on a purity measure.\n\nThe most common splitting criterion is called information gain, and it is based on a\npurity measure called entropy. Both concepts were invented by one of the pioneers of\ninformation theory, Claude Shannon, in his seminal work in the field (Shannon, 1948).\n\nEntropy is a measure of disorder that can be applied to a set, such as one of our individual\nsegments. Consider that we have a set of properties of members of the set, and each\nmember has one and only one of the properties. In supervised segmentation, the member properties will correspond to the values of the target variable. Disorder corresponds\nto how mixed (impure) the segment is with respect to these properties of interest. So,\nfor example, a mixed up segment with lots of write-offs and lots of non-write-offs would\nhave high entropy.\n\nMore technically, entropy is defined as:\n\nEquation 3-1. Entropy\n\nentropy = - p1 log ( p1) - p2 log ( p2) - ⋯\n\nEach pi is the probability (the relative percentage) of property i within the set, ranging\nfrom pi = 1 when all members of the set have property i, and pi = 0 when no members\nof the set have property i. The ... simply indicates that there may be more than just two\nproperties (and for the technically minded, the logarithm is generally taken as base 2).\n\nSince the entropy equation might not lend itself to intuitive understanding, Figure 3-3\nshows a plot of the entropy of a set containing 10 instances of two classes, + and --. We\ncan see then that entropy measures the general disorder of the set, ranging from zero\nat minimum disorder (the set has members all with the same, single property) to one\nat maximal disorder (the properties are equally mixed). Since there are only two classes,\np+ = 1--p--. Starting with all negative instances at the lower left, p+ = 0, the set has minimal\ndisorder (it is pure) and the entropy is zero. If we start to switch class labels of elements\nof the set from -- to +, the entropy increases. Entropy is maximized at 1 when the instance\nclasses are balanced (five of each), and p+ = p-- = 0.5. As more class labels are switched,\nthe + class starts to predominate and the entropy lowers again. When all instances are\npositive, p+ = 1 and entropy is minimal again at zero.\n\nAs a concrete example, consider a set S of 10 people with seven of the non-write-off class\nand three of the write-off class. So:\n\np(non-write-off) = 7 / 10 = 0.7\n\np(write-off) = 3 / 10 = 0.3\n\nSupervised Segmentation\n\n|\n\n51", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00076", "page_num": 76, "segment": "Figure 3-3. Entropy of a two-class set as a function of p(+).\n\nentropy(S) =\n≈\n\n- 0.7 × log2 (0.7) + 0.3 × log2 (0.3)\n- 0.7 × - 0.51 + 0.3 × - 1.74\n\n≈\n\n0.88\n\nEntropy is only part of the story. We would like to measure how informative an attribute\nis with respect to our target: how much gain in information it gives us about the value\nof the target variable. An attribute segments a set of instances into several subsets. Entropy only tells us how impure one individual subset is. Fortunately, with entropy to\nmeasure how disordered any set is, we can define information gain (IG) to measure how\nmuch an attribute improves (decreases) entropy over the whole segmentation it creates.\nStrictly speaking, information gain measures the change in entropy due to any amount\nof new information being added; here, in the context of supervised segmentation, we\nconsider the information gained by splitting the set on all values of a single attribute.\nLet’s say the attribute we split on has k different values. Let’s call the original set of\nexamples the parent set, and the result of splitting on the attribute values the k children sets. Thus, information gain is a function of both a parent set and of the children\n\n52\n\n|\n\nChapter 3: Introduction to Predictive Modeling: From Correlation to Supervised Segmentation", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00077", "page_num": 77, "segment": "resulting from some partitioning of the parent set---how much information has this\nattribute provided? That depends on how much purer the children are than the parent.\nStated in the context of predictive modeling, if we were to know the value of this attribute, how much would it increase our knowledge of the value of the target variable?\n\nSpecifically, the definition of information gain (IG) is:\n\nEquation 3-2. Information gain\n\nIG(parent, children) =\n\nentropy(parent) -\np(c1) × entropy(c1) + p(c2) × entropy(c2) + ⋯\n\nNotably, the entropy for each child (ci) is weighted by the proportion of instances belonging to that child, p(ci). This addresses directly our concern from above that splitting\noff a single example, and noticing that that set is pure, may not be as good as splitting\nthe parent set into two nice large, relatively pure subsets, even if neither is pure.\n\nAs an example, consider the split in Figure 3-4. This is a two-class problem (• and ★).\nExamining the figure, the children sets certainly seem “purer” than the parent set. The\nparent set has 30 instances consisting of 16 dots and 14 stars, so:\n\nentropy( parent) =\n≈\n\n- p( • ) × log2 p( • ) + p( ☆ ) × log2 p( ☆ )\n- 0.53 × - 0.9 + 0.47 × - 1.1\n\n≈\n\n0.99\n\n(very impure)\n\nThe entropy of the left child is:\n\nentropy(Balance < 50K ) =\n≈\n\n≈\n\n- p( • ) × log2 p( • ) + p( ☆ ) × log2 p( ☆ )\n- 0.92 × ( - 0.12) + 0.08 × ( - 3.7)\n0.39\n\nThe entropy of the right child is:\n\nentropy(Balance ≥ 50K ) =\n≈\n\n≈\n\n- p( • ) × log2 p( • ) + p( ☆ ) × log2 p( ☆ )\n- 0.24 × ( - 2.1) + 0.76 × ( - 0.39)\n0.79\n\nUsing Equation 3-2, the information gain of this split is:\n\nSupervised Segmentation\n\n|\n\n53", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": null, "section_title": null}
{"segment_id": "00078", "page_num": 78, "segment": "IG =\n\nentropy( parent) - p(Balance < 50K) × entropy(Balance < 50K)\n+ p(Balance ≥ 50K) × entropy(Balance ≥ 50K)\n\n≈\n\n≈\n\n0.99 - 0.43 × 0.39 + 0.57 × 0.79\n\n0.37\n\nSo this split reduces entropy substantially. In predictive modeling terms, the attribute\nprovides a lot of information on the value of the target.\n\nFigure 3-4. Splitting the “write-off” sample into two segments, based on splitting the\nBalance attribute (account balance) at 50K.\n\nAs a second example, consider another candidate split shown in Figure 3-5. This is the\nsame parent set as in Figure 3-4, but instead we consider splitting on the attribute\nResidence with three values: OWN, RENT, and OTHER. Without showing the detailed\ncalculations:\n\n54\n\n|\n\nChapter 3: Introduction to Predictive Modeling: From Correlation to Supervised Segmentation", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00079", "page_num": 79, "segment": "entropy( parent) ≈ 0.99\nentropy(Residence=OWN) ≈ 0.54\nentropy(Residence=RENT) ≈ 0.97\nentropy(Residence=OTHER) ≈ 0.98\nIG ≈ 0.13\n\nFigure 3-5. A classification tree split on the three-valued Residence attribute.\n\nThe Residence variable does have a positive information gain, but it is lower than that\nof Balance. Intuitively, this is because, while the one child Residence=OWN has considerably reduced entropy, the other values RENT and OTHER produce children that\nare no more pure than the parent. Thus, based on these data, the Residence variable is\nless informative than Balance.\n\nLooking back at our concerns from above about creating supervised segmentation for\nclassification problems, information gain addresses them all. It does not require absolute\n\nSupervised Segmentation\n\n|\n\n55", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00080", "page_num": 80, "segment": "purity. It can be applied to any number of child subsets. It takes into account the relative\nsizes of the children, giving more weight to larger subsets.3\n\nNumeric variables\nWe have not discussed what exactly to do if the attribute is numeric. Numeric variables can be “discretized” by choosing a split point\n(or many split points) and then treating the result as a categorical\nattribute. For example, Income could be divided into two or more\nranges. Information gain can be applied to evaluate the segmentation created by this discretization of the numeric attribute. We still\nare left with the question of how to choose the split point(s) for the\nnumeric attribute. Conceptually, we can try all reasonable split points,\nand choose the one that gives the highest information gain.\n\nFinally, what about supervised segmentations for regression problems---problems with\na numeric target variable? Looking at reducing the impurity of the child subsets still\nmakes intuitive sense, but information gain is not the right measure, because entropybased information gain is based on the distribution of the properties in the segmentation.\nInstead, we would want a measure of the purity of the numeric (target) values in the\nsubsets.\n\nWe will not go through a derivation here, but the fundamental idea is important: a\nnatural measure of impurity for numeric values is variance. If the set has all the same\nvalues for the numeric target variable, then the set is pure and the variance is zero. If\nthe numeric target values in the set are very different, then the set will have high variance.\nWe can create a similar notion to information gain by looking at reductions in variance\nbetween parent and children. The process proceeds in direct analogy to the derivation\nfor information gain above. To create the best segmentation given a numeric target, we\nmight choose the one that produces the best weighted average variance reduction. In\nessence, we again would be finding variables that have the best correlation with the\ntarget, or alternatively, are most predictive of the target.\n\nExample: Attribute Selection with Information Gain\nNow we are ready to apply our first concrete data mining technique. For a dataset with\ninstances described by attributes and a target variable, we can determine which attribute\nis the most informative with respect to estimating the value of the target variable. (We\nwill delve into this more deeply below.) We also can rank a set of attributes by their\ninformativeness, in particular by their information gain. This can be used simply to\n\n3. Technically, there remains a concern with attributes with very many values, as splitting on them may result\nin large information gain, but not be predictive. This problem (“overfitting”) is the subject of Chapter 5.\n\n56\n\n|\n\nChapter 3: Introduction to Predictive Modeling: From Correlation to Supervised Segmentation", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00081", "page_num": 81, "segment": "understand the data better. It can be used to help predict the target. Or it can be used\nto reduce the size of the data to be analyzed, by selecting a subset of attributes in cases\nwhere we can not or do not want to process the entire dataset.\n\nTo illustrate the use of information gain, we introduce a simple but realistic dataset taken\nfrom the machine learning dataset repository at the University of California at Irvine.4\nIt is a dataset describing edible and poisonous mushrooms taken from The Audubon\nSociety Field Guide to North American Mushrooms. From the description:\n\nThis dataset includes descriptions of hypothetical samples corresponding to 23 species\nof gilled mushrooms in the Agaricus and Lepiota Family (pp. 500--525). Each species is\nidentified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly\nstates that there is no simple rule for determining the edibility of a mushroom; no rule\nlike “leaflets three, let it be” for Poisonous Oak and Ivy.\n\nEach data example (instance) is one mushroom sample, described in terms of its observable attributes (the features). The twenty-odd attributes and the values for each are\nlisted in Table 3-1. For a given example, each attribute takes on a single discrete value\n(e.g., gill-color=black). We use 5,644 examples from the dataset, comprising 2,156 poisonous and 3,488 edible mushrooms.\n\nThis is a classification problem because we have a target variable, called edible?, with\ntwo values yes (edible) and no (poisonous), specifying our two classes. Each of the rows\nin the training set has a value for this target variable. We will use information gain to\nanswer the question: “Which single attribute is the most useful for distinguishing edible\n(edible?=Yes) mushrooms from poisonous (edible?=No) ones?” This is a basic attribute\nselection problem. In much larger problems we could imagine selecting the best ten or\nfifty attributes out of several hundred or thousand, and often you want do this if you\nsuspect there are far too many attributes for your mining problem, or that many are not\nuseful. Here, for simplicity, we will find the single best attribute instead of the top ten.\n\nTable 3-1. The attributes of the Mushroom dataset\n\nAttribute name\n\nPossible values\n\nCAP-SHAPE\n\nCAP-SURFACE\n\nCAP-COLOR\n\nBRUISES?\n\nODOR\n\nbell, conical, convex, flat, knobbed, sunken\n\nfibrous, grooves, scaly, smooth\n\nbrown, buff, cinnamon, gray, green, pink, purple, red,\nwhite, yellow\n\nyes, no\n\nalmond, anise, creosote, fishy, foul, musty, none,\npungent, spicy\n\nGILL-ATTACHMENT\n\nattached, descending, free, notched\n\n4. See this UC Irvine Machine Learning Repository page.\n\nSupervised Segmentation\n\n|\n\n57", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00082", "page_num": 82, "segment": "Attribute name\n\nGILL-SPACING\n\nGILL-SIZE\n\nGILL-COLOR\n\nSTALK-SHAPE\n\nSTALK-ROOT\n\nPossible values\n\nclose, crowded, distant\n\nbroad, narrow\n\nblack, brown, buff, chocolate, gray, green, orange, pink,\npurple, red, white, yellow\n\nenlarging, tapering\n\nbulbous, club, cup, equal, rhizomorphs, rooted, missing\n\nSTALK-SURFACE-ABOVE-RING\n\nfibrous, scaly, silky, smooth\n\nSTALK-SURFACE-BELOW-RING\n\nfibrous, scaly, silky, smooth\n\nSTALK-COLOR-ABOVE-RING\n\nSTALK-COLOR-BELOW-RING\n\nbrown, buff, cinnamon, gray, orange, pink, red, white,\nyellow\n\nbrown, buff, cinnamon, gray, orange, pink, red, white,\nyellow\n\nVEIL-TYPE\n\nVEIL-COLOR\n\nRING-NUMBER\n\nRING-TYPE\n\nSPORE-PRINT-COLOR\n\nPOPULATION\n\nHABITAT\n\npartial, universal\n\nbrown, orange, white, yellow\n\nnone, one, two\n\ncobwebby, evanescent, flaring, large, none, pendant, sheath\ning, zone\n\nblack, brown, buff, chocolate, green, orange, purple, white,\nyellow\n\nabundant, clustered, numerous, scattered, several, solitary\n\ngrasses, leaves, meadows, paths, urban, waste, woods\n\nEDIBLE? (Target variable)\n\nyes, no\n\nSince we now have a way to measure information gain this is straightforward: we are\nasking for the single attribute that gives the highest information gain.\n\nTo do this, we calculate the information gain achieved by splitting on each attribute.\nThe information gain from Equation 3-2 is defined on a parent and a set of children.\nThe parent in each case is the whole dataset. First we need entropy(parent), the entropy\nof the whole dataset. If the two classes were perfectly balanced in the dataset it would\nhave an entropy of 1. This dataset is slightly unbalanced (more edible than poisonous\nmushrooms are represented) and its entropy is 0.96.\n\nTo illustrate entropy reduction graphically, we’ll show a number of entropy graphs for\nthe mushroom domain (Figure 3-6 through Figure 3-8). Each graph is a twodimensional description of the entire dataset’s entropy as it is divided in various ways\nby different attributes. On the x axis is the proportion of the dataset (0 to 1), and on the\ny axis is the entropy (also 0 to 1) of a given piece of the data. The amount of shaded area\nin each graph represents the amount of entropy in the dataset when it is divided by some\n\n58\n\n|\n\nChapter 3: Introduction to Predictive Modeling: From Correlation to Supervised Segmentation", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00083", "page_num": 83, "segment": "chosen attribute (or not divided, in the case of Figure 3-6). Our goal of having the lowest\nentropy corresponds to having as little shaded area as possible.\n\nThe first chart, Figure 3-6, shows the entropy of the entire dataset. In such a chart, the\nhighest possible entropy corresponds to the entire area being shaded; the lowest possible\nentropy corresponds to the entire area being white. Such a chart is useful for visualizing\ninformation gain from different partitions of a dataset, because any partition can be\nshown simply as slices of the graph (with widths corresponding to the proportion of\nthe dataset), each with its own entropy. The weighted sum of entropies in the information gain calculation will be depicted simply by the total amount of shaded area.\n\nFigure 3-6. Entropy chart for the entire Mushroom dataset. The entropy for the entire\ndataset is 0.96, so 96% of the area is shaded.\n\nSupervised Segmentation\n\n|\n\n59", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00084", "page_num": 84, "segment": "Figure 3-7. Entropy chart for the Mushroom dataset as split by GILL-COLOR. The\namount of shading corresponds to the total (weighted sum) entropy, with each bar corresponding to the entropy of one of the attribute’s values, and the width of the bar corresponding to the prevalence of that value in the data.\n\nFor our entire dataset, the global entropy is 0.96, so Figure 3-6 shows a large shaded\narea below the line y = 0.96. We can think of this as our starting entropy---any informative attribute should produce a new graph with less shaded area. Now we show the\nentropy charts of three sample attributes. Each value of an attribute occurs in the dataset\nwith a different frequency, so each attribute splits the set in a different way.\n\nFigure 3-7 shows the dataset split apart by the attribute GILL-COLOR, whose values\nare coded as y (yellow), u (purple), n (brown), and so on. The width of each attribute\nrepresents what proportion of the dataset has that value, and the height is its entropy.\nWe can see that GILL-COLOR reduces the entropy somewhat; the shaded area in\nFigure 3-7 is considerably less than the area in Figure 3-6.\n\n60\n\n|\n\nChapter 3: Introduction to Predictive Modeling: From Correlation to Supervised Segmentation", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00085", "page_num": 85, "segment": "Figure 3-8. Entropy chart for the Mushroom dataset as split by SPORE-PRINTCOLOR. The amount of shading corresponds to the total (weighted sum) entropy, with\neach bar corresponding to the entropy of one of the attribute’s values, and the width of\nthe bar corresponding to the prevalence of that value in the data.\n\nSimilarly, Figure 3-8 shows how SPORE-PRINT-COLOR decreases uncertainty (entropy). A few of the values, such as h (chocolate), specify the target value perfectly and\nthus produce zero-entropy bars. But notice that they don’t account for very much of the\npopulation, only about 30%.\n\nFigure 3-9 shows the graph produced by ODOR. Many of the values, such as a (al\nmond), c (creosote), and m (musty) produce zero-entropy partitions; only n (no odor)\nhas a considerable entropy (about 20%). In fact, ODOR has the highest information\ngain of any attribute in the Mushroom dataset. It can reduce the dataset’s total entropy\nto about 0.1, which gives it an information gain of 0.96 -- 0.1 = 0.86. What is this saying?\nMany odors are completely characteristic of poisonous or edible mushrooms, so odor\nis a very informative attribute to check when considering mushroom edibility.5 If you’re\n\n5. This assumes odor can be measured accurately, of course. If your sense of smell is poor you may not want to\nbet your life on it. Frankly, you probably wouldn’t want to bet your life on the results of mining data from a\nfield guide. Nevertheless, it makes a nice example.\n\nSupervised Segmentation\n\n|\n\n61", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00086", "page_num": 86, "segment": "going to build a model to determine the mushroom edibility using only a single feature,\nyou should choose its odor. If you were going to build a more complex model you might\nstart with the attribute ODOR before considering adding others. In fact, this is exactly\nthe topic of the next section.\n\nFigure 3-9. Entropy chart for the Mushroom dataset as split by ODOR. The amount of\nshading corresponds to the total (weighted sum) entropy, with each bar corresponding\nto the entropy of one of the attribute’s values, and the width of the bar corresponding to\nthe prevalence of that value in the data.\n\nSupervised Segmentation with Tree-Structured Models\nWe have now introduced one of the fundamental ideas of data mining: finding informative attributes from the data. Let’s continue on the topic of creating a supervised\nsegmentation, because as important as it is, attribute selection alone does not seem to\nbe sufficient. If we select the single variable that gives the most information gain, we\ncreate a very simple segmentation. If we select multiple attributes each giving some\ninformation gain, it’s not clear how to put them together. Recall from earlier that we\nwould like to create segments that use multiple attributes, such as “Middle-aged professionals who reside in New York City on average have a churn rate of 5%.” We now\n\n62\n\n|\n\nChapter 3: Introduction to Predictive Modeling: From Correlation to Supervised Segmentation", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00087", "page_num": 87, "segment": "introduce an elegant application of the ideas we’ve developed for selecting important\nattributes, to produce a multivariate (multiple attribute) supervised segmentation.\n\nConsider a segmentation of the data to take the form of a “tree,” such as that shown in\nFigure 3-10. In the figure, the tree is upside down with the root at the top. The tree is\nmade up of nodes, interior nodes and terminal nodes, and branches emanating from\nthe interior nodes. Each interior node in the tree contains a test of an attribute, with\neach branch from the node representing a distinct value of the attribute. Following the\nbranches from the root node down (in the direction of the arrows), each path eventually\nterminates at a terminal node, or leaf. The tree creates a segmentation of the data: every\ndata point will correspond to one and only one path in the tree, and thereby to one and\nonly one leaf. In other words, each leaf corresponds to a segment, and the attributes and\nvalues along the path give the characteristics of the segment. So the rightmost path in\nthe tree in Figure 3-10 corresponds to the segment “Older, unemployed people with\nhigh balances.” The tree is a supervised segmentation, because each leaf contains a value\nfor the target variable. Since we are talking about classification, here each leaf contains\na classification for its segment. Such a tree is called a classification tree or more loosely\na decision tree.\n\nClassification trees often are used as predictive models---“tree structured models.” In\nuse, when presented with an example for which we do not know its classification, we\ncan predict its classification by finding the corresponding segment and using the class\nvalue at the leaf. Mechanically, one would start at the root node and descend through\nthe interior nodes, choosing branches based on the specific attribute values in the example. The nonleaf nodes are often referred to as “decision nodes,” because when descending through the tree, at each node one uses the values of the attribute to make a\ndecision about which branch to follow. Following these branches ultimately leads to a\nfinal decision about what class to predict: eventually a terminal node is reached, which\ngives a class prediction. In a tree, no two parents share descendants and there are no\ncycles; the branches always “point downwards” so that every example always ends up at\na leaf node with some specific class determination.\n\nConsider how we would use the classification tree in Figure 3-10 to classify an example\nof the person named Claudio from Figure 3-1. The values of Claudio’s attributes are\nBalance=115K, Employed=No, and Age=40. We begin at the root node that tests Employed. Since the value is No we take the right branch. The next test is Balance. The value\nof Balance is 115K, which is greater than 50K so we take a right branch again to a node\nthat tests Age. The value is 40 so we take the left branch. This brings us to a leaf node\nspecifying class=Not Write-off, representing a prediction that Claudio will not default.\nAnother way of saying this is that we have classified Claudio into a segment defined by\n(Employed=No, Balance=115K, Age<45) whose classification is Not Write -off.\n\nSupervised Segmentation\n\n|\n\n63", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00088", "page_num": 88, "segment": "Figure 3-10. A simple classification tree.\n\nClassification trees are one sort of tree-structured model. As we will see later, in business\napplications often we want to predict the probability of membership in the class (e.g.,\nthe probability of churn or the probability of write-off), rather than the class itself. In\nthis case, the leaves of the probability estimation tree would contain these probabilities\nrather than a simple value. If the target variable is numeric, the leaves of the regression\ntree contain numeric values. However, the basic idea is the same for all.\n\nTrees provide a model that can represent exactly the sort of supervised segmentation\nwe often want, and we know how to use such a model to predict values for new cases\n(in “use”). However, we still have not addressed how to create such a model from the\ndata. We turn to that now.\n\nThere are many techniques to induce a supervised segmentation from a dataset. One of\nthe most popular is to create a tree-structured model (tree induction). These techniques\nare popular because tree models are easy to understand, and because the induction\nprocedures are elegant (simple to describe) and easy to use. They are robust to many\ncommon data problems and are relatively efficient. Most data mining packages include\nsome type of tree induction technique.\n\nHow do we create a classification tree from data? Combining the ideas introduced above,\nthe goal of the tree is to provide a supervised segmentation---more specifically, to partition the instances, based on their attributes, into subgroups that have similar values\n\n64\n\n|\n\nChapter 3: Introduction to Predictive Modeling: From Correlation to Supervised Segmentation", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00089", "page_num": 89, "segment": "Figure 3-11. First partitioning: splitting on body shape (rectangular versus oval).\n\nfor their target variables. We would like for each “leaf ” segment to contain instances\nthat tend to belong to the same class.\n\nTo illustrate the process of classification tree induction, consider the very simple example set shown previously in Figure 3-2.\n\nTree induction takes a divide-and-conquer approach, starting with the whole dataset\nand applying variable selection to try to create the “purest” subgroups possible using\nthe attributes. In the example, one way is to separate people based on their body type:\nrectangular versus oval. This creates the two groups shown in Figure 3-11. How good\nis this partitioning? The rectangular-body people on the left are mostly Yes, with a single\nNo person, so it is mostly pure. The oval-body group on the right has mostly No people,\nbut two Yes people. This step is simply a direct application of the attribute selection\nideas presented above. Let’s consider this “split” to be the one that yields the largest\ninformation gain.\n\nLooking at Figure 3-11, we can now see the elegance of tree induction, and why it\nresonates well with so many people. The left and right subgroups are simply smaller\nversions of the problem with which we initially were faced! We can simply take each\ndata subset and recursively apply attribute selection to find the best attribute to partition\nit. So in our example, we recursively consider the oval-body group (Figure 3-12). To\nsplit this group again we now consider another attribute: head shape. This splits the\ngroup in two on the right side of the figure. How good is this partitioning? Each new\ngroup has a single target label: four (square heads) of No, and two (round heads) of\n\nSupervised Segmentation\n\n|\n\n65", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00090", "page_num": 90, "segment": "Figure 3-13. Third partitioning: the rectangular body people subgrouped by body color.\n\nYes. These groups are “maximally pure” with respect to class labels and there is no need\nto split them further.\n\nFigure 3-12. Second partitioning: the oval body people sub-grouped by head type.\n\n66\n\n|\n\nChapter 3: Introduction to Predictive Modeling: From Correlation to Supervised Segmentation", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00091", "page_num": 91, "segment": "We still have not done anything with the rectangular body group on the left side of\nFigure 3-11, so let’s consider how to split them. There are five Yes people and one No\nperson. There are two attributes we could split upon: head shape (square or round), and\nbody color (white or gray). Either of these would work, so we arbitrarily choose body\ncolor. This produces the groupings in Figure 3-13. These are pure groups (all of one\ntype) so we are finished. The classification tree corresponding to these groupings is\nshown in Figure 3-14.\n\nIn summary, the procedure of classification tree induction is a recursive process of\ndivide and conquer, where the goal at each step is to select an attribute to partition the\ncurrent group into subgroups that are as pure as possible with respect to the target\nvariable. We perform this partitioning recursively, splitting further and further until we\nare done. We choose the attributes to split upon by testing all of them and selecting\nwhichever yields the purest subgroups. When are we done? (In other words, when do\nwe stop recursing?) It should be clear that we would stop when the nodes are pure, or\nwhen we run out of variables to split on. But we may want to stop earlier; we will return\nto this question in Chapter 5.\n\nVisualizing Segmentations\nContinuing with the metaphor of predictive model building as supervised segmentation,\nit is instructive to visualize exactly how a classification tree partitions the instance space.\nThe instance space is simply the space described by the data features. A common form\nof instance space visualization is a scatterplot on some pair of features, used to compare\none variable against another to detect correlations and relationships.\n\nThough data may contain dozens or hundreds of variables, it is only really possible to\nvisualize segmentations in two or three dimensions at once. Still, visualizing models in\ninstance space in a few dimensions is useful for understanding the different types of\nmodels because it provides insights that apply to higher dimensional spaces as well. It\nmay be difficult to compare very different families of models just by examining their\nform (e.g., a mathematical formula versus a set of rules) or the algorithms that generate\nthem. Often it is easier to compare them based on how they partition the instance space.\n\nFor example, Figure 3-15 shows a simple classification tree next to a two-dimensional\ngraph of the instance space: Balance on the x axis and Age on the y axis. The root node\nof the classification tree tests Balance against a threshold of 50K. In the graph, this\ncorresponds to a vertical line at 50K on the x axis splitting the plane into Balance<50K\nand Balance≥50K. At the left of this line lie the instances whose Balance values are less\nthan 50K; there are 13 examples of class Write-off (black dot) and 2 examples of class\nnon-Write-off (plus sign) in this region.\n\nOn the right branch out of the root node are instances with Balance≥50K. The next\nnode in the classification tree tests the Age attribute against the threshold 45. In the\n\nVisualizing Segmentations\n\n|\n\n67", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00092", "page_num": 92, "segment": "Figure 3-14. The classification tree resulting from the splits done in Figure 3-11 to\nFigure 3-13.\n\n68\n\n|\n\nChapter 3: Introduction to Predictive Modeling: From Correlation to Supervised Segmentation", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00093", "page_num": 93, "segment": "graph this corresponds to the horizontal dotted line at Age=45. It appears only on the\nright side of the graph because this partition only applies to examples with Balance≥50.\nThe Age decision node assigns to its left branch instances with Age<45, corresponding\nto the lower right segment of the graph, representing: (Balance≥50K AND Age<45).\n\nNotice that each internal (decision) node corresponds to a split of the instance space.\nEach leaf node corresponds to an unsplit region of the space (a segment of the population). Whenever we follow a path in the tree out of a decision node we are restricting\nattention to one of the two (or more) subregions defined by the split. As we descend\nthrough a classification tree we consider progressively more focused subregions of the\ninstance space.\n\nDecision lines and hyperplanes\nThe lines separating the regions are known as decision lines (in two\ndimensions) or more generally decision surfaces or decision boundaries. Each node of a classification tree tests a single variable against a\nfixed value so the decision boundary corresponding to it will always\nbe perpendicular to the axis representing this variable. In two dimensions, the line will be either horizontal or vertical. If the data had three\nvariables the instance space would be three-dimensional and each\nboundary surface imposed by a classification tree would be a twodimensional plane. In higher dimensions, since each node of a classification tree tests one variable it may be thought of as “fixing” that one\ndimension of a decision boundary; therefore, for a problem of n variimposes an (n--1)-\nables, each node of a classification tree\ndimensional “hyperplane” decision boundary on the instance space.\n\nYou will often see the term hyperplane used in data mining literature\nto refer to the general separating surface, whatever it may be. Don’t be\nintimidated by this terminology. You can always just think of it as a\ngeneralization of a line or a plane.\n\nOther decision surfaces are possible, as we shall see later.\n\nVisualizing Segmentations\n\n|\n\n69", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00094", "page_num": 94, "segment": "Figure 3-15. A classification tree and the partitions it imposes in instance space. The\nblack dots correspond to instances of the class Write-off, the plus signs correspond to\ninstances of class non-Write-off. The shading shows how the tree leaves correspond to\nsegments of the population in instance space.\n\n70\n\n|\n\nChapter 3: Introduction to Predictive Modeling: From Correlation to Supervised Segmentation", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00095", "page_num": 95, "segment": "Trees as Sets of Rules\nBefore moving on from the interpretation of classification trees, we should mention\ntheir interpretation as logical statements. Consider again the tree shown at the top of\nFigure 3-15. You classify a new unseen instance by starting at the root node and following\nthe attribute tests downward until you reach a leaf node, which specifies the instance’s\npredicted class. If we trace down a single path from the root node to a leaf, collecting\nthe conditions as we go, we generate a rule. Each rule consists of the attribute tests along\nthe path connected with AND. Starting at the root node and choosing the left branches\nof the tree, we get the rule:\n\nIF (Balance < 50K) AND (Age < 50) THEN Class=Write-off\n\nWe can do this for every possible path to a leaf node. From this tree we get three more\nrules:\n\nIF (Balance < 50K) AND (Age ≥ 50) THEN Class=No Write-off\nIF (Balance ≥ 50K) AND (Age < 45) THEN Class=Write-off\nIF (Balance ≥ 50K) AND (Age < 45) THEN Class=No Write-off\n\nThe classification tree is equivalent to this rule set. If these rules look repetitive, that’s\nbecause they are: the tree gathers common rule prefixes together toward the top of the\ntree. Every classification tree can be expressed as a set of rules this way. Whether the\ntree or the rule set is more intelligible is a matter of opinion; in this simple example,\nboth are fairly easy to understand. As the model becomes larger, some people will prefer\nthe tree or the rule set.\n\nProbability Estimation\nIn many decision-making problems, we would like a more informative prediction than\njust a classification. For example, in our churn-prediction problem, rather than simply\npredicting whether a person will leave the company within 90 days of contract expiration, we would much rather have an estimate of the probability that he will leave the\ncompany within that time. Such estimates can be used for many purposes. We will\ndiscuss some of these in detail in later chapters, but briefly: you might then rank prospects by their probability of leaving, and then allocate a limited incentive budget to the\nhighest probability instances. Alternatively, you may want to allocate your incentive\nbudget to the instances with the highest expected loss, for which you’ll need (an estimate\nof) the probability of churn. Once you have such probability estimates you can use them\nin a more sophisticated decision-making process than these simple examples, as we’ll\ndescribe in later chapters.\n\nTrees as Sets of Rules\n\n|\n\n71", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00096", "page_num": 96, "segment": "There is another, even more insidious problem with models that give simple classifications, rather than estimates of class membership probability. Consider the problem of\nestimating credit default. Under normal circumstances, for just about any segment of\nthe population to which we would be considering giving credit, the probability of writeoff will be very small---far less than 0.5. In this case, when we build a model to estimate\nthe classification (write-off or not), we’d have to say that for each segment, the members\nare likely not to default---and they will all get the same classification (not write-off). For\nexample, in a naively built tree model every leaf will be labeled “not write-off.” This turns\nout to be a frustrating experience for new data miners: after all that work, the model\nreally just says that no one is likely to default? This does not mean that the model is\nuseless. It may be that the different segments indeed have very different probabilities of\nwrite-off, they just all are less than 0.5. If instead we use these probabilities for assigning\ncredit, we may be able reduce our risk substantially.\n\nSo, in the context of supervised segmentation, we would like each segment (leaf of a tree\nmodel) to be assigned an estimate of the probability of membership in the different\nclasses. Figure 3-15 more generally shows a “probability estimation tree” model for our\nsimple write-off prediction example, giving not only a prediction of the class but also\nthe estimate of the probability of membership in the class.6\n\nFortunately, the tree induction ideas we have discussed so far can easily produce probability estimation trees instead of simple classification trees.7 Recall that the tree induction procedure subdivides the instance space into regions of class purity (low entropy).\nIf we are satisfied to assign the same class probability to every member of the segment\ncorresponding to a tree leaf, we can use instance counts at each leaf to compute a class\nprobability estimate. For example, if a leaf contains n positive instances and m negative\ninstances, the probability of any new instance being positive may be estimated as n/(n\n+m). This is called a frequency-based estimate of class membership probability.\n\nAt this point you may spot a problem with estimating class membership probabilities\nthis way: we may be overly optimistic about the probability of class membership for\nsegments with very small numbers of instances. At the extreme, if a leaf happens to have\nonly a single instance, should we be willing to say that there is a 100% probability that\nmembers of that segment will have the class that this one instance happens to have?\n\n6. We often deal with binary classification problems, such as write-off or not, or churn or not. In these cases it\nis typical just to report the probability of membership in one chosen class p(c), because the other is just 1 -- p(c).\n\n7. Often these are still called classification trees, even if the decision maker intends to use the probability estimates rather than the simple classifications.\n\n72\n\n|\n\nChapter 3: Introduction to Predictive Modeling: From Correlation to Supervised Segmentation", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00097", "page_num": 97, "segment": "This phenomenon is one example of a fundamental issue in data science (“overfitting”),\nto which we devote a chapter later in the book. For completeness, let’s quickly discuss\none easy way to address this problem of small samples for tree-based class probability\nestimation. Instead of simply computing the frequency, we would often use a “smoothed” version of the frequency-based estimate, known as the Laplace correction, the purpose of which is to moderate the influence of leaves with only a few instances. The\nequation for binary class probability estimation becomes:\n\np(c) =\n\nn + 1\nn + m + 2\n\nwhere n is the number of examples in the leaf belonging to class c, and m is the number\nof examples not belonging to class c.\n\nLet’s walk through an example with and without the Laplace correction. A leaf node\nwith two positive instances and no negative instances would produce the same\nfrequency-based estimate (p = 1) as a leaf node with 20 positive instances and no negatives. However, the first leaf node has much less evidence and may be extreme only\ndue to there being so few instances. Its estimate should be tempered by this consideration. The Laplace equation smooths its estimate down to p = 0.75 to reflect this uncertainty; the Laplace correction has much less effect on the leaf with 20 instances (p ≈\n0.95). As the number of instances increases, the Laplace equation converges to the\nfrequency-based estimate. Figure 3-16 shows the effect of Laplace correction on several\nclass ratios as the number of instances increases (2/3, 4/5, and 1/1). For each ratio the\nsolid horizontal\nline shows the uncorrected (constant) estimate, while the\ncorresponding dashed line shows the estimate with the Laplace correction applied. The\nuncorrected line is the asymptote of the Laplace correction as the number of instances\ngoes to infinity.\n\nExample: Addressing the Churn Problem with Tree\nInduction\nNow that we have a basic data mining technique for predictive modeling, let’s consider\nthe churn problem again. How could we use tree induction to help solve it?\n\nFor this example, we have a historical data set of 20,000 customers. At the point of\ncollecting the data, each customer either had stayed with the company or had left\n(churned). Each customer is described by the variables listed in Table 3-2.\n\nExample: Addressing the Churn Problem with Tree Induction\n\n|\n\n73", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00098", "page_num": 98, "segment": "Figure 3-16. The effect of Laplace smoothing on probability estimation for several instance ratios.\n\nTable 3-2. Attributes for the cellular phone churn-prediction problem\n\nVariable\n\nCOLLEGE\n\nINCOME\n\nOVERAGE\n\nLEFTOVER\n\nHOUSE\n\nExplanation\n\nIs the customer college educated?\n\nAnnual income\n\nAverage overcharges per month\n\nAverage number of leftover minutes per month\n\nEstimated value of dwelling (from census tract)\n\nHANDSET_PRICE\n\nCost of phone\n\nLONG_CALLS_PER_MONTH Average number of long calls (15 mins or over) per month\n\nAVERAGE_CALL_DURATION Average duration of a call\n\nREPORTED_SATISFACTION\n\nReported level of satisfaction\n\nREPORTED_USAGE_LEVEL\n\nSelf-reported usage level\n\nLEAVE (Target variable)\n\nDid the customer stay or leave (churn)?\n\n74\n\n|\n\nChapter 3: Introduction to Predictive Modeling: From Correlation to Supervised Segmentation", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00099", "page_num": 99, "segment": "These variables comprise basic demographic and usage information available from the\ncustomer’s application and account. We want to use these data with our tree induction\ntechnique to predict which new customers are going to churn.\n\nBefore starting to build a classification tree with these variables, it is worth asking, How\ngood are each of these variables individually? For this we measure the information gain\nof each attribute, as discussed earlier. Specifically, we apply Equation 3-2 to each variable\nindependently over the entire set of instances, to see what each gains us.\n\nThe results are in Figure 3-17, with a table listing the exact values. As you can see, the\nfirst three variables---the house value, the number of leftover minutes, and the number\nof long calls per month---have a higher information gain than the rest.8 Perhaps surprisingly, neither the amount the phone is used nor the reported degree of satisfaction\nseems, in and of itself, to be very predictive of churning.\n\nApplying a classification tree algorithm to the data, we get the tree shown in\nFigure 3-18. The highest information gain feature (HOUSE) according to Figure 3-17\nis at the root of the tree. This is to be expected since it will always be chosen first. The\nsecond best feature, OVERAGE, also appears high in the tree. However, the order in\nwhich features are chosen for the tree doesn’t exactly correspond to their ranking in\nFigure 3-17. Why is this?\n\nThe answer is that the table ranks each feature by how good it is independently, evaluated\nseparately on the entire population of instances. Nodes in a classification tree depend\non the instances above them in the tree. Therefore, except for the root node, features in\na classification tree are not evaluated on the entire set of instances. The information\ngain of a feature depends on the set of instances against which it is evaluated, so the\nranking of features for some internal node may not be the same as the global ranking.\n\nWe have not yet discussed how we decide to stop building the tree. The dataset has\n20,000 examples yet the tree clearly doesn’t have 20,000 leaf nodes. Can’t we just keep\nselecting more attributes to split upon, building the tree downwards until we’ve exhausted the data? The answer is yes, we can, but we should stop long before the model\nbecomes that complex. This issue ties in closely with model generality and overfitting,\nwhose discussion we defer to Chapter 5.\n\n8. Note that the information gains for the attributes in this churn data set are much smaller than those shown\n\npreviously for the mushroom data set.\n\nExample: Addressing the Churn Problem with Tree Induction\n\n|\n\n75", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00100", "page_num": 100, "segment": "Figure 3-17. Churn attributes from Table 3-2 ranked by information gain.\n\n76\n\n|\n\nChapter 3: Introduction to Predictive Modeling: From Correlation to Supervised Segmentation", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00101", "page_num": 101, "segment": "Figure 3-18. Classification tree learned from the cellular phone churn data.\n\nConsider a final issue with this dataset. After building a tree model from the data, we\nmeasured its accuracy against the data to see how good of a model it is. Specifically, we\nused a training set consisting half of people who churned and the other half who did\nnot; after learning a classification tree from this, we applied the tree to the dataset to see\nhow many of the examples it could classify correctly. The tree achieved 73% accuracy\non its decisions. This raises two questions:\n\n1. First, do you trust this number? If we applied the tree to another sample of 20,000\npeople from the same dataset, do you think we’d still get about 73% accuracy?\n\n2. If you do trust the number, does it mean this model is good? In other words, is a\n\nmodel with 73% accuracy worth using?\n\nExample: Addressing the Churn Problem with Tree Induction\n\n|\n\n77", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00102", "page_num": 102, "segment": "We will revisit these questions in Chapter 7 and Chapter 8, which delve into issues of\nmodel evaluation.\n\nSummary\nIn this chapter, we introduced basic concepts of predictive modeling, one of the main\ntasks of data science, in which a model is built that can estimate the value of a target\nvariable for a new unseen example. In the process, we introduced one of data science’s\nfundamental notions: finding and selecting informative attributes. Selecting informative attributes can be a useful data mining procedure in and of itself. Given a large\ncollection of data, we now can find those variables that correlate with or give us information about another variable of interest. For example, if we gather historical data on\nwhich customers have or have not left the company (churned) shortly after their contracts expire, attribute selection can find demographic or account-oriented variables\nthat provide information about the likelihood of customers churning. One basic measure of attribute information is called information gain, which is based on a purity measure called entropy; another is variance reduction.\n\nSelecting informative attributes forms the basis of a common modeling technique called\ntree induction. Tree induction recursively finds informative attributes for subsets of the\ndata. In so doing it segments the space of instances into similar regions. The partitioning\nis “supervised” in that it tries to find segments that give increasingly precise information\nabout the quantity to be predicted, the target. The resulting tree-structured model partitions the space of all possible instances into a set of segments with different predicted\nvalues for the target. For example, when the target is a binary “class” variable such as\nchurn versus not churn, or write-off versus not write-off, each leaf of the tree corresponds to a population segment with a different estimated probability of class\nmembership.\n\nAs an exercise, think about what would be different in building a treestructured model for regression rather than for classification. What\nwould need to be changed from what you’ve learned about classification tree induction?\n\nHistorically, tree induction has been a very popular data mining procedure because it\nis easy to understand, easy to implement, and computationally inexpensive. Research\non tree induction goes back at least to the 1950s and 1960s. Some of the earliest popular\ntree induction systems include CHAID (Chi-squared Automatic Interaction Detection)\n(Kass, 1980) and CART (Classification and Regression Trees) (Breiman, Friedman,\nOlshen, & Stone, 1984), which are still widely used. C4.5 and C5.0 are also very popular\ntree induction algorithms, which have a notable lineage (Quinlan, 1986, 1993). J48 is a\nreimplementation of C4.5 in the Weka package (Witten & Frank, 2000; Hall et al., 2001).\n\n78\n\n|\n\nChapter 3: Introduction to Predictive Modeling: From Correlation to Supervised Segmentation", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00103", "page_num": 103, "segment": "In practice, tree-structured models work remarkably well, though they may not be the\nmost accurate model one can produce from a particular data set. In many cases, especially early in the application of data mining, it is important that models be understood\nand explained easily. This can be useful not just for the data science team but for communicating results to stakeholders not knowledgeable about data mining.\n\nSummary\n\n|\n\n79", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00104", "page_num": 104, "segment": "", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "none", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00105", "page_num": 105, "segment": "CHAPTER 4\nFitting a Model to Data\n\nFundamental concepts: Finding “optimal” model parameters based on data; Choosing\nthe goal for data mining; Objective functions; Loss functions.\n\nExemplary techniques: Linear regression; Logistic regression; Support-vector machines.\n\nAs we have seen, predictive modeling involves finding a model of the target variable in\nterms of other descriptive attributes. In Chapter 3, we constructed a supervised segmentation model by recursively finding informative attributes on ever-more-precise\nsubsets of the set of all instances, or from the geometric perspective, ever-more-precise\nsubregions of the instance space. From the data we produced both the structure of the\nmodel (the particular tree model that resulted from the tree induction) and the numeric\n“parameters” of the model (the probability estimates at the leaf nodes).\n\nAn alternative method for learning a predictive model from a dataset is to start by\nspecifying the structure of the model with certain numeric parameters left unspecified.\nThen the data mining calculates the best parameter values given a particular set of\ntraining data. A very common case is where the structure of the model is a parameterized\nmathematical function or equation of a set of numeric attributes. The attributes used in\nthe model could be chosen based on domain knowledge regarding which attributes\nought to be informative in predicting the target variable, or they could be chosen based\non other data mining techniques, such as the attribute selection procedures introduced\nin Chapter 3. The data miner specifies the form of the model and the attributes; the goal\nof the data mining is to tune the parameters so that the model fits the data as well as\npossible. This general approach is called parameter learning or parametric modeling.\n\nIn certain fields of statistics and econometrics, the bare model with\nunspecified parameters is called “the model.” We will clarify that this\nis the structure of the model, which still needs to have its parameters\nspecified to be useful.\n\n81", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00106", "page_num": 106, "segment": "Many data mining procedures fall within this general framework. We will illustrate with\nsome of the most common, all of which are based on linear models. If you’ve taken a\nstatistics course, you’re probably already familiar with one linear modeling technique:\nlinear regression. We will see the same differences in models that we’ve seen already,\nsuch as the differences in task between classification, class probability estimation, and\nregression. As examples we will present some common techniques used for predicting\n(estimating) unknown numeric values, unknown binary values (such as whether a\ndocument or web page is relevant to a query), as well as likelihoods of events, such as\ndefault on credit, response to an offer, fraud on an account, and so on.\n\nWe also will explicitly discuss something that we skirted in Chapter 3: what exactly do\nwe mean when we say a model fits the data well? This is the crux of the fundamental\nconcept of this chapter---fitting a model to data by finding “optimal” model parameters\n---and is a notion that will resurface in later chapters. Because of its fundamental concepts, this chapter is more mathematically focused than the rest. We will keep the math\nto a minimum, and encourage the less mathematical reader to proceed boldly.\n\nSidebar: Simplifying Assumptions in This Chapter\nThe point of this chapter is to introduce and explain parametric modeling. To keep the\ndiscussion focused, and to avoid excessive footnotes, we’ve made some simplifying assumptions:\n\n• First, for classification and class probability estimation we will consider only binary\nclasses: the models predict events that either take place or do not, such as responding\nto an offer, leaving the company, being defrauded, etc. The methods here can all be\ngeneralized to work with multiple (nonbinary) classes, but the generalization complicates the description unnecessarily.\n\n• Second, because we’re dealing with equations, this chapter assumes all attributes\nare numeric. There are techniques for converting categorical (symbolic) attributes\ninto numerical values for use with these equations.\n\n• Finally, we ignore the need to normalize numeric measurements to a common scale.\nAttributes such as Age and Income have vastly different ranges and they are usually\nnormalized to a common scale to help with model interpretability, as well as other\nthings (to be discussed later).\n\nWe ignore these complications in this chapter. However, dealing with them is ultimately\nimportant and often necessary regardless of the data mining technique.\n\n82\n\n|\n\nChapter 4: Fitting a Model to Data", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00107", "page_num": 107, "segment": "Figure 4-1. A dataset split by a classification tree with four leaf nodes.\n\nClassification via Mathematical Functions\nRecall the instance-space view of tree models from Chapter 3. One such diagram is\nreplicated in Figure 4-1. It shows the space broken up into regions by horizontal and\nvertical decision boundaries that partition the instance space into similar regions. Examples in each region should have similar values for the target variable. In the last\nchapter we saw how the entropy measure gives us a way of measuring homogeneity so\nwe can choose such boundaries.\n\nA main purpose of creating homogeneous regions is so that we can predict the target\nvariable of a new, unseen instance by determining which segment it falls into. For example, in Figure 4-1, if a new customer falls into the lower-left segment, we can conclude\nthat the target value is very likely to be “•”. Similarly, if it falls into the upper-right\nsegment, we can predict its value as “+”.\n\nThe instance-space view is helpful because if we take away the axis-parallel boundaries\n(see Figure 4-2) we can see that there clearly are other, possibly better, ways to partition\n\nClassification via Mathematical Functions\n\n|\n\n83", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00108", "page_num": 108, "segment": "Figure 4-2. The raw data points of Figure 4-1, without decision lines.\n\nthe space. For example, we can separate the instances almost perfectly (by class) if we\nare allowed to introduce a boundary that is still a straight line, but is not perpendicular\nto the axes (Figure 4-3).\n\nFigure 4-3. The dataset of Figure 4-2 with a single linear split.\n\nThis is called a linear classifier and is essentially a weighted sum of the values for the\nvarious attributes, as we will describe next.\n\n84\n\n|\n\nChapter 4: Fitting a Model to Data", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00109", "page_num": 109, "segment": "Linear Discriminant Functions\nOur goal is going to be to fit our model to the data, and to do so it is quite helpful to\nrepresent the model mathematically. You may recall that the equation of a line in two\ndimensions is y = mx + b, where m is the slope of the line and b is the y intercept (the y\nvalue when x = 0). The line in Figure 4-3 can be expressed in this form (with Balance\nin thousands) as:\n\nAge = ( - 1.5) × Balance + 60\n\nWe would classify an instance x as a + if it is above the line, and as a • if it is below the\nline. Rearranging this mathematically leads to the function that is the basis of all the\ntechniques discussed in this chapter. First, for this example form the classification\nsolution is shown in Equation 4-1.\n\nEquation 4-1. Classification function\n\nclass() = { + if 1.0 × Age - 1.5 × Balance + 60 > 0\n\n• if 1.0 × Age - 1.5 × Balance + 60 ≤ 0\n\nThis is called a linear discriminant because it discriminates between the classes, and the\nfunction of the decision boundary is a linear combination---a weighted sum---of the\nattributes. In the two dimensions of our example, the linear combination corresponds\nto a line. In three dimensions, the decision boundary is a plane, and in higher dimensions it is a hyperplane (see Decision lines and hyperplanes in “Visualizing Segmentations” on page 67). For our purposes, the important thing is that we can express the\nmodel as a weighted sum of the attribute values.\n\nThus, this linear model is a different sort of multivariate supervised segmentation. Our\ngoal with supervised segmentation still is to separate the data into regions with different\nvalues of the target variable. The difference is that the method for taking multiple attributes into account is to create a mathematical function of them.\n\nIn “Trees as Sets of Rules” on page 71 we showed how a classification tree corresponds\nto a rule set---a logical classification model of the data. A linear discriminant function\nis a numeric classification model. For example, consider our feature vector x, with the\nindividual component features being xi. A linear model then can be written as follows\nin Equation 4-2.\n\nEquation 4-2. A general linear model\n\nf () = w0 + w1x1 + w2x2 + ⋯\n\nClassification via Mathematical Functions\n\n|\n\n85", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00110", "page_num": 110, "segment": "Figure 4-4. A basic instance space in two dimensions containing points of two classes.\n\nThe concrete example from Equation 4-1 can be written in this form:\n\nf () = 60 + 1.0 × Age - 1.5 × Balance\n\nTo use this model as a linear discriminant, for a given instance represented by a feature\nvector x, we check whether f(x) is positive or negative. As discussed above, in the twodimensional case, this corresponds to seeing whether the instance x falls above or below\nthe line.\n\nLinear functions are one of the workhorses of data science; now we finally come to the\ndata mining. We now have a parameterized model: the weights of the linear function\n(wi) are the parameters.1 The data mining is going to “fit” this parameterized model to\na particular dataset---meaning specifically, to find a good set of weights on the features.\n\nAfter learning, these weights are often loosely interpreted as importance indicators of\nthe features. Roughly, the larger the magnitude of a feature’s weight, the more important\nthat feature is for classifying the target (recalling the assumptions discussed earlier). By\nthe same token, if a feature’s weight is near zero the corresponding feature can usually\nbe ignored or discarded. For now, we are interested in a set of weights that discriminate\nthe training data well and predict as accurately as possible the value of the target variable\nfor cases where we don’t know it.\n\n1. In order that the line need not go through the origin, it is typical to include the weight w0, which is the\n\nintercept.\n\n86\n\n|\n\nChapter 4: Fitting a Model to Data", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00111", "page_num": 111, "segment": "Figure 4-5. Many different possible linear boundaries can separate the two groups of\npoints of Figure 4-4.\n\nUnfortunately, it’s not trivial to choose the “best” line to separate the classes. Let’s consider a simple case, illustrated in Figure 4-4. Here the training data can indeed be separated by class using a linear discriminant. However, as shown in Figure 4-5, there\nactually are many different linear discriminants that can separate the classes perfectly.\nThey have very different slopes and intercepts, and each represents a different model\nof the data. In fact, there are infinitely many lines (models) that classify this training set\nperfectly. Which should we pick?\n\nOptimizing an Objective Function\nThis brings us to one of the most important fundamental ideas in data mining---one\nthat surprisingly is often overlooked even by data scientists themselves: we need to ask,\nwhat should be our goal or objective in choosing the parameters? In our case, this would\nallow us to answer the question: what weights should we choose? Our general procedure\nwill be to define an objective function that represents our goal, and can be calculated for\na particular set of weights and a particular set of data. We will then find the optimal\nvalue for the weights by maximizing or minimizing the objective function. What can\neasily be overlooked is that these weights are “best” only if we believe that the objective\nfunction truly represents what we want to achieve, or practically speaking, is the best\nproxy we can come up with. We will return to this later in the book.\n\nUnfortunately, creating an objective function that matches the true goal of the data\nmining is usually impossible, so data scientists often choose based on faith2 and expe2. And sometimes it can be surprisingly hard for them to admit it.\n\nClassification via Mathematical Functions\n\n|\n\n87", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00112", "page_num": 112, "segment": "rience. Several choices have been shown to be remarkably effective. One of these choices\ncreates the so-called “support vector machine,” about which we will say a few words\nafter presenting a concrete example with a simpler objective function. After that, we\nwill briefly discuss linear models for regression, rather than classification, and end with\none of the most useful data mining techniques of all: logistic regression. Its name is\nsomething of a misnomer---logistic regression doesn’t really do what we call regression,\nwhich is the estimation of a numeric target value. Logistic regression applies linear\nmodels to class probability estimation, which is particularly useful for many applications.\n\nLinear regression, logistic regression, and support vector machines are all very similar\ninstances of our basic fundamental technique: fitting a (linear) model to data. The key\ndifference is that each uses a different objective function.\n\nAn Example of Mining a Linear Discriminant from Data\nTo illustrate linear discriminant functions, we use an adaptation of the Iris dataset taken\nfrom the UCI Dataset Repository (Bache & Lichman, 2013). This is an old and fairly\nsimple dataset representing various types of iris, a genus of flowering plant. The original\ndataset includes three species of irises represented with four attributes, and the data\nmining problem is to classify each instance as belonging to one of the three species based\non the attributes.\n\nFigure 4-6. Two parts of a flower. Width measurements of these are used in the Iris dataset.\n\nFor this illustration we’ll use just two species of irises, Iris Setosa and Iris Versicolor. The\ndataset describes a collection of flowers of these two species, each described with two\nmeasurements: the Petal width and the Sepal width (Figure 4-6). The flower dataset is\n\n88\n\n|\n\nChapter 4: Fitting a Model to Data", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00113", "page_num": 113, "segment": "plotted in Figure 4-7, with these two attributes on the x and y axis, respectively. Each\ninstance is one flower and corresponds to one dot on the graph. The filled dots are of\nthe species Iris Setosa and the circles are instances of the species Iris Versicolor.\n\nFigure 4-7. A dataset and two learned linear classifiers.\n\nTwo different separation lines are shown in the figure, onelogistic regression and the second by another linear method, a support vector machine (which will\nbe described shortly). Note that the data comprise two fairly distinct clumps, with a few\noutliers. Logistic regression separates the two classes completely: all the Iris Versicolor\nexamples are to the left of its line and all the Iris Setosa to the right. The Support vector\nmachine line is almost midway between the clumps, though it misclassifies the starred\npoint at (3, 1). Which separator do you think is better? In Chapter 5, we will get into\ndetails of why these separators are different and why one might be preferable to the\nother. For now it’s enough just to notice that the methods produce different boundaries\nbecause they’re optimizing different functions.\n\nClassification via Mathematical Functions\n\n|\n\n89", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00114", "page_num": 114, "segment": "Linear Discriminant Functions for Scoring and Ranking Instances\nIn many applications, we don’t simply want a yes or no prediction of whether an instance\nbelongs to the class, but we want some notion of which examples are more or less likely\nto belong to the class. For example, which consumers are most likely to respond to this\noffer? Which customers are most likely to leave when their contracts expire? One option\nis to build a model that produces an estimate of class membership probability, as we did\nwith tree induction for class probability estimation in Chapter 3. We can do this with\nlinear models as well, and will treat this in detail below when we introduce logistic\nregression.\n\nIn other applications, we do not need a precise probability estimate. We simply need a\nscore that will rank cases by the likelihood of belonging to one class or the other. For\nexample, for targeted marketing we may have a limited budget for targeting prospective\ncustomers. We would like to have a list of consumers ranked by their predicted likelihood of responding positively to our offer. We don’t necessarily need to be able to estimate the exact probability of response accurately, as long as the list is ranked reasonably\nwell, and the consumers at the top of the list are the ones most likely to respond.\n\nLinear discriminant functions can give us such a ranking for free. Look at Figure 4-4,\nand consider the + instances to be responders and • instances to be nonresponders.\nAssume we are presented with a new instance x for which we do not yet know the class\n(i.e., we have not yet made an offer to x). In which portion of the instance space would\nwe like x to fall in order to expect the highest likelihood of response? Where would we\nbe most certain that x would not respond? Where would we be most uncertain?\n\nMany people suspect that right near the decision boundary we would be most uncertain\nabout a class (and see the discussion below on the “margin”). Far away from the decision\nboundary, on the + side would be where we would expect the highest likelihood of\nresponse. In the equation of the separating boundary, given above in Equation 4-2, f(x)\nwill be zero when x is sitting on the decision boundary (technically, x in that case is one\nof the points of the line or hyperplane). f(x) will be relatively small when x is near the\nboundary. And f(x) will be large (and positive) when x is far from the boundary in the\n+ direction. Thus f(x) itself---the output of the linear discriminant function---gives an\nintuitively satisfying ranking of the instances by their (estimated) likelihood of belonging to the class of interest.\n\n90\n\n|\n\nChapter 4: Fitting a Model to Data", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00115", "page_num": 115, "segment": "Support Vector Machines, Briefly\nIf you’re even on the periphery of the world of data science these days, you eventually\nwill run into the support vector machine or “SVM.” This is a notion that can strike fear\ninto the hearts even of people quite knowledgeable in data science. Not only is the name\nitself opaque, but the method often is imbued with the sort of magic that derives from\nperceived effectiveness without understanding.\n\nFortunately, we now have the concepts necessary to understand support vector machines. In short, support vector machines are linear discriminants. For many business\nusers interacting with data scientists, that will be sufficient. Nevertheless, let’s look at\nSVMs a little more carefully; if we can get through some minor details, the procedure\nfor fitting the linear discriminant is intuitively satisfying.\n\nAs with linear discriminants generally, SVMs classify instances based on a linear function of the features, described above in Equation 4-2.\n\nYou may also hear of nonlinear support vector machines. Oversimplifying slightly, a nonlinear SVM uses different features (that are functions of the original features), so that the linear discriminant with the\nnew features is a nonlinear discriminant with the original features.\n\nSo, as we’ve discussed, the crucial question becomes: what is the objective function that\nis used to fit an SVM to data? For now we will skip the mathematical details in order to\ngain an intuitive understanding. There are two main ideas.\n\nRecall Figure 4-5 showing the infinitude of different possible linear discriminants that\nwould separate the classes, and recall that choosing an objective function for fitting the\ndata amounts to choosing which of these lines is the best. SVMs choose based on a\nsimple, elegant idea: instead of thinking about separating with a line, first fit the fattest\nbar between the classes. This is shown by the parallel dashed lines in Figure 4-8.\n\nThe SVM’s objective function incorporates the idea that a wider bar is better. Then once\nthe widest bar is found, the linear discriminant will be the center line through the bar\n(the solid middle line in Figure 4-8). The distance between the dashed parallel lines is\ncalled the margin around the linear discriminant, and thus the objective is to maximize\nthe margin.\n\nClassification via Mathematical Functions\n\n|\n\n91", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00116", "page_num": 116, "segment": "Figure 4-8. The points of Figure 4-2 and the maximal margin classifier.\n\nThe idea of maximizing the margin is intuitively satisfying for the following reason. The\ntraining dataset is just a sample from some population. In predictive modeling, we are\ninterested in predicting the target for instances that we have not yet seen. These instances\nwill be scattered about. Hopefully they will be distributed similarly to the training data,\nbut they will in fact be different points. In particular, some of the positive examples will\nlikely fall closer to the discriminant boundary than any positive example we have yet\nseen. All else being equal, the same applies to the negative examples. In other words,\nthey may fall in the margin. The margin-maximizing boundary gives the maximal leeway for classifying such points. Specifically, by choosing the SVM decision boundary,\nin order for a new instance to be misclassified, one would have to place it further into\nthe margin than with any other linear discriminant. (Or, of course, completely on the\nwrong side of the margin bar altogether.)\n\nThe second important idea of SVMs lies in how they handle points falling on the wrong\nside of the discrimination boundary. The original example of Figure 4-2 shows a situation in which a single line cannot perfectly separate the data into classes. This is true of\nmost data from complex real-world applications---some data points will inevitably be\n\n92\n\n|\n\nChapter 4: Fitting a Model to Data", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00117", "page_num": 117, "segment": "misclassified by the model. This does not pose a problem for the general notion of linear\ndiscriminants, as their classifications don’t necessarily have to be correct for all points.\nHowever, when fitting the linear function to the data we cannot simply ask which of all\nthe lines that separate the data perfectly should we choose. There may be no such perfect\nseparating line!\n\nOnce again, the support-vector machine’s solution is intuitively satisfying. Skipping the\nmath, the idea is as follows. In the objective function that measures how well a particular\nmodel fits the training points, we will simply penalize a training point for being on the\nwrong side of the decision boundary. In the case where the data indeed are linearly\nseparable, we incur no penalty and simply maximize the margin. If the data are not\nlinearly separable, the best fit is some balance between a fat margin and a low total error\npenalty. The penalty for a misclassified point is proportional to the distance from the\ndecision boundary, so if possible the SVM will make only “small” errors. Technically,\nthis error function is known as hinge loss (see “Sidebar: Loss functions” on page 94 and\nFigure 4-9).\n\nFigure 4-9. Two loss functions illustrated. The x axis shows the distance from the decision boundary. The y axis shows the loss incurred by a negative instance as a function\nof its distance from the decision boundary. (The case of a positive instance is symmetric.) If the negative instance is on the negative side of the boundary, there is no loss. If it\nis on the positive (wrong) side of the boundary, the different loss functions penalize it\ndifferently. (See “Sidebar: Loss functions” on page 94.)\n\nClassification via Mathematical Functions\n\n|\n\n93", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00118", "page_num": 118, "segment": "Regression via Mathematical Functions\nThe previous chapter introduced the fundamental notion of selecting informative variables. We showed that this notion applies to classification, to regression, and to class\nprobability estimation. Here too, this chapter’s basic notion of fitting linear functions\nto data applies to classification, regression, and to class probability estimation. Let’s now\ndiscuss regression briefly.3\n\nSidebar: Loss functions\nThe term “loss” is used across data science as a general term for error penalty. A loss\nfunction determines how much penalty should be assigned to an instance based on the\nerror in the model’s predicted value---in our present context, based on its distance from\nthe separation boundary. Several loss functions are commonly used (two are shown in\nFigure 4-9). In the figure, the horizontal axis is the distance from the separating boundary. Errors have positive distances from the separator in Figure 4-9, while correct classifications have negative distances (the choice is arbitrary in this diagram).\n\nSupport vector machines use hinge loss, so called because the loss graph looks like a\nhinge. Hinge loss incurs no penalty for an example that is not on the wrong side of the\nmargin. The hinge loss only becomes positive when an example is on the wrong side of\nthe boundary and beyond the margin. Loss then increases linearly with the example’s\ndistance from the margin, thereby penalizing points more the farther they are from the\nseparating boundary.\n\nZero-one loss, as its name implies, assigns a loss of zero for a correct decision and one\nfor an incorrect decision.\n\nFor contrast, consider a different sort of loss function. Squared error specifies a loss\nproportional to the square of the distance from the boundary. Squared error loss usually\nis used for numeric value prediction (regression), rather than classification. The squaring of the error has the effect of greatly penalizing predictions that are grossly wrong.\nFor classification, this would apply large penalties to points far over on the “wrong side”\nof the separating boundary. Unfortunately, using squared error for classification also\npenalizes points far on the correct side of the decision boundary. For most business\nproblems, choosing squared-error loss for classification or class-probability estimation\nthus would violate our principle of thinking carefully about whether the loss function\n\n3. There is an immense literature on linear regression for descriptive analysis of data, and we encourage the\nreader to delve into it. In this book, we treat linear regression simply as one of many modeling techniques.\nOur treatment does differ from what you are likely to have learned about regression analysis, because we\nfocus on linear regression for making predictions. Other authors have discussed in detail the differences\nbetween descriptive modeling and predictive modeling (Shmueli, 2010).\n\n94\n\n|\n\nChapter 4: Fitting a Model to Data", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00119", "page_num": 119, "segment": "is aligned with the business goal. (Hinge-like versions of squared error have been created\nbecause of this misalignment [Rosset & Zhu, 2007].)\n\nWe have already discussed most of what we need for linear regression. The linear regression model structure is exactly the same as for the linear discriminant function\nEquation 4-2:\n\nf () = w0 + w1x1 + w2x2 + ⋯\n\nSo, following our general framework for thinking about parametric modeling, we need\nto decide on the objective function we will use to optimize the model’s fit to the data.\nThere are many possibilities. Each different linear regression modeling procedure uses\none particular choice (and the data scientist should think carefully about whether it is\nappropriate for the problem).\n\nThe most common (“standard”) linear regression procedure makes a powerful and\nconvenient choice. Recall that for regression problems the target variable is numeric.\nThe linear function estimates this numeric target value using Equation 4-2, and of course\nthe training data have the actual target value. Therefore, an intuitive notion of the fit of\nthe model is: how far away are the estimated values from the true values on the training\ndata? In other words, how big is the error of the fitted model? Presumably we’d like to\nminimize this error. For a particular training dataset, we could compute this error for\neach individual data point and sum up the results. Then the model that fits the data best\nwould be the model with the minimum sum of errors on the training data. And that is\nexactly what regression procedures do.\n\nYou might notice that we really have not actually specified the objective function, because there are many ways to compute the error between an estimated value and an\nactual value. The method that is most natural is to simply subtract one from the other\n(and take the absolute value). So if I predict 10 and the actual value is 12 or 8, I make\nan error of 2. This is called absolute error, and we could then minimize the sum of\nabsolute errors or equivalently the mean of the absolute errors across the training data.\nThis makes a lot of sense, but it is not what standard linear regression procedures do.\n\nStandard linear regression procedures instead minimize the sum or mean of the squares\nof these errors---which gives the procedure its common name “least squares” regression.\nSo why do so many people use least squares regression without much thought to alternatives? The short answer is convenience. It is the technique we learn in basic statistics\nclasses (and beyond). It is available to us to use in various software packages. Originally,\nthe least squared error function was introduced by the famous 18th century mathematician Carl Friedrich Gauss, and there are certain theoretical arguments for its use (relating to the normal or “Gaussian” distribution). Often, more importantly, it turns out\n\nRegression via Mathematical Functions\n\n|\n\n95", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00120", "page_num": 120, "segment": "that squared error is particularly convenient mathematically.4 This was helpful in the\ndays before computers. From a data science perspective, the convenience extends to\ntheoretical analyses, including a clean decomposition of model error into different\nsources. More pragmatically, analysts often claim to prefer squared error because it\nstrongly penalizes very large errors. Whether the quadratic penalty is actually appropriate is specific to each application. (Why not take the fourth power of the errors, and\npenalize large errors even more strongly?)\n\nImportantly, any choice for the objective function has both advantages and drawbacks.\nFor least squares regression a serious drawback is that it is very sensitive to the data:\nerroneous or otherwise outlying data points can severely skew the resultant linear function. For some business applications, we may not have the resources to spend as much\ntime on manual massaging of the data as we would in other applications. At the extreme,\nfor systems that build and apply models totally automatically, the modeling needs to be\nmuch more robust than when doing a detailed regression analysis “by hand.” Therefore,\nfor the former application we may want to use a more robust modeling procedure (e.g.,\nuse as the objective function absolute error instead of squared error). An important\nthing to remember is that once we see linear regression simply as an instance of fitting\na (linear) model to data, we see that we have to choose the objective function to optimize\n---and we should do so with the ultimate business application in mind.\n\nClass Probability Estimation and Logistic “Regression”\nAs mentioned earlier, for many applications we would like to estimate the probability\nthat a new instance belongs to the class of interest. In many cases, we would like to use\nthe estimated probability in a decision-making context that includes other factors such\nas costs and benefits. For example, predictive modeling from large consumer data is\nused widely in fraud detection across many industries, especially banking, telecommunications, and online commerce. A linear discriminant could be used to identify accounts or transactions as likely to have been defrauded. The director of the fraud control\noperation may want the analysts to focus not simply on the cases most likely to be fraud,\nbut on the cases where the most money is at stake---that is, accounts where the company’s\nmonetary loss is expected to be the highest. For this we need to estimate the actual\nprobability of fraud. (Chapter 7 will discuss in detail the use of expected value to frame\nbusiness problems.)\n\nFortunately, within this same framework for fitting linear models to data, by choosing\na different objective function we can produce a model designed to give accurate estimates of class probability. The most common procedure by which we do this is called\nlogistic regression.\n\n4. Gauss agreed with objections to the arbitrariness of this choice.\n\n96\n\n|\n\nChapter 4: Fitting a Model to Data", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00121", "page_num": 121, "segment": "What exactly is an accurate estimate of class membership probability\nis a subject of debate beyond the scope of this book. Roughly, we would\nlike (i) the probability estimates to be well calibrated, meaning that if\nyou take 100 cases whose class membership probability is estimated to\nbe 0.2, then about 20 of them will actually belong to the class. We would\nalso like (ii) the probability estimates to be discriminative, in that if\npossible they give meaningfully different probability estimates to different examples. The latter condition keeps us from simply giving the\n“base rate” (the overall prevalence in the population) as the prediction for every example. Say 0.5% of accounts overall are fraudulent.\nWithout condition (ii) we could simply predict the same 0.5% probability for each account; those estimates would be well calibrated---but\nnot discriminative at all.\n\nTo understand logistic regression, it is instructive to first consider: exactly what is the\nproblem with simply using our basic linear model (Equation 4-2) to estimate the class\nprobability? As we discussed, an instance being further from the separating boundary\nintuitively ought to lead to a higher probability of being in one class or the other, and\nthe output of the linear function, f(x), gives the distance from the separating boundary.\nHowever, this also shows the problem: f(x) ranges from --∞ to ∞, and a probability\nshould range from zero to one.\n\nSo let’s take a brief stroll down a garden path and ask how else we might cast our distance\nfrom the separator, f(x), in terms of the likelihood of class membership. Is there another\nrepresentation of the likelihood of an event that we use in everyday life? If we could\ncome up with one that ranges from --∞ to ∞, then we might model this other notion of\nlikelihood with our linear equation.\n\nOne very useful notion of the likelihood of an event is the odds. The odds of an event\nis the ratio of the probability of the event occurring to the probability of the event not\noccurring. So, for example, if the event has an 80% probability of occurrence, the odds\nare 80:20 or 4:1. And if the linear function were to give us the odds, a little algebra would\ntell us the probability of occurrence. Let’s look at a more detailed example. Table 4-1\nshows the odds corresponding to various probabilities.\n\nTable 4-1. Probabilities and the corresponding odds.\n\nProbability\n\nCorresponding odds\n\n0.5\n\n0.9\n\n0.999\n\n0.01\n\n0.001\n\n50:50 or 1\n\n90:10 or 9\n\n999:1 or 999\n\n1:99 or 0.0101\n\n1:999 or 0.001001\n\nClass Probability Estimation and Logistic “Regression”\n\n|\n\n97", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00122", "page_num": 122, "segment": "Looking at the range of the odds in Table 4-1, we can see that it still is not quite right as\nan interpretation of the distance from the separating boundary. Again, the distance from\nthe boundary is between --∞ and ∞, but as we can see from the example, the odds range\nfrom 0 to ∞. Nonetheless, we can solve our garden-path problem simply by taking the\nlogarithm of the odds (called the “log-odds”), since for any number in the range 0 to\n∞ its log will be between --∞ to ∞. These are shown in Table 4-2.\n\nTable 4-2. Probabilities, odds, and the corresponding log-odds.\n\nProbability Odds\n\nLog-odds\n\n0.5\n\n0.9\n\n0.999\n\n0.01\n\n0.001\n\n50:50 or 1\n\n90:10 or 9\n\n999:1 or 999\n\n1:99 or 0.0101\n\n0\n\n2.19\n\n6.9\n\n--4.6\n\n1:999 or 0.001001 --6.9\n\nSo if we only cared about modeling some notion of likelihood, rather than the class\nmembership probability specifically, we could model the log-odds with f(x).\n\nLo and behold, our garden path has taken us directly back to our main topic. This is\nexactly a logistic regression model: the same linear function f(x) that we’ve examined\nthroughout the chapter is used as a measure of the log-odds of the “event” of interest.\nMore specifically, f(x) is the model’s estimation of the log-odds that x belongs to the\npositive class. For example, the model might estimate the log-odds that a customer\ndescribed by feature vector x will leave the company when her contract expires. Moreover, with a little algebra we can translate these log-odds into the probability of class\nmembership. This is a little more technical than most of the book, so we’ve relegated it\nto a special “technical details” subsection (next), which also discusses what exactly is the\nobjective function that is optimized to fit a logistic regression to the data. You can read\nthat section in detail or just skim it. The most important points are:\n\n• For probability estimation, logistic regression uses the same linear model as do our\nlinear discriminants for classification and linear regression for estimating numeric\ntarget values.\n\n• The output of the logistic regression model is interpreted as the log-odds of class\n\nmembership.\n\n• These log-odds can be translated directly into the probability of class membership.\nTherefore, logistic regression often is thought of simply as a model for the probability of class membership. You have undoubtedly dealt with logistic regression\nmodels many times without even knowing it. They are used widely to estimate\nquantities like the probability of default on credit, the probability of response to an\n\n98\n\n|\n\nChapter 4: Fitting a Model to Data", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00123", "page_num": 123, "segment": "offer, the probability of fraud on an account, the probability that a document is\nrelevant to a topic, and so on.\n\nAfter the technical details section, we will compare the linear models we’ve developed\nin this chapter with the tree-structured models we developed in Chapter 3.\n\nNote: Logistic regression is a misnomer\nAbove we mentioned that the name logistic regression is a misnomer\nunder the modern use of data science terminology. Recall that the\ndistinction between classification and regression is whether the value\nfor the target variable is categorical or numeric. For logistic regression, the model produces a numeric estimate (the estimation of the\nlog-odds). However, the values of the target variable in the data are\ncategorical. Debating this point is rather academic. What is important to understand is what logistic regression is doing. It is estimating the log-odds or, more loosely, the probability of class membership (a numeric quantity) over a categorical class. So we consider it to\nbe a class probability estimation model and not a regression model,\ndespite its name.\n\n* Logistic Regression: Some Technical Details\n\nSince logistic regression is used so widely, and is not as intuitive as linear regression, let’s examine a few of the technical details. You may skip\nthis subsection without it affecting your understanding of the rest of the\nbook.\n\nSo, technically, what is the bottom line for the logistic regression model? Let’s use p+()\nto represent the model’s estimate of the probability of class membership of a data item\nrepresented by feature vector x.5 Recall that the class + is whatever is the (binary) event\nthat we are modeling: responding to an offer, leaving the company after contract expiration, being defrauded, etc. The estimated probability of the event not occurring is\ntherefore 1 - p+().\n\n5. Often technical treatments use the “hat” notation, p̂, to differentiate the model’s estimate of the probability\nof class membership from the actual probability of class membership. We will not use the hat, but the technically savvy reader should keep that in mind.\n\nClass Probability Estimation and Logistic “Regression”\n\n|\n\n99", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00124", "page_num": 124, "segment": "Equation 4-3. Log-odds linear function\n\nlog ( p+()\n\n1 - p+()) = f () = w0 + w1x1 + w2x2 + ⋯\n\nThus, Equation 4-3 specifies that for a particular data item, described by feature-vector\nx, the log-odds of the class is equal to our linear function, f(x). Since often we actually\nwant the estimated probability of class membership, not the log-odds, we can solve for\np+() in Equation 4-3. This yields the not-so-pretty quantity in Equation 4-4.\n\nEquation 4-4. The logistic function\n\np+() =\n\n1\n1 + e - f ()\n\nAlthough the quantity in Equation 4-4 is not very pretty, by plotting it in a particular\nway we can see that it matches exactly our intuitive notion that we would like there to\nbe relative certainty in the estimations of class membership far from the decision\nboundary, and uncertainty near the decision boundary.\n\nFigure 4-10. Logistic regression’s estimate of class probability as a function of f(x), (i.e.,\nthe distance from the separating boundary). This curve is called a “sigmoid” curve because of its “S” shape, which squeezes the probabilities into their correct range (between\nzero and one).\n\n100\n\n|\n\nChapter 4: Fitting a Model to Data", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00125", "page_num": 125, "segment": "Figure 4-10 plots the estimated probability p+() (vertical axis) as a function of the\ndistance from the decision boundary (horizontal axis). The figure shows that at the\ndecision boundary (at distance x = 0), the probability is 0.5 (a coin toss). The probability\nvaries approximately linearly near to the decision boundary, but then approaches certainty farther away. Part of the “fitting” of the model to the data includes determining\nthe slope of the almost-linear part, and thereby how quickly we are certain of the class\nas we move away from the boundary.\n\nThe other main technical point that we omitted in our main discussion above is: what\nthen is the objective function we use to fit the logistic regression model to the data?\nRecall that the training data have binary values of the target variable. The model can be\napplied to the training data to produce estimates that each of the training data points\nbelongs to the target class. What would we want? Ideally, any positive example x+ would\nhave p+(+) = 1 and any negative example x• would have p+(•) = 0. Unfortunately, with\nreal-world data it is unlikely that we will be able to estimate these probabilities perfectly\n(consider the task of estimating that a consumer described by demographic variables\nwould respond to a particular offer). Nevertheless, we still would like p+(+) to be as\nclose as possible to one and p+(•) to be as close as possible to zero.\n\nThis leads us to the standard objective function for fitting a logistic regression model\nto data. Consider the following function computing the “likelihood” that a particular\nlabeled example belongs to the correct class, given a set of parameters w that produces\nclass probability estimates p+():\n\ng(, ) ={\n\np+()\n1 - p+()\n\nif is a +\n\nif is a •\n\nThe g function gives the model’s estimated probability of seeing x’s actual class given\nx’s features. Now consider summing the g values across all the examples in a labeled\ndataset. And do that for different parameterized models---in our case, different sets of\nweights (w) for the logistic regression. The model (set of weights) that gives the highest\nsum is the model that gives the highest “likelihood” to the data---the “maximum likelihood” model. The maximum likelihood model “on average” gives the highest probabilities to the positive examples and the lowest probabilities to the negative examples.\n\nClass Labels and Probabilities\nOne may be tempted to think that the target variable is a representation of the probability\nof class membership, and the observed values of the target variable in the training data\nsimply report probabilities of p(x) = 1 for cases that are observed to be in the class and\np(x) = 0 for instances that are observed not to be in the class. However, this is not\ngenerally consistent with how logistic regression models are used. Take an application\n\nClass Probability Estimation and Logistic “Regression”\n\n|\n\n101", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00126", "page_num": 126, "segment": "to targeted marketing for example. For a consumer c, our model may estimate the probability of responding to the offer to be p(c responds) = 0.02. In the data, we see that the\nperson indeed does respond. That does not mean that this consumer’s probability of\nresponding actually was 1.0, nor that the model incurred a large error on this example.\nThe consumer’s probability may indeed have been around p(c responds) = 0.02, which\nactually is a high probability of response for many campaigns, and the consumer just\nhappened to respond this time.\n\nA more satisfying way to think about it is that the training data comprise a set of statistical\n“draws” from the underlying probabilities, rather than representing the underlying\nprobabilities themselves. The logistic regression procedure then tries to estimate the\nprobabilities (the probability distribution over the instance space) with a linear-logodds model, based on the observed data on the result of the draws from the distribution.\n\nExample: Logistic Regression versus Tree Induction\nThough classification trees and linear classifiers both use linear decision boundaries,\nthere are two important differences between them:\n\n1. A classification tree uses decision boundaries that are perpendicular to the instancespace axes (see Figure 4-1), whereas the linear classifier can use decision boundaries\nof any direction or orientation (see Figure 4-3). This is a direct consequence of the\nfact that classification trees select a single attribute at a time whereas linear classifiers\nuse a weighted combination of all attributes.\n\n2. A classification tree is a “piecewise” classifier that segments the instance space recursively when it has to, using a divide-and-conquer approach. In principle, a classification tree can cut up the instance space arbitrarily finely into very small regions\n(though we will see reasons to avoid that in Chapter 5). A linear classifier places a\nsingle decision surface through the entire space. It has great freedom in the orientation of the surface, but it is limited to a single division into two segments. This is\na direct consequence of there being a single (linear) equation that uses all of the\nvariables, and must fit the entire data space.\n\nIt is usually not easy to determine in advance which of these characteristics are a better\nmatch to a given dataset. You likely will not know what the best decision boundary will\nlook like. So practically speaking, what are the consequences of these differences?\n\nWhen applied to a business problem, there is a difference in the comprehensibility of\nthe models to stakeholders with different backgrounds. For example, what exactly a\nlogistic regression model is doing can be quite understandable to people with a strong\nbackground in statistics, and very difficult to understand for those who do not. A decision tree, if it is not too large, may be considerably more understandable to someone\nwithout a strong statistics or mathematics background.\n\n102\n\n|\n\nChapter 4: Fitting a Model to Data", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00127", "page_num": 127, "segment": "Why is this important? For many business problems, the data science team does not\nhave the ultimate say in which models are used or implemented. Often there is at least\none manager who must “sign off ” on the use of a model in practice, and in many cases\na set of stakeholders need to be satisfied with the model. For example, to put in place a\nnew model to dispatch technicians to repair problems after customer calls to the telephone company, managers from operations support, customer service, and technical\ndevelopment all need to be satisfied that the new model will do more good than harm\n---since for this problem no model is perfect.\n\nLet’s try logistic regression on a simple but realistic dataset, the Wisconsin Breast Cancer\nDataset. As with the Mushroom dataset from the previous chapter, this is another popular dataset from the the machine learning dataset repository at the University of California at Irvine.\n\nEach example describes characteristics of a cell nuclei image, which has been labeled as\neither benign or malignant (cancerous), based on an expert’s diagnosis of the cells. A\nsample cell image is shown in Figure 4-11.\n\nFigure 4-11. One of the cell images from which the Wisconsin Breast Cancer dataset\nwas derived. (Image courtesy of Nick Street and Bill Wolberg.)\n\nFrom each image 10 fundamental characteristics were extracted, listed in Table 4-3.\n\nExample: Logistic Regression versus Tree Induction\n\n|\n\n103", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00128", "page_num": 128, "segment": "Table 4-3. The attributes of the Wisconsin Breast Cancer dataset.\n\nAttribute name\n\nDescription\n\nRADIUS\n\nTEXTURE\n\nMean of distances from center to points on the perimeter\n\nStandard deviation of grayscale values\n\nPERIMETER\n\nPerimeter of the mass\n\nAREA\n\nSMOOTHNESS\n\nCOMPACTNESS\n\nArea of the mass\n\nLocal variation in radius lengths\n\nComputed as: perimeter2/area -- 1.0\n\nCONCAVITY\n\nSeverity of concave portions of the contour\n\nCONCAVE POINTS\n\nNumber of concave portions of the contour\n\nSYMMETRY\n\nA measure of the nucleii’s symmetry\n\nFRACTAL DIMENSION 'Coastline approximation' -- 1.0\n\nDIAGNOSIS (Target)\n\nDiagnosis of cell sample: malignant or benign\n\nThese were “computed from a digitized image of a fine needle aspirate (FNA) of a breast\nmass. They describe characteristics of the cell nuclei present in the image.” From each\nof these basic characteristics, three values were computed: the mean (_mean), standard\nerror (_SE), and “worst” or largest (mean of the three largest values, _worst). This resulted in 30 measured attributes in the dataset. There are 357 benign images and 212\nmalignant images.\n\nTable 4-4. Linear equation learned by logistic regression on the Wisconsin Breast Cancer dataset (see text and Table 4-3 for a description of the attributes).\n\nAttribute\n\nWeight (learned parameter)\n\nSMOOTHNESS_worst\n\n22.3\n\nCONCAVE_mean\n\nCONCAVE_worst\n\nSYMMETRY_worst\n\nCONCAVITY_worst\n\nCONCAVITY_mean\n\nRADIUS_worst\n\nTEXTURE_worst\n\nAREA_SE\n\nTEXTURE_mean\n\n19.47\n\n11.68\n\n4.99\n\n2.86\n\n2.34\n\n0.25\n\n0.13\n\n0.06\n\n0.03\n\nTEXTURE_SE\n\n--0.29\n\nCOMPACTNESS_mean --7.1\n\nCOMPACTNESS_SE\n\nw0 (intercept)\n\n--27.87\n\n--17.7\n\n104\n\n|\n\nChapter 4: Fitting a Model to Data", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00129", "page_num": 129, "segment": "Table 4-4 shows the linear model learned by logistic regression to predict benign versus\nmalignant for this dataset. Specifically, it shows the nonzero weights ordered from\nhighest to lowest.\n\nThe performance of this model is quite good---it makes only six mistakes on the entire\ndataset, yielding an accuracy of about 98.9% (the percentage of the instances that the\nmodel classifies correctly). For comparison, a classification tree was learned from the\nsame dataset (using Weka’s J48 implementation). The resulting tree is shown in\nFigure 4-13. The tree has 25 nodes altogether, with 13 leaf nodes. Recall that this means\nthat the tree model partitions the instances into 13 segments. The classification tree’s\naccuracy is 99.1%, slightly higher than that of logistic regression.\n\nThe intent of this experiment is only to illustrate the results of two different methods\non a dataset, but it is worth digressing briefly to think about these performance results.\nFirst, an accuracy figure like 98.9% sounds like a very good result. Is it? We see many\nsuch accuracy numbers thrown around in the data mining literature, but evaluating\nclassifiers on real-world problems like cancer diagnosis is often difficult and complex.\nWe discuss evaluation in detail in Chapter 7 and Chapter 8.\n\nSecond, consider the two performance results here: 98.9% versus 99.1%. Since the classification tree gives slightly higher accuracy, we might be tempted to conclude that it’s\nthe better model. Should we believe this? This difference is caused by only a single\nadditional error out of the 569 examples. Furthermore, the accuracy numbers were\nderived by evaluating each model on the same set of examples it was built from. How\nconfident should we be in this evaluation? Chapter 5, Chapter 7, and Chapter 8 discuss\nguidelines and pitfalls of model evaluation.\n\nNonlinear Functions, Support Vector Machines, and\nNeural Networks\nSo far this chapter has focused on the numeric functions most commonly used in data\nscience: linear models. This set of models includes a wide variety of different techniques.\nIn addition, in Figure 4-12 we show that such linear functions can actually represent\nnonlinear models, if we include more complex features in the functions. In this example,\nwe used the Iris dataset from “An Example of Mining a Linear Discriminant from Data” on page 88 and added a squared term to the input data: Sepal width2. The resulting\nmodel is a curved line (a parabola) in the original feature space. Sepal width2. We also\nadded a single data point to the original dataset, an Iris Versicolor example added at\n(4,0.7), shown starred.\n\nNonlinear Functions, Support Vector Machines, and Neural Networks\n\n|\n\n105", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00130", "page_num": 130, "segment": "Figure 4-12. The Iris dataset with a nonlinear feature. In this figure, logistic regression\nand support vector machine---both linear models---are provided an additional feature,\nSepal width2, which allows both the freedom to create more complex, nonlinear models\n(boundaries), as shown.\n\nOur fundamental concept is much more general than just the application of fitting linear\nfunctions. Of course, we could specify arbitrarily complex numeric functions and fit\ntheir parameters to the data. The two most common families of techniques that are\nbased on fitting the parameters of complex, nonlinear functions are nonlinear supportvector machines and neural networks.\n\nOne can think of nonlinear support vector machines as essentially a systematic way of\nimplementing the “trick” we just discussed of adding more complex terms and fitting a\nlinear function to them. Support vector machines have a so-called “kernel function”\nthat maps the original features to some other feature space. Then a linear model is fit\nto this new feature space, just as in our simple example in Figure 4-12. Generalizing\nthis, one could implement a nonlinear support vector machine with a “polynomial kernel,” which essentially means it would consider “higher-order” combinations of the\n\n106\n\n|\n\nChapter 4: Fitting a Model to Data", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00131", "page_num": 131, "segment": "original features (e.g., squared features, products of features). A data scientist would\nbecome familiar with the different alternatives for kernel functions (linear, polynomial,\nand others).\n\nNeural networks also implement complex nonlinear numeric functions, based on the\nfundamental concepts of this chapter. Neural networks offer an intriguing twist. One\ncan think of a neural network as a “stack” of models. On the bottom of the stack are the\noriginal features. From these features are learned a variety of relatively simple models.\nLet’s say these are logistic regressions. Then, each subsequent layer in the stack applies\na simple model (let’s say, another logistic regression) to the outputs of the next layer\ndown. So in a two-layer stack, we would learn a set of logistic regressions from the\noriginal features, and then learn a logistic regression using as features the outputs of the\nfirst set of logistic regressions. We could think of this very roughly as first creating a set\nof “experts” in different facets of the problem (the first-layer models), and then learning\nhow to weight the opinions of these different experts (the second-layer model).6\n\nThe idea of neural networks gets even more intriguing. We might ask: if we are learning\nthose lower-layer logistic regressions---the different experts---what would be the target variable for each? While some practitioners build stacked models where the lowerlayer experts are built to represent specific things using specific target variables (e.g.,\nPerlich et al., 2013), more generally with neural networks target labels for training are\nprovided only for the final layer (the actual target variable). So how are the lower-layer\nlogistic regressions trained? We can understand by returning to the fundamental concept of this chapter. The stack of models can be represented by one big parameterized\nnumeric function. The paramenters now are the coefficients of all the models, taken\ntogether. So once we have decided on an objective function representing what we want\nto optimize (e.g., the fit to the training data, based on some fitting function), we can\nthen apply an optimization procedure to find the best parameters to this very complex\nnumeric function. When we’re done, we have the parameters to all the models, and\nthereby have learned the “best” set of lower-level experts and also the best way to combine them, all simultaneously.\n\nNote: Neural networks are useful for many tasks\nThis section describes neural networks for classification and regression. The field of neural networks is broad and deep, with a long\nhistory. Neural networks have found wide application throughout\ndata mining. They are commonly used for many other tasks mentioned in Chapter 2, such as clustering, time series analysis, profiling, and so on.\n\n6. Compare this with the notion of ensemble methods described in Chapter 12.\n\nNonlinear Functions, Support Vector Machines, and Neural Networks\n\n|\n\n107", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00132", "page_num": 132, "segment": "So, given how cool that sounds, why wouldn’t we want to do that all the time? The\ntradeoff is that as we increase the amount of flexibility we have to fit the data, we increase\nthe chance that we fit the data too well. The model can fit details of its particular training\nset rather than finding patterns or models that apply more generally. Specifically, we\nreally want models that apply to other data drawn from the same population or application. This concern is not specific to neural networks, but is very general. It is one of\nthe most important concepts in data science---and it is the subject of the next chapter.\n\nSummary\nThis chapter introduced a second type of predictive modeling technique called function\nfitting or parametric modeling. In this case the model is a partially specified equation:\na numeric function of the data attributes, with some unspecified numeric parameters.\nThe task of the data mining procedure is to “fit” the model to the data by finding the\nbest set of parameters, in some sense of “best.”\n\nThere are many varieties of function fitting techniques, but most use the same linear\nmodel structure: a simple weighted sum of the attribute values. The parameters to be\nfit by the data mining are the weights on the attributes. Linear modeling techniques\ninclude linear discriminants such as support-vector machines, logistic regression, and\ntraditional linear regression. Conceptually the key difference between these techniques\nis their answer to a key issue, What exactly do we mean by best fitting the data? The\ngoodness of fit is described by an “objective function,” and each technique uses a different function. The resulting techniques may be quite different.\n\nWe now have seen two very different sorts of data modeling, tree induction and function\nfitting, and have compared them (in “Example: Logistic Regression versus Tree Induction” on page 102). We have also introduced two criteria by which models can be evaluated: the predictive performance of a model and its intelligibility. It is often advantageous to build different sorts of models from a dataset to gain insight.\n\nThis chapter focused on the fundamental concept of optimizing a model’s fit to data.\nHowever, doing this leads to the most important fundamental problem with data mining\n---if you look hard enough, you will find structure in a dataset, even if it’s just there by\nchance. This tendency is known as overfitting. Recognizing and avoiding overfitting is\nan important general topic in data science; and we devote the entire next chapter to it.\n\n108\n\n|\n\nChapter 4: Fitting a Model to Data", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00133", "page_num": 133, "segment": "Figure 4-13. Decision tree learned from the Wisconsin Breast Cancer dataset.\n\nSummary\n\n|\n\n109", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00134", "page_num": 134, "segment": "", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "none", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00135", "page_num": 135, "segment": "CHAPTER 5\nOverfitting and Its Avoidance\n\nFundamental concepts: Generalization; Fitting and overfitting; Complexity control.\n\nExemplary techniques: Cross-validation; Attribute selection; Tree pruning; Regularization.\n\nOne of the most important fundamental notions of data science is that of overfitting\nand generalization. If we allow ourselves enough flexibility in searching for patterns in\na particular dataset, we will find patterns. Unfortunately, these “patterns” may be just\nchance occurrences in the data. As discussed previously, we are interested in patterns\nthat generalize---that predict well for instances that we have not yet observed. Finding\nchance occurrences in data that look like interesting patterns, but which do not generalize, is called overfitting the data.\n\nGeneralization\nConsider the following (extreme) example. You’re a manager at MegaTelCo, responsible\nfor reducing customer churn. I run a data mining consulting group. You give my data\nscience team a set of historical data on customers who have stayed with the company\nand customers who have departed within six months of contract expiration. My job is\nto build a model to distinguish customers who are likely to churn based on some features, as we’ve discussed previously. I mine the data and build a model. I give you back\nthe code for the model, to implement in your company’s churn-reduction system.\n\nOf course you are interested in whether my model is any good, so you ask your technical\nteam to check the performance of the model on the historical data. You understand that\nhistorical performance is no guarantee of future success, but your experience tells you\nthat churn patterns remain relatively stable, except for major changes to the industry\n(such as the introduction of the iPhone), and you know of no such major changes since\nthese data were collected. So, the tech team runs the historical dataset through the model.\nYour technical lead reports back that this data science team is amazing. The model is\n\n111", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00136", "page_num": 136, "segment": "100% accurate. It does not make a single mistake, identifying correctly all the churners\nas well as the nonchurners.\n\nYou’re experienced enough not to be comfortable with that answer. You’ve had experts\nlooking at churn behavior for a long time, and if there really were 100% accurate indicators, you figure you would be doing better than you currently are. Maybe this is just\na lucky fluke?\n\nIt was not a lucky fluke. Our data science team can do that every time. Here is how we\nbuilt the model. We stored the feature vector for each customer who has churned in a\ndatabase table. Let’s call that Tc. Then, in use, when the model is presented with a customer to determine the likelihood of churning, it takes the customer’s feature vector,\nlooks her up in Tc, and reports “100% likelihood of churning” if she is in Tc and “0%\nlikelihood of churning” if she is not in Tc. So, when the tech team applies our model to\nthe historical dataset, the model predicts perfectly.1\n\nCall this simple approach a table model. It memorizes the training data and performs\nno generalization. What is the problem with this? Consider how we’ll use the model in\npractice. When a previously unseen customer’s contract is about to expire, we’ll want to\napply the model. Of course, this customer was not part of the historical dataset, so the\nlookup will fail since there will be no exact match, and the model will predict “0%\nlikelihood of churning” for this customer. In fact, the model will predict this for every\ncustomer (not in the training data). A model that looked perfect would be completely\nuseless in practice!\n\nThis may seem like an absurd scenario. In reality, no one would throw raw customer\ndata into a table and claim it was a “predictive model” of anything. But it is important\nto think about why this is a bad idea, because it fails for the same reason other, more\nrealistic data mining efforts may fail. It is an extreme example of two related fundamental\nconcepts of data science: generalization and overfitting. Generalization is the property\nof a model or modeling process, whereby the model applies to data that were not used\nto build the model. In this example, the model does not generalize at all beyond the data\nthat were used to build it. It is tailored, or “fit,” perfectly to the training data. In fact, it\nis “overfit.”\n\nThis is the important point. Every dataset is a finite sample of a population---in this case,\nthe population of phone customers. We want models to apply not just to the exact\ntraining set but to the general population from which the training data came. We may\nworry that the training data were not representative of the true population, but that is\nnot the problem here. The data were representative, but the data mining did not create\na model that generalized beyond the training data.\n\n1. Technically, this is not necessarily true: there may be two customers with the same feature vector description,\none of whom churns and the other does not. We can ignore that possibility for the sake of this example. For\nexample, we can assume that the unique customer ID is one of the features.\n\n112\n\n|\n\nChapter 5: Overfitting and Its Avoidance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00137", "page_num": 137, "segment": "Overfitting\nOverfitting is the tendency of data mining procedures to tailor models to the training\ndata, at the expense of generalization to previously unseen data points. The example\nfrom the previous section was contrived; the data mining built a model using pure\nmemorization, the most extreme overfitting procedure possible. However, all data mining procedures have the tendency to overfit to some extent---some more than others.\nThe idea is that if we look hard enough we will find patterns in a dataset. As the Nobel\nLaureate Ronald Coase said, “If you torture the data long enough, it will confess.”\n\nUnfortunately, the problem is insidious. The answer is not to use a data mining procedure that doesn’t overfit because all of them do. Nor is the answer to simply use models\nthat produce less overfitting, because there is a fundamental trade-off between model\ncomplexity and the possibility of overfitting. Sometimes we may simply want more\ncomplex models, because they will better capture the real complexities of the application\nand thereby be more accurate. There is no single choice or procedure that will eliminate\noverfitting. The best strategy is to recognize overfitting and to manage complexity in a\nprincipled way.\n\nThe rest of this chapter discusses overfitting in more detail, methods for assessing the\ndegree of overfitting at modeling time, as well as methods for avoiding overfitting as\nmuch as possible.\n\nOverfitting Examined\nBefore discussing what to do about overfitting, we need to know how to recognize it.\n\nHoldout Data and Fitting Graphs\nLet’s now introduce a simple analytic tool: the fitting graph. A fitting graph shows the\naccuracy of a model as a function of complexity. To examine overfitting, we need to\nintroduce a concept that is fundamental to evaluation in data science: holdout data.\n\nThe problem in the prior section was that the model was evaluated on the training data\n---exactly the same data that were used to build it. Evaluation on training data provides\nno assessment of how well the model generalizes to unseen cases. What we need to do\nis to “hold out” some data for which we know the value of the target variable, but which\nwill not be used to build the model. These are not the actual use data, for which we\nultimately would like to predict the value of the target variable. Instead, creating holdout\ndata is like creating a “lab test” of generalization performance. We will simulate the use\nscenario on these holdout data: we will hide from the model (and possibly the modelers)\nthe actual values for the target on the holdout data. The model will predict the values.\nThen we estimate the generalization performance by comparing the predicted values\nwith the hidden true values. There is likely to be a difference between the model’s acOverfitting\n\n|\n\n113", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00138", "page_num": 138, "segment": "Figure 5-1. A typical fitting graph. Each point on a curve represents an accuracy estimation of a model with a specified complexity (as indicated on the horizontal axis). Accuracy estimates on training data and testing data vary differently based on how complex we allow a model to be. When the model is not allowed to be complex enough, it is\nnot very accurate. As the models get too complex, they look very accurate on the training data, but in fact are overfitting---the training accuracy diverges from the holdout\n(generalization) accuracy.\n\ncuracy on the training set (sometimes called the “in-sample” accuracy) and the model’s\ngeneralization accuracy, as estimated on the holdout data. Thus, when the holdout data\nare used in this manner, they often are called the “test set.”\n\nThe accuracy of a model depends on how complex we allow it to be. A model can be\ncomplex in different ways, as we will discuss in this chapter. First let us use this distinction between training data and holdout data to define the fitting graph more precisely.\nThe fitting graph (see Figure 5-1) shows the difference between a modeling procedure’s\naccuracy on the training data and the accuracy on holdout data as model complexity\nchanges. Generally, there will be more overfitting as one allows the model to be more\ncomplex. (Technically, the chance of overfitting increases as one allows the modeling\nprocedure more flexibility in the models it can produce; we will ignore that distinction\nin this book).\n\nFigure 5-2 shows a fitting graph for the customer churn “table model” described earlier.\nSince this was an extreme example the fitting graph will be peculiar. Again, the x axis\nmeasures the complexity of the model; in this case, the number of rows allowed in the\ntable. The y axis measures the error. As we allow the table to increase in size, we can\nmemorize more and more of the training set, and with each new row the training set\nerror decreases. Eventually the table is large enough to contain the entire training set\n(marked N on the x axis) and the error goes to zero and remains there. However, the\ntesting (holdout) set error starts at some value (let’s call it b) and never decreases, because\nthere is never an overlap between the training and holdout sets. The large gap between\nthe two is a strong indication of memorization.\n\n114\n\n|\n\nChapter 5: Overfitting and Its Avoidance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00139", "page_num": 139, "segment": "Figure 5-2. A fitting graph for the customer churn (table) model.\n\nNote: Base rate\nWhat would b be? Since the table model always predicts no churn for\nevery new case with which it is presented, it will get every no churn\ncase right and every churn case wrong. Thus the error rate will be the\npercentage of churn cases in the population. This is known as the base\nrate, and a classifier that always selects the majority class is called a\nbase rate classifier.\n\nA corresponding baseline for a regression model is a simple model that\nalways predicts the mean or median value of the target variable.\n\nYou will occasionally hear reference to “base rate performance,” and\nthis is what it refers to. We will revisit the base rate again in the next\nchapter.\n\nWe’ve discussed in the previous chapters two very different sorts of modeling procedures: recursive partitioning of the data as done for tree induction, and fitting a numeric\nmodel by finding an optimal set of parameters, for example the weights in a linear model.\nWe can now examine overfitting for each of these procedures.\n\nOverfitting Examined\n\n|\n\n115", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00140", "page_num": 140, "segment": "Overfitting in Tree Induction\nRecall how we built tree-structured models for classification. We applied a fundamental\nability to find important, predictive individual attributes repeatedly (recursively) to\nsmaller and smaller data subsets. Let’s assume for illustration that the dataset does not\nhave two instances with exactly the same feature vector but different target values. If we\ncontinue to split the data, eventually the subsets will be pure---all instances in any chosen\nsubset will have the same value for the target variable. These will be the leaves of our\ntree. There might be multiple instances at a leaf, all with the same value for the target\nvariable. If we have to, we can keep splitting on attributes, and subdividing our data\nuntil we’re left with a single instance at each leaf node, which is pure by definition.\n\nWhat have we just done? We’ve essentially built a version of the lookup table discussed\nin the prior section as an extreme example, of overfitting! Any training instance given\nto the tree for classification will make its way down, eventually landing at the appropriate\nleaf---the leaf corresponding to the subset of the data that includes this particular training instance. What will be the accuracy of this tree on the training set? It will be perfectly\naccurate, predicting correctly the class for every training instance.\n\nWill it generalize? Possibly. This tree should be slightly better than the lookup table\nbecause every previously unseen instance will arrive at some classification, rather than\njust failing to match; the tree will give a nontrivial classification even for instances it has\nnot seen before. Therefore, it is useful to examine empirically how well the accuracy on\nthe training data tends to correspond to the accuracy on test data.\n\nA procedure that grows trees until the leaves are pure tends to overfit. Tree-structured\nmodels are very flexible in what they can represent. Indeed, they can represent any\nfunction of the features, and if allowed to grow without bound they can fit it to arbitrary\nprecision. But the trees may need to be huge in order to do so. The complexity of the\ntree lies in the number of nodes.\n\nFigure 5-3 shows a typical fitting graph for tree induction. Here we artificially limit the\nmaximum size of each tree, as measured by the number of nodes it’s allowed to have,\nindicated on the x axis (which is log scale for convenience). For each tree size we create\na new tree from scratch, using the training data. We measure two values: its accuracy\non the training set and its accuracy on the holdout (test) set. If the data subsets at the\nleaves are not pure, we will predict the target variable based on some average over the\ntarget values in the subset, as we discussed in Chapter 3.\n\n116\n\n|\n\nChapter 5: Overfitting and Its Avoidance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00141", "page_num": 141, "segment": "Figure 5-3. A typical fitting graph for tree induction.\n\nBeginning at the left, the tree is very small and has poor performance. As it is allowed\nmore and more nodes it improves rapidly, and both training-set accuracy and holdoutset accuracy improve. Also we see that training-set accuracy always is at least a little\nbetter than holdout-set accuracy, since we did get to look at the training data when\nbuilding the model. But at some point the tree starts to overfit: it acquires details of the\ntraining set that are not characteristic of the population in general, as represented by\nthe holdout set. In this example overfitting starts to happen at around x = 100 nodes,\ndenoted the “sweet spot” in the graph. As the trees are allowed to get larger, the trainingset accuracy continues to increase---in fact, it is capable of memorizing the entire training set if we let it, leading to an accuracy of 1.0 (not shown). But the holdout accuracy\ndeclines as the tree grows past its “sweet spot”; the data subsets at the leaves get smaller\nand smaller, and the model generalizes from fewer and fewer data. Such inferences will\nbe increasingly error-prone and the performance on the holdout data suffers.\n\nOverfitting Examined\n\n|\n\n117", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00142", "page_num": 142, "segment": "In summary, from this fitting graph we may infer that overfitting on this dataset starts\nto dominate at around 100 nodes, so we should restrict tree size to this value.2 This\nrepresents the best trade-off between the extremes of (i) not splitting the data at all and\nsimply using the average target value in the entire dataset, and (ii) building a complete\ntree out until the leaves are pure.\n\nUnfortunately, no one has come up with a procedure to determine this exact sweet spot\ntheoretically, so we have to rely on empirically based techniques. Before discussing\nthose, let’s examine overfitting in our second sort of modeling procedure.\n\nOverfitting in Mathematical Functions\nThere are different ways to allow more or less complexity in mathematical functions.\nThere are entire books on the topic. This section discusses one very important way, and\n“* Avoiding Overfitting for Parameter Optimization” on page 136 discuss a second one.\nWe urge you to at least skim that advanced (starred) section because it introduces concepts and vocabulary in common use by data scientists these days, that can make a nondata scientist’s head swim. Here we will summarize and give you enough to understand\nsuch discussions at a conceptual level.3 But first, let’s discuss a much more straightforward way in which functions can become too complex.\n\nOne way mathematical functions can become more complex is by adding more variables\n(more attributes). For example, say that we have a linear model as described in\nEquation 4-2:\n\nf () = w0 + w1x1 + w2x2 + w3x3\n\nAs we add more xi’s, the function becomes more and more complicated. Each xi has a\ncorresponding wi, which is a learned parameter of the model.\n\nModelers sometimes even change the function from being truly linear in the original\nattributes by adding new attributes that are nonlinear versions of original attributes.\nFor example, I might add a fourth attribute x4=x2\n1. Also, we might expect that the ratio\nof x2 and x3 is important, so we add a new attribute x5 = x2/x3. Now we’re trying to find\nthe parameters (weights) of:\n\nf () = w0 + w1x1 + w2x2 + w3x3 + w4x4 + w5x5\n\n2. Note that 100 nodes is not some special universal value. It is specific to this particular dataset. If we changed\nthe data significantly, or even just used a different tree-building algorithm, we’d probably want to make\nanother fitting graph to find the new sweet spot.\n\n3. We also will have enough of a conceptual toolkit by that point to understand support vector machines a little\n\nbetter---as being almost equivalent to logistic regression with complexity (overfitting) control.\n\n118\n\n|\n\nChapter 5: Overfitting and Its Avoidance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00143", "page_num": 143, "segment": "Either way, a dataset may end up with a very large number of attributes, and using all\nof them gives the modeling procedure much leeway to fit the training set. You might\nrecall from geometry that in two dimensions you can fit a line to any two points and in\nthree dimensions you can fit a plane to any three points. This concept generalizes: as\nyou increase the dimensionality, you can perfectly fit larger and larger sets of arbitrary\npoints. And even if you cannot fit the dataset perfectly, you can fit it better and better\nwith more dimensions---that is, with more attributes.\n\nOften, modelers carefully prune the attributes in order to avoid overfitting. Modelers\nwill use a sort of holdout technique introduced above to assess the information in the\nindividual attributes. Careful manual attribute selection is a wise practice in cases where\nconsiderable human effort can be spent on modeling, and where there are reasonably\nfew attributes. In many modern applications, where large numbers of models are built\nautomatically, and/or where there are very large sets of attributes, manual selection may\nnot be feasible. For example, companies that do data science-driven targeting of online\ndisplay advertisements can build thousands of models each week, sometimes with millions of possible features. In such cases there is no choice but to employ automatic feature\nselection (or to ignore feature selection all together).\n\nExample: Overfitting Linear Functions\nIn “An Example of Mining a Linear Discriminant from Data” on page 88, we introduced\na simple dataset called iris, comprising data describing two species of Iris flowers. Now\nlet’s revisit that to see the effects of overfitting in action.\n\nFigure 5-4 shows the original Iris dataset graphed with its two attributes, Petal width\nand Sepal width. Recall that each instance is one flower and corresponds to one dot on\nthe graph. The filled dots are of the species Iris Setosa and the circles are instances of\nthe species Iris Versicolor. Note several things here: first, the two classes of iris are very\ndistinct and separable. In fact, there is a wide gap between the two “clumps” of instances.\nBoth logistic regression and support vector machines place separating boundaries\n(lines) in the middle. In fact, the two separating lines are so similiar that they’re indistinguishable in the graph.\n\nExample: Overfitting Linear Functions\n\n|\n\n119", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00144", "page_num": 144, "segment": "Figure 5-4. The original Iris dataset and the models (boundary lines) that two linear\nmethods learn. In this case, both linear regression and a support vector machine learn\nthe same model (the decision boundary, shown as a line).\n\nIn Figure 5-5, we’ve added a single new example: an Iris Setosa point at (3,1). Realistically,\nwe might consider this example to be an outlier or an error since it’s much closer to the\nVersicolor examples than the Setosas. Notice how the logistic regression line moves in\nresponse: it separates the two groups perfectly, while the SVM line barely moves at all.\n\n120\n\n|\n\nChapter 5: Overfitting and Its Avoidance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00145", "page_num": 145, "segment": "Figure 5-5. The Iris dataset from Figure 5-4 with a single new Iris Setosa example added (shown by star). Note how logistic regression has changed its model considerably.\n\nIn Figure 5-6 we’ve added a different outlier at (4,0.7), this time a Versicolor example\ndown in the Setosa region. Again, the support vector machine line moves very little in\nresponse, but the logistic regression line moves considerably.\n\nExample: Overfitting Linear Functions\n\n|\n\n121", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00146", "page_num": 146, "segment": "Figure 5-6. The Iris dataset from Figure 5-4 with a single new Iris Versicolor example\nadded (shown by star). Note how logistic regression again changes its model considerably.\n\nIn Figure 5-5 and Figure 5-6, Logistic regression appears to be overfitting. Arguably,\nthe examples introduced in each are outliers that should not have a strong influence on\nthe model---they contribute little to the “mass” of the species examples. Yet in the case\nof logistic regression they clearly do. If a linear boundary exists, logistic regression will\nfind it,4 even if this means moving the boundary to accommodate outliers. The SVM\ntends to be less sensitive to individual examples. The SVM training procedure incorporates complexity control, which we will describe technically later.\n\n4. Technically, only some logistic regression algorithms are guaranteed to find it. Some do not have this guarantee. However, this fact is not germane to the overfitting point we’re making here.\n\n122\n\n|\n\nChapter 5: Overfitting and Its Avoidance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00147", "page_num": 147, "segment": "Figure 5-7. The Iris dataset from Figure 5-6 with its Iris Versicolor example added\n(shown by star). In this figure, both logistic regression and support vector machine are\ngiven an additional feature, Sepal width2, which allows both the freedom to create\nmore complex, nonlinear models (boundaries).\n\nAs we said earlier, another way mathematical functions can become more complex is\nby adding more variables. In Figure 5-7, we have done just this: we used the same dataset\nas in Figure 5-6 but we added a single extra attribute, the square of the Sepal width.\nProviding this attribute gives each method more flexibility in fitting the data because it\nmay assign weights to the squared term. Geometrically, this means the separating\nboundary can be not just a line but a parabola. This additional freedom allows both\nmethods to create curved surfaces that can fit the regions more closely. In cases where\ncurved surfaces may be necessary, this freedom may be necessary, but it also gives the\nmethods far more opportunity to overfit. Note however that the SVM, even though its\nboundary now is curved, the training procedure still has opted for the larger margin\naround the boundary, rather than the perfect separation of the positive different classes.\n\nExample: Overfitting Linear Functions\n\n|\n\n123", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00148", "page_num": 148, "segment": "* Example: Why Is Overfitting Bad?\n\nAt the beginning of the chapter, we said that a model that only memorizes is useless because it always overfits and is incapable of generalizing. But technically this only demonstrates that overfitting hinders us\nfrom improving a model after a certain complexity. It does not explain\nwhy overfitting often causes models to become worse, as Figure 5-3\nshows. This section goes into a detailed example showing how this happens and why. It may be skipped without loss of continuity.\n\nWhy does performance degrade? The short answer is that as a model gets more complex\nit is allowed to pick up harmful spurious correlations. These correlations are idiosyncracies of the specific training set used and do not represent characteristics of the population in general. The harm occurs when these spurious correlations produce incorrect generalizations in the model. This is what causes performance to decline when\noverfitting occurs. In this section we go through an example in detail to show how this\ncan happen.\n\nTable 5-1. A small set of training examples\n\nInstance\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\nx\n\np\n\np\n\np\n\nq\n\np\n\nq\n\nq\n\nq\n\ny\n\nr\n\nr\n\nr\n\ns\n\ns\n\nr\n\ns\n\nr\n\nClass\nc1\nc1\nc1\nc1\nc2\nc2\nc2\nc2\n\nConsider a simple two-class problem with classes c1 and c2 and attributes x and y. We\nhave a population of examples, evenly balanced between the classes. Attribute x has two\nvalues, p and q, and y has two values, r and s. In the general population, x = p occurs\n75% of the time in class c1 examples and in 25% of the c2 examples, so x provides some\nprediction of the class. By design, y has no predictive power at all, and indeed we see\nthat in the data sample both of y’s values occur in both classes equally. In short, the\ninstances in this domain are difficult to separate, with only x providing some predictive\npower. The best we can achieve is 75% accuracy by looking at x.\n\n124\n\n|\n\nChapter 5: Overfitting and Its Avoidance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00149", "page_num": 149, "segment": "Figure 5-8. Classification trees for the overfitting example. (a) The optimal tree has only three nodes. (b) An overfit tree, which fits the training data better, has worse generalization accuracy because the extraneous structure makes suboptimal predictions.\n\nTable 5-1 shows a very small training set of examples from this domain. What would a\nclassification tree learner do with these? We won’t go into the entropy calculations, but\nattribute x provides some leverage so a tree learner would split on it and create the tree\nshown in Figure 5-8. Since x provides the only leverage, this should be the optimal tree.\nIts error rate is 25%---equal to the theoretical minimum error rate.\n\nHowever, observe from Table 5-1 that in this particular dataset y’s values of r and s are\nnot evenly split between the classes, so y does seem to provide some predictiveness.\nSpecifically, once we choose x=p (instances 1-4), we see that y=r predicts c1 perfectly\n(instances 1-3). Hence, from this dataset, tree induction would achieve information gain\nby splitting on y’s values and create two new leaf nodes, shown in Figure 5-8.\n\nBased on our training set, the tree in (b) performs well, better than (a). It classifies seven\nof the eight training examples correctly, whereas the tree in (a) classifies only six out of\neight correct. But this is due to the fact that y=r purely by chance correlates with class\nc1 in this data sample; in the general population there is no such correlation. We have\nbeen misled, and the extra branch in (b) is not simply extraneous, it is harmful. Recall\nthat we defined the general population to have x=p occuring in 75% of the class c1\nexamples and 25% of the c2 examples. But the spurious y=s branch predicts c2, which is\nwrong in the general population. In fact, we expect this spurious branch to contribute\none in eight errors made by the tree. Overall, the (b) tree will have a total expected error\nrate of 30%, while (a) will have an error rate of 25%.\n\nWe conclude this example by emphasizing several points. First, this phenomenon is not\nparticular to classification trees. Trees are convenient for this example because it is easy\n\n* Example: Why Is Overfitting Bad?\n\n|\n\n125", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00150", "page_num": 150, "segment": "to point to a portion of a tree and declare it to be spurious, but all model types are\nsusceptible to overfitting effects. Second, this phenomenon is not due to the training\ndata in Table 5-1 being atypical or biased. Every dataset is a finite sample of a larger\npopulation, and every sample will have variations even when there is no bias in the\nsampling. Finally, as we have said before, there is no general analytic way to determine\nin advance whether a model has overfit or not. In this example we defined what the\npopulation looked like so we could declare that a given model had overfit. In practice,\nyou will not have such knowledge and it will be necessary to use a holdout set to detect\noverfitting.\n\nFrom Holdout Evaluation to Cross-Validation\nLater we will present a general technique in broad use to try to avoid overfitting, which\napplies to attribute selection as well as tree complexity, and beyond. But first, we need\nto discuss holdout evaluation in more detail. Before we can work to avoid overfitting,\nwe need to be able to avoid being fooled by overfitting. At the beginning of this chapter\nwe introduced the idea that in order to have a fair evaluation of the generalization\nperformance of a model, we should estimate its accuracy on holdout data---data not\nused in building the model, but for which we do know the actual value of the target\nvariable. Holdout testing is similar to other sorts of evaluation in a “laboratory” setting.\n\nWhile a holdout set will indeed give us an estimate of generalization performance, it is\njust a single estimate. Should we have any confidence in a single estimate of model\naccuracy? It might have just been a single particularly lucky (or unlucky) choice of\ntraining and test data. We will not go into the details of computing confidence intervals\non such quantities, but it is important to discuss a general testing procedure that will\nend up helping in several ways.\n\nCross-validation is a more sophisticated holdout training and testing procedure. We\nwould like not only a simple estimate of the generalization performance, but also some\nstatistics on the estimated performance, such as the mean and variance, so that we can\nunderstand how the performance is expected to vary across datasets. This variance is\ncritical for assessing confidence in the performance estimate, as you might have learned\nin a statistics class.\n\nCross-validation also makes better use of a limited dataset. Unlike splitting the data into\none training and one holdout set, cross-validation computes its estimates over all the\ndata by performing multiple splits and systematically swapping out samples for testing.\n\n126\n\n|\n\nChapter 5: Overfitting and Its Avoidance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00151", "page_num": 151, "segment": "Sidebar: Building a modeling “laboratory”\nBuilding the infrastructure for a modeling lab may be costly and time consuming, but\nafter this investment many aspects of model performance can be evaluated quickly in a\ncontrolled environment. However, holdout testing cannot capture all the complexities\nof the real world where the model will be used. Data scientists should work to understand\nthe actual use scenario so as to make the lab setting as much like it as possible, to avoid\nsurprises when the two do not match. For example, consider a company that wants to\nuse data science to improve its targeting of costly personally targeted advertisements.\nAs a campaign progresses, more and more data arrive on people who make purchases\nafter having seen the ad versus those who do not. These data can be used to build models\nto discriminate between those to whom we should and should not advertise. Examples\ncan be put aside to evaluate how accurate the models are in predicting whether consumers will respond to the ad.\n\nWhen the resultant models are put into production, targeting consumers “in the wild,”\nthe company is surprised that the models do not work as well as they did in the lab. Why\nnot? There could be many reasons, but notice one in particular: the training and holdout\ndata do not really match the data to which the model will be applied in the field. Specifically, the training data all are consumers who had been targeted in the campaign.\nOtherwise, we would not know the value of the target variable (whether they responded).\nEven before the data mining, the company did not simply target randomly; they had\nsome criteria for targeting people they believed would respond. In the field, the model\nis applied to consumers more broadly---not just to consumers who meet these criteria.\nThe fact that the training and deployment populations are different is a likely source of\nperformance degradation.\n\nThis phenomenon is not limited to advertisement targeting. Consider credit scoring,\nwhere we would like to build models to predict the likelihood of a consumer defaulting\non credit. Again, the data we have on write-offs versus non-write-offs are based on those\nto whom we previously extended credit, who presumably were those thought to be low\nrisk.\n\nIn both of these cases, think about what you might do as a business to gather a more\nappropriate dataset from which to build predictive models. Remember to apply the\nfundamental concept introduced in Chapter 1: think of data as an asset in which you\nmay want to invest.\n\nCross-validation begins by splitting a labeled dataset into k partitions called folds. Typically, k will be five or ten. The top pane of Figure 5-9 shows a labeled dataset (the original\ndataset) split into five folds. Cross-validation then iterates training and testing k times,\nin a particular way. As depicted in the bottom pane of Figure 5-9, in each iteration of\nthe cross-validation, a different fold is chosen as the test data. In this iteration, the other\n\nFrom Holdout Evaluation to Cross-Validation\n\n|\n\n127", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00152", "page_num": 152, "segment": "k--1 folds are combined to form the training data. So, in each iteration we have (k--\n1)/k of the data used for training and 1/k used for testing.\n\nFigure 5-9. An illustration of cross-validation. The purpose of cross-validation is to use\nthe original labeled data efficiently to estimate the performance of a modeling procedure. Here we show five-fold cross-validation: the original dataset is split randomly into five equal-sized pieces. Then, each piece is used in turn as the test set, with the other\nfour used to train a model. The result is five different accuracy results, which then can\nbe used to compute the average accuracy and its variance.\n\n128\n\n|\n\nChapter 5: Overfitting and Its Avoidance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00153", "page_num": 153, "segment": "Each iteration produces one model, and thereby one estimate of generalization performance, for example, one estimate of accuracy. When cross-validation is finished, every\nexample will have been used only once for testing but k--1 times for training. At this\npoint we have performance estimates from all the k folds and we can compute the\naverage and standard deviation.\n\nThe Churn Dataset Revisited\nConsider again the churn dataset introduced in “Example: Addressing the Churn Problem with Tree Induction” on page 73. In that section we used the entire dataset both for\ntraining and testing, and we reported an accuracy of 73%. We ended that section by\nasking a question, Do you trust this number? By this point you should know enough to\nmistrust any performance measurement done on the training set, because overfitting is\na very real possibility. Now that we have introduced cross-validation we can redo the\nevaluation more carefully.\n\nFigure 5-10 shows the results of ten-fold cross-validation. In fact, two model types are\nshown. The top graph shows results with logistic regression, and the bottom graph\nshows results with classification trees. To be precise: the dataset was first shuffled, then\ndivided into ten partitions. Each partition in turn served as a single holdout set while\nthe other nine were collectively used for training. The horizontal line in each graph is\nthe average of accuracies of the ten models of that type.\n\nThere are several things to observe here. First, the average accuracy of the folds with\nclassification trees is 68.6%---significantly lower than our previous measurement of\n73%. This means there was some overfitting occurring with the classification trees, and\nthis new (lower) number is a more realistic measure of what we can expect. Second,\nthere is variation in the performances in the different folds (the standard deviation of\nthe fold accuracies is 1.1), and thus it is a good idea to average them to get a notion of\nthe performance as well as the variation we can expect from inducing classification trees\non this dataset.\n\nFinally, compare the fold accuracies between logistic regression and classification trees.\nThere are certain commonalities in both graphs---for example, neither model type did\nvery well on Fold Three and both performed well on Fold Ten. But there are definite\ndifferences between the two. An important thing to notice is that logistic regression\nmodels show slightly lower average accuracy (64.1%) and with higher variation (standard deviation of 1.3) than the classification trees do. On this particular dataset, trees\nmay be preferable to logistic regression because of their greater stability and performance. But this is not absolute; other datasets will produce different results, as we shall\nsee.\n\nThe Churn Dataset Revisited\n\n|\n\n129", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00154", "page_num": 154, "segment": "Figure 5-10. Fold accuracies for cross-validation on the churn problem. At the top are\naccuracies of logistic regression models trained on a dataset of 20,000 instances divided\ninto ten folds. At the bottom are accuracies of classification trees on the same folds. In\neach graph the horizontal line shows the average accuracy of the folds. (Note the selection of the range of the y axis, which emphasizes the differences in accuracy.)\n\nLearning Curves\nIf training set size changes, you may also expect different generalization performance\nfrom the resultant model. All else being equal, the generalization performance of datadriven modeling generally improves as more training data become available, up to a\n\n130\n\n|\n\nChapter 5: Overfitting and Its Avoidance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00155", "page_num": 155, "segment": "point. A plot of the generalization performance against the amount of training data is\ncalled a learning curve. The learning curve is another important analytical tool.\n\nLearning curves for tree induction and logistic regression are shown in Figure 5-11 for\nthe telecommunications churn problem.5 Learning curves usually have a characteristic\nshape. They are steep initially as the modeling procedure finds the most apparent regularities in the dataset. Then as the modeling procedure is allowed to train on larger\nand larger datasets, it finds more accurate models. However, the marginal advantage of\nhaving more data decreases, so the learning curve becomes less steep. In some cases,\nthe curve flattens out completely because the procedure can no longer improve accuracy\neven with more training data.\n\nIt is important to understand the difference between learning curves and fitting graphs\n(or fitting curves). A learning curve shows the generalization performance---the performance only on testing data, plotted against the amount of training data used. A fitting\ngraph shows the generalization performance as well as the performance on the training\ndata, but plotted against model complexity. Fitting graphs generally are shown for a\nfixed amount of training data.\n\nEven on the same data, different modeling procedures can produce very different learning curves. In Figure 5-11, observe that for smaller training-set sizes, logistic regression\nyields better generalization accuracy than tree induction. However, as the training sets\nget larger, the learning curve for logistic regression levels off faster, the curves cross,\nand tree induction soon is more accurate. This performance relates back to the fact that\nwith more flexibility comes more overfitting. Given the same set of features, classification trees are a more flexible model representation than linear logistic regression. This\nmeans two things: for smaller data, tree induction will tend to overfit more. Often, as\nwe see for the data in Figure 5-11, this leads logistic regression to perform better for\nsmaller datasets (not always, though). On the other hand, the figure also shows that the\nflexibility of tree induction can be an advantage with larger training sets: the tree can\nrepresent substantially nonlinear relationships between the features and the target.\nWhether the tree induction can actually capture those relationships needs to be evaluated empirically---using an analytical tool such as learning curves.\n\n5. Perlich et al. (2003) show learning curves for tree induction and logistic regression for dozens of classification\n\nproblems.\n\nLearning Curves\n\n|\n\n131", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00156", "page_num": 156, "segment": "Figure 5-11. Learning curves for tree induction and logistic regression for the churn\nproblem. As the training size grows (x axis), generalization performance (y axis) improves. Importantly, the improvement rates are different for the two induction technique, and change over time. Logistic regression has less flexibility, which allows it to\noverfit less with small data, but keeps it from modeling the full complexity of the data.\nTree induction is much more flexible, leading it to overfit more with small data, but to\nmodel more complex regularities with larger training sets.\n\nThe learning curve has additional analytical uses. For example, we’ve\nmade the point that data can be an asset. The learning curve may show\nthat generalization perforance has leveled off so investing in more\ntraining data is probably not worthwhile; instead, one should accept\nthe current performance or look for another way to improve the model, such as by devising better features. Alternatively, the learning curve\nmight show generalization accuracy continuing to improve, so obtaining more training data could be a good investment.\n\n132\n\n|\n\nChapter 5: Overfitting and Its Avoidance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00157", "page_num": 157, "segment": "Overfitting Avoidance and Complexity Control\nTo avoid overfitting, we control the complexity of the models induced from the data.\nLet’s start by examining complexity control in tree induction, since tree induction has\nmuch flexibility and therefore will tend to overfit a good deal without some mechanism\nto avoid it. This discussion in the context of trees will lead us to a very general mechanism\nthat will be applicable to other models.\n\nAvoiding Overfitting with Tree Induction\nThe main problem with tree induction is that it will keep growing the tree to fit the\ntraining data until it creates pure leaf nodes. This will likely result in large, overly complex trees that overfit the data. We have seen how this can be detrimental. Tree induction\ncommonly uses two techniques to avoid overfitting. These strategies are (i) to stop\ngrowing the tree before it gets too complex, and (ii) to grow the tree until it is too large,\nthen “prune” it back, reducing its size (and thereby its complexity).\n\nThere are various methods for accomplishing both. The simplest method to limit tree\nsize is to specify a minimum number of instances that must be present in a leaf. The\nidea behind this minimum-instance stopping criterion is that for predictive modeling,\nwe essentially are using the data at the leaf to make a statistical estimate of the value of\nthe target variable for future cases that would fall to that leaf. If we make predictions of\nthe target based on a very small subset of data, we might expect them to be inaccurate\n---especially when we built the tree specifically to try to get pure leaves. A nice property\nof controlling complexity in this way is that tree induction will automatically grow the\ntree branches that have a lot of data and cut short branches that have fewer data---thereby\nautomatically adapting the model based on the data distribution.\n\nA key question becomes what threshold we should use. How few instances are we willing\nto tolerate at a leaf? Five instances? Thirty? One hundred? There is no fixed number,\nalthough practitioners tend to have their own preferences based on experience. However, researchers have developed techniques to decide the stopping point statistically.\nStatistics provides the notion of a “hypothesis test,” which you might recall from a basic\nstatistics class. Roughly, a hypothesis test tries to assess whether a difference in some\nstatistic is not due simply to chance. In most cases, the hypothesis test is based on a “pvalue,” which gives a limit on the probability that the difference in statistic is due to\nchance. If this value is below a threshold (often 5%, but problem specific), then the\nhypothesis test concludes that the difference is likely not due to chance. So, for stopping\ntree growth, an alternative to setting a fixed size for the leaves is to conduct a hypothesis\ntest at every leaf to determine whether the observed difference in (say) information gain\ncould have been due to chance. If the hypothesis test concludes that it was likely not\ndue to chance, then the split is accepted and the tree growing continues. (See “Sidebar:\nBeware of “multiple comparisons”” on page 139.)\n\nOverfitting Avoidance and Complexity Control\n\n|\n\n133", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00158", "page_num": 158, "segment": "The second strategy for reducing overfitting is to “prune” an overly large tree. Pruning\nmeans to cut off leaves and branches, replacing them with leaves. There are many ways\nto do this, and the interested reader can look into the data mining literature for details.\nOne general idea is to estimate whether replacing a set of leaves or a branch with a leaf\nwould reduce accuracy. If not, then go ahead and prune. The process can be iterated on\nprogressive subtrees until any removal or replacement would reduce accuracy.\n\nWe conclude our example of avoiding overfitting in tree induction with the method that\nwill generalize to many different data modeling techniques. Consider the following idea:\nwhat if we built trees with all sorts of different complexities? For example, say we stop\nbuilding the tree after only one node. Then build a tree with two nodes. Then three\nnodes, etc. We have a set of trees of different complexities. Now, if only there were a way\nto estimate their generalization performance, we could pick the one that is (estimated\nto be) the best!\n\nA General Method for Avoiding Overfitting\nMore generally, if we have a collection of models with different complexities, we could\nchoose the best simply by estimating the generalization performance of each. But how\ncould we estimate their generalization performance? On the (labeled) test data? There’s\none big problem with that: test data should be strictly independent of model building\nso that we can get an independent estimate of model accuracy. For example, we might\nwant to estimate the ultimate business performance or to compare the best model we\ncan build from one family (say, classification trees) against the best model from another\nfamily (say, logistic regression). If we don’t care about comparing models or getting an\nindependent estimate of the model accuracy and/or variance, then we could pick the\nbest model based on the testing data.\n\nHowever, even if we do want these things, we still can proceed. The key is to realize that\nthere was nothing special about the first training/test split we made. Let’s say we are\nsaving the test set for a final assessment. We can take the training set and split it again\ninto a training subset and a testing subset. Then we can build models on this training\nsubset and pick the best model based on this testing subset. Let’s call the former the subtraining set and the latter the validation set for clarity. The validation set is separate from\nthe final test set, on which we are never going to make any modeling decisions. This\nprocedure is often called nested holdout testing.\n\nReturning to our classification tree example, we can induce trees of many complexities\nfrom the subtraining set, then we can estimate the generalization performance for each\nfrom the validation set. This would correspond to choosing the top of the inverted-Ushaped holdout curve in Figure 5-3. Say the best model by this assessment has a complexity of 122 nodes (the “sweet spot”). Then we could use this model as our best choice,\npossibly estimating the actual generalization performance on the final holdout test set.\nWe also could add one more twist. This model was built on a subset of our training data,\n\n134\n\n|\n\nChapter 5: Overfitting and Its Avoidance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00159", "page_num": 159, "segment": "since we had to hold out the validation set in order to choose the complexity. But once\nwe’ve chosen the complexity, why not induce a new tree with 122 nodes from the whole,\noriginal training set? Then we might get the best of both worlds: using the subtraining/\nvalidation split to pick the best complexity without tainting the test set, and building a\nmodel of this best complexity on the entire training set (subtraining plus validation).\n\nThis approach is used in many sorts of modeling algorithms to control complexity. The\ngeneral method is to choose the value for some complexity parameter by using some\nsort of nested holdout procedure. Again, it is nested because a second holdout procedure\nis performed on the training set selected by the first holdout procedure.\n\nOften, nested cross-validation is used. Nested cross-validation is more complicated, but\nit works as you might suspect. Say we would like to do cross-validation to assess the\ngeneralization accuracy of a new modeling technique, which has an adjustable complexity parameter C, but we do not know how to set it. So, we run cross-validation as\ndescribed above. However, before building the model for each fold, we take the training\nset (refer to Figure 5-9) and first run an experiment: we run another entire crossvalidation on just that training set to find the value of C estimated to give the best\naccuracy. The result of that experiment is used only to set the value of C to build the\nactual model for that fold of the cross-validation. Then we build another model using\nthe entire training fold, using that value for C, and test on the corresponding test fold.\nThe only difference from regular cross-validation is that for each fold we first run this\nexperiment to find C, using another, smaller, cross-validation.\n\nIf you understood all that, you would realize that if we used 5-fold cross-validation in\nboth cases, we actually have built 30 total models in the process (yes, thirty). This sort\nof experimental complexity-controlled modeling only gained broad practical application over the last decade or so, because of the obvious computational burden involved.\n\nThis idea of using the data to choose the complexity experimentally, as well as to build\nthe resulting model, applies across different induction algorithms and different sorts of\ncomplexity. For example, we mentioned that complexity increases with the size of the\nfeature set, so it is usually desirable to cull the feature set. A common method for doing\nthis is to run with many different feature sets, using this sort of nested holdout procedure\nto pick the best.\n\nFor example, sequential forward selection (SFS) of features uses a nested holdout procedure to first pick the best individual feature, by looking at all models built using just\none feature. After choosing a first feature, SFS tests all models that add a second feature\nto this first chosen feature. The best pair is then selected. Next the same procedure is\ndone for three, then four, and so on. When adding a feature does not improve classification accuracy on the validation data, the SFS process stops. (There is a similar procedure called sequential backward elimination of features. As you might guess, it works\nby starting with all features and discarding features one at a time. It continues to discard\nfeatures as long as there is no performance loss.)\n\nOverfitting Avoidance and Complexity Control\n\n|\n\n135", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00160", "page_num": 160, "segment": "This is a common approach. In modern environments with plentiful data and computational power, the data scientist routinely sets modeling parameters by experimenting\nusing some tactical, nested holdout testing (often nested cross-validation).\n\nThe next section shows a different way that this method applies to controlling overfitting\nwhen learning numerical functions (as described in Chapter 4). We urge you to at least\nskim the following section because it introduces concepts and vocabulary in common\nuse by data scientists these days.\n\n* Avoiding Overfitting for Parameter Optimization\nAs just described, avoiding overfitting involves complexity control: finding the “right”\nbalance between the fit to the data and the complexity of the model. In trees we saw\nvarious ways for trying to keep the tree from getting too big (too complex) when fitting\nthe data. For equations, such as logistic regression, that unlike trees do not automatically\nselect what attributes to include, complexity can be controlled by choosing a “right” set\nof attributes.\n\nChapter 4 introduced the popular family of methods that builds models by explicitly\noptimizing the fit to the data via a set of numerical parameters. We discussed various\nlinear members of this family, including linear discriminant learners, linear regression,\nand logistic regression. Many nonlinear models are fit to the data in exactly the same\nway.\n\nAs might be expected given our discussion so far in this chapter and the figures in\n“Example: Overfitting Linear Functions” on page 119, these procedures also can overfit\nthe data. However, their explicit optimization framework provides an elegant, if technical, method for complexity control. The general strategy is that instead of just optimizing the fit to the data, we optimize some combination of fit and simplicity. Models\nwill be better if they fit the data better, but they also will be better if they are simpler.\nThis general methodology is called regularization, a term that is heard often in data\nscience discussions.\n\nThe rest of this section discusses briefly (and slightly technically) how\nregularization is done. Don’t worry if you don’t really understand the\ntechnical details. Do remember that regularization is trying to optimize\nnot just the fit to the data, but a combination of fit to the data and\nsimplicity of the model.\n\nRecall from Chapter 4 that to fit a model involving numeric parameters w to the data\nwe find the set of parameters that maximizes some “objective function” indicating how\nwell it fits the data:\n\n136\n\n|\n\nChapter 5: Overfitting and Its Avoidance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00161", "page_num": 161, "segment": "arg max\n\nfit(, )\n\n(The arg maxw just means that you want to maximize the fit over all possible arguments\nw, and are interested in the particular argument w that gives the maximum. These would\nbe the parameters of the final model.)\n\nComplexity control via regularization works by adding to this objective function a penalty for complexity:\n\narg max\n\nfit(, ) - λ · penalty()\n\nThe λ term is simply a weight that determines how much importance the optimization\nprocedure should place on the penalty, compared to the data fit. At this point, the modeler has to choose λ and the penalty function.\n\nSo, as a concrete example, recall from “* Logistic Regression: Some Technical Details”\non page 99 that to learn a standard logistic regression model, from data, we find the\nnumeric parameters w that yield the linear model most likely to have generated the\nobserved data---the “maximum likelihood” model. Let’ represent that as:\n\narg max\n\nglikelihood(, )\n\nTo learn a “regularized” logistic regression model we would instead compute:\n\narg max\n\nglikelihood(, ) - λ · penalty()\n\nThere are different penalties that can be applied, with different properties.6 The most\ncommonly used penalty is the sum of the squares of the weights, sometimes called the\n“L2-norm” of w. The reason is technical, but basically functions can fit data better if\nthey are allowed to have very large positive and negative weights. The sum of the squares\nof the weights gives a large penalty when weights have large absolute values.\n\n6. The book The Elements of Statistical Learning (Hastie, Tibshirani, & Friedman, 2009) contains an excellent\n\ntechnical discussion of these.\n\nOverfitting Avoidance and Complexity Control\n\n|\n\n137", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00162", "page_num": 162, "segment": "If we incorporate the L2-norm penalty into standard least-squares linear regression, we\nget the statistical procedure called ridge regression. If instead we use the sum of the\nabsolute values (rather than the squares), known as the L1-norm, we get a procedure\nknown as the lasso (Hastie et al., 2009). More generally, this is called L1-regularization.\nFor reasons that are quite technical, L1-regularization ends up zeroing out many coefficients. Since these coefficients are the multiplicative weights on the features, L1regularization effectively performs an automatic form of feature selection.\n\nNow we have the machinery to describe in more detail the linear support vector machine, introduced in “Support Vector Machines, Briefly” on page 91. There we waved\nour hands and told you that the support vector machine “maximizes the margin” between the classes by fitting the “fattest bar” between the classes. Separately we discussed\nthat it uses hinge loss (see “Sidebar: Loss functions” on page 94) to penalize errors. We\nnow can connect these together, and directly to logistic regression. Specifically, linear\nsupport vector machine learning is almost equivalent to the L2-regularized logistic regression just discussed; the only difference is that a support vector machine uses hinge\nloss instead of likelihood in its optimization. The support vector machine optimizes\nthis equation:\n\narg max\n\n- ghinge(, ) - λ · penalty()\n\nwhere ghinge, the hinge loss term, is negated because lower hinge loss is better.\n\nFinally, you may be saying to yourself: all this is well and good, but a lot of magic seems\nto be hidden in this λ parameter, which the modeler has to choose. How in the world\nwould the modeler choose that for some real domain like churn prediction, or online\nad targeting, or fraud detection?\n\nIt turns out that we already have a straightforward way to choose λ. We’ve discussed\nhow a good tree size and a good feature set can be chosen via nested cross-validation\non the training data. We can choose λ the same way. This cross-validation would essentially conduct automated experiments on subsets of the training data and find a good\nλ value. Then this λ would be used to learn a regularized model on all the training data.\nThis has become the standard procedure for building numerical models that give a good\nbalance between data fit and model complexity. This general approach to optimizing\nthe parameter values of a data mining procedure is known as grid search.\n\n138\n\n|\n\nChapter 5: Overfitting and Its Avoidance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00163", "page_num": 163, "segment": "Sidebar: Beware of “multiple comparisons”\nConsider the following scenario. You run an investment firm. Five years ago, you wanted\nto have some marketable small-cap mutual fund products to sell, but your analysts had\nbeen awful at picking small-cap stocks. So you undertook the following procedure. You\nstarted 1,000 different mutual funds, each including a small set of stocks randomly\nchosen from those that make up the Russell 2000 index (the main index for small-cap\nstocks). Your firm invested in all 1,000 of these funds, but told no one about them. Now,\nfive years later, you look at their performance. Since they have different stocks in them,\nthey will have had different returns. Some will be about the same as the index, some will\nbe worse, and some will be better. The best one might be a lot better. Now, you liquidate\nall the funds but the best few, and you present these to the public. You can “honestly”\nclaim that their 5-year return is substantially better than the return of the Russell 2000\nindex.\n\nSo, what’s the problem? The problem is that you randomly chose the stocks! You have\nno idea whether the stocks in these “best” funds performed better because they indeed\nare fundamentally better, or because you cherry-picked the best from a large set that\nsimply varied in performance. If you flip 1,000 fair coins many times each, one of them\nwill have come up heads much more than 50% of the time. However, choosing that coin\nas the “best” of the coins for later flipping obviously is silly. These are instances of “the\nproblem of multiple comparisons,” a very important statistical phenomenon that business analysts and data scientists should always keep in mind. Beware whenever someone\ndoes many tests and then picks the results that look good. Statistics books will warn\nagainst running multiple statistical hypothesis tests, and then looking at the ones that\ngive “significant” results. These usually violate the assumptions behind the statistical\ntests, and the actual significance of the results is dubious.\n\nThe underlying reasons for overfitting when building models from data are essentially\nproblems of multiple comparisons (Jensen & Cohen, 2000). Note that even the procedures for avoiding overfitting themselves undertake multiple comparisons (e.g., choosing the best complexity for a model by comparing many complexities). There is no silver\nbullet or magic formula to truly get “the optimal” model to fit the data. Nonetheless,\ncare can be taken to reduce overfitting as much as possible, by using the holdout procedures described in this chapter and if possible by looking carefully at the results before\ndeclaring victory. For example, if the fitting graph truly has an inverted-U-shape, one\ncan be much more confident that the top represents a “good” complexity than if the\ncurve jumps around randomly.\n\nOverfitting Avoidance and Complexity Control\n\n|\n\n139", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00164", "page_num": 164, "segment": "Summary\nData mining involves a fundamental trade-off between model complexity and the possibility of overfitting. A complex model may be necessary if the phenomenon producing\nthe data is itself complex, but complex models run the risk of overfitting training data\n(i.e., modeling details of the data that are not found in the general population). An overfit\nmodel will not generalize to other data well, even if they are from the same population.\n\nAll model types can be overfit. There is no single choice or technique to eliminate\noverfitting. The best strategy is to recognize overfitting by testing with a holdout set.\nSeveral types of curves can help detect and measure overfitting. A fitting graph has two\ncurves showing the model performance on the training and testing data as a function\nof model complexity. A fitting curve on testing data usually has an approximate U or\ninverted-U-shape (depending on whether error or accuracy is plotted). The accuracy\nstarts off low when the model is simple, increases as complexity increases, flattens out,\nthen starts to decrease again as overfitting sets in. A learning curve shows model performance on testing data plotted against the amount of training data used. Usually\nmodel performance increases with the amount of data, but the rate of increase and the\nfinal asymptotic performance can be quite different between models.\n\nA common experimental methodology called cross-validation specifies a systematic way\nof splitting up a single dataset such that it generates multiple performance measures.\nThese values tell the data scientist what average behavior the model yields as well as the\nvariation to expect.\n\nThe general method for reining in model complexity to avoid overfitting is called model\nregularization. Techniques include tree pruning (cutting a classification tree back when\nit has become too large), feature selection, and employing explicit complexity penalties\ninto the objective function used for modeling.\n\n140\n\n|\n\nChapter 5: Overfitting and Its Avoidance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00165", "page_num": 165, "segment": "CHAPTER 6\nSimilarity, Neighbors, and Clusters\n\nFundamental concepts: Calculating similarity of objects described by data; Using similarity for prediction; Clustering as similarity-based segmentation.\n\nExemplary techniques: Searching for similar entities; Nearest neighbor methods; Clustering methods; Distance metrics for calculating similarity.\n\nSimilarity underlies many data science methods and solutions to business problems. If\ntwo things (people, companies, products) are similar in some ways they often share\nother characteristics as well. Data mining procedures often are based on grouping things\nby similarity or searching for the “right” sort of similarity. We saw this implicitly in\nprevious chapters where modeling procedures create boundaries for grouping instances\ntogether that have similar values for their target variables. In this chapter we will look\nat similarity directly, and show how it applies to a variety of different tasks. We include\nsections with some technical details, in order that the more mathematical reader can\nunderstand similarity in more depth; these sections can be skipped.\n\nDifferent sorts of business tasks involve reasoning from similar examples:\n\n• We may want to retrieve similar things directly. For example, IBM wants to find\ncompanies that are similar to their best business customers, in order to have the\nsales staff look at them as prospects. Hewlett-Packard maintains many highperformance servers for clients; this maintenance is aided by a tool that, given a\nserver configuration, retrieves information on other similarly configured servers.\nAdvertisers often want to serve online ads to consumers who are similar to their\ncurrent good customers.\n\n• Similarity can be used for doing classification and regression. Since we now know\na good bit about classification, we will illustrate the use of similarity with a classification example below.\n\n• We may want to group similar items together into clusters, for example to see\nwhether our customer base contains groups of similar customers and what these\n\n141", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00166", "page_num": 166, "segment": "groups have in common. Previously we discussed supervised segmentation; this is\nunsupervised segmetation. After discussing the use of similarity for classification,\nwe will discuss its use for clustering.\n\n• Modern retailers such as Amazon and Netflix use similarity to provide recommendations of similar products or from similar people. Whenever you see statements\nlike “People who like X also like Y” or “Customers with your browsing history have\nalso looked at ...” similarity is being applied. In Chapter 12, we will discuss how a\ncustomer can be similar to a movie, if the two are described by the same “taste\ndimensions.” In this case, to make recommendations we can find the movies that\nare most similar to the customer (and which the customer has not already seen).\n\n• Reasoning from similar cases of course extends beyond business applications; it is\nnatural to fields such as medicine and law. A doctor may reason about a new difficult\ncase by recalling a similar case (either treated personally or documented in a journal) and its diagnosis. A lawyer often argues cases by citing legal precedents, which\nare similar historical cases whose dispositions were previously judged and entered\ninto the legal casebook. The field of Artificial Intelligence has a long history of\nbuilding systems to help doctors and lawyers with such case-based reasoning. Similarity judgments are a key component.\n\nIn order to discuss these applications further, we need to take a minute to formalize\nsimilarity and its cousin, distance.\n\nSimilarity and Distance\nOnce an object can be represented as data, we can begin to talk more precisely about\nthe similarity between objects, or alternatively the distance between objects. For example, let’s consider the data representation we have used throughout the book so far:\nrepresent each object as a feature vector. Then, the closer two objects are in the space\ndefined by the features, the more similar they are.\n\nRecall that when we build and apply predictive models, the goal is to determine the\nvalue of a target characteristic. In doing so, we’ve used the implicit similarity of objects\nalready. “Visualizing Segmentations” on page 67 discussed the geometric interpretation\nof some classification models and “Classification via Mathematical Functions” on page\n83 discussed how two different model types divide up an instance space into regions\nbased on closeness of instances with similar class labels. Many methods in data science\nmay be seen in this light: as methods for organizing the space of data instances (representations of important objects) so that instances near each other are treated similarly\nfor some purpose. Both classification trees and linear classifiers establish boundaries\nbetween regions of differing classifications. They have in common the view that instances sharing a common region in space should be similar; what differs between the\nmethods is how the regions are represented and discovered.\n\n142\n\n|\n\nChapter 6: Similarity, Neighbors, and Clusters", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00167", "page_num": 167, "segment": "So why not reason about the similarity or distance between objects directly? To do so,\nwe need a basic method for measuring similarity or distance. What does it mean that\ntwo companies or two consumers are similar? Let’s examine this carefully. Consider two\ninstances from our simplified credit application domain:\n\nAttribute\n\nAge\n\nYears at current address\n\nResidential status (1=Owner, 2=Renter, 3=Other)\n\nPerson A Person B\n\n23\n\n2\n\n2\n\n40\n\n10\n\n1\n\nThese data items have multiple attributes, and there’s no single best method for reducing\nthem to a single similarity or distance measurement. There are many different ways to\nmeasure the similarity or distance between Person A and Person B. A good place to\nbegin is with measurements of distance from basic geometry.\n\nFigure 6-1. Euclidean distance.\n\nRecall from our prior discussions of the geometric interpretation that if we have two\n(numeric) features, then each object is a point in a two-dimensional space. Figure 6-1\nshows two data items, A and B, located on a two-dimensional plane. Object A is at\ncoordinates (xA, yA) and B is at (xB, yB). At the risk of too much repetition, note that these\ncoordinates are just the values of the two features of the objects. We can draw a right\ntriangle between the two objects, as shown, whose base is the difference in the x’s: (xA --\nxB) and whose height is the difference in the y’s: (yA -- yB). The Pythagorean theorem tells\nus that the distance between A and B is given by the length of the hypotenuse, and is\nequal to the square root of the summed squares of the lengths of the other two sides of\nthe triangle, which in this case is (xA - xB)2 + (yA - yB)2. Essentially, we can compute\nthe overall distance by computing the distances of the individual dimensions---the individual features in our setting. This is called the Euclidean distance 1 between two\npoints, and it’s probably the most common geometric distance measure.\n\n1. After Euclid, the 4th century B.C. Greek mathematician known as the Father of Geometry.\n\nSimilarity and Distance\n\n|\n\n143", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00168", "page_num": 168, "segment": "Euclidean distance is not limited to two dimensions. If A and B were objects described\nby three features, they could be represented by points in three-dimensional space and\ntheir positions would then be represented as (xA, yA, zA) and (xB, yB, zB). The distance\nbetween A and B would then include the term (zA--zB)2. We can add arbitrarily many\nfeatures, each a new dimension. When an object is described by n features, n dimensions\n(d1, d2, ..., dn), the general equation for Euclidean distance in n dimensions is shown in\nEquation 6-1:\n\nEquation 6-1. General Euclidean distance\n\n(d1,A - d1,B)2 + (d2,A - d2,B)2 + ... + (dn,A - dn,B)2\n\nWe now have a metric for measuring the distance between any two objects described\nby vectors of numeric features---a simple formula based on the distances of the objects’\nindividual features. Recalling persons A and B, above, their Euclidean distance is:\n\nd (A, B) =\n\n(23 - 40)2 + (2 - 10)2 + (2 - 1)2\n\n≈\n\n18.8\n\nSo the distance between these examples is about 19. This distance is just a number---it\nhas no units, and no meaningful interpretation. It is only really useful for comparing\nthe similarity of one pair of instances to that of another pair. It turns out that comparing\nsimilarities is extremely useful.\n\nNearest-Neighbor Reasoning\nNow that we have a way to measure distance, we can use it for many different dataanalysis tasks. Recalling examples from the beginning of the chapter, we could use this\nmeasure to find the companies most similar to our best corporate customers, or the\nonline consumers most similar to our best retail customers. Once we have found these,\nwe can take whatever action is appropriate in the business context. For corporate customers, IBM does this to help direct its sales force. Online advertisers do this to target\nads. These most-similar instances are called nearest neighbors.\n\nExample: Whiskey Analytics\nLet’s talk about a fresh example. One of us (Foster) likes single malt Scotch whiskey. If\nyou’ve had more than one or two, you realize that there is a lot of variation among the\nhundreds of different single malts. When Foster finds a single malt he really likes, he\nwants to find other similar ones---both because he likes to explore the “space” of single\nmalts, but also because any given liquor store or restaurant only has a limited selection.\n\n144\n\n|\n\nChapter 6: Similarity, Neighbors, and Clusters", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00169", "page_num": 169, "segment": "He wants to be able to pick one he’ll really like. For example, the other evening a dining\ncompanion recommended trying the single malt “Bunnahabhain.”2 It was unusual and\nvery good. Out of all the many single malts, how could Foster find other ones like that?\n\nLet’s take a data science approach. Recall from Chapter 2 that we first should think about\nthe exact question we would like to answer, and what are the appropriate data to answer\nit. How can we describe single malt Scotch whiskeys as feature vectors, in such a way\nthat we think similar whiskeys will have similar taste? This is exactly the project undertaken by François-Joseph Lapointe and Pierre Legendre of the University of Montréal (Lapointe & Legendre, 1994). They were interested in several classification and\norganizational questions about Scotch whiskeys. We’ll adopt some of their approach\nhere.\n\nIt turns out that tasting notes are published for many whiskeys. For example, Michael\nJackson is a well-known whiskey and beer connoisseur who has written Michael Jackson’s Malt Whisky Companion: A Connoisseur’s Guide to the Malt Whiskies of Scotland (Jackson, 1989), which describes 109 different single malt Scotches of Scotland.\nThe descriptions are in the form of tasting notes on each Scotch, such as: “Appetizing\naroma of peat smoke, almost incense-like, heather honey with a fruity softness.”\n\nAs data scientists, we are making progress. We have found a potentially useful source\nof data. However, we do not yet have whiskeys described by feature vectors, only by\ntasting notes. We need to press on with our data formulation. Following Lapointe and\nLegendre (1994), let’s create some numeric features that, for any whiskey, will summarize\nthe information in the tasting notes. Define five general whiskey attributes, each with\nmany possible values:\n\n1.\n\n2.\n\n3.\n\n4.\n\n5.\n\nColor: yellow, very pale, pale, pale gold, gold, old gold, full gold, amber, etc.\n\nNose: aromatic, peaty, sweet, light, fresh, dry, grassy, etc.\n\nBody: soft, medium, full, round, smooth, light, firm, oily.\n\nPalate: full, dry, sherry, big, fruity, grassy, smoky, salty, etc.\n\nFinish: full, dry, warm, light, smooth, clean, fruity, grassy, smoky, etc.\n\n(14 values)\n\n(12 values)\n\n(8 values)\n\n(15 values)\n\n(19 values)\n\nIt is important to note that these category values are not mutually exclusive (e.g., Aberlour’s palate is described as medium, full, soft, round and smooth). In general, any of\nthe values can co-occur (though some of them, like Color being both light and smoky,\nnever do) but because they can co-occur, each value of each variable was coded as a\nseparate feature by Lapointe and Legendre. Consequently there are 68 binary features\nof each whiskey.\n\n2. No, he can’t pronounce it properly either.\n\nNearest-Neighbor Reasoning\n\n|\n\n145", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00170", "page_num": 170, "segment": "Foster likes Bunnahabhain, so we can use Lapointe and Legendre’s representation of\nwhiskeys with Euclidean distance to find similar ones for him. For reference, here is\ntheir description of Bunnahabhain:\n\n• Color: gold\n\n• Nose: fresh and sea\n\n• Body: firm, medium, and light\n\n• Palate: sweet, fruity, and clean\n\n• Finish: full\n\nHere is Bunnahabhain’s description and the five single-malt Scotches most similar to\nBunnahabhain, by increasing distance:\n\nWhiskey\n\nDistance Descriptors\n\nBunnahabhain ---\n\ngold; firm,med,light; sweet,fruit,clean; fresh,sea; full\n\nGlenglassaugh 0.643\n\ngold; firm,light,smooth; sweet,grass; fresh,grass\n\nTullibardine\n\nArdberg\n\nBruichladdich\n\n0.647\n\n0.667\n\n0.667\n\ngold; firm,med,smooth; sweet,fruit,full,grass,clean; sweet; big,arome,sweet\n\nsherry; firm,med,full,light; sweet; dry,peat,sea;salt\n\npale; firm,light,smooth; dry,sweet,smoke,clean; light; full\n\nGlenmorangie\n\n0.667\n\np.gold; med,oily,light; sweet,grass,spice; sweet,spicy,grass,sea,fresh; full,long\n\nUsing this list we could find a Scotch similar to Bunnahabhain. At any particular shop\nwe might have to go down the list a bit to find one they stock, but since the Scotches are\nordered by similarity we can easily find the most similar Scotch (and also have a vague\nidea as to how similar the closest available Scotch is as compared to the alternatives that\nare not available).\n\nThis is an example of the direct application of similarity to solve a problem. Once we\nunderstand this fundamental notion, we have a powerful conceptual tool for approaching a variety of problems, such as those laid out above (finding similar companies,\nsimilar consumers, etc.). As we see in the whiskey example, the data scientist often still\nhas work to do to actually define the data so that the similarity will be with respect to a\nuseful set of characteristics. Later we will present some other notions of similarity and\ndistance. Now, let’s move on to another very common use of similarity in data science.\n\nNearest Neighbors for Predictive Modeling\nWe also can use the idea of nearest neighbors to do predictive modeling in a different\nway. Take a minute to recall everything you now know about predictive modeling from\nprior chapters. To use similarity for predictive modeling, the basic procedure is beautifully simple: given a new example whose target variable we want to predict, we scan\nthrough all the training examples and choose several that are the most similar to the\n\n146\n\n|\n\nChapter 6: Similarity, Neighbors, and Clusters", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00171", "page_num": 171, "segment": "Figure 6-2. Nearest neighbor classification. The point to be classified, labeled with a\nquestion mark, would be classified + because the majority of its nearest (three) neighbors are +.\n\nnew example. Then we predict the new example’s target value, based on the nearest\nneighbors’ (known) target values. How to do that last step needs to be defined; for now,\nlet’s just say that we have some combining function (like voting or averaging) operating\non the neighbors’ known target values. The combining function will give us a prediction.\n\nClassification\n\nSince we have focused a great deal on classification tasks so far in the book, let’s begin\nby seeing how neighbors can be used to classify a new instance in a super-simple setting.\nFigure 6-2 shows a new example whose label we want to predict, indicated by a “?.”\nFollowing the basic procedure introduced above, the nearest neighbors (in this example,\nthree of them) are retrieved and their known target variables (classes) are consulted. In\nthis this case, two examples are positive and one is negative. What should be our combining function? A simple combining function in this case would be majority vote, so\nthe predicted class would be positive.\n\nAdding just a little more complexity, consider a credit card marketing problem. The\ngoal is to predict whether a new customer will respond to a credit card offer based on\nhow other, similar customers have responded. The data (still oversimplified of course)\nare shown in Table 6-1.\n\nNearest-Neighbor Reasoning\n\n|\n\n147", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00172", "page_num": 172, "segment": "Table 6-1. Nearest neighbor example: Will David respond or not?\n\nCustomer Age\n\nIncome (1000s)\n\nCards Response (target) Distance from David\n\nDavid\n\nJohn\n\nRachael\n\nRuth\n\nJefferson\n\nNorah\n\n37\n\n35\n\n22\n\n63\n\n59\n\n25\n\n50\n\n35\n\n50\n\n200\n\n170\n\n40\n\n2\n\n3\n\n2\n\n1\n\n1\n\n4\n\n?\n\nYes\n\nNo\n\nNo\n\nNo\n\nYes\n\n0\n\n(35 - 37)2 + (35 - 50)2 + (3 - 2)2 = 15.16\n\n(22 - 37)2 + (50 - 50)2 + (2 - 2)2 = 15\n\n(63 - 37)2 + (200 - 50)2 + (1 - 2)2 = 152.23\n\n(59 - 37)2 + (170 - 50)2 + (1 - 2)2 = 122\n\n(25 - 37)2 + (40 - 50)2 + (4 - 2)2 = 15.74\n\nIn this example data, there are five existing customers we previously have contacted\nwith a credit card offer. For each of them we have their name, age, income, the number\nof cards they already have, and whether they responded to the offer. For a new person,\nDavid, we want to predict whether he will respond to the offer or not.\n\nThe last column in Table 6-1 shows a distance calculation, using Equation 6-1, of how\nfar each instance is from David. Three customers (John, Rachael, and Norah) are fairly\nsimilar to David, with a distance of about 15. The other two customers (Ruth and Jefferson) are much farther away. Therefore, David’s three nearest neighbors are Rachael,\nthen John, then Norah. Their responses are No, Yes, and Yes, respectively. If we take a\nmajority vote of these values, we predict Yes (David will respond). This touches upon\nsome important issues with nearest-neighbor methods: how many neighbors should we\nuse? Should they have equal weights in the combining function? We discuss these later\nin the chapter.\n\nProbability Estimation\n\nWe’ve made the point that it’s usually important not just to classify a new example but\nto estimate its probability---to assign a score to it, because a score gives more information\nthan just a Yes/No decision. Nearest neighbor classification can be used to do this fairly\neasily. Consider again the classification task of deciding whether David will be a responder or not. His nearest neighbors (Rachael, John, and Norah) have classes of No,\nYes, and Yes, respectively. If we score for the Yes class, so that Yes=1 and No=0, we can\naverage these into a score of 2/3 for David. If we were to do this in practice, we might\nwant to use more than just three nearest neighbors to compute the probability estimates\n(and recall the discussion of estimating probabilities from small samples in “Probability\nEstimation” on page 71).\n\nRegression\n\nOnce we can retrieve nearest neighbors, we can use them for any predictive mining task\nby combining them in different ways. We just saw how to do classification by taking a\nmajority vote of a target. We can do regression in a similar way.\n\n148\n\n|\n\nChapter 6: Similarity, Neighbors, and Clusters", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00173", "page_num": 173, "segment": "Assume we had the same dataset as in Table 6-1, but this time we want to predict David’s\nIncome. We won’t redo the distance calculation, but assume that David’s three nearest\nneighbors were again Rachael, John, and Norah. Their respective incomes are 50, 35,\nand 40 (in thousands). We then use these values to generate a prediction for David’s\nincome. We could use the average (about 42) or the median (40).\n\nIt is important to note that in retrieving neighbors we do not use the\ntarget variable because we’re trying to predict it. Thus Income would\nnot enter into the distance calculation as it does in Table 6-1. However, we’re free to use any other variables whose values are available to\ndetermine distance.\n\nHow Many Neighbors and How Much Influence?\nIn the course of explaining how classification, regression, and scoring may be done, we\nhave used an example with only three neighbors. Several questions may have occurred\nto you. First, why three neighbors, instead of just one, or five, or one hundred? Second,\nshould we treat all neighbors the same? Though all are called “nearest” neighbors, some\nare nearer than others, and shouldn’t this influence how they’re used?\n\nThere is no simple answer to how many neighbors should be used. Odd numbers are\nconvenient for breaking ties for majority vote classification with two-class problems.\nNearest neighbor algorithms are often referred to by the shorthand k-NN, where the k\nrefers to the number of neighbors used, such as 3-NN.\n\nIn general, the greater k is the more the estimates are smoothed out among neighbors.\nIf you have understood everything so far, with a little thought you should realize that if\nwe increase k to the maximum possible (so that k = n) the entire dataset would be used\nfor every prediction. Elegantly, this simply predicts the average over the entire dataset\nfor any example. For classification, this would predict the majority class in the entire\ndataset; for regression, the average of all the target values; for class probability estimation, the “base rate” probability (see Note: Base rate in “Holdout Data and Fitting\nGraphs” on page 113).\n\nEven if we’re confident about the number of neighbor examples we should use, we may\nrealize that neighbors have different similarity to the example we’re trying to predict.\nShouldn’t this influence how they’re used?\n\nFor classification we started with a simple strategy of majority voting, retrieving an odd\nnumber of neighbors to break ties. However, this ignores an important piece of information: how close each neighbor is to the instance. For example, consider what would\nhappen if we used k = 4 neighbors to classify David. We would retrieve the responses\n(Yes, No, Yes, No), causing the responses to be evenly mixed. But the first three are very\nclose to David (distance ≈ 15) while the fourth is much further away (distance ≈ 122).\n\nNearest-Neighbor Reasoning\n\n|\n\n149", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00174", "page_num": 174, "segment": "Intuitively, this fourth instance shouldn’t contribute as much to the vote as the first three.\nTo incorporate this concern, nearest-neighbor methods often use weighted voting or\nsimilarity moderated voting such that each neighbor’s contribution is scaled by its similarity.\n\nConsider again the data in Table 6-1, involving predicting whether David will respond\nto a credit card offer. We showed that if we predict David’s class by majority vote it\ndepends greatly on the number of neighbors we choose. Let’s redo the calculations, this\ntime using all neighbors but scaling each by its similarity to David, using as the scaling\nweight the reciprocal of the square of the distance. Here are the neighbors ordered by\ntheir distance from David:\n\nName\n\nDistance\n\nSimilarity weight\n\nContribution Class\n\nRachael\n\nJohn\n\nNorah\n\n15.0\n\n15.2\n\n15.7\n\nJefferson 122.0\n\nRuth\n\n152.2\n\n0.004444\n\n0.004348\n\n0.004032\n\n0.000067\n\n0.000043\n\n0.344\n\n0.336\n\n0.312\n\n0.005\n\n0.003\n\nNo\n\nYes\n\nYes\n\nNo\n\nNo\n\nThe Contribution column is the amount that each neighbor contributes to the final\ncalculation of the target probability prediction (the contributions are proportional to\nthe weights, but adding up to one). We see that distances greatly effect contributions:\nRachael, John and Norah are most similar to David and effectively determine our prediction of his response, while Jefferson and Ruth are so far away that they contribute\nvirtually nothing. Summing the contributions for the positive and negative classes, the\nfinal probability estimates for David are 0.65 for Yes and 0.35 for No.\n\nThis concept generalizes to other sorts of prediction tasks, for example regression and\nclass probability estimation. Generally, we can think of the procedure as weighted scoring. Weighted scoring has a nice consequence in that it reduces the importance of deciding how many neighbors to use. Because the contribution of each neighbor is moderated by its distance, the influence of neighbors naturally drops off the farther they are\nfrom the instance. Consequently, when using weighted scoring the exact value of k is\nmuch less critical than with majority voting or unweighted averaging. Some methods\navoiding committing to a k by retrieving a very large number of instances (e.g., all\ninstances, k = n) and depend upon distance weighting to moderate the influences.\n\nSidebar: Many names for nearest-neighbor reasoning\nAs with many things in data mining, different terms exist for nearest-neighbor classifiers, in part because similar ideas were pursued independently. Nearest-neighbor classifiers were established long ago in statistics and pattern recognition (Cover & Hart,\n1967). The idea of classifying new instances directly by consulting a database (a “mem150\n\n|\n\nChapter 6: Similarity, Neighbors, and Clusters", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00175", "page_num": 175, "segment": "ory”) of instances has been termed instance-based learning (Aha, Kibler, & Albert, 1991)\nand memory-based learning (Lin & Vitter, 1994). Because no model is built during\n“training” and most effort is deferred until instances are retrieved, this general idea is\nknown as lazy learning (Aha, 1997).\n\nA related technique in artificial intelligence is Case-Based Reasoning (Kolodner, 1993;\nAamodt & Plaza, 1994), abbreviated CBR. Past cases are commonly used by doctors and\nlawyers to reason about new cases, so case-based reasoning has a well-established history\nin these fields.\n\nHowever, there are also significant differences between case-based reasoning and\nnearest-neighbor methods. Cases in CBR are typically not simple feature vector instances but instead are very detailed summaries of an episode, including items such as a\npatient’s symptoms, medical history, diagnosis, treatment, and outcome; or the details\nof a legal case including plaintiff and defendant arguments, precedents cited, and judgement. Because cases are so detailed, in CBR they are used not just to provide a class label\nbut to provide diagnostic and planning information that can be used to deal with the\ncase after it is retrieved. Adapting historical cases to be used in a new situation is usually\na complex process that requires significant effort.\n\nGeometric Interpretation, Overfitting, and Complexity Control\nAs with other models we’ve seen, it is instructive to visualize the classification regions\ncreated by a nearest-neighbor method. Although no explicit boundary is created, there\nare implicit regions created by instance neighborhoods. These regions can be calculated\nby systematically probing points in the instance space, determining each point’s classification, and constructing the boundary where classifications change.\n\nFigure 6-3. Boundaries created by a 1-NN classifier.\n\nNearest-Neighbor Reasoning\n\n|\n\n151", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00176", "page_num": 176, "segment": "Figure 6-3 illustrates such a region created by a 1-NN classifier around the instances of\nour “Write-off ” domain. Compare this with the classification tree regions from\nFigure 3-15 and the regions created by the linear boundary in Figure 4-3.\n\nNotice that the boundaries are not lines, nor are they even any recognizable geometric\nshape; they are erratic and follow the frontiers between training instances of different\nclasses. The nearest-neighbor classifier follows very specific boundaries around the\ntraining instances. Note also the one negative instance isolated inside the positive instances creates a “negative island” around itself. This point might be considered noise\nor an outlier, and another model type might smooth over it.\n\nSome of this sensitivity to outliers is due to the use of a 1-NN classifier, which retrieves\nonly single instances, and so has a more erratic boundary than one that averages multiple\nneighbors. We will return to that in a minute. More generally, irregular concept boundaries are characteristic of all nearest-neighbor classifiers, because they do not impose\nany particular geometric form on the classifier. Instead, they form boundaries in instance space tailored to the specific data used for training.\n\nThis should recall our discussions of overfitting and complexity control from Chapter 5. If you’re thinking that 1-NN must overfit very strongly, then you are correct. In\nfact, think about what would happen if you evaluated a 1-NN classifier on the training\ndata. When classifying each training data point, any reasonable distance metric would\nlead to the retrieval of that training point itself as its own nearest neighbor! Then its\nown value for the target variable would be used to predict itself, and voilà, perfect classification. The same goes for regression. The 1-NN memorizes the training data. It does\na little better than our strawman lookup table from the beginning of Chapter 5, though.\nSince the lookup table did not have any notion of similarity, it simply predicted perfectly\nfor exact training examples, and gave some default prediction for all others. The 1-NN\nclassifier predicts perfectly for training examples, but it also can make an often reasonable prediction on other examples: it uses the most similar training example.\n\nThus, in terms of overfitting and its avoidance, the k in a k-NN classifier is a complexity\nparameter. At one extreme, we can set k = n and we do not allow much complexity at\nall in our model. As described previously, the n-NN model (ignoring similarity weighting) simply predicts the average value in the dataset for each case. At the other extreme,\nwe can set k = 1, and we will get an extremely complex model, which places complicated\nboundaries such that every training example will be in a region labeled by its own class.\n\nNow let’s return to an earlier question: how should one choose k? We can use the same\nprocedure discussed in “A General Method for Avoiding Overfitting” on page 134 for\nsetting other complexity parameters: we can conduct cross-validation or other nested\nholdout testing on the training set, for a variety of different values of k, searching for\none that gives the best performance on the training data. Then when we have chosen a\nvalue of k, we build a k-NN model from the entire training set. As discussed in detail in\nChapter 5, since this procedure only uses the training data, we can still evaluate it on\n\n152\n\n|\n\nChapter 6: Similarity, Neighbors, and Clusters", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00177", "page_num": 177, "segment": "the test data and get an unbiased estimate of its generalization performance. Data mining\ntools usually have the ability to do such nested cross-validation to set k automatically.\n\nFigure 6-4 and Figure 6-5 show different boundaries created by nearest-neighbor classifiers. Here a simple three-class problem is classified using different numbers of neighbors. In Figure 6-4, only a single neighbor is used, and the boundaries are erratic and\nvery specific to the training examples in the dataset. In Figure 6-5, 30 nearest neighbors\nare averaged to form a classification. The boundaries are obviously different from\nFigure 6-4 and are much less jagged. Note, however, that in neither case are the boundaries smooth curves or regular piecewise geometric regions that we would expect to see\nwith a linear model or a tree-structured model. The boundaries for k-NN are more\nstrongly defined by the data.\n\nFigure 6-4. Classification boundaries created on a three-class problem created by 1-NN\n(single nearest neighbor).\n\nNearest-Neighbor Reasoning\n\n|\n\n153", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00178", "page_num": 178, "segment": "Figure 6-5. Classification boundaries created on a three-class problem created by 30NN (averaging 30 nearest neighbors).\n\nIssues with Nearest-Neighbor Methods\nBefore concluding a discussion of nearest-neighbor methods as predictive models, we\nshould mention several issues regarding their use. These often come into play in realworld applications.\n\nIntelligibility\n\nIntelligibility of nearest-neighbor classifiers is a complex issue. As mentioned, in some\nfields such as medicine and law, reasoning about similar historical cases is a natural way\nof coming to a decision about a new case. In such fields, a nearest-neighbor method\nmay be a good fit. In other areas, the lack of an explicit, interpretable model may pose\na problem.\n\nThere are really two aspects to this issue of intelligibility: the justification of a specific\ndecision and the intelligibility of an entire model.\n\nWith k-NN, it usually is easy to describe how a single instance is decided: the set of\nneighbors participating in the decision can be presented, along with their contributions.\nThis was done for the example involving the prediction of whether David would respond, earlier in Table 6-1. Some careful phrasing and judicious presentation of nearest\n\n154\n\n|\n\nChapter 6: Similarity, Neighbors, and Clusters", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00179", "page_num": 179, "segment": "neighbors are useful. For example, Netflix uses a form of nearest-neighbor classification\nfor their recommendations, and explains their movie recommendations with sentences\nlike:\n\n“The movie Billy Elliot was recommended based on your interest in Amadeus, The Constant Gardener and Little Miss Sunshine”\n\nAmazon presents recommendations with phrases like: “Customers with similar searches purchased...” and “Related to Items You’ve Viewed.”\n\nWhether such justifications are adequate depends on the application. An Amazon customer may be satisfied with such an explanation for why she got a recommendation.\nOn the other hand, a mortgage applicant may not be satisfied with the explanation, “We\ndeclined your mortgage application because you remind us of the Smiths and the\nMitchells, who both defaulted.” Indeed, some legal regulations restrict the sorts of models that can be used for credit scoring to models for which very simple explanations can\nbe given based on specific, important variables. For example, with a linear model, one\nmay be able to say: “all else being equal, if your income had been $20,000 higher you\nwould have been granted this particular mortgage.”\n\nIt also is easy to explain how the entire nearest-neighbor model generally decides new\ncases. The idea of finding the most similar cases and looking at how they were classified,\nor what value they had, is intuitive to many.\n\nWhat is difficult is to explain more deeply what “knowledge” has been mined from the\ndata. If a stakeholder asks “What did your system learn from the data about my customers? On what basis does it make its decisions?” there may be no easy answer because\nthere is no explicit model. Strictly speaking, the nearest-neighbor “model” consists of\nthe entire case set (the database), the distance function, and the combining function. In\ntwo dimensions we can visualize this directly as we did in the prior figures. However,\nthis is not possible when there are many dimensions. The knowledge embedded in this\nmodel is not usually understandable, so if model intelligibility and justification are critical, nearest-neighbor methods should be avoided.\n\nDimensionality and domain knowledge\n\nNearest-neighbor methods typically take into account all features when calculating the\ndistance between two instances. “Heterogeneous Attributes” on page 157 below discusses\none of the difficulties with attributes: numeric attributes may have vastly different ranges, and unless they are scaled appropriately the effect of one attribute with a wide range\ncan swamp the effect of another with a much smaller range. But apart from this, there\nis a problem with having too many attributes, or many that are irrelevant to the similarity\njudgment.\n\nFor example, in the credit card offer domain, a customer database could contain much\nincidental information such as number of children, length of time at job, house size,\nmedian income, make and model of car, average education level, and so on. Conceivably\n\nNearest-Neighbor Reasoning\n\n|\n\n155", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00180", "page_num": 180, "segment": "some of these could be relevant to whether the customer would accept the credit card\noffer, but probably most would be irrelevant. Such problems are said to be highdimensional---they suffer from the so-called curse of dimensionality---and this poses\nproblems for nearest neighbor methods. Much of the reason and effects are quite technical,3 but roughly, since all of the attributes (dimensions) contribute to the distance\ncalculations, instance similarity can be confused and misled by the presence of too many\nirrelevant attributes.\n\nThere are several ways to fix the problem of many, possibly irrelevant attributes. One is\nfeature selection, the judicious determination of features that should be included in the\ndata mining model. Feature selection can be done manually by the data miner, using\nbackground knowledge as what attributes are relevant. This is one of the main ways in\nwhich a data mining team injects domain knowledge into the data mining process. As\ndiscussed in Chapter 3 and Chapter 5 there are also automated feature selection methods\nthat can process the data and make judgments about which attributes give information\nabout the target.\n\nAnother way of injecting domain knowledge into similarity calculations is to tune the\nsimilarity/distance function manually. We may know, for example, that the attribute\nNumber of Credit Cards should have a strong influence on whether a customer accepts\nan offer for another one. A data scientist can tune the distance function by assigning\ndifferent weights to the different attributes (e.g., giving a larger weight to Number of\nCredit Cards). Domain knowledge can be added not only because we believe we know\nwhat will be more predictive, but more generally because we know something about the\nsimilar entities we want to find. When looking for similar whiskeys, I may know that\n“peatiness” is important to my judging a single malt as tasting similar, so I could give\npeaty a higher weight in the similarity calculation. If another taste variable is unimportant, I could remove it or simply give it a low weight.\n\nComputational efficiency\n\nOne benefit of nearest-neighbor methods is that training is very fast because it usually\ninvolves only storing the instances. No effort is expended in creating a model. The main\ncomputational cost of a nearest neighbor method is borne by the prediction/classification step, when the database must be queried to find nearest neighbors of a new instance.\nThis can be very expensive, and the classification expense should be a consideration.\nSome applications require extremely fast predictions; for example, in online advertisement targeting, decisions may need to be made in a few tens of milliseconds. For such\napplications, a nearest neighbor method may be impractical.\n\n3. For example, it turns out that for technical reasons, with large numbers of features, certain particular instances\nappear extremely frequently in other instances’ sets of k nearest neighbors. These particular instances thereby\nhave a very large influence on many classifications.\n\n156\n\n|\n\nChapter 6: Similarity, Neighbors, and Clusters", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00181", "page_num": 181, "segment": "There are techniques for speeding up neighbor retrievals. Specialized\ndata structures like kd-trees and hashing methods (Shakhnarovich,\nDarrell, & Indyk, 2005; Papadopoulos & Manolopoulos, 2005) are\nemployed in some commerical database and data mining systems to\nmake nearest neighbor queries more efficient. However, be aware that\nmany small-scale and research data mining tools usually do not employ such techniques, and still rely on naive brute-force retrieval.\n\nSome Important Technical Details Relating to Similarities\nand Neighbors\n\nHeterogeneous Attributes\nUp to this point we have been using Euclidean distance, showing that it was easy to\ncalculate. If attributes are numeric and are directly comparable, the distance calculation\nis indeed straightforward. When examples contain complex, heterogeneous attributes\nthings become more complicated. Consider another example in the same domain but\nwith a few more attributes:\n\nAttribute\n\nSex\n\nAge\n\nYears at current address\n\nResidential status (1=Owner, 2=Renter, 3=Other)\n\nIncome\n\nPerson A Person B\n\nMale\n\nFemale\n\n23\n\n2\n\n2\n\n40\n\n10\n\n1\n\n50,000\n\n90,000\n\nSeveral complications now arise. First, the equation for Euclidean distance is numeric,\nand Sex is a categorical (symbolic) attribute. It must be encoded numerically. For binary\nvariables, a simple encoding like M=0, F=1 may be sufficient, but if there are multiple\nvalues for a categorical attribute this will not be good enough.\n\nAlso important, we have variables that, though numeric, have very different scales and\nranges. Age might have a range from 18 to 100, while Income might have a range from\n$10 to $10,000,000. Without scaling, our distance metric would consider ten dollars of\nincome difference to be as significant as ten years of age difference, and this is clearly\nwrong. For this reason nearest-neighbor-based systems often have variable-scaling front\nends. They measure the ranges of variables and scale values accordingly, or they apportion values to a fixed number of bins. The general principle at work is that care must\nbe taken that the similarity/distance computation is meaningful for the application.\n\nSome Important Technical Details Relating to Similarities and Neighbors\n\n|\n\n157", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00182", "page_num": 182, "segment": "* Other Distance Functions\n\nFor simplicity, up to this point we have used only a single metric, Euclidean distance. Here we include more details about distance functions\nand some alternatives.\n\nIt is important to note that the similarity measures presented here represent only a tiny fraction of all the similarity measures that have been\nused. These ones are particularly popular, but both the data scientist and\nthe business analyst should keep in mind that it is important to use a\nmeaningful similarity metric with respect to the business problem at\nhand. This section may be skipped without loss of continuity.\n\nAs noted previously, Euclidean distance is probably the most widely used distance metric in data science. It is general, intuitive and computationally very fast. Because it employs the squares of the distances along each individual dimension, it is sometimes\ncalled the L2 norm and sometimes represented by || · ||2. Equation 6-2 shows how it\nlooks formally.\n\nEquation 6-2. Euclidean distance (L2 norm)\n\ndEuclidean(, ) = ∥ - ∥ 2 = (x1 - y1)2 + (x2 - y2)2 + ⋯\n\nThough Euclidean distance is widely used, there are many other distance calculations.\nThe Dictionary of Distances by Deza & Deza (Elsevier Science, 2006) lists several hundred, of which maybe a dozen or so are used regularly for mining data. The reason there\nare so many is that in a nearest-neighbor method the distance function is critical. It\nbasically reduces a comparison of two (potentially complex) examples into a single\nnumber. The data types and specifics of the domain of application greatly influence how\nthe differences in individual attributes should combine.\n\nThe Manhattan distance or L1-norm is the sum of the (unsquared) pairwise distances,\nas shown in Equation 6-3.\n\nEquation 6-3. Manhattan distance (L1 norm)\n\ndManhattan(, ) = ∥ - ∥ 1 = | x1 - y1 | + | x2 - y2 | + ⋯\n\nThis simply sums the differences along the different dimensions between X and Y. It is\ncalled Manhattan (or taxicab) distance because it represents the total street distance you\nwould have to travel in a place like midtown Manhattan (which is arranged in a grid)\n\n158\n\n|\n\nChapter 6: Similarity, Neighbors, and Clusters", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00183", "page_num": 183, "segment": "to get between two points---the total east-west distance traveled plus the total northsouth distance traveled.\n\nResearchers studying the whiskey analytics problem introduced above used another\ncommon distance metric.4 Specifically, they used Jaccard distance. Jaccard distance\ntreats the two objects as sets of characteristics. Thinking about the objects as sets allows\none to think about the size of the union of all the characteristics of two objects X and\nY, |X ∪ Y|, and the size of the set of characteristics shared by the two objects (the intersection), |X ∩ Y|. Given two objects, X and Y, the Jaccard distance is the proportion\nof all the characteristics (that either has) that are shared by the two. This is appropriate\nfor problems where the possession of a common characteristic between two items is\nimportant, but the common absence of a characteristic is not. For example, in finding\nsimilar whiskeys it is significant if two whiskeys are both peaty, but it may not be significant that they are both not salty. In set notation, the Jaccard distance metric is shown\nin Equation 6-4.\n\nEquation 6-4. Jaccard distance\n\ndJaccard(X , Y ) = 1 -\n\n| X ∩ Y |\n| X ∪ Y |\n\nCosine distance is often used in text classification to measure the similarity of two documents. It is defined in Equation 6-5.\n\nEquation 6-5. Cosine distance\n\ndcosine(, ) = 1 -\n\n ·\n∥ ∥ 2 · ∥ ∥ 2\n\nwhere ||·||2 again represents the L2 norm, or Euclidean length, of each feature vector\n(for a vector this is simply the distance from the origin).\n\nThe information retrieval literature more commonly talks about\ncosine similarity, which is simply the fraction in Equation 6-5.\nAlternatively, it is 1 -- cosine distance.\n\n4. See Lapointe and Legendre (1994), Section 3 (“Classification of Pure Malt Scotch Whiskies”), for a detailed\n\ndiscussion of how they engineered their problem formulation. Available here.\n\nSome Important Technical Details Relating to Similarities and Neighbors\n\n|\n\n159", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00184", "page_num": 184, "segment": "In text classification, each word or token corresponds to a dimension, and the location\nof a document along each dimension is the number of occurrences of the word in that\ndocument. For example, suppose document A contains seven occurrences of the word\nperformance, three occurrences of transition, and two occurrences of monetary. Document B contains two occurrences of performance, three occurrences of transition, and\nno occurrences of monetary. The two documents would be represented as vectors of\ncounts of these three words: A = <7,3,2> and B = <2,3,0>. The cosine distance of the\ntwo documents is:\n\ndcosine(A, B) =\n\n1 -\n\n=\n\n=\n\n1 -\n\n1 -\n\n7, 3, 2 · 2, 3, 0\n∥ 7, 3, 2 ∥ 2 · ∥ 2, 3, 0 ∥ 2\n7 · 2 + 3 · 3 + 2 · 0\n\n49 + 9 + 4 · 4 + 9\n\n23\n28.4\n\n≈ 0.19\n\nCosine distance is particularly useful when you want to ignore differences in scale across\ninstances---technically, when you want to ignore the magnitude of the vectors. As a\nconcrete example, in text classification you may want to ignore whether one document\nis much longer than another, and just concentrate on the textual content. So in our\nexample above, suppose we have a third document, C, which has seventy occurrences\nof the word performance, thirty occurrences of transition, and twenty occurrences of\nmonetary. The vector representing C would be C = <70, 30, 20>. If you work through\nthe math you’ll find that the cosine distance between A and C is zero---because C is\nsimply A multiplied by 10.\n\nAs a final example illustrating the variety of distance metrics, let’s again consider text\nbut in a very different way. Sometimes you may want to measure the distance between\ntwo strings of characters. For example, often a business application needs to be able to\njudge when two data records correspond to the same person. Of course, there may be\nmisspellings. We would want to be able to say how similar two text fields are. Let’s say\nwe have two strings:\n\n1. 1113 Bleaker St.\n\n2. 113 Bleecker St.\n\nWe want to determine how similar these are. For this purpose, another type of distance\nfunction is useful, called edit distance or the Levenshtein metric. This metric counts the\nminimum number of edit operations required to convert one string into the other, where\nan edit operation consists of either inserting, deleting, or replacing a character (one\ncould choose other edit operators). In the case of our two strings, the first could be\ntransformed into the second with this sequence of operations:\n\n160\n\n|\n\nChapter 6: Similarity, Neighbors, and Clusters", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00185", "page_num": 185, "segment": "1. Delete a 1,\n\n2. Insert a c, and\n\n3. Replace an a with an e.\n\nSo these two strings have an edit distance of three. We might compute a similar edit\ndistance calculation for other fields, such as name (thereby dealing with missing middle\ninitials, for example), and then calculate a higher-level similarity that combines the\nvarious edit-distance similarities.\n\nEdit distance is also used commonly in biology where it is applied to\nmeasure the genetic distance between strings of alleles. In general, edit\ndistance is a common choice when data items consist of strings or\nsequences where order is very important.\n\n* Combining Functions: Calculating Scores from Neighbors\n\nFor completeness, let us also briefly discuss “combining functions”---\nthe formulas used for calculating the prediction of an instance from a set\nof the instance’s nearest neighbors.\n\nWe began with majority voting, a simple strategy. This decision rule can be seen in\nEquation 6-6:\n\nEquation 6-6. Majority vote classification\n\nc() = arg max\nc∈classes\n\nscore(c, neighborsk())\n\nHere neighborsk(x) returns the k nearest neighbors of instance x, arg max returns the\nargument (c in this case) that maximizes the quantity that follows it, and the score\nfunction is defined, shown in Equation 6-7.\n\nEquation 6-7. Majority scoring function\n\nscore(c, N ) = ∑\n\nclass() = c\n\n∈N\n\nHere the expression [class(y)=c] has the value one if class(y) = c and zero otherwise.\n\nSome Important Technical Details Relating to Similarities and Neighbors\n\n|\n\n161", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00186", "page_num": 186, "segment": "Similarity-moderated voting, discussed in “How Many Neighbors and How Much Influence?” on page 149, can be accomplished by modifying Equation 6-6 to incorporate\na weight, as shown in Equation 6-8.\n\nEquation 6-8. Similarity-moderated classification\n\nscore(c, N ) = ∑\n\nw(, ) × class() = c\n\n∈N\n\nwhere w is a weighting function based on the similarity between examples x and y. The\ninverse of the square of the distance is commonly used:\n\nw(, ) =\n\n1\ndist(, )2\n\nwhere dist is whatever distance function is being used in the domain.\n\nIt is straightforward to alter Equation 6-6 and Equation 6-8 to produce a score that can\nbe used as a probability estimate. Equation 6-8 already produces a score so we just have\nto scale it by the total scores contributed by all neighbors so that it is between zero and\none, as shown in Equation 6-9.\n\nEquation 6-9. Similarity-moderated scoring\n\np(c | ) =\n\n∑\n∈neighbors()\n\nw(, ) × class() = c\n\n∑\n∈neighbors()\n\nw(, )\n\nFinally, with one more step we can generalize this equation to do regression. Recall that\nin a regression problem, instead of trying to estimate the class of a new instance x we\nare trying to estimate some value f(x) given the f values of the neighbors of x. We can\nsimply replace the bracketed class-specific part of Equation 6-9 with numeric values.\nThis will estimate the regression value as the weighted average of the neighbors’ target\nvalues (although depending on the application, alternative combining functions might\nbe sensible, such as the median).\n\nEquation 6-10. Similarity-moderated regression\n\nf () =\n\n∑\n∈neighbors()\n∑\n∈neighbors()\n\nw(, ) × t()\n\nw(, )\n\n162\n\n|\n\nChapter 6: Similarity, Neighbors, and Clusters", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00187", "page_num": 187, "segment": "where t( y ) is the target value for example y.\n\nSo, for example, for estimating the expected spending of a prospective customer with a\nparticular set of characteristics, Equation 6-10 would estimate this amount as the\ndistance-weighted average of the neighbors’ historical spending amounts.\n\nClustering\nAs noted at the beginning of the chapter, the notions of similarity and distance underpin\nmuch of data science. To increase our appreciation of this, let’s look at a very different\nsort of task. Recall the first application of data science that we looked at deeply: supervised segmentation---finding groups of objects that differ with respect to some target\ncharacteristic of interest. For example, find groups of customers that differ with respect\nto their propensity to leave the company when their contracts expire. Why, in talking\nabout supervised segmentation, do we always use the modifier “supervised”?\n\nIn other applications we may want to find groups of objects, for example groups of\ncustomers, but not driven by some prespecified target characteristic. Do our customers\nnaturally fall into different groups? This may be useful for many reasons. For example,\nwe may want to step back and consider our marketing efforts more broadly. Do we\nunderstand who our customers are? Can we develop better products, better marketing\ncampaigns, better sales methods, or better customer service by understanding the natural subgroups? This idea of finding natural groupings in the data may be called unsupervised segmentation, or more simply clustering.\n\nClustering is another application of our fundamental notion of similarity. The basic idea\nis that we want to find groups of objects (consumers, businesses, whiskeys, etc.), where\nthe objects within groups are similar, but the objects in different groups are not so\nsimilar.\n\nSupervised modeling involves discovering patterns to predict the value of a specified target variable, based on data where we know the\nvalues of the target variable. Unsupervised modeling does not focus\non a target variable. Instead it looks for other sorts of regularities in a\nset of data.\n\nExample: Whiskey Analytics Revisited\nBefore getting into details, let’s revisit our example problem of whiskey analytics. We\ndiscussed using similarity measures to find similar single malt scotch whiskeys. Why\nmight we want to take a step further and find clusters of similar whiskeys?\n\nOne reason we might want to find clusters of whiskeys is simply to understand the\nproblem better. This is an example of exploratory data analysis, to which data-rich\n\nClustering\n\n|\n\n163", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00188", "page_num": 188, "segment": "businesses should continually devote some energy and resources, as such exploration\ncan lead to useful and profitable discoveries. In our example, if we are interested in\nScotch whiskeys, we may simply want to understand the natural groupings by taste---\nbecause we want to understand our “business,” which might lead to a better product or\nservice. Let’s say that we run a small shop in a well-to-do neighborhood, and as part of\nour business strategy we want to be known as the place to go for single-malt scotch\nwhiskeys. We may not be able to have the largest selection, given our limited space and\nability to invest in inventory, but we might choose a strategy of having a broad and\neclectic collection. If we understood how the single malts grouped by taste, we could\n(for example) choose from each taste group a popular member and a lesser-known\nmember. Or an expensive member and a more affordable member. Each of these is based\non having a good understanding of how the whiskeys group by taste.\n\nLet’s now talk about clustering more generally. We will present the two main sorts of\nclustering, illustrating the concept of similarity in action. In the process, we can examine\nactual clusters of whiskeys.\n\nHierarchical Clustering\nLet’s start with a very simple example. At the top of Figure 6-6 we see six points, A-F,\narranged on a plane (i.e., a two-dimensional instance space). Using Euclidean distance\nrenders points more similar to each other if they are closer to each other in the plane.\nCircles labeled 1-5 are placed over the points to indicate clusters. This diagram shows\nthe key aspects of what is called “hierarchical” clustering. It is a clustering because it\ngroups the points by their similarity. Notice that the only overlap between clusters is\nwhen one cluster contains other clusters. Because of this structure, the circles actually\nrepresent a hierarchy of clusterings. The most general (highest-level) clustering is just\nthe single cluster that contains everything---cluster 5 in the example. The lowest-level\nclustering is when we remove all the circles, and the points themselves are six (trivial)\nclusters. Removing circles in decreasing order of their numbers in the figure produces\na collection of different clusterings, each with a larger number of clusters.\n\nThe graph on the bottom of the figure is called a dendrogram, and it shows explicitly\nthe hierarchy of the clusters. Along the x axis are arranged (in no particular order except\nto avoid line crossings) the individual data points. The y axis represents the distance\nbetween the clusters (we’ll talk more about that presently). At the bottom (y = 0) each\npoint is in a separate cluster. As y increases, different groupings of clusters fall within\nthe distance constraint: first A and C are clustered together, then B and E are merged,\nthen the BE cluster is merged with D, and so on, until all clusters are merged at the top.\nThe numbers at the joins of the dendrograms correspond to the numbered circles in\nthe top diagram.\n\n164\n\n|\n\nChapter 6: Similarity, Neighbors, and Clusters", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00189", "page_num": 189, "segment": "Figure 6-6. Six points and their possible clusterings. At top are shown six points, A-F,\nwith circles 1-5 showing different distance-based groupings that could be imposed.\nThese groups form an implicit hierarchy. At the bottom is a dendrogram corresponding\nto the groupings, which makes the hierarchy explicit.\n\nBoth parts of Figure 6-6 show that hierarchical clustering doesn’t just create “a clustering,” or a single set of groups of objects. It creates a collection of ways to group the points.\nTo see this clearly, consider “clipping” the dendrogram with a horizontal line, ignoring\neverything above the line. As the line moves downward, we get different clusterings\n\nClustering\n\n|\n\n165", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00190", "page_num": 190, "segment": "with increasing numbers of clusters, as shown in the figure. Clip the dendrogram at the\nline labeled “2 clusters,” and below that we see two different groups; here, the singleton\npoint F and the group containing all the other points. Referring back to the top part of\nthe figure, we see that indeed F stands apart from the rest. Clipping the dendrogram at\nthe 2-cluster point corresponds to removing circle 5. If we move down to the horizonal\nline labeled “3 clusters,” and clip the dendrogram there, we see that the dendrogram is\nleft with three groups below the line (AC, BED, F), which corresponds in the plot to\nremoving circles 5 and 4, and we then see the same three clusters. Intuitively, the clusters\nmake sense. F is still off by itself. A and C form a close group. B, E, and D form a close\ngroup.\n\nAn advantage of hierarchical clustering is that it allows the data analyst to see the\ngroupings---the “landscape” of data similarity---before deciding on the number of clusters to extract. As shown by the horizontal dashed lines, the diagram can be cut across\nat any point to give any desired number of clusters. Note also that once two clusters are\njoined at one level, they remain joined in all higher levels of the hierarchy.\n\nHierarchical clusterings generally are formed by starting with each node as its own\ncluster. Then clusters are merged iteratively until only a single cluster remains. The\nclusters are merged based on the similarity or distance function that is chosen. So far\nwe have discussed distance between instances. For hierarchical clustering, we need a\ndistance function between clusters, considering individual instances to be the smallest\nclusters. This is sometimes called the linkage function. So, for example, the linkage\nfunction could be “the Euclidean distance between the closest points in each of the\nclusters,” which would apply to any two clusters.\n\nNote: Dendrograms\nTwo things can usually be noticed in a dendrogram. Because the y axis\nrepresents the distance between clusters, the dendrogram can give an\nidea of where natural clusters may occur. Notice in the dendrogram of\nFigure 6-6 there is a relatively long distance between cluster 3 (at about\n0.10) and cluster 4 (at about 0.17). This suggests that this segmentation of the data, yielding three clusters, might be a good division. Also\nnotice point F in the dendrogram. Whenever a single point merges\nhigh up in a dendrogram, this is an indication that it seems different\nfrom the rest, which we might call an “outlier,” and want to investigate it.\n\nOne of the best known uses of hierarchical clustering is in the “Tree of Life” (Sugden et\nal., 2003; Pennisi, 2003), a hierarchical phylogenetic chart of all life on earth. This chart\nis based on a hierarchical clustering of RNA sequences. A portion of a tree from the\nInteractive Tree of Life is shown in Figure 6-7 (Letunic & Bork, 2006). Large hierarchical\ntrees are often displayed radially to conserve space, as is done here. This diagram shows\n\n166\n\n|\n\nChapter 6: Similarity, Neighbors, and Clusters", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00191", "page_num": 191, "segment": "a global phylogeny (taxonomy) of fully sequenced genomes, automatically reconstructed by Francesca Ciccarelli and colleagues (2006). The center is the “last universal ancestor” of all life on earth, from which branch the three domains of life (eukaryota,\nbacteria, and archaea). Figure 6-8 shows a magnified portion of this tree containing the\nparticular bacterium Helicobacter pylori, which causes ulcers.\n\nFigure 6-7. The phylogenetic Tree of Life, a huge hierarchical clustering of species, displayed radially.\n\nClustering\n\n|\n\n167", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00192", "page_num": 192, "segment": "Figure 6-8. A portion of the Tree of Life.\n\nReturning to our example from the outset of the chapter, the top of Figure 6-9 shows,\nas a dendrogram, the 50 single malt Scotch whiskeys clustered using the methodology\ndescribed by Lapointe and Legendre (1994). By clipping the dendrogram we can obtain\nany number of clusters we would like, so for example, removing the top-most 11 connecting segments leaves us with 12 clusters.\n\nAt the bottom of Figure 6-9 is a close up of a portion of the hierarchy, focusing on Foster’s\nnew favorite, Bunnahabhain. Previously in “Example: Whiskey Analytics” on page 144\nwe retrieved whiskeys similar to it. This excerpt shows that most of its nearest neighbors\n(Tullibardine, Glenglassaugh, etc.) do indeed cluster near it in the hierarchy. (You may\nwonder why the clusters don’t correspond exactly to the similarity ranking. The reason\nis that, while the five whiskeys we found are the most similar to Bunnahabhain, some\nof these five are more similar to other whiskeys in the dataset, so they are clustered with\nthese closer neighbors before joining Bunnahabhain.)\n\nInterestingly from the point of view of whiskey classification, the groups of single malts\nresulting from this taste-based clustering do not correspond neatly with regions of\nScotland---the basis of the usual categorizations of Scotch whiskeys. There is a correlation, however, as Lapointe and Legendre (1994) point out.\n\n168\n\n|\n\nChapter 6: Similarity, Neighbors, and Clusters", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00193", "page_num": 193, "segment": "Figure 6-9. Hierarchical clustering of Scotch whiskeys. At top is the entire dendrogram;\nat bottom is a small excerpt including Bunnahabhain and neighbors.\n\nSo instead of simply stocking the most recognizable Scotches, or a few Highland, Lowland, and Islay brands, our specialty shop owner could choose to stock single malts from\nthe different clusters. Alternatively, one could create a guide to Scotch whiskeys that\nmight help single malt lovers to choose whiskeys.5 For example, since Foster loves the\nBunnahabhain recommended to him by his friend at the restaurant the other night, the\nclustering suggests a set of other “most similar” whiskeys (Bruichladdich, Tullibardine,\netc.) The most unusual tasting single malt in the data appears to be Aultmore, at the\nvery top, which is the last whiskey to join any others.\n\nNearest Neighbors Revisited: Clustering Around Centroids\nHierarchical clustering focuses on the similarities between the individual instances and\nhow similarities link them together. A different way of thinking about clustering data\nis to focus on the clusters themselves---the groups of instances. The most common\nmethod for focusing on the clusters themselves is to represent each cluster by its “cluster\ncenter,” or centroid. Figure 6-10 illustrates the idea in two dimensions: here we have\nthree clusters, whose instances are represented by the circles. Each cluster has a centroid,\nrepresented by the solid-lined star. The star is not necessarily one of the instances; it is\nthe geometric center of a group of instances. This same idea applies to any number of\ndimensions, as long as we have a numeric instance space and a distance measure (of\ncourse, we can’t visualize the clusters so nicely, if at all, in high-dimensional space).\n\nThe most popular centroid-based clustering algorithm is called k-means clustering\n(MacQueen, 1967; Lloyd, 1982; MacKay, 2003), and the main idea behind it deserves\nsome discussion as k-means clustering is mentioned frequently in data science. In kmeans the “means” are the centroids, represented by the arithmetic means (averages)\nof the values along each dimension for the instances in the cluster. So in Figure 6-10, to\ncompute the centroid for each cluster, we would average all the x values of the points in\nthe cluster to form the x coordinate of the centroid, and average all the y values to form\n\n5. This has been done: see David Wishart’s (2006) book Whisky Classified: Choosing Single Malts by Flavour\n\nPavilion.\n\nClustering\n\n|\n\n169", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00194", "page_num": 194, "segment": "the centroid’s y coordinate. Generally, the centroid is the average of the values for each\nfeature of each example in the cluster. The result is shown in Figure 6-11.\n\nFigure 6-10. The first step of the k-means algorithm: find the points closest to the chosen\ncenters (possibly chosen randomly). This results in the first set of clusters.\n\nThe k in k-means is simply the number of clusters that one would like to find in the\ndata. Unlike hierarchical clustering, k-means starts with a desired number of clusters\nk. So, in Figure 6-10, the analyst would have specified k=3, and the k-means clustering\nmethod would return (i) the three cluster centroids when cluster method terminates\n(the three solid-lined stars in Figure 6-11), plus (ii) information on which of the data\npoints belongs to each cluster. This is sometimes referred to as nearest-neighbor clustering because the answer to (ii) is simply that each cluster contains those points that\nare nearest to its centroid (rather than to one of the other centroids).\n\n170\n\n|\n\nChapter 6: Similarity, Neighbors, and Clusters", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00195", "page_num": 195, "segment": "Figure 6-11. The second step of the k-means algorithm: find the actual center of the\nclusters found in the first step.\n\nThe k-means algorithm for finding the clusters is simple and elegant, and therefore is\nworth mentioning. It is represented by Figure 6-10 and Figure 6-11. The algorithm starts\nby creating k initial cluster centers, usually randomly, but sometimes by choosing k of\nthe actual data points, or by being given specific initial starting points by the user, or\nvia a pre-processing of the data to determine a good set of starting centers (Arthur &\nVassilvitskii, 2007). Think of the stars in Figure 6-10 as being these initial (k=3) cluster\ncenters. Then the algorithm proceeds as follows. As shown in Figure 6-10, the clusters\ncorresponding to these cluster centers are formed, by determining which is the closest\ncenter to each point.\n\nNext, for each of these clusters, its center is recalculated by finding the actual centroid\nof the points in the cluster. As shown in Figure 6-11, the cluster centers typically shift;\nin the figure, we see that the new solid-lined stars are indeed closer to what intuitively\nseems to be the center of each cluster. And that’s pretty much it. The process simply\niterates: since the cluster centers have shifted, we need to recalculate which points belong\nto each cluster (as in Figure 6-10). Once these are reassigned, we might have to shift the\ncluster centers again. The k-means procedure keeps iterating until there is no change\nin the clusters (or possibly until some other stopping criterion is met).\n\nClustering\n\n|\n\n171", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00196", "page_num": 196, "segment": "Figure 6-12 and Figure 6-13 show an example run of k-means on 90 data points with\nk=3. This dataset is a little more realistic in that it does not have such well-defined\nclusters as in the previous example. Figure 6-12 shows the initial data points before\nclustering. Figure 6-13 shows the final results of clustering after 16 iterations. The three\n(erratic) lines show the path from each centroid’s initial (random) location to its final\nlocation. The points in the three clusters are denoted by different symbols (circles, x’s,\nand triangles).\n\nFigure 6-12. A k-means clustering example using 90 points on a plane and k=3 centroids. This figure shows the initial set of points.\n\nThere is no guarantee that a single run of the k-means algorithm will result in a good\nclustering. The result of a single clustering run will find a local optimum---a locally best\nclustering---but this will be dependent upon the initial centroid locations. For this reason, k-means is usually run many times, starting with different random centroids each\ntime. The results can be compared by examining the clusters (more on that in a minute),\nor by a numeric measure such as the clusters’ distortion, which is the sum of the squared\ndifferences between each data point and its corresponding centroid. In the latter case,\nthe clustering with the lowest distortion value can be deemed the best clustering.\n\n172\n\n|\n\nChapter 6: Similarity, Neighbors, and Clusters", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00197", "page_num": 197, "segment": "Figure 6-13. A k-means clustering example using 90 points on a plane and k=3 centroids. This figure shows the movement paths of centroids (each of three lines) through\n16 iterations of the clustering algorithm. The marker shape of each point represents the\ncluster identity to which it is finally assigned.\n\nIn terms of run time, the k-means algorithm is efficient. Even with multiple runs it is\ngenerally relatively fast, because it only computes the distances between each data point\nand the cluster centers on each interation. Hierarchical clustering is generally slower,\nas it needs to know the distances between all pairs of clusters on each iteration, which\nat the start is all pairs of data points.\n\nA common concern with centroid algorithms such as k-means is how to determine a\ngood value for k. One answer is simply to experiment with different k values and see\nwhich ones generate good results. Since k-means is often used for exploratory data\nmining, the analyst must examine the clustering results anyway to determine whether\nthe clusters make sense. Usually this can reveal whether the number of clusters is appropriate. The value for k can be decreased if some clusters are too small and overly\nspecific, and increased if some clusters are too broad and diffuse.\n\nFor a more objective measure, the analyst can experiment with increasing values of k\nand graph various metrics (sometimes obliquely called indices) of the quality of the\nresulting clusterings. As k increases the quality metrics should eventually stabilize or\nplateau, either bottoming out if the metric is to be minimized or topping out if maxiClustering\n\n|\n\n173", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00198", "page_num": 198, "segment": "mized. Some judgment will be required, but the minimum k where the stabilization\nbegins is often a good choice. Wikipedia’s article Determining the number of clusters in\na data set describes various metrics for evaluating sets of candidate clusters.\n\nExample: Clustering Business News Stories\nAs a concrete example of centroid-based clustering, consider the task of identifying\nsome natural groupings of business news stories released by a news aggregator. The\nobjective of this example is to identify, informally, different groupings of news stories\nreleased about a particular company. This may be useful for a specific application, for\nexample: to get a quick understanding of the news about a company without having to\nread every news story; to categorize forthcoming news stories for a news prioritization\nprocess; or simply to understand the data before undertaking a more focused data mining project, such as relating business news stories to stock performance.\n\nFor this example we chose a large collection of (text) news stories: the Thomson Reuters\nText Research Collection (TRC2), a corpus of news stories created by the Reuters news\nagency, and made available to researchers. The entire corpus comprises 1,800,370 news\nstories from January of 2008 through February of 2009 (14 months). To make the example tractable but still realistic, we’re going to extract only those stories that mention\na particular company---in this case, Apple (whose stock symbol is AAPL).\n\nData preparation\n\nFor this example, it is useful to discuss data preparation in a little detail, as we will be\ntreating text as data, and we have not previously discussed that. See Chapter 10 for more\ndetails on mining text.\n\nIn this corpus, large companies are always mentioned when they are the primary subject\nof a story, such as in earnings reports and merger announcements; but they are often\nmentioned peripherally in weekly business summaries, lists of active stocks, and stories\nmentioning significant events within their industry sectors. For example, many stories\nabout the personal computer industry mention how HP’s and Dell’s stock prices reacted\non that day even if neither company was involved in the event. For this reason, we\nextracted stories whose headlines specifically mentioned Apple---thus assuring that the\nstory is very likely news about Apple itself. There were 312 such stories but they covered\na wide variety of topics, as we shall see.\n\nPrior to clustering, the stories underwent basic web text preprocessing, with HTML and\nURLs stripped out and the text case-normalized. Words that occurred rarely (fewer than\ntwo documents) or too commonly (more than 50% documents) in the corpus were\neliminated, and the rest formed the vocabulary for the next step. Then each document\nwas represented by a numeric feature vector using “TFIDF scores” scoring for each\nvocabulary word in the document. TFIDF (Term Frequency times Inverse Document\n\n174\n\n|\n\nChapter 6: Similarity, Neighbors, and Clusters", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00199", "page_num": 199, "segment": "Frequency) scores represent the frequency of the word in the document, penalized by\nthe frequency of the word in the corpus. TFIDF is explained in detail later in Chapter 10.\n\nThe news story clusters\n\nWe chose to cluster the stories into nine groups (so k=9 for k-means). Here we present\na description of the clusters, along with some headlines of the stories contained in that\ncluster. It is important to remember that the entire news story was used in the clustering,\nnot just the headline.\n\nCluster 1. These stories are analysts’ announcements concerning ratings changes and\nprice target adjustments:\n\n• RBC RAISES APPLE <AAPL.O> PRICE TARGET TO $200 FROM $190; KEEPS OUT\n\nPERFORM RATING\n\n• THINKPANMURE ASSUMES APPLE <AAPL.O> WITH BUY RATING; $225 PRICE TARGET\n\n• AMERICAN TECHNOLOGY RAISES APPLE <AAPL.O> TO BUY FROM NEUTRAL\n\n• CARIS RAISES APPLE <AAPL.O> PRICE TARGET TO $200 FROM $170; RATING\n\nABOVE AVERAGE\n\n• CARIS CUTS APPLE <AAPL.O> PRICE TARGET TO $155 FROM $165; KEEPS ABOVE\n\nAVERAGE RATING\n\nCluster 2. This cluster contains stories about Apple’s stock price movements, during and\nafter each day of trading:\n\n• Apple shares pare losses, still down 5 pct\n\n• Apple rises 5 pct following strong results\n\n• Apple shares rise on optimism over iPhone demand\n\n• Apple shares decline ahead of Tuesday event\n\n• Apple shares surge, investors like valuation\n\nCluster 3. In 2008, there were many stories about Steve Jobs, Apple’s charismatic CEO,\nand his struggle with pancreatic cancer. Jobs’ declining health was a topic of frequent\ndiscussion, and many business stories speculated on how well Apple would continue\nwithout him. Such stories clustered here:\n\n• ANALYSIS-Apple success linked to more than just Steve Jobs\n\n• NEWSMAKER-Jobs used bravado, charisma as public face of Apple\n\n• COLUMN-What Apple loses without Steve: Eric Auchard\n\n• Apple could face lawsuits over Jobs' health\n\nClustering\n\n|\n\n175", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00200", "page_num": 200, "segment": "• INSTANT VIEW 1-Apple CEO Jobs to take medical leave\n\n• ANALYSIS-Investors fear Jobs-less Apple\n\nCluster 4. This cluster contains various Apple announcements and releases. Superficially, these stories were similar, though the specific topics varied:\n\n• Apple introduces iPhone \"push\" e-mail software\n\n• Apple CFO sees 2nd-qtr margin of about 32 pct\n\n• Apple says confident in 2008 iPhone sales goal\n\n• Apple CFO expects flat gross margin in 3rd-quarter\n\n• Apple to talk iPhone software plans on March 6\n\nCluster 5. This cluster’s stories were about the iPhone and deals to sell iPhones in other\ncountries:\n\n• MegaFon says to sell Apple iPhone in Russia\n\n• Thai True Move in deal with Apple to sell 3G iPhone\n\n• Russian retailers to start Apple iPhone sales Oct 3\n\n• Thai AIS in talks with Apple on iPhone launch\n\n• Softbank says to sell Apple's iPhone in Japan\n\nCluster 6. One class of stories reports on stock price movements outside of normal\ntrading hours (known as Before and After the Bell):\n\n• Before the Bell-Apple inches up on broker action\n\n• Before the Bell-Apple shares up 1.6 pct before the bell\n\n• BEFORE THE BELL-Apple slides on broker downgrades\n\n• After the Bell-Apple shares slip\n\n• After the Bell-Apple shares extend decline\n\nCentroid 7. This cluster contained little thematic consistency:\n\n• ANALYSIS-Less cheer as Apple confronts an uncertain 2009\n\n• TAKE A LOOK - Apple Macworld Convention\n\n• TAKE A LOOK-Apple Macworld Convention\n\n• Apple eyed for slim laptop, online film rentals\n\n• Apple's Jobs finishes speech announcing movie plan\n\n176\n\n|\n\nChapter 6: Similarity, Neighbors, and Clusters", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00201", "page_num": 201, "segment": "Cluster 8. Stories on iTunes and Apple’s position in digital music sales formed this cluster:\n\n• PluggedIn-Nokia enters digital music battle with Apple\n\n• Apple's iTunes grows to No. 2 U.S. music retailer\n\n• Apple may be chilling iTunes competition\n\n• Nokia to take on Apple in music, touch-screen phones\n\n• Apple talking to labels about unlimited music\n\nCluster 9. A particular kind of Reuters news story is a News Brief, which is usually just\na few itemized lines of very terse text (e.g. “• Says purchase new movies on itunes\nsame day as dvd release”). The contents of these New Briefs varied, but because of\ntheir very similar form they clustered together:\n\n• BRIEF-Apple releases Safari 3.1\n\n• BRIEF-Apple introduces ilife 2009\n\n• BRIEF-Apple announces iPhone 2.0 software beta\n\n• BRIEF-Apple to offer movies on iTunes same day as DVD release\n\n• BRIEF-Apple says sold one million iPhone 3G's in first weekend\n\nAs we can see, some of these clusters are interesting and thematically consistent while\nothers are not. Some are just collections of superficially similar text. There is an old\ncliché in statistics: Correlation is not causation, meaning that just because two things\nco-occur doesn’t mean that one causes another. A similar caveat in clustering could be:\nSyntactic similarity is not semantic similarity. Just because two things---particularly text\npassages---have common surface characteristics doesn’t mean they’re necessarily related\nsemantically. We shouldn’t expect every cluster to be meaningful and interesting. Nevertheless, clustering is often a useful tool to uncover structure in our data that we did\nnot foresee. Clusters can suggest new and interesting data mining opportunities.\n\nUnderstanding the Results of Clustering\nOnce we have formulated the instances and clustered them, then what? As we mentioned\nabove, the result of clustering is either a dendrogram or a set of cluster centers plus the\ncorresponding data points for each cluster. How can we understand the clustering? This\nis particularly important because clustering often is used in exploratory analysis, so the\nwhole point is to understand whether something was discovered, and if so, what?\n\nHow to understand clusterings and clusters depends on the sort of data being clustered\nand the domain of application, but there are several methods that apply broadly. We\nhave seen some of them in action already.\n\nClustering\n\n|\n\n177", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00202", "page_num": 202, "segment": "Consider our whiskey example. Our whiskey researchers Lapointe and Legendre cut\ntheir dendrogram into 12 clusters; here are two of them:\n\nGroup A\n\nScotches: Aberfeldy, Glenugie, Laphroaig, Scapa\n\nGroup H\n\nScotches: Bruichladdich, Deanston, Fettercairn, Glenfiddich, Glen Mhor, Glen\nSpey, Glentauchers, Ladyburn, Tobermory\n\nThus, to examine the clusters, we simply can look at the whiskeys in each cluster. That\nseems rather easy, but remember that this whiskey example was chosen as an illustration\nin a book. What is it about the application that allowed relatively easy examination of\nthe clusters (and thereby made it a good example in the book)? We might think, well,\nthere are only a small number of whiskeys in total; that allows us to actually look at them\nall. This is true, but it actually is not so critical. If we had had massive numbers of\nwhiskeys, we still could have sampled whiskeys from each cluster to show the composition of each.\n\nThe more important factor to understanding these clusters---at least for someone who\nknows a little about single malts---is that the elements of the cluster can be represented\nby the names of the whiskeys. In this case, the names of the data points are meaningful\nin and of themselves, and convey meaning to an expert in the field.\n\nThis gives us a guideline that can be applied to other applications. For example, if we\nare clustering customers of a large retailer, probably a list of the names of the customers\nin a cluster would have little meaning, so this technique for understanding the result of\nclustering would not be useful. On the other hand, if IBM is clustering business customers it may be that the names of the businesses (or at least many of them) carry\nconsiderable meaning to a manager or member of the sales force.\n\nWhat can we do in cases where we cannot simply show the names of our data points,\nor for which showing the names does not give sufficient understanding? Let’s look again\nat our whiskey clusters, but this time looking at more information on the clusters:\n\nGroup A\n\n• Scotches: Aberfeldy, Glenugie, Laphroaig, Scapa\n\n• The best of its class: Laphroaig (Islay), 10 years, 86 points\n\n• Average characteristics: full gold; fruity, salty; medium; oily, salty, sherry; dry\n\nGroup H\n\n• Scotches: Bruichladdich, Deanston, Fettercairn, Glenfiddich, Glen Mhor, Glen\n\nSpey, Glentauchers, Ladyburn, Tobermory\n\n• The best of its class: Bruichladdich (Islay), 10 years, 76 points\n\n178\n\n|\n\nChapter 6: Similarity, Neighbors, and Clusters", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00203", "page_num": 203, "segment": "• Average characteristics: white wyne, pale; sweet; smooth, light; sweet, dry,\n\nfruity, smoky; dry, light\n\nHere we see two additional pieces of information useful for understanding the results\nof clustering. First, in addition to listing out the members, an “exemplar” member is\nlisted. Here it is the “best of its class” whiskey, taken from Jackson (1989) (this additional\ninformation was not provided to the clustering algorithm). Alternatively, it could be the\nbest known or highest-selling whiskey in the cluster. These techniques could be especially useful when there are massive numbers of instances in each cluster, so randomly\nsampling some may not be as telling as carefully selecting exemplars. However, this still\npresumes that the names of the instances are meaningful. Our other example, clustering\nthe business news stories, shows a slight twist on this general idea: show exemplar stories\nand their headlines, because there the headlines can be meaningful summaries of the\nstories.\n\nThe example also illustrates a different way of understanding the result of the clustering:\nit shows the average characteristics of the members of the cluster---essentially, it shows\nthe cluster centroid. Showing the centroid can be applied to any clustering; whether it\nis meaningful depends on whether the data values themselves are meaningful.\n\n* Using Supervised Learning to Generate Cluster Descriptions\n\nThis section describes a way to automatically generate cluster descriptions. It is more complicated than the ones already discussed. It involves mixing unsupervised learning (the clustering) with supervised\nlearning in order to create differential descriptions of the clusters. If this\nchapter is your first introduction to clustering and unsupervised learning, this may seem confusing to you, so we’ve made it a starred (advanced material) chapter. It may be skipped without loss of continuity.\n\nHowever clustering was done, it provides us with a list of assignments indicating which\nexamples belong to which cluster. A cluster centroid, in effect, describes the average\ncluster member. The problem is that these descriptions may be very detailed and they\ndon’t tell us how the clusters differ. What we may want to know is, for each cluster, what\ndifferentiates this cluster from all the others? This is essentially what supervised learning\nmethods do so we can use them here.\n\nThe general strategy is this: we use the cluster assignments to label examples. Each\nexample will be given a label of the cluster it belongs to, and these can be treated as class\nlabels. Once we have a labeled set of examples, we run a supervised learning algorithm\non the example set to generate a classifier for each class/cluster. We can then inspect the\nclassifier descriptions to get a (hopefully) intelligible and concise description of the\n\nClustering\n\n|\n\n179", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00204", "page_num": 204, "segment": "corresponding cluster. The important thing to note is that these will be differential\ndescriptions: for each cluster, what differentiates it from the others?\n\nIn this section, from this point on we equate clusters with classes. We will use the terms\ninterchangeably.\n\nIn principle we could use any predictive (supervised) learning method for this, but what\nis important here is intelligibility: we’re going to use the learned classifier definition as\na cluster description so we want a model that will serve this purpose. “Trees as Sets of\nRules” on page 71 showed how rules could be extracted from classification trees, so this\nis a useful method for the task.\n\nThere are two ways to set up the classification task. We have k clusters so we could set\nup a k-class task (one class per cluster). Alternatively, we could set up a k separate\nlearning tasks, each trying to differentiate one cluster from all the other (k--1) clusters.\n\nWe’ll use the second approach on the whiskey-clustering task, using Lapointe and Legendre’s cluster assignments (Appendix A of A Classification of Pure Malt Scotch Whiskies). This gives us 12 whiskey clusters labeled A through L. We go back to our raw data\nand append each whiskey description with its cluster assignment. We’re going to use\nthe binary approach: choose each cluster in turn to classify against the others. We’ll\nchoose cluster J, which Lapointe and Legendre describe this way:\n\nGroup J\n\n• Scotches: Glen Albyn, Glengoyne, Glen Grant, Glenlossie, Linkwood, North\n\nPort, Saint Magdalene, Tamdhu\n\n• The best of its class: Linkwood (Speyside), 12 years, 83\n\n• Average characteristics: full gold; dry, peaty, sherry; light to medium, round;\n\nsweet; dry\n\nYou may recall from “Example: Whiskey Analytics” on page 144 that each whiskey is\ndescribed using 68 binary features. The dataset now has a label (J or not_J) for each\nwhiskey indicating whether it belongs to the J cluster. An excerpt of the dataset looks\nlike this:\n\n0,0,0,...,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,J % Glen Grant\n0,0,0,...,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,not_J % Glen Keith\n0,0,0,...,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,not_J % Glen Mhor\n\nThe text after the “%” is a comment indicating the name of the whiskey.\n\nThis dataset is passed to a classification tree learner.6 The result is shown in Figure 6-14.\n\n6. Specifically, the J48 procedure of Weka with pruning turned off.\n\n180\n\n|\n\nChapter 6: Similarity, Neighbors, and Clusters", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00205", "page_num": 205, "segment": "Figure 6-14. The decision tree learned from cluster J on the Scotches data. In this tree,\nthe right branches from each node correspond to the presence of the node’s attribute (a\nvalue of 1 in the binary representation). The left branches correspond to the absence of\nthe node’s attribute (a value of 0). So, the leftmost leaf corresponds to the segment of the\npopulation with round body and sherry nose, and the whiskeys in this segment are\nmostly from cluster J.\n\nFrom this tree we concentrate only on the leaves labeled J (ignoring the ones labeled\nnot_J). There are only two such leaves. Tracing paths from the root to these leaves, we\ncan extract the two rules:\n\n1. (ROUND_BODY = 1) AND (NOSE_SHERRY = 1) ⇒ J\n\n2. (ROUND_BODY = 0) AND (color_red = 0) AND (color_f.gold = 1) AND\n\n(BODY_light = 1) AND (FIN_dry = 1) ⇒ J\n\nTranslating these loosely into English, the J cluster is distinguished by Scotches having\neither:\n\n1. A round body and a sherry nose, or\n\nClustering\n\n|\n\n181", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00206", "page_num": 206, "segment": "2. A full gold (but not red) color with a light (but not round) body and a dry finish.\n\nIs this description of cluster J better than the one given by Lapointe and Legendre, above?\nYou can decide which you prefer, but it’s important to point out they are different types\nof descriptions. Lapointe and Legendre’s is a characteristic description; it describes what\nis typical or characteristic of the cluster, ignoring whether other clusters might share\nsome of these characteristics. The onethe decision tree is a differential\ndescription; it describes only what differentiates this cluster from the others, ignoring\nthe characteristics that may be shared by whiskeys within it. To put it another way:\ncharacteristic descriptions concentrate on intergroup commonalities, whereas differential descriptions concentrate on intragroup differences. Neither is inherently better\n---it depends on what you’re using it for.\n\nStepping Back: Solving a Business Problem Versus Data\nExploration\nWe now have seen various examples of our fundamental concepts of data science in\naction. You may have realized that the clustering examples seem somehow different\nfrom the predictive modeling examples, and even the examples of finding similar objects. Let’s examine why.\n\nIn our predictive modeling examples, as well as our examples of using similarity directly,\nwe focused on solving a very specific business problem. As we have emphasized, one of\nthe fundamental concepts of data science is that one should work to define as precisely\nas possible the goal of any data mining. Recall the CRISP data mining process, replicated\nin Figure 6-15. We should spend as much time as we can in the business understanding/\ndata understanding mini-cycle, until we have a concrete, specific definition of the problem we are trying to solve. In predictive modeling applications, we are aided by our need\nto define the target variable precisely, and we will see in Chapter 7 that we can get more\nand more precise about defining the problem as we get more sophisticated in our understanding of data science. In our similarity-matching examples, again we had a very\nconcrete notion of what exactly we were looking for: we want to find similar companies\nto optimize our efforts, and we will define specifically what it means to be similar. We\nwant to find similar whiskeys---specifically in terms of taste---and we again work to\ngather and represent the data so that we can find exactly these. Later in the book we will\ndiscuss how we often expend considerable effort applying data science frameworks to\ndecompose business problems into multiple, well-defined components, each of which\nwe might apply data science methods to solve.\n\nHowever, not all problems are so well defined. What do we do when in the business\nunderstanding phase we conclude: we would like to explore our data, possibly with only\na vague notion of the exact problem we are solving? The problems to which we apply\nclustering often fall into this category. We want to perform unsupervised segmentation:\n\n182\n\n|\n\nChapter 6: Similarity, Neighbors, and Clusters", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00207", "page_num": 207, "segment": "Figure 6-15. The CRISP data mining process.\n\nfinding groups that “naturally” occur (subject, of course, to how we define our similarity\nmeasures).\n\nFor the sake of discussion, let’s simplify by separating our problems into supervised\n(e.g., predictive modeling) and unsupervised (e.g., clustering). The world is not so cutand-dried and just about any of the data mining techniques we have presented could\nbe used for data exploration, but the discussion will be much clearer if we do simply\nseparate into supervised versus unsupervised. There is a direct trade-off in where and\nhow effort is expended in the data mining process. For the supervised problems, since\nwe spent so much time defining precisely the problem we were going to solve, in the\nEvaluation stage of the data mining process we already have a clear-cut evaluation\nquestion: do the results of the modeling seem to solve the problem we have defined?\nFor example, if we had defined our goal as improving prediction of defection when a\ncustomer’s contract is about to expire, we could assess whether our model has done this.\n\nStepping Back: Solving a Business Problem Versus Data Exploration\n\n|\n\n183", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00208", "page_num": 208, "segment": "In contrast, unsupervised problems often are much more exploratory. We may have a\nnotion that if we could cluster companies, news stories, or whiskeys, we would understand our business better, and therefore be able to improve something. However, we\nmay not have a precise formulation. We should not let our desire to be concrete and\nprecise keep us from making important discoveries from data. But there is a trade-off.\nThe tradeoff is that for problems where we did not achieve a precise formulation of the\nproblem in the early stages of the data mining process, we have to spend more time later\nin the process---in the Evaluation stage.\n\nFor clustering, specifically, it often is difficult even to understand what (if anything) the\nclustering reveals. Even when the clustering does seem to reveal interesting information,\nit often is not clear how to use that to make better decisions. Therefore, for clustering,\nadditional creativity and business knowledge must be applied in the Evaluation stage of\nthe data mining process.\n\nIra Haimowitz and Henry Schwartz (1997) show a concrete example of how clustering\nwas used to improve decisions about how to set credit lines for new credit customers.\nThey clustered existing GE Capital customers based on similarity in their use of their\ncards, payment of their bills, and profitability to the company. After some work, they\nsettled on five clusters that represented very different consumer credit behavior (e.g.,\nthose who spend a lot but pay off their cards in full each month versus those who spend\na lot and keep their balance near their credit limit). These different sorts of customers\ncan tolerate very different credit lines (in the two examples, extra care must be taken\nwith the latter to avoid default). The problem with using this clustering immediately for\ndecision making is that the data are not available when the initial credit line is set. Briefly,\nHaimowitz and Schwarz took this new knowledge and cycled back to the beginning of\nthe data mining process. They used the knowledge to define a precise predictive modeling problem: using data that are available at the time of credit approval, predict the\nprobability that a customer will fall into each of these clusters. This predictive model\nthen can be used to improve initial credit line decisions.\n\nSummary\nThe fundamental concept of similarity between data items occurs throughout data\nmining. In this chapter we first discussed a wide variety of uses of similarity ranging\nfrom finding similar entities (or objects) based on their data descriptions, to predictive\nmodeling, to clustering entities. We discussed these various uses and illustrated with\nexamples.\n\nA very common proxy for the similarity of two entities is the distance between them in\nthe instance space defined by their feature vector representation. We presented similarity and distance computations, generally and in technical detail. We also introduced\na family of methods, called nearest-neighbor methods, that perform prediction tasks\nby calculating explicitly the similarity between a new example and a set of training\n\n184\n\n|\n\nChapter 6: Similarity, Neighbors, and Clusters", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00209", "page_num": 209, "segment": "examples (with known values for the target). Once we can retrieve a set of nearest\nneighbors (most similar examples) we can use these for various data mining tasks: classification, regression and instance scoring. Finally, we showed how the same fundamental concept---similarity---also underlies the most common methods for unsupervised data mining: clustering.\n\nWe also discussed another important concept that raises its head once we begin to look\nseriously at methods (such as clustering) that are employed for more exploratory data\nanalysis. When exploring the data, especially with unsupervised methods, we usually\nend up spending less time at the outset in the business understanding phase of the data\nmining process, but more time in the evaluation stage, and in iterating around the cycle.\nTo illustrate, we discussed a variety of methodologies for understanding the results of\nclusterings.\n\nSummary\n\n|\n\n185", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00210", "page_num": 210, "segment": "", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "none", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00211", "page_num": 211, "segment": "CHAPTER 7\nDecision Analytic Thinking I: What Is a Good\nModel?\n\nFundamental concepts: Careful consideration of what is desired from data science results;\nExpected value as a key evaluation framework; Consideration of appropriate comparative\nbaselines.\n\nExemplary techniques: Various evaluation metrics; Estimating costs and benefits; Calculating expected profit; Creating baseline methods for comparison.\n\nRecall from the beginning of Chapter 5: as a manager at MegaTelCo, you wanted to\nassess whether the model my consulting firm had produced was any good. Overfitting\naside, how would you go about measuring that?\n\nFor data science to add value to an application, it is important for the data scientists and\nother stakeholders to consider carefully what they would like to achieve by mining data.\nThis sounds obvious, so it is sometimes surprising how often it is ignored. Both data\nscientists themselves and the people who work with them often avoid---perhaps without\neven realizing it---connecting the results of mining data back to the goal of the undertaking. This may manifest itself in the reporting of a statistic without a clear understanding of why it is the right statistic, or in the failure to figuring out how to measure\nperformance in a meaningful way.\n\nWe should be careful with such a criticism, though. Often it is not possible to measure\nperfectly one’s ultimate goal, for example because the systems are inadequate, or because\nit is too costly to gather the right data, or because it is difficult to assess causality. So, we\nmight conclude that we need to measure some surrogate for what we’d really like to\nmeasure. It is nonetheless crucial to think carefully about what we’d really like to measure. If we have to choose a surrogate, we should do it via careful, data-analytic thinking.\n\nA challenge with writing a chapter on this topic is that every application is different. We\ncannot offer a single evaluation metric that is “right” for any classification problem, or\nregression problem, or whatever problem you may encounter. Nevertheless, there are\n\n187", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00212", "page_num": 212, "segment": "various common issues and themes in evaluation, and frameworks and techniques for\ndealing with them.\n\nWe will work through a set of such frameworks and metrics for tasks of classification\n(in this chapter) and instance scoring (e.g., ordering consumers by their likelihood of\nresponding to an offer), and class probability estimation (in the following chapter). The\nspecific techniques should be seen as examples illustrating the general concept of thinking deeply about the needs of the application. Fortunately, these specific techniques do\napply quite broadly. We also will describe a very general framework for thinking about\nevaluation, using expected value, that can cover a very wide variety of applications. As\nwe will show in a later chapter, it also can be used as an organizational tool for dataanalytic thinking generally, all the way back to problem formulation.\n\nEvaluating Classifiers\nRecall that a classification model takes an instance for which we do not know know the\nclass and predicts its class. Let’s consider binary classification, for which the classes often\nare simply called “positive” and “negative.” How shall we evaluate how well such a model\nperforms? In Chapter 5 we discussed how for evaluation we should use a holdout test\nset to assess the generalization performance of the model. But how should we measure\ngeneralization performance?\n\nSidebar: Bad Positives and Harmless Negatives\nIn discussing classifiers, we often refer to a bad outcome as a “positive” example, and a\nnormal or good outcome as “negative.” This may seem odd to you, given the everyday\ndefinitions of positive and negative. Why, for example, is a case of fraud considered\npositive and a legitimate case considered negative? This terminology is conventional in\nmany fields, including machine learning and data mining, and we will use it throughout\nthis book. Some explanation may be useful to avoid confusion.\n\nIt is useful to think of a positive example as one worthy of attention or alarm, and a\nnegative example as uninteresting or benign. For example, a medical test (which is a\ntype of classifier) performed on a biological sample tries to detect disease or an unusual\ncondition by examining certain aspects of the sample. If the test comes back positive it\nmeans the disease or condition is present; if the test is negative there is no cause for\nalarm and usually no need for treatment. Similarly, if a fraud detector finds unusual\nactivity on a customer account and decides there is cause for alarm, this is called a\npositive. On the other hand, negatives (accounts with only legitimate activity) are profitable but from a fraud detection perspective they are unworthy of attention.\n\nThere are advantages to maintaining this general orientation rather than redefining the\nmeaning of positive and negative for every domain we introduce. You can think of a\nclassifier as sifting through a large population consisting mostly of negatives---the un188\n\n|\n\nChapter 7: Decision Analytic Thinking I: What Is a Good Model?", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00213", "page_num": 213, "segment": "interesting cases---looking for a small number of positive instances. By convention, then,\nthe positive class is often rare, or at least rarer than the negative class. In consequence,\nthe number of mistakes made on negative examples (the false positive errors) may dominate, though the cost of each mistake made on a positive example (a false negative error)\nwill be higher.\n\nPlain Accuracy and Its Problems\nUp to this point we have assumed that some simple metric, such as classifier error rate\nor accuracy, was being used to measure a model’s performance.\n\nClassification accuracy is a popular metric because it’s very easy to measure. Unfortunately, it is usually too simplistic for applications of data mining techniques to real\nbusiness problems. This section discusses it and some of the alternatives.\n\nThe term “classifier accuracy” is sometimes used informally to mean any general measure of classifier performance. Here we will reserve accuracy for its specific technical\nmeaning as the proportion of correct decisions:\n\naccuracy =\n\nNumber of correct decisions made\nTotal number of decisions made\n\nThis is equal to 1--error rate. Accuracy is a common evaluation metric that is often used\nin data mining studies because it reduces classifier performance to a single number and\nit is very easy to measure. Unfortunately, it is simplistic and has some well-known\nproblems (Provost, Fawcett, & Kohavi, 1998). To understand these problems we need\na way to decompose and count the different types of correct and incorrect decisions\nmade by a classifier. For this we use the confusion matrix.\n\nThe Confusion Matrix\nTo evaluate a classifier properly it is important to understand the notion of class confusion and the confusion matrix, which is one sort of contingency table. A confusion\nmatrix for a problem involving n classes is an n × n matrix with the columns labeled\nwith actual classes and the rows labeled with predicted classes. Each example in a test\nset has an actual class label as well as the class predicted by the classifier (the predicted\nclass), whose combination determines which matrix cell the instance counts into. For\nsimplicity we will deal with two-class problems having 2 × 2 confusion matrices.\n\nA confusion matrix separates out the decisions made by the classifier, making explicit\nhow one class is being confused for another. In this way different sorts of errors may be\ndealt with separately. Let’s differentiate between the true classes and the classes predicted\nby the model by using different symbols. We will consider two-class problems, and will\ndenote the true classes as p(ositive) and n(egative), and the classes predicted by the\n\nEvaluating Classifiers\n\n|\n\n189", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00214", "page_num": 214, "segment": "model (the “predicted” classes) as Y(es) and N(o), respectively (think: the model says\n“Yes, it is a positive” or “No, it is not a positive”).\n\nTable 7-1. The layout of a 2 × 2 confusion matrix showing the names of the correct\npredictions (main diagonal) and errors (off-diagonal) entries.\n\np\n\nn\n\nY\n\nTrue positives\n\nFalse positives\n\nN False negatives\n\nTrue negatives\n\nIn the confusion matrix, the main diagonal contains the counts of correct decisions.\nThe errors of the classifier are the false positives (negative instances classified as positive) and false negatives (positives classified as negative).\n\nProblems with Unbalanced Classes\nAs an example of how we need to think carefully about model evaluation, consider a\nclassification problem where one class is rare. This is a common situation in applications, because classifiers often are used to sift through a large population of normal or\nuninteresting entities in order to find a relatively small number of unusual ones; for\nexample, looking for defrauded customers, checking an assembly line for defective parts,\nor targeting consumers who actually would respond to an offer. Because the unusual or\ninteresting class is rare among the general population, the class distribution is unbalanced or skewed (Ezawa, Singh, & Norton, 1996; Fawcett & Provost, 1996; Japkowicz &\nStephen, 2002).\n\nUnfortunately, as the class distribution becomes more skewed, evaluation based on accuracy breaks down. Consider a domain where the classes appear in a 999:1 ratio. A\nsimple rule---always choose the most prevalent class---gives 99.9% accuracy. Presumably\nthis is not satisfactory if a nontrivial solution is sought. Skews of 1:100 are common in\nfraud detection, and skews greater than 1:106 have been reported in other classifier\nlearning applications (Clearwater & Stern, 1991; Attenberg & Provost, 2010). Chapter 5 mentioned the “base rate” of a class, which corresponds to how well a classifier\nwould perform by simply choosing that class for every instance. With such skewed\ndomains the base rate for the majority class could be very high, so a report of 99.9%\naccuracy may tell us little about what data mining has really accomplished.\n\nEven when the skew is not so great, in domains where one class is more prevalent than\nanother accuracy can be greatly misleading. Consider again our cellular-churn example.\nLet’s say you are a manager at MegaTelCo and as an analyst I report that our churnprediction model generates 80% accuracy. This sounds good, but is it? My coworker\nreports that her model generates an accuracy of 37%. That’s pretty bad, isn’t it?\n\nYou might say, wait---we need more information about the data. And you would be\nexactly right to do so (and would be engaging in data-analytic thinking). What do we\n\n190\n\n|\n\nChapter 7: Decision Analytic Thinking I: What Is a Good Model?", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00215", "page_num": 215, "segment": "need? Considering the line of discussion so far in this subsection, you might rightly say:\nwe need to know what is the proportion of churn in the population we are considering.\nLet’s say you know that in these data the baseline churn rate is approximately 10% per\nmonth. Let’s consider a customer who churns to be a positive example, so within our\npopulation of customers we expect a positive to negative class ratio of 1:9. So if we simply\nclassify everyone as negative we could achieve a base rate accuracy of 90%!\n\nDigging deeper, you discover that my coworker and I evaluated on two different datasets.\nThis would not be suprising if we had not coordinated our data analysis efforts. My\ncoworker calculated the accuracy on a representative sample from the population,\nwhereas I created artificially balanced datasets for training and testing (both common\npractices). Now my coworker’s model looks really bad---she could have achieved 90%\naccuracy, but only got 37%. However, when she applies her model to my balanced data\nset, she also sees an accuracy of 80%. Now it’s really confusing.\n\nThe bottom line is that accuracy simply is the wrong thing to measure. In this admittedly\ncontrived example, my coworker’s model (call it Model A) achieves 80% accuracy on\nthe balanced sample by correctly identifying all positive examples but only 30% of the\nnegative examples. My model (Model B) does this, conversely, by correctly identifying\nall the negative examples but only 30% of the positive examples.\n\nLet’s look at these two models more carefully, using confusion matrices as a conceptual\ntool. In a training population of 1,000 customers, the confusion matrices are as follows.\nRecall that a model’s predicted classes are denoted Y and N.\n\nTable 7-2. Confusion matrix of A\n\nchurn not churn\n\nY\n\n500\n\nN 0\n\n200\n\n300\n\nTable 7-3. Confusion matrix of B\n\nchurn not churn\n\nY\n\n300\n\nN 200\n\n0\n\n500\n\nFigure 7-1 illustrates these classifications on a balanced population and on a representative population. As mentioned, both models correctly classify 80% of the balanced\npopulation, but the confusion matrices and the figure show that they operate very differently. Classifier A often falsely predicts that customers will churn when they will not,\nwhile classifier B makes many opposite errors of predicting that customers will not\nchurn when in fact they will. When applied to the original, unbalanced population of\ncustomers, model A’s accuracy declines to 37% while model B’s rises to 93%. This is a\nhuge change. So which model is better?\n\nEvaluating Classifiers\n\n|\n\n191", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00216", "page_num": 216, "segment": "Figure 7-1. Two churn models, A and B, can make an equal number of errors on a balanced population used for training (top) but a very different number of errors when tested against the true population (bottom).\n\nMy model (B) now appears to be better than A because B seems to have greater performance on the population we care about---the 1:9 mix of customers we expect to see. But\nwe still can’t say for sure because of another problem with accuracy: we don’t know how\nmuch we care about the different errors and correct decisions. This issue is the topic of\nthe next section.\n\n192\n\n|\n\nChapter 7: Decision Analytic Thinking I: What Is a Good Model?", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00217", "page_num": 217, "segment": "Problems with Unequal Costs and Benefits\nAnother problem with simple classification accuracy as a metric is that it makes no\ndistinction between false positive and false negative errors. By counting them together,\nit makes the tacit assumption that both errors are equally important. With real-world\ndomains this is rarely the case. These are typically very different kinds of errors with\nvery different costs because the classifications have consequences of differing severity.\n\nConsider a medical diagnosis domain where a patient is wrongly informed he has cancer\nwhen he does not. This is a false positive error. The result would likely be that the patient\nwould be given further tests or a biopsy, which would eventually disconfirm the initial\ndiagnosis of cancer. This mistake might be expensive, inconvenient, and stressful for\nthe patient, but it would not be life threatening. Compare this with the opposite error:\na patient who has cancer but she is wrongly told she does not. This is a false negative.\nThis second type of error would mean a person with cancer would miss early detection,\nwhich could have far more serious consequences. These two errors are very different,\nshould be counted separately, and should have different costs.\n\nReturning to our cellular-churn example, consider the cost of giving a customer a retention incentive which still results in departure (a false positive error). Compare this\nwith the cost of losing a customer because no incentive was offered (a false negative).\nWhatever costs you might decide for each, it is unlikely they would be equal; and the\nerrors should be counted separately regardless.\n\nIndeed, it is hard to imagine any domain in which a decision maker can safely be indifferent to whether she makes a false positive or a false negative error. Ideally, we should\nestimate the cost or benefit of each decision a classifier can make. Once aggregated,\nthese will produce an expected profit (or expected benefit or expected cost) estimate for\nthe classifier.\n\nGeneralizing Beyond Classification\nWe have been using classification modeling to illustrate many data science issues concretely. Most of these issues are applicable beyond classification.\n\nThe general principle we’re developing here is that when we are applying data science\nto an actual application it is vital to return to the question: what is important in the\napplication? What is the goal? Are we assessing the results of data mining appropriately\ngiven the actual goal?\n\nAs another example, let’s apply this thinking to regression modeling rather than classification. Say our data science team has to build a movie recommendation model. It\npredicts how much a given customer will like a given movie, which we will use to help\nus provide personalized recommendations. Let’s say each customer rates a movie by\ngiving it one to five stars, and the recommendation model predicts how many stars a\n\nGeneralizing Beyond Classification\n\n|\n\n193", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00218", "page_num": 218, "segment": "user will give an unseen movie. One of our analysts describes each model by reporting\nthe mean-squared-error (or root-mean-squared-error, or R2, or whatever metric) for\nthe model. We should ask: mean-squared-error of what? The analyst replies: the value\nof the target variable, which is the number of stars that a user would give as a rating for\nthe movie. Why is the mean-squared-error on the predicted number of stars an appropriate metric for our recommendation problem? Is it meaningful? Is there a better metric? Hopefully, the analyst has thought this through carefully. It is surprising how often\none finds that an analyst has not, and is simply reporting some measure he learned about\nin a class in school.\n\nA Key Analytical Framework: Expected Value\nWe now are ready to discuss a very broadly useful conceptual tool to aid data analytic\nthinking: expected value. The expected value computation provides a framework that\nis extremely useful in organizing thinking about data-analytic problems. Specifically, it\ndecomposes data-analytic thinking into (i) the structure of the problem, (ii) the elements\nof the analysis that can be extracted from the data, and (iii) the elements of the analysis\nthat need to be acquired from other sources (e.g., business knowledge of subject matter\nexperts).\n\nIn an expected value calculation the possible outcomes of a situation are enumerated.\nThe expected value is then the weighted average of the values of the different possible\noutcomes, where the weight given to each value is its probability of occurrence. For\nexample, if the outcomes represent different possible levels of profit, an expected profit\ncalculation weights heavily the highly likely levels of profit, while unlikely levels of profit\nare given little weight. For this book, we will assume that we are considering repeated\ntasks (like targeting a large number of consumers, or diagnosing a large number of\nproblems) and we are interested in maximizing expected profit.1\n\nThe expected value framework provides structure to an analyst’s thinking (i) via the\ngeneral form shown in Equation 7-1 .\n\nEquation 7-1. The general form of an expected value calculation\n\nEV = p(o1) · v(o1) + p(o2) · v(o2) + p(o3) · v(o3) ...\n\nEach oi is a possible decision outcome; p(oi) is its probability and v(oi) is its value. The\nprobabilities often can be estimated from the data (ii), but the business values often need\nto be acquired from other sources (iii). As we will see in Chapter 11, data-driven mod1. A course in decision theory would lead you into a thicket of interesting related issues.\n\n194\n\n|\n\nChapter 7: Decision Analytic Thinking I: What Is a Good Model?", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00219", "page_num": 219, "segment": "eling may help estimate business values, but usually the values must come from external\ndomain knowledge.\n\nWe now will illustrate the use of expected value as an analytical framework with two\ndifferent data science scenarios. The two scenarios are often confused but it is vital to\nbe able to distinguish them. To do so, recall the difference from Chapter 2 between the\nmining (or induction) of a model, and the model’s use.\n\nUsing Expected Value to Frame Classifier Use\nIn use, we have many individual cases for which we would like to predict a class, which\nmay then lead to an action. In targeted marketing, for example, we may want to assign\neach consumer a class of likely responder versus not likely responder, then we could target\nthe likely responders. Unfortunately, for targeted marketing often the probability of\nresponse for any individual consumer is very low---maybe one or two percent---so no\nconsumer may seem like a likely responder. If we choose a “common sense” threshold\nof 50% for deciding what a likely responder is, we would probably not target anyone.\nMany inexperienced data miners are surprised when the application of data mining\nmodels results in everybody being classified as not likely responder (or a similar negative\nclass).\n\nHowever, with the expected value framework we can see the crux of the problem. Let’s\nwalk through a targeted marketing scenario.2 Consider that we have an offer for a product that, for simplicity, is only available via this offer. If the offer is not made to a consumer, the consumer will not buy the product. We have a model, mined from historical\ndata, that gives an estimated probability of response pR() for any consumer whose\nfeature vector description x is given as input. The model could be a classification tree\nor a logistic regression model or some other model we haven’t talked about yet. Now\nwe would like to decide whether to target a particular consumer described by feature\nvector x.\n\nExpected value provides a framework for carrying out the analysis. Specifically, let’s\ncalculate the expected benefit (or cost) of targeting consumer x:\n\nExpected benefit of targeting = pR() · vR + 1 - pR() · vNR\n\nwhere vR is the value we get from a response and vNR is the value we get from no response.\nSince everyone either responds or does not, our estimate of the probability of not responding is just (1 - pR()). As mentioned, the probabilities came from the historical\n\n2. We use targeted marketing here, rather than the churn example, because the expected value framework\nactually reveals an important complexity to the churn example that we’re not ready to deal with. We will\nreturn to that later in Chapter 11 when we’re ready to deal with it.\n\nA Key Analytical Framework: Expected Value\n\n|\n\n195", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00220", "page_num": 220, "segment": "data, as summarized in our predictive model. The benefits vR and vNR need to be determined separately, as part of the Business Understanding step (recall Chapter 2). Since\na customer can only purchase the product by responding to the offer (as discussed\nabove), the expected benefit of not targeting her conveniently is zero.\n\nTo be concrete, let’s say that a consumer buys the product for $200 and our productrelated costs are $100. To target the consumer with the offer, we also incur a cost. Let’s\nsay that we mail some flashy marketing materials, and the overall cost including postage\nis $1, yielding a value (profit) of vR = $99 if the consumer responds (buys the product).\nNow, what about vNR, the value to us if the consumer does not respond? We still mailed\nthe marketing materials, incurring a cost of $1 or equivalently a benefit of -$1.\n\nNow we are ready to say precisely whether we want to target this consumer: do we expect\nto make a profit? Technically, is the expected value (profit) of targeting greater than\nzero? Mathematically, this is:\n\npR() · $99 - 1 - pR() · $1 > 0\n\nA little rearranging of the equation gives us a decision rule: Target a given customer x\nonly if:\n\npR() · $99 > 1 - pR() · $1\n\npR() > 0.01\n\nWith these example values, we should target the consumer as long as the estimated\nprobability of responding is greater than 1%.\n\nThis shows how an expected value calculation can express how we will use the model.\nMaking this explicit helps to organize problem formulation and analysis. We will return\nto this in Chapter 11. Now, let’s move on to the other important application of the\nexpected value framework, to organize our analysis of whether the model we have induced from the data is any good.\n\nUsing Expected Value to Frame Classifier Evaluation\nAt this point we want to shift our focus from individual decisions to collections of\ndecisions. Specifically, we need to evaluate the set of decisions made by a model when\napplied to a set of examples. Such an evaluation is necessary in order to compare one\nmodel to another. For example, does our data-driven model perform better than the\nhand-crafted model suggested by the marketing group? Does a classification tree work\nbetter than a linear discriminant model for a particular problem? Do any of the models\ndo substantially better than a baseline “model,” such as randomly choosing consumers\n\n196\n\n|\n\nChapter 7: Decision Analytic Thinking I: What Is a Good Model?", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00221", "page_num": 221, "segment": "to target? It is likely that each model will make some decisions better than the other\nmodel. What we care about is, in aggregate, how well does each model do: what is its\nexpected value.\n\nFigure 7-2. A diagram of the expected value calculation. The Π and Σ refer to the multiplication and summation in the expected value calculation.\n\nWe can use the expected value framework just described to determine the best decisions\nfor each particular model, and then use the expected value in a different way to compare\nthe models. If we are to calculate the expected profit for a model in aggregate, each oi\nin Equation 7-1 corresponds to one of the possible combinations of the class we predict,\nand the actual class. We want to aggregate all the different possible cases: overall, when\nwe decide to target consumers, what is the probability that they respond? What is the\nprobability that they do not? What about when we do not target consumers, would they\nhave responded? Fortunately, as you may recall, we already have the counts necessary\nto calculate all these---in the confusion matrix. Each oi corresponds to one cell of the\nconfusion matrix. For example, what is the probability associated with the particular\ncombination of a consumer being predicted to churn and actually does not churn? That\n\nA Key Analytical Framework: Expected Value\n\n|\n\n197", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00222", "page_num": 222, "segment": "would be estimated by the number of test-set consumers who fell into the confusion\nmatrix cell (Y,n), divided by the total number of test-set consumers.\n\nLet’s walk through an entire expected profit calculation at the aggregate (model) level,\nin the process computing these probabilities. Figure 7-2 shows a schematic diagram of\nthe expected value calculation in the context of model induction and evaluation. At the\ntop left of the diagram, a training portion of a dataset is taken as input by an induction\nalgorithm, which produces the model that we will evaluate. That model is applied to a\nholdout (test) portion of the data, and the counts for the different cells of the confusion\nmatrix are tallied. Let’s consider a concrete example of a classifier confusion matrix in\nTable 7-4.\n\nTable 7-4. A sample confusion matrix with counts.\n\np\n\n56\n\nn\n\n7\n\nY\n\nN 5\n\n42\n\nError rates\n\nWhen calculating expected values for a business problem, the analyst is often faced with\nthe question: where do these probabilities actually come from? When evaluating a model\non testing data, the answer is straightforward: these probabilities (of errors and correct\ndecisions) can be estimated from the tallies in the confusion matrix by computing the\nrates of the errors and correct decisions. Each cell of the confusion matrix contains a\ncount of the number of decisions corresponding to the corresponding combination of\n(predicted, actual), which we will express as count(h,a) (we use h for “hypothesized”\nsince p is already being used). For the expected value calculation we reduce these counts\nto rates or estimated probabilities, p(h,a). We do this by dividing each count by the total\nnumber of instances:\n\np(h , a) = count(h , a) / T\n\nHere are the calculations of the rates for each of the raw statistics in the confusion matrix.\nThese rates are estimates of the probabilities that we will use in the expected value\ncomputation of Equation 7-1.\n\nT = 110\n\np(Y,p) = 56/110 = 0.51\n\np(Y,n) = 7/110 = 0.06\n\np(N,p) = 5/110 = 0.05\n\np(N,n) = 42/110 = 0.38\n\nCosts and benefits\n\nTo compute expected profit (recall Equation 7-1), we also need the cost and benefit\nvalues that go with each decision pair. These will form the entries of a cost-benefit matrix\n\n198\n\n|\n\nChapter 7: Decision Analytic Thinking I: What Is a Good Model?", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00223", "page_num": 223, "segment": "with the same dimensions (rows and columns) as the confusion matrix. However, the\ncost-benefit matrix specifies, for each (predicted,actual) pair, the cost or benefit of\nmaking such a decision (see Figure 7-3). Correct classifications (true positives and true\nnegatives) correspond to the benefits b(Y,p) and b(N, n), respectively. Incorrect classifications (false positives and false negatives) correspond to the “benefit” b(Y,n) and b(N,\np), respectively, which may well actually be a cost (a negative benefit), and often are\nexplicitly referred to as costs c(Y, n) and c(N, p).\n\nFigure 7-3. A cost-benefit matrix.\n\nWhile the probabilities can be estimated from data, the costs and benefits often cannot.\nThey generally depend on external information provided via analysis of the consequences of decisions in the context of the specific business problem. Indeed, specifying\nthe costs and benefits may take a great deal of time and thought. In many cases they\ncannot be specified exactly but only as approximate ranges. Chapter 8 will return to\naddress what we might do when these values are not known exactly. For example, in\nour churn problem, how much is it really worth us to retain a customer? The value\ndepends on future cell phone usage and probably varies a great deal between customers.\nIt may be that data on customers’ prior usage can be helpful in this estimation. In many\ncases, average estimated costs and benefits are used rather than individual-specific costs\nand benefits, for simplicity of problem formulation and calculation. Therefore, we will\nignore customer-specific cost/benefit calculations for the rest of our example, but will\nreturn to it in Chapter 11.\n\nSo, let’s return to our targeted marketing example. What are the costs and benefits? We\nwill express all values as benefits, with costs being negative benefits, so the function we’re\nspecifying is b(predicted, actual). For simplicity, all numbers will be expressed as dollars.\n\n• A false positive occurs when we classify a consumer as a likely responder and therefore target her, but she does not respond. We’ve said that the cost of preparing and\n\nA Key Analytical Framework: Expected Value\n\n|\n\n199", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00224", "page_num": 224, "segment": "mailing the marketing materials is a fixed cost of $1 per consumer. The benefit in\nthis case is negative: b(Y, n) = --1.\n\n• A false negative is a consumer who was predicted not to be a likely responder (so\nwas not offered the product), but would have bought it if offered. In this case, no\nmoney was spent and nothing was gained, so b(N, p) = 0.\n\n• A true positive is a consumer who is offered the product and buys it. The benefit in\nthis case is the profit from the revenue ($200) minus the product-related costs\n($100) and the mailing costs ($1), so b(Y, p) = 99.\n\n• A true negative is a consumer who was not offered a deal and who would not have\nbought it even if it had been offered. The benefit in this case is zero (no profit but\nno cost), so b(N, n) = 0.\n\nThese cost-benefit estimations can be summarized in a 2 × 2 cost-benefit matrix, as in\nFigure 7-4. Note that the rows and columns are the same as for our confusion matrix,\nwhich is exactly what we’ll need to compute the overall expected value for the classification model.\n\nFigure 7-4. A cost-benefit matrix for the targeted marketing example.\n\nGiven a matrix of costs and benefits, these are multiplied cell-wise against the matrix\nof probabilities, then summed into a final value representing the total expected profit.\nThe result is:\n\nExpected profit = p(, ) · b(, ) + p(, ) · b(, ) +\np(, ) · b(, ) + p(, ) · b(, )\n\nUsing this equation, we can now compute and compare the expected profits for various\nmodels and other targeting strategies. All we need is to be able to compute the confusion\nmatrices over a set of test instances, and to generate the cost-benefit matrix.\n\nThis equation is sufficient for comparing classifiers, but let’s continue along this path a\nlittle further, because an alternative calculation of this equation is often used in practice.\nThis alternative view is closely related to some techniques used to visualize classifier\nperformance (see Chapter 8). Furthermore, by examining the alternative formulation\nwe can see exactly how to deal with the model comparison problem we introduced at\n\n200\n\n|\n\nChapter 7: Decision Analytic Thinking I: What Is a Good Model?", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00225", "page_num": 225, "segment": "the beginning of the chapter---where one analyst had reported performance statistics\nover a representative (but unbalanced) population, and another had used a classbalanced population.\n\nA common way of expressing expected profit is to factor out the probabilities of seeing\neach class, often referred to as the class priors. The class priors, p(p) and p(n), specify\nthe likelihood of seeing positive and negative instances, respectively. Factoring these\nout allows us to separate the influence of class imbalance from the fundamental predictive power of the model, as we will discuss in more detail in Chapter 8.\n\nA rule of basic probability is:\n\np(x, y) = p(y) · p(x | y)\n\nThis says that the probability of two different events both occurring is equal to the\nprobability of one of them occurring times the probability of the other occurring if we\nknow that the first occurs. Using this rule we can re-express our expected profit as:\n\nExpected profit =\n\np( | ) · p() · b(, ) + p( | ) · p() · b(, ) +\np( | ) · p() · b(, ) + p( | ) · p() · b(, )\n\nFactoring out the class priors p(y) and p(n), we get the final equation:\n\nEquation 7-2. Expected profit equation with priors p(p) and p(n) factored.\n\nExpected profit =\n\np() · p( | ) · b(, ) + p( | ) · c(, ) +\np() · p( | ) · b(, ) + p( | ) · c(, )\n\nFrom this mess, notice that we now have one component (the first one) corresponding\nto the expected profit from the positive examples, and another (the second one) corresponding to the expected profit from the negative examples. Each of these is weighted\nby the probability that we see that sort of example. So, if positive examples are very rare,\ntheir contribution to the overall expected profit will be correspondingly small.3 In this\nalternative formulation, the quantities p(Y|p), p(Y|n), etc. correspond directly to the\ntrue positive rate, the false positive rate, etc., that also can be calculated directly from\nthe confusion matrix (see “Sidebar: Other Evaluation Metrics” on page 203).\n\n3. This could be extended to any number of classes, though to keep things simple(r) we’ll use two.\n\nA Key Analytical Framework: Expected Value\n\n|\n\n201", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00226", "page_num": 226, "segment": "Here again is our sample confusion matrix in Table 7-5.\n\nTable 7-5. Our sample confusion matrix (raw counts)\n\np\n\n56\n\nn\n\n7\n\nY\n\nN 5\n\n42\n\nTable 7-6 shows the class priors and various error rates we need.\n\nTable 7-6. The class priors and the rates of true positives, false positives, and so on.\n\nT = 110\n\nP = 61\n\np(p) = 0.55\n\nN = 49\n\np(n) = 0.45\n\ntp rate = 56/61 = 0.92\n\nfp rate = 7/49 = 0.14\n\nfn rate = 5/61 = 0.08\n\ntn rate = 42/49 = 0.86\n\nReturning to the targeted marketing example, what is the expected profit of the model\nlearned? We can calculate it using Equation 7-2:\n\nexpected profit =\n\n=\n\n=\n\n=\n≈\n\np() · p( | ) · b(, ) + p( | ) · c(, ) +\np() · p( | ) · b(, ) + p( | ) · c(, )\n0.55 · 0.92 · b(, ) + 0.08 · b(, ) +\n0.45 · 0.86 · b(, ) + 0.14 · p(, )\n\n0.55 · 0.92 · 99 + 0.08 · 0 +\n\n0.45 · 0.86 · 0 + 0.14 · - 1\n\n50.1 - 0.063\n$50.04\n\nThis expected value means that if we apply this model to a population of prospective\ncustomers and mail offers to those it classifies as positive, we can expect to make an\naverage of about $50 profit per consumer.\n\nWe now can see one way to deal with our motivating example from the beginning of\nthe chapter. Instead of computing accuracies for the competing models, we would compute expected values. Furthermore, using this alternative formulation, we can compare\nthe two models even though one analyst tested using a representative distribution and\nthe other tested using a class-balanced distribution. In each calculation, we simply can\nreplace the priors. Using a balanced distribution corresponds to priors of p(p) = 0.5 and\np(n) = 0.5. The mathematically savvy reader is encouraged to convince herself that the\nother factors in the equation will not change if the test-set priors change.\n\n202\n\n|\n\nChapter 7: Decision Analytic Thinking I: What Is a Good Model?", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00227", "page_num": 227, "segment": "To close this section on estimated profit, we emphasize two pitfalls that\nare common when formulating cost-benefit matrices:\n\n• It is important to make sure the signs of quantities in the costbenefit matrix are consistent. In this book we take benefits to be\npositive and costs to be negative. In many data mining studies,\nthe focus is on minimizing cost rather than maximizing profit, so\nthe signs are reversed. Mathematically, there is no difference.\nHowever, it is important to pick one view and be consistent.\n\n• An easy mistake in formulating cost-benefit matrices is to “double count” by putting a benefit in one cell and a negative cost for\nthe same thing in another cell (or vice versa). A useful practical\ntest is to compute the benefit improvement for changing the decision on an example test instance.\n\nFor example, say you’ve built a model to predict which accounts\nhave been defrauded. You’ve determined that a fraud case costs\n$1,000 on average. If you decide that the benefit of catching fraud\nis therefore +$1,000/case on average, and the cost of missing fraud\nis -$1,000/case, then what would be the improvement in benefit\nfor catching a case of fraud? You would calculate:\n\nb(Y,p) - b(N,p) = $1000 - (-$1000) = $2000\n\nBut intuitively you know that this improvement should only be\nabout $1,000, so this error indicates double counting. The solution is to specify either that the benefit of catching fraud is $1,000\nor that the cost of missing fraud is -$1,000, but not both. One\nshould be zero.\n\nSidebar: Other Evaluation Metrics\nThere are many evaluation metrics you will likely encounter in data science. All of them\nare fundamentally summaries of the confusion matrix. Referring to the counts in the\nconfusion matrix, let’s abbreviate the number of true positives, false positives, true negatives, and false negatives by TP, FP, TN, and FN, respectively. We can describe various\nevaluation metrics using these counts. True positive rate and False negative rate refer to\nthe frequency of being correct and incorrect, respectively, when the instance is actually\npositive: TP/(TP + FN) and FN/(TP + FN). The True negative rate and False positive\nrate are analogous for the instances that are actually negative. These are often taken as\nestimates of the probability of predicting Y when the instance is actually p, that is p(Y|\np), etc. We will continue to explore these measures in Chapter 8.\n\nThe metrics Precision and Recall are often used, especially in text classification and\ninformation retrieval. Recall is the same as true positive rate, while precision is TP/(TP\n\nA Key Analytical Framework: Expected Value\n\n|\n\n203", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00228", "page_num": 228, "segment": "+ FP), which is the accuracy over the cases predicted to be positive. The F-measure is\nthe harmonic mean of precision and recall at a given point, and is:\n\nF-measure = 2 ·\n\nprecision · recall\nprecision + recall\n\nPractitioners in many fields such as statistics, pattern recognition, and epidemiology\nspeak of the sensitivity and specificity of a classifier:\n\nSensitivity =\n\nTN / (TN + FP) = True negative rate = 1 - False positive rate\n\nSpecificity =\n\nTP / (TP + FN ) = True positive rate\n\nYou may also hear about the positive predictive value, which is the same as precision.\n\nAccuracy, as mentioned before, is simply the count of correct decisions divided by the\ntotal number of decisions, or:\n\nAccuracy =\n\nTP + TN\nP + N\n\nSwets (1996) lists many other evaluation metrics and their relationships to the confusion\nmatrix.\n\nEvaluation, Baseline Performance, and Implications for\nInvestments in Data\nUp to this point we have talked about model evaluation in isolation. In some cases just\ndemonstrating that a model generates some (nonzero) profit, or a positive return on\ninvestment, will be informative by itself. Nevertheless, another fundamental notion in\ndata science is: it is important to consider carefully what would be a reasonable baseline\nagainst which to compare model performance. This is important for the data science team\nin order to understand whether they indeed are improving performance, and is equally\nimportant for demonstrating to stakeholders that mining the data has added value. So,\nwhat is an appropriate baseline for comparison?\n\nThe answer of course depends on the actual application, and coming up with suitable\nbaselines is one task for the business understanding phase of the data mining process.\nHowever, there are some general principles that can be very helpful.\n\nFor classification models it is easy to simulate a completely random model and measure\nits performance. The visualization frameworks we will discuss in Chapter 8 have natural\nbaselines showing what random classification should achieve. This is useful for very\n\n204\n\n|\n\nChapter 7: Decision Analytic Thinking I: What Is a Good Model?", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00229", "page_num": 229, "segment": "difficult problems or initial explorations. Comparison against a random model establishes that there is some information to be extracted from the data.\n\nHowever, beating a random model may be easy (or may seem easy), so demonstrating\nsuperiority to it may not be very interesting or informative. A data scientist will often\nneed to implement an alternative model, usually one that is simple but not simplistic,\nin order to justify continuing the data mining effort.\n\nIn Nate Silver’s book on prediction, The Signal and the Noise (2012), he mentions the\nbaseline issue with respect to weather forecasting:\n\nThere are two basic tests that any weather forecast must pass to demonstrate its merit: It\nmust do better than what meteorologists call persistence: the assumption that the weather\nwill be the same tomorrow (and the next day) as it was today. It must also beat climatology,\nthe long-term historical average of conditions on a particular date in a particular area.\n\nIn other words, weather forecasters have two simple---but not simplistic---baseline\nmodels that they compare against. One (persistence) predicts that the weather tomorrow\nis going to be whatever it was today. The other (climatology) predicts whatever the\naverage historical weather has been on this day from prior years. Each model performs\nconsiderably better than random guessing, and both are so easy to compute that they\nmake natural baselines of comparison. Any new, more complex model must beat these.\n\nWhat are some general guidelines for good baselines? For classification tasks, one good\nbaseline is the majority classifier, a naive classifier that always chooses the majority class\nof the training dataset (see Note: Base rate in “Holdout Data and Fitting Graphs” on\npage 113). This may seem like advice so obvious it can be passed over quickly, but it is\nworth spending an extra moment here. There are many cases where smart, analytical\npeople have been tripped up in skipping over this basic comparison. For example, an\nanalyst may see a classification accuracy of 94% from her classifier and conclude that it\nis doing fairly well---when in fact only 6% of the instances are positive. So, the simple\nmajority prediction classifier also would have an accuracy of 94%. Indeed, many beginning data science students are surprised to find that the models they build from the\ndata simply predict everything to be of the majority class. Note that this may make sense\nif the modeling procedure is set up to build maximum accuracy models---it may be hard\nto beat 94% accuracy. The answer, of course, is to apply the central idea of this chapter:\nconsider carefully what is desired from the data mining results. Maximizing simple\nprediction accuracy is usually not an appropriate goal. If that’s what our algorithm is\ndoing, we’re using the wrong algorithm. For regression problems we have a directly\nanalogous baseline: predict the average value over the population (usually the mean or\nmedian).\n\nIn some applications there are multiple simple averages that one may want to combine.\nFor example, when evaluating recommender systems that internally predict how many\n“stars” a particular customer would give to a particular movie, we have the average\nnumber of stars a movie gets across the population (how well liked it is) and the average\n\nEvaluation, Baseline Performance, and Implications for Investments in Data\n\n|\n\n205", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00230", "page_num": 230, "segment": "number of stars a particular customer gives to movies (what that customer’s overall bias\nis). A simple prediction based on these two may do substantially better than using one\nor the other in isolation.\n\nMoving beyond these simple baseline models, a slightly more complex alternative is a\nmodel that only considers a very small amount of feature information. For example,\nrecall from Chapter 3 our very first example of a data mining procedure: finding informative variables. If we find the one variable that correlates best with the target, we\ncan build a classification or regression model that uses just that variable, which gives\nanother view of baseline performance: how well does a simple “conditional” model\nperform? “Conditional” here means that it predicts differently based on, or conditioned\non, the value of the feature(s). The overall population average is therefore sometimes\ncalled the “unconditional” average.\n\nOne example of mining such single-feature predictive models from data is to use tree\ninduction to build a “decision stump” --- a decision tree with only one internal node,\nthe root node. A tree limited to one internal node simply means that the tree induction\nselects the single most informative feature to make a decision. In a well-known paper\nin machine learning, Robert Holte (1993) showed that decision stumps often produce\nquite good baseline performance on many of the test datasets used in machine learning\nresearch. A decision stump is an example of the strategy of choosing the single most\ninformative piece of information available (recall Chapter 3) and basing all decisions\non it. In some cases most of the leverage may be coming from a single feature, and this\nmethod assesses whether and to what extent that is the case.\n\nThis idea can be extended to data sources, and relates to our fundamental principle from\nChapter 1 that we should regard data as an asset to be invested in. If you are considering\nbuilding models that integrate data from various sources, you should compare the result\nto models built from the individual sources. Often there are substantial costs to acquiring new sources of data. In some cases these are actual monetary costs; in other cases\nthey involve commitments of personnel time for managing relationships with data providers and monitoring data feeds. To be thorough, for each data source the data science\nteam should compare a model that uses the source to one that does not. Such comparisons help to justify the cost of each source by quantifying its value. If the contribution\nis negligible, the team may be able to reduce costs by eliminating it.\n\nBeyond comparing simple models (and reduced-data models), it is often useful to implement simple, inexpensive models based on domain knowledge or “received wisdom”\nand evaluate their performance. For example, in one fraud detection application it was\ncommonly believed that most defrauded accounts would experience a sudden increase\nin usage, and so checking accounts for sudden jumps in volume was sufficient for\ncatching a large proportion of fraud. Implementing this idea was straightforward and\nit provided a useful baseline for demonstrating the benefit of data mining. (This essentially was a single-feature predictive model.) Similarly, an IBM team that used data\n\n206\n\n|\n\nChapter 7: Decision Analytic Thinking I: What Is a Good Model?", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00231", "page_num": 231, "segment": "mining to direct sales efforts chose to implement a simple sales model that prioritized\nexisting customers by the size of previous revenue and other companies by annual sales.\n4 They were able to demonstrate that their data mining added significant value beyond\nthis simpler strategy. Whatever the data mining group chooses as a baseline for comparison, it should be something the stakeholders find informative, and hopefully persuasive.\n\nSummary\nA vital part of data science is arranging for proper evaluation of models. This can be\nsurprisingly difficult to get right and will often require multiple iterations. It is tempting\nto use simple measures, such as classification accuracy, since these are simple to calculate, are used in many research papers, and may be what one learned in school. However,\nin real-world domains simplistic measures rarely capture what is actually important for\nthe problem at hand, and often mislead. Instead, the data scientist should give careful\nthought to how the model will be used in practice and devise an appropriate metric.\n\nThe expected value calculation is a good framework for organizing this thinking. It will\nhelp to frame the evaluation, and in the event that the final deployed model produces\nunacceptable results, it will help identify what is wrong.\n\nThe characteristics of the data should be taken into account carefully when evaluating\ndata science results. For example, real classification problems often present data with\nvery unbalanced class distributions (that is, the classes will not occur with equal prevalence). Adjusting class proportions may be useful (or even necessary) to learn a model\nfrom the data; however, evaluation should use the original, realistic population so that\nthe results reflect what will actually be achieved.\n\nTo calculate the overall expected value of a model, the costs and benefits of decisions\nmust be specified. If this is possible, the data scientist can calculate an expected cost per\ninstance for each model and choose whichever model produces the lowest expected cost\nor greatest profit.\n\nIt also is vital to consider what one should compare a data-driven model against, to\njudge whether it performs well or better. The answer to this question is intimately tied\nto the business understanding, but there are a variety of general best practices that data\nscience teams should follow.\n\nWe illustrated the ideas of this chapter with applications of the concepts presented in\nthe chapters that preceded. The concepts are more general of course, and relate to our\nvery first fundamental concept: data should be considered an asset and we need to\n\n4. They refer to these as Willy Sutton models, after the famed bank robber who robbed banks because “that’s\n\nwhere the money is.”\n\nSummary\n\n|\n\n207", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00232", "page_num": 232, "segment": "consider how to invest. We illustrated this point by discussing briefly that one not only\ncan compare different models and different baselines, but also compare results with\ndifferent data sources. Different data sources may have different associated costs, and\ncareful evaluation may show which can be chosen to maximize the return on investment.\n\nAs a final summary point, this chapter has discussed single quantitative numbers as\nsummary estimates of model performance. They can answer questions like “How much\nprofit can I expect to make?” and “Should I use model A or model B?” Such answers\nare useful but provide only “single-point values” that hold under a specific set of assumptions. It is often revealing to visualize model behavior under a broad range of\nconditions. The next chapter discusses graphical views of model behavior that can do\njust this.\n\n208\n\n|\n\nChapter 7: Decision Analytic Thinking I: What Is a Good Model?", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00233", "page_num": 233, "segment": "CHAPTER 8\nVisualizing Model Performance\n\nFundamental concepts: Visualization of model performance under various kinds of uncertainty; Further consideration of what is desired from data mining results.\n\nExemplary techniques: Profit curves; Cumulative response curves; Lift curves; ROC\ncurves.\n\nThe previous chapter introduced basic issues of model evaluation and explored the\nquestion of what makes for a good model. We developed detailed calculations based on\nthe expected value framework. That chapter was much more mathematical than previous ones, and if this is your first introduction to that material you may have felt overwhemed by the equations. Though they form the basis for what comes next, by themselves they may not be very intuitive. In this chapter we will take a different view to\nincrease our understanding of what they are revealing.\n\nThe expected profit calculation of Equation 7-2 takes a specific set of conditions and\ngenerates a single number, representing the expected profit in that scenario. Stakeholders outside of the data science team may have little patience for details, and will often\nwant a higher-level, more intuitive view of model performance. Even data scientists who\nare comfortable with equations and dry calculations often find such single estimates to\nbe impoverished and uninformative, because they rely on very stringent assumptions\n(e.g., of precise knowledge of the costs and benefits, or that the models’ estimates of\nprobabilities are accurate). In short, it is often useful to present visualizations rather\nthan just calculations, and this chapter presents some useful techniques.\n\nRanking Instead of Classifying\n“A Key Analytical Framework: Expected Value” on page 194 discussed how the score\nassigned by a model can be used to compute a decision for each individual case based\non its expected value. A different strategy for making decisions is to rank a set of cases\nby these scores, and then take actions on the cases at the top of the ranked list. Instead\n\n209", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00234", "page_num": 234, "segment": "of deciding each case separately, we may decide to take the top n cases (or, equivalently,\nall cases that score above a given threshold). There are several practical reasons for doing\nthis.\n\nIt may be that the model gives a score that ranks cases by their likelihood of belonging\nto the class of interest, but which is not a true probability (recall our discussion in\nChapter 4 of the distance from the separating boundary as a classifier score). More\nimportantly, for some reason we may not be able to obtain accurate probability estimates\nfrom the classifier. This happens, for example, in targeted marketing applications when\none cannot get a sufficiently representative training sample. The classifier scores may\nstill be very useful for deciding which prospects are better than others, even if a 1%\nprobability estimate doesn’t exactly correspond to a 1% probability of responding.\n\nA common situation is where you have a budget for actions, such as a fixed marketing\nbudget for a campaign, and so you want to target the most promising candidates. If one\nis going to target the highest expected value cases using costs and benefits that are\nconstant for each class, then ranking cases by likelihood of the target class is sufficient.\nThere is no great need to care about the precise probability estimates. The only caveat\nis that the budget be small enough so that the actions do not go into negative expectedvalue territory. For now, we will leave that as a business understanding task.\n\nIt also may be that costs and benefits cannot be specified precisely, but nevertheless we\nwould like to take actions (and are happy to do so on the highest likelihood cases). We’ll\nreturn to this situation in the next section.\n\nIf individual cases have different costs and benefits, then our expected value discussion in “A Key Analytical Framework: Expected Value” on page 194 should make it clear that simply ranking by likelihood will not be sufficient.\n\nWhen working with a classifier that gives scores to instances, in some situations the\nclassifier decisions should be very conservative, corresponding to the fact that the classifier should have high certainty before taking the positive action. This corresponds to\nusing a high threshold on the output score. Conversely, in some situations the classifier\ncan be more permissive, which corresponds to lowering the threshold.1\n\nThis introduces a complication for which we need to extend our analytical framework\nfor assessing and comparing models. “The Confusion Matrix” on page 189 stated that\na classifier produces a confusion matrix. With a ranking classifier, a classifier plus a\n\n1. Indeed, in some applications, scores from the same model may be used in several places with different\nthresholds to make different decisions. For example, a model may be used first in a decision to grant or deny\ncredit. The same model may be used later in setting a new customer’s credit line.\n\n210\n\n|\n\nChapter 8: Visualizing Model Performance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00235", "page_num": 235, "segment": "threshold produces a single confusion matrix. Whenever the threshold changes, the\nconfusion matrix may change as well because the numbers of true positives and false\npositives change.\n\nFigure 8-1. Thresholding a list of instances sorted by scores. Here, a set of test instances\nis scored by a model and sorted decreasing by these scores. We then apply a series of\nthresholds (represented by each horizontal line) to classify all instances above it as positive and those below it as negative. Each threshold results in a specific confusion matrix.\n\nFigure 8-1 illustrates this basic idea. As the threshold is lowered, instances move up\nfrom the N row into the Y row of the confusion matrix: an instance that was considered\na negative is now classified as positive, so the counts change. Which counts change\ndepends on the example’s true class. If the instance was a positive (in the “p” column)\nit moves up and becomes a true positive (Y,p). If it was a negative (n), it becomes a false\npositive (Y,n). Technically, each different threshold produces a different classifier, represented by its own confusion matrix.\n\nThis leaves us with two questions: how do we compare different rankings? And, how\ndo we choose a proper threshold? If we have accurate probability estimates and a wellspecified cost-benefit matrix, then we already answered the second question in our\ndiscussion of expected value: we determine the threshold where our expected profit is\nabove a desired level (usually zero). Let’s explore and extend this idea.\n\nRanking Instead of Classifying\n\n|\n\n211", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00236", "page_num": 236, "segment": "Profit Curves\nFrom “A Key Analytical Framework: Expected Value” on page 194, we know how to\ncompute expected profit, and we’ve just introduced the idea of using a model to rank\ninstances. We can combine these ideas to construct various performance visualizations\nin the form of curves. Each curve is based on the idea of examining the effect of thresholding the value of a classifier at successive points, implicitly dividing the list of instances\ninto many successive sets of predicted positive and negative instances. As we move the\nthreshold “down” the ranking, we get additional instances predicted as being positive\nrather than negative. Each threshold, i.e., each set of predicted positives and negatives,\nwill have a corresponding confusion matrix. The previous chapter showed that once we\nhave a confusion matrix, along with knowledge of the cost and benefits of decisions, we\ncan generate an expected value corresponding to that confusion matrix.\n\nMore specifically, with a ranking classifier, we can produce a list of instances and their\npredicted scores, ranked by decreasing score, and then measure the expected profit that\nwould result from choosing each successive cut-point in the list. Conceptually, this\namounts to ranking the list of instances by score from highest to lowest and sweeping\ndown through it, recording the expected profit after each instance. At each cut-point\nwe record the percentage of the list predicted as positive and the corresponding estimated profit. Graphing these values gives us a profit curve. Three profit curves are shown\nin Figure 8-2.\n\nThis graph is based on a test set of 1,000 consumers---say, a small random population\nof people to whom you test-marketed earlier. (When interpreting results, we normally\nwill talk about percentages of consumers, so as to generalize to the population as a\nwhole.) For each curve, the consumers are ordered from highest to lowest probability\nof accepting an offer based on some model. For this example, let’s assume our profit\nmargin is small: each offer costs $5 to make and market, and each accepted offer earns\n$9, for a profit of $4. The cost matrix is thus:\n\np\n\n$4\n\nn\n\n-$5\n\nY\n\nN $0\n\n$0\n\nThe curves show that profit can go negative---not always, but sometimes they will, depending on the costs and the class ratio. In particular, this will happen when the profit\nmargin is thin and the number of responders is small, because the curves show you\n“going into the red” by working too far down the list and making offers to too many\npeople who won’t respond, thereby spending too much on the costs of the offers.2\n\n2. For simplicity in the example we will ignore inventory and other realistic issues that would require a more\n\ncomplicated profit calculation.\n\n212\n\n|\n\nChapter 8: Visualizing Model Performance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00237", "page_num": 237, "segment": "Figure 8-2. Profit curves of three classifiers. Each curve shows the expected cumulative\nprofit for that classifier as progressively larger proportions of the consumer base are targeted.\n\nNotice that all four curves begin and end at the same point. This should make sense\nbecause, at the left side, when no customers are targeted there are no expenses and zero\nprofit; at the right side everyone is targeted, so every classifier performs the same. In\nbetween, we’ll see some differences depending on how the classifiers order the customers. The random classifier performs worst because it has an even chance of choosing a\nresponder or a nonresponder. Among the classifiers tested here, the one labeled Classifier 2 produces the maximum profit of $200 by targeting the top-ranked 50% of consumers. If your goal was simply to maximize profit and you had unlimited resources,\nyou should choose Classifier 2, use it to score your population of customers, and target\nthe top half (highest 50%) of customers on the list.\n\nNow consider a slightly different but very common situation where you’re constrained\nby a budget. You have a fixed amount of money available and you must plan how to\nspend it before you see any profit. This is common in situations such as marketing\ncampaigns. As before, you still want to target the highest-ranked people, but now you\n\nProfit Curves\n\n|\n\n213", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00238", "page_num": 238, "segment": "have a budgetary constraint3 that may affect your strategy. Say you have 100,000 total\ncustomers and a budget of $40,000 for the marketing campaign. You want to use the\nmodeling results (the profit curves in Figure 8-2) to figure out how best to spend your\nbudget. What do you do in this case? Well, first you figure out how many offers you can\nafford to make. Each offer costs $5 so you can target at most $40,000/$5 = 8,000 customers. As before, you want to identify the customers most likely to respond, but each\nmodel ranks customers differently. Which model should you use for this campaign?\n8,000 customers is 8% of your total customer base, so check the performance curves at\nx=8%. The best-performing model at this performance point is Classifier 1. You should\nuse it to score the entire population, then send offers to the highest-ranked 8,000 customers.\n\nIn summary, from this scenario we see that adding a budgetary constraint causes not\nonly a change in the operating point (targeting 8% of the population instead of 50%)\nbut also a change in the choice of classifier to do the ranking.\n\nROC Graphs and Curves\nProfit curves are appropriate when you know fairly certainly the conditions under which\na classifier will be used. Specifically, there are two critical conditions underlying the\nprofit calculation:\n\n1. The class priors; that is, the proportion of positive and negative instances in the\ntarget population, also known as the base rate (usually referring to the proportion\nof positives). Recall that Equation 7-2 is sensitive to p(p) and p(n).\n\n2. The costs and benefits. The expected profit is specifically sensitive to the relative\n\nlevels of costs and benefits for the different cells of the cost-benefit matrix.\n\nIf both class priors and cost-benefit estimates are known and are expected to be stable,\nprofit curves may be a good choice for visualizing model performance.\n\nHowever, in many domains these conditions are uncertain or unstable. In fraud detection domains, for example, the amount of fraud changes from place to place, and from\none month to the next (Leigh, 1995; Fawcett & Provost, 1997). The amount of fraud\ninfluences the priors. In the case of mobile phone churn management, marketing campaigns can have different budgets and offers may have different costs, which will change\nthe expected costs.\n\n3. Another common situation is to have a workforce constraint. It’s the same idea: you have a fixed allocation of\nresources (money or personnel) available to address a problem and you want the most “bang for the buck.”\nAn example might be that you have a fixed workforce of fraud analysts, and you want to give them the topranked cases of potential fraud to process.\n\n214\n\n|\n\nChapter 8: Visualizing Model Performance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00239", "page_num": 239, "segment": "One approach to handling uncertain conditions is to generate many different expected\nprofit calculations for each model. This may not be very satisfactory: the sets of models,\nsets of class priors, and sets of decision costs multiply in complexity. This often leaves\nthe analyst with a large stack of profit graphs that are difficult to manage, difficult to\nunderstand the implications of, and difficult to explain to a stakeholder.\n\nAnother approach is to use a method that can accomodate uncertainty by showing the\nentire space of performance possibilities. One such method is the Receiver Operating\nCharacteristics (ROC) graph (Swets, 1988; Swets, Dawes, & Monahan, 2000; Fawcett,\n2006). A ROC graph is a two-dimensional plot of a classifier with false positive rate on\nthe x axis against true positive rate on the y axis. As such, a ROC graph depicts relative\ntrade-offs that a classifier makes between benefits (true positives) and costs (false positives). Figure 8-3 shows a ROC graph with five classifiers labeled A through E.\n\nFigure 8-3. ROC space and five different classifiers (A-E) with their performance\nshown.\n\nA discrete classifier is one that outputs only a class label (as opposed to a ranking). As\nalready discussed, each such classifier produces a confusion matrix, which can be summarized by certain statistics regarding the numbers and rates of true positives, false\npositives, true negatives, and false negatives. Note that although the confusion matrix\n\nROC Graphs and Curves\n\n|\n\n215", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00240", "page_num": 240, "segment": "contains four numbers, we really only need two of the rates: either the true positive rate\nor the false negative rate, and either the false positive rate or the true negative rate. Given\none from either pair the other can be derived since they sum to one. It is conventional\nto use the true positive rate (tp rate) and the false positive rate (fp rate), and we will keep\nto that convention so the ROC graph will make sense. Each discrete classifier produces\nan (fp rate, tp rate) pair corresponding to a single point in ROC space. The classifiers\nin Figure 8-3 are all discrete classifiers. Importantly for what follows, the tp rate is\ncomputed using only the actual positive examples, and the fp rate is computed using\nonly the actual negative examples.\n\nRemembering exactly what statistics the tp rate and fp rate refer to can\nbe confusing for someone who does not deal with such things on a\ndaily basis. It can be easier to remember by using less formal but more\nintuitive names for the statistics: the tp rate is sometimes referred to\nas the hit rate---what percent of the actual positives does the classifier get right. The fp rate is sometimes referred to as the false alarm rate\n---what percent of the actual negative examples does the classifier get\nwrong (i.e., predict to be positive).\n\nSeveral points in ROC space are important to note. The lower left point (0, 0) represents\nthe strategy of never issuing a positive classification; such a classifier commits no false\npositive errors but also gains no true positives. The opposite strategy, of unconditionally\nissuing positive classifications, is represented by the upper right point (1, 1). The point\n(0, 1) represents perfect classification, represented by a star. The diagonal line connecting (0, 0) to (1, 1) represents the policy of guessing a class. For example, if a classifier\nrandomly guesses the positive class half the time, it can be expected to get half the\npositives and half the negatives correct; this yields the point (0.5, 0.5) in ROC space. If\nit guesses the positive class 90% of the time, it can be expected to get 90% of the positives\ncorrect but its false positive rate will increase to 90% as well, yielding (0.9, 0.9) in ROC\nspace. Thus a random classifier will produce a ROC point that moves back and forth\non the diagonal based on the frequency with which it guesses the positive class. In order\nto get away from this diagonal into the upper triangular region, the classifier must exploit\nsome information in the data. In Figure 8-3, E’s performance at (0.6, 0.6) is virtually\nrandom. E may be said to be guessing the positive class 60% of the time. Note that no\nclassifier should be in the lower right triangle of a ROC graph. This represents performance that is worse than random guessing.\n\nOne point in ROC space is superior to another if it is to the northwest of the first (tp\nrate is higher and fp rate is no worse; fp rate is lower and tp rate is no worse, or both are\nbetter). Classifiers appearing on the lefthand side of a ROC graph, near the x axis, may\nbe thought of as “conservative”: they raise alarms (make positive classifications) only\nwith strong evidence so they make few false positive errors, but they often have low true\n\n216\n\n|\n\nChapter 8: Visualizing Model Performance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00241", "page_num": 241, "segment": "positive rates as well. Classifiers on the upper righthand side of a ROC graph may be\nthought of as “permissive”: they make positive classifications with weak evidence so they\nclassify nearly all positives correctly, but they often have high false positive rates. In\nFigure 8-3, A is more conservative than B, which in turn is more conservative than C.\nMany real-world domains are dominated by large numbers of negative instances (see\nthe discussion in “Sidebar: Bad Positives and Harmless Negatives” on page 188), so\nperformance in the far left-hand side of the ROC graph is often more interesting than\nelsewhere. If there are very many negative examples, even a moderate false alarm rate\ncan be unmanageable. A ranking model produces a set of points (a curve) in ROC space.\nAs discussed previously, a ranking model can be used with a threshold to produce a\ndiscrete (binary) classifier: if the classifier output is above the threshold, the classifier\nproduces a Y, else an N. Each threshold value produces a different point in ROC space,\nas shown in Figure 8-4.\n\nFigure 8-4. Each different point in ROC space corresponds to a specific confusion matrix.\n\nConceptually, we may imagine sorting the instances by score and varying a threshold\nfrom --∞ to +∞ while tracing a curve through ROC space, as shown in Figure 8-5.\n\nROC Graphs and Curves\n\n|\n\n217", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00242", "page_num": 242, "segment": "Whenever we pass a positive instance, we take a step upward (increasing true positives);\nwhenever we pass a negative instance, we take a step rightward (increasing false positives). Thus the “curve” is actually a step function for a single test set, but with enough\ninstances it appears smooth.4\n\nFigure 8-5. An illustration of how a ROC “curve” (really, a stepwise graph) is constructed from a test set. The example set, at left, consists of 100 positives and 100 negatives.\nThe model assigns a score to each instance and the instances are ordered decreasing\nfrom bottom to top. To construct the curve, start at the bottom with an initial confusion matrix where everything is classified as N. Moving upward, every instance moves a\ncount of 1 from the N row to the Y row, resulting in a new confusion matrix. Each confusion matrix maps to a (fp rate, tp rate) pair in ROC space.\n\nAn advantage of ROC graphs is that they decouple classifier performance from the\nconditions under which the classifiers will be used. Specifically, they are independent\nof the class proportions as well as the costs and benefits. A data scientist can plot the\n\n4. Technically, if there are runs of examples with the same score, we should count the positive and negatives\n\nacross the entire run, and thus the ROC curve will have a sloping step rather than square step.\n\n218\n\n|\n\nChapter 8: Visualizing Model Performance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00243", "page_num": 243, "segment": "performance of classifiers on a ROC graph as they are generated, knowing that the\npositions and relative performance of the classifiers will not change. The region(s) on\nthe ROC graph that are of interest may change as costs, benefits, and class proportions\nchange, but the curves themselves should not.\n\nBoth Stein (2005) and Provost & Fawcett (1997, 2001) show how the operating conditions of the classifier (the class priors and error costs) can be combined to identify the\nregion of interest on its ROC curve. Briefly, knowledge about the range of possible class\npriors can be combined with knowledge about the cost and benefits of decisions; together these describe a family of tangent lines that can identify which classifier(s) should\nbe used under those conditions. Stein (2005) presents an example from finance (loan\ndefaulting) and shows how this technique can be used to choose models.\n\nThe Area Under the ROC Curve (AUC)\nAn important summary statistic is the area under the ROC curve (AUC). As the name\nimplies, this is simply the area under a classifier’s curve expressed as a fraction of the\nunit square. Its value ranges from zero to one. Though a ROC curve provides more\ninformation than its area, the AUC is useful when a single number is needed to summarize performance, or when nothing is known about the operating conditions. Later,\nin “Example: Performance Analytics for Churn Modeling” on page 223, we will show a\nuse of the AUC statistic. For now it is enough to realize that it’s a good general summary\nstatistic of the predictiveness of a classifier.\n\nAs a technical note, the AUC is equivalent to the Mann-WhitneyWilcoxon measure, a well-known ordering measure in Statistics (Wilcoxon, 1945). It is also equivalent to the Gini Coefficient, with a minor algebraic transformation (Adams & Hand, 1999; Stein, 2005). Both\nare equivalent to the probability that a randomly chosen positive instance will be ranked ahead of a randomly chosen negative instance.\n\nCumulative Response and Lift Curves\nROC curves are a common tool for visualizing model performance for classification,\nclass probability estimation, and scoring. However, as you may have just experienced if\nyou are new to all this, ROC curves are not the most intuitive visualization for many\nbusiness stakeholders who really ought to understand the results. It is important for the\ndata scientist to realize that clear communication with key stakeholders is not only a\nprimary goal of her job, but also is essential for doing the right modeling (in addition\nto doing the modeling right). Therefore, it can be useful also to consider visualization\nframeworks that might not have all of the nice properties of ROC curves, but are more\nintuitive. (It is important for the business stakeholder to realize that the theoretical\n\nThe Area Under the ROC Curve (AUC)\n\n|\n\n219", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00244", "page_num": 244, "segment": "properties that are sacrificed sometimes are important, so it may be necessary in certain\ncircumstances to pull out the more complex visualizations.)\n\nOne of the most common examples of the use of an alternate visualization is the use of\nthe “cumulative response curve,” rather than the ROC curve. They are closely related,\nbut the cumulative response curve is more intuitive. Cumulative response curves plot\nthe hit rate (tp rate; y axis), i.e., the percentage of positives correctly classified, as a function\nof the percentage of the population that is targeted (x axis). So, conceptually as we move\ndown the list of instances ranked by the model, we target increasingly larger proportions\nof all the instances. Hopefully in the process, if the model is any good, when we are at\nthe top of the list we will target a larger proportion of the actual positives than actual\nnegatives. As with ROC curves, the diagonal line x=y represents random performance.\nIn this case, the intuition is clear: if you target 20% of all instances completely randomly,\nyou should target 20% of the positives as well. Any classifier above the diagonal is providing some advantage.\n\nThe cumulative response curve is sometimes called a lift curve, because one can see the increase over simply targeting randomly as how\nmuch the line representing the model performance is lifted up over the\nrandom performance diagonal. We will call these curves cumulative\nresponse curves, because “lift curve” also refers to a curve that specifically plots the numeric lift.\n\nIntuitively, the lift of a classifier represents the advantage it provides over random\nguessing. The lift is the degree to which it “pushes up” the positive instances in a list\nabove the negative instances. For example, consider a list of 100 customers, half of whom\nchurn (positive instances) and half who do not (negative instances). If you scan down\nthe list and stop halfway (representing 0.5 targeted), how many positives would you\nexpect to have seen in the first half? If the list were sorted randomly, you would expect\nto have seen only half the positives (0.5), giving a lift of 0.5/0.5 = 1. If the list had been\nordered by an effective ranking classifier, more than half the positives should appear in\nthe top half of the list, producing a lift greater than 1. If the classifier were perfect, all\npositives would be ranked at the top of the list so by the midway point we would have\nseen all of them (1.0), giving a lift of 1.0/0.5 = 2.\n\nFigure 8-6 shows four sample cumulative response curves, and Figure 8-7 shows the lift\ncurves of the same four.\n\n220\n\n|\n\nChapter 8: Visualizing Model Performance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00245", "page_num": 245, "segment": "Figure 8-6. Four example classifiers (A--D) and their cumulative response curves.\n\nThe lift curve is essentially the value of the cumulative response curve at a given x point\ndivided by the diagonal line (y=x) value at that point. The diagonal line of a cumulative\nresponse curve becomes a horizontal line at y=1 on the lift curve.\n\nSometimes you will hear claims like “our model gives a two times (or a 2X) lift”; this\nmeans that at the chosen threshold (often not mentioned), the lift curve shows that the\nmodel’s targeting is twice as good as random. On the cumulative response curve, the\ncorresponding tp rate for the model will be twice the tp rate for the random-performance\ndiagonal. (You might also compute a version of lift with respect to some other baseline.)\nThe lift curve plots this numeric lift on the y axis, against the percent of the population\ntargeted on the x axis (the same x axis as the cumulative response curve).\n\nCumulative Response and Lift Curves\n\n|\n\n221", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00246", "page_num": 246, "segment": "Figure 8-7. The four classifiers (A--D) of Figure 8-6 and their lift curves.\n\nBoth lift curves and cumulative response curves must be used with care if the exact\nproportion of positives in the population is unknown or is not represented accurately\nin the test data. Unlike for ROC curves, these curves assume that the test set has exactly\nthe same target class priors as the population to which the model will be applied. This\nis one of the simplifying assumptions that we mentioned at the outset, that can allow\nus to use a more intuitive visualization.\n\nAs an example, in online advertising the base rate of observed response to an advertisement may be very small. One in ten million (1:107) is not unusual. Modelers may\nnot want to have to manage datasets that have ten million nonresponders for every\nresponder, so they down-sample the nonresponders, and create a more balanced dataset\nfor modeling and evaluation. When visualizing classifier performance with ROC curves,\nthis will have no effect (because as mentioned above, the axes each correspond only to\nproportions of one class). However, lift and cumulative response curves will be different\n---the basic shapes of the curves may still be informative, but the relationships between\nthe values on the axes will not be valid.\n\n222\n\n|\n\nChapter 8: Visualizing Model Performance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00247", "page_num": 247, "segment": "Example: Performance Analytics for Churn Modeling\nThe last few chapters have covered a lot of territory in evaluation. We’ve introduced\nseveral important methods and issues in evaluating models. In this section we tie them\ntogether with a single application case study to show the results of different evaluation\nmethods. The example we’ll use is our ongoing domain of cell phone churn. However,\nin this section we use a different (and more difficult) churn dataset than was used in\nprevious chapters. It is a dataset from the 2009 KDD Cup data mining competition. We\ndid not use this dataset in earlier examples, such as Table 3-2 and Figure 3-18, because\nthese attribute names and values have been anonymized extensively to preserve customer privacy. This leaves very little meaning in the attributes and their values, which\nwould have interfered with our discussions. However, we can demonstrate the model\nperformance analytics with the sanitized data. From the website:\n\nThe KDD Cup 2009 offers the opportunity to work on large marketing databases from\nthe French Telecom company Orange to predict the propensity of customers to switch\nprovider (churn), buy new products or services (appetency), or buy upgrades or add-ons\nproposed to them to make the sale more profitable (up-selling). The most practical way,\nin a CRM system, to build knowledge on customer is to produce scores.\n\nA score (the output of a model) is an evaluation for all instances of a target variable to\nexplain (i.e., churn, appetency or up-selling). Tools which produce scores allow to project,\non a given population, quantifiable information. The score is computed using input variables which describe instances. Scores are then used by the information system (IS), for\nexample, to personalize the customer relationship.\n\nLittle of the dataset is worth describing because it has been thoroughly sanitized, but its\nclass skew is worth mentioning. There are about 47,000 instances altogether, of which\nabout 7% are marked as churn (positive examples) and the remaining 93% are not\n(negatives). This is not severe skew, but it’s worth noting for reasons that will become\nclear.\n\nWe emphasize that the intention is not to propose good solutions for this problem, or\nto suggest which models might work well, but simply to use the domain as a testbed to\nillustrate the ideas about evaluation we’ve been developing. Little effort has been done\nto tune performance. We will train and test several models: a classification tree, a logistic\nregression equation, and a nearest-neighbor model. We will also use a simple Bayesian\nclassifier called Naive Bayes, not discussed until Chapter 9. For the purpose of this\nsection, details of the models are unimportant; all the models are “black boxes” with\ndifferent performance characteristics. We’re using the evaluation and visualization\ntechniques introduced in the last chapters to understand their characteristics.\n\nExample: Performance Analytics for Churn Modeling\n\n|\n\n223", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00248", "page_num": 248, "segment": "Let’s begin with a very naive evaluation. We’ll train on the complete dataset and then\ntest on the same dataset we trained on. We’ll also measure simple classification accuracies. The results are shown in Table 8-1.\n\nTable 8-1. Accuracy values of four classifiers trained and tested on the complete KDD\nCup 2009 churn problem.\n\nModel\n\nAccuracy\n\nClassification tree\n\nLogistic regression\n\n95%\n\n93%\n\nk-Nearest Neighbor\n\n100%\n\nNaive Bayes\n\n76%\n\nSeveral things are striking here. First, there appears to be a wide range of performance\n---from 76% to 100%. Also, since the dataset has a base rate of 93%, any classifier should\nbe able to achieve at least this minimum accuracy. This makes the Naive Bayes result\nlook strange since it’s significantly worse. Also, at 100% accuracy, the k-Nearest Neighbor classifier looks suspiciously good.5\n\nBut this test was performed on the training set, and by now (having read Chapter 5)\nyou realize such numbers are unreliable, if not completely meaningless. They are more\nlikely to be an indication of how well each classifier can memorize (overfit) the training\nset than anything else. So instead of investigating these numbers further, let’s redo the\nevaluation properly using separate training and test sets. We could just split the dataset\nin half, but instead we’ll use the cross-validation procedure discussed in “From Holdout\nEvaluation to Cross-Validation” on page 126. This will not only ensure proper separation\nof datasets but also provide a measure of variation in results. The results are shown in\nTable 8-2.\n\nTable 8-2. Accuracy and AUC values of four classifiers on the KDD Cup 2009 churn\nproblem. These values are from ten-fold cross-validation.\n\nModel\n\nClassification Tree\n\nLogistic Regression\n\nk-Nearest Neighbor\n\nNaive Bayes\n\nAccuracy (%)\n\n91.8 ± 0.0\n\n93.0 ± 0.1\n\n93.0 ± 0.0\n\n76.5 ± 0.6\n\nAUC\n\n0.614 ± 0.014\n\n0.574 ± 0.023\n\n0.537 ± 0.015\n\n0.632 ± 0.019\n\n5. Optimism can be a fine thing, but as a rule of thumb in data mining, any results that show perfect performance\n\non a real-world problem should be mistrusted.\n\n224\n\n|\n\nChapter 8: Visualizing Model Performance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00249", "page_num": 249, "segment": "Each number is an average of ten-fold cross validation followed by a “±” sign and the\nstandard deviation of the measurements. Including a standard deviation may be regarded as a kind of “sanity check”: a large standard deviation indicates the test results are\nvery erratic, which could be the source of various problems such as the dataset being\ntoo small or the model being a very poor match to a portion of the problem.\n\nThe accuracy numbers have all dropped considerably, except for Naive Bayes, which is\nstill oddly low. The standard deviations are fairly small compared to the means so there\nis not a great deal of variation in the performance on the folds. This is good.\n\nAt the far right is a second value, the Area Under the ROC Curve (commonly abbreviated\nAUC). We briefly discussed this AUC measure back in “The Area Under the ROC Curve\n(AUC)” on page 219, noting it as a good general summary statistic of the predictiveness\nof a classifier. It varies from zero to one. A value of 0.5 corresponds to randomness (the\nclassifier cannot distinguish at all between positives and negatives) and a value of one\nmeans that it is perfect in distinguishing them. One of the reasons accuracy is a poor\nmetric is that it is misleading when datasets are skewed, which this one is (93% negatives\nand 7% positives).\n\nRecall that we introduced fitting curves back in “Overfitting Examined” on page 113 as\na way to detect when a model is overfitting. Figure 8-8 shows fitting curves for the\nclassification tree model on this churn domain. The idea is that as a model is allowed\nto get more and more complex it typically fits the data more and more closely, but at\nsome point it is simply memorizing idiosyncracies of the particular training set rather\nthan learning general characteristics of the population. A fitting curve plots model\ncomplexity (in this case, the number of nodes in the tree) against a performance measure\n(in this case, AUC) using two datasets: the set it was trained upon and a separate holdout\nset. When performance on the holdout set starts to decrease, overfitting is occurring,\nand Figure 8-8 does indeed follow this general pattern.6 The classification tree definitely\nis overfitting, and the other models probably are too. The “sweet spot” where holdout\nperformance is maximum is at about 100 tree nodes, beyond which the performance\non the holdout data declines.\n\n6. Note that the x axis is log scale so the righthand side of the graph looks compressed.\n\nExample: Performance Analytics for Churn Modeling\n\n|\n\n225", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00250", "page_num": 250, "segment": "Figure 8-8. Fitting curves for a classification tree on the churn data: the change in the\narea under the ROC curve (AUC) as we increase the allowed complexity (size) of the\ntree. The performance on the training data (upper curve) continues to increase whereas\nthe performance on the holdout data peaks and then declines.\n\nLet’s return to the model comparison figures in Table 8-2. These values are taken from\na reasonably careful evaluation using holdout data, so they are less suspicious. However,\nthey do raise some questions. There are two things to note about the AUC values. One\nis that they are all fairly modest. This is unsurprising with real-world domains: many\ndatasets simply have little signal to be exploited, or the data science problem is formulated after the easier problems have already been solved. Customer churn is a difficult\nproblem so we shouldn’t be too surprised by these modest AUC scores. Even modest\nAUC scores may lead to good business results.\n\nThe second interesting point is that Naive Bayes, which has the lowest accuracy of the\ngroup, has the highest AUC score in Table 8-2. What’s going on here? Let’s take a look\nat a sample confusion matrix of Naive Bayes, with the highest AUC and lowest accuracy,\nand compare it with the confusion matrix of k-NN (lowest AUC and high accuracy) on\nthe same dataset. Here is the Naive Bayes confusion matrix:\n\n226\n\n|\n\nChapter 8: Visualizing Model Performance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00251", "page_num": 251, "segment": "p\n\nn\n\nY\n\n127 (3%)\n\n848 (18%)\n\nN 200 (4%)\n\n3518 (75%)\n\nHere is the k-Nearest Neighbors confusion matrix on the same test data:\n\np\n\nn\n\nY\n\n3 (0%)\n\n15 (0%)\n\nN 324 (7%)\n\n4351 (93%)\n\nWe see from the k-NN matrix that it rarely predicts churn---the Y row is almost empty.\nIn other words, it is performing very much like a base-rate classifier, with a total accuracy\nof just about 93%. On the other hand, the Naive Bayes classifier makes more mistakes\n(so its accuracy is lower) but it identifies many more of the churners. Figure 8-9 shows\nthe ROC curves of a typical fold of the cross-validation procedure. Note that the curves\ncorresponding to Naive Bayes (NB) and Classification Tree (Tree) are somewhat more\n“bowed” than the others, indicating their predictive superiority.\n\nFigure 8-9. ROC curves of the classifiers on one fold of cross-validation for the churn\nproblem.\n\nAs we said, ROC curves have a number of nice technical properties but they can be hard\nto read. The degree of “bowing” and the relative superiority of one curve to another can\nbe difficult to judge by eye. Lift and profit curves are sometimes preferable, so let’s\nexamine these.\n\nExample: Performance Analytics for Churn Modeling\n\n|\n\n227", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00252", "page_num": 252, "segment": "Lift curves have the advantage that they don’t require us to commit to any costs yet so\nwe begin with those, shown in Figure 8-10.\n\nFigure 8-10. Lift curves for the churn domain.\n\nThese curves are averaged over the 10 test sets of the cross-validation. The classifiers\ngenerally peak very early then trail off down to random performance (Lift=1). Both Tree\n(Classification tree) and NB (Naive Bayes) perform very well. Tree is superior up\nthrough about the first 25% of the instances, after which it is dominated by NB. Both\nk-NN and Logistic Regression (LR) perform poorly here and have no regions of superiority. Looking at this graph, if you wanted to target the top 25% or less of customers,\nyou’d choose the classification tree model; if you wanted to go further down the list you\nshould choose NB. Lift curves are sensitive to the class proportions, so if the ratio of\nchurners to nonchurners changed these curves would change also.\n\n228\n\n|\n\nChapter 8: Visualizing Model Performance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00253", "page_num": 253, "segment": "A note on combining classifiers\nLooking at these curves, you might wonder, “If Tree is best for the top\n25%, and NB is best for the remainder, why don’t we just use Tree’s top\n25% then switch to NB’s list for the rest?” This is a clever idea, but you\nwon’t necessarily get the best of both classifiers that way. The reason,\nin short, is that the two orderings are different and you can’t simply\npick-and-choose segments from each and expect the result to be optimal. The evaluation curves are only valid for each model individually, and all bets are off when you start mixing orderings from each.\n\nBut classifiers can be combined in principled ways, such that the combination outperforms any individual classifier. Such combinations are\ncalled ensembles, and they are discussed in “Bias, Variance, and Ensemble Methods” on page 306.\n\nAlthough the lift curve shows you the relative advantage of each model, it does not tell\nyou how much profit you should expect to make---or even whether you’d make a profit\nat all. For that purpose we use a profit curve, which incorporates assumptions about\ncosts and benefits and displays expected value.\n\nLet’s ignore the actual details of churn in wireless for the moment (we will return to\nthese explicitly in Chapter 11). To make things interesting with this dataset, let’s make\ntwo sets of assumptions about costs and benefits. In the first scenario, let’s assume an\nexpense of $3 for each offer and a gross benefit of $30, so a true positive gives us a net\nprofit of $27 and a false positive gives a net loss of $3. This is a 9-to-1 profit ratio. The\nresulting profit curves are shown in Figure 8-11. The classification tree is superior for\nthe highest cutoff thresholds, and Naive Bayes dominates for the remainder of the possible cutoff thresholds. Maximum profit would be achieved in this scenario by targeting\nroughly the first 20% of the population.\n\nExample: Performance Analytics for Churn Modeling\n\n|\n\n229", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00254", "page_num": 254, "segment": "Figure 8-11. Profit curves of four classifiers on the churn domain, assuming a 9-to-1\nratio of benefit to cost.\n\nIn the second scenario, we assume the same expense of $3 for each offer (so the false\npositive cost doesn’t change) but we assume a higher gross benefit ($39), so a true positive now nets us a profit of $36. This is a 12-to-1 profit ratio. The curves are shown in\nFigure 8-12. As you might expect, this scenario has much higher maximum profit than\nthe previous scenario. More importantly it demonstrates different profit maxima. One\npeak is with the Classification Tree at about 20% of the population and the second peak,\nslightly higher, occurs when we target the top 35% of the population with NB. The\ncrossover point between Tree and LR occurs at the same place on both graphs, however:\nat about 25% of the population. This illustrates the sensitivity of profit graphs to the\nparticular assumptions about costs and benefits.\n\n230\n\n|\n\nChapter 8: Visualizing Model Performance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00255", "page_num": 255, "segment": "Figure 8-12. Profit curves of four classifiers on the churn domain. These curves assume\na more lucrative 12-to-1 ratio (compare with Figure 8-11).\n\nWe conclude this section by reiterating that these graphs are just meant to illustrate the\ndifferent techniques for model evaluation. Little effort was made to tune the induction\nmethods to the problem, and no general conclusions should be drawn about the relative\nmerits of these model types or their suitability for churn prediction. We deliberately\nproduced a range of classifier performance to illustrate how the graphs could reveal\ntheir differences.\n\nSummary\nA critical part of the data scientist’s job is arranging for proper evaluation of models and\nconveying this information to stakeholders. Doing this well takes experience, but it is\nvital in order to reduce surprises and to manage expectations among all concerned.\nVisualization of results is an important piece of the evaluation task.\n\nWhen building a model from data, adjusting the training sample in various ways may\nbe useful or even necessary; but evaluation should use a sample reflecting the original,\nrealistic population so that the results reflect what will actually be achieved.\n\nSummary\n\n|\n\n231", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00256", "page_num": 256, "segment": "When the costs and benefits of decisions can be specified, the data scientist can calculate\nan expected cost per instance for each model and simply choose whichever model produces the best value. In some cases a basic profit graph can be useful to compare models\nof interest under a range of conditions. These graphs may be easy to comprehend for\nstakeholders who are not data scientists, since they reduce model performance to their\nbasic “bottom line” cost or profit.\n\nThe disadvantage of a profit graph is that it requires that operating conditions be known\nand specified exactly. With many real-world problems, the operating conditions are\nimprecise or change over time, and the data scientist must contend with uncertainty. In\nsuch cases other graphs may be more useful. When costs and benefits cannot be specified\nwith confidence, but the class mix will likely not change, a cumulative response or lift\ngraph is useful. Both show the relative advantages of classifiers, independent of the value\n(monetary or otherwise) of the advantages.\n\nFinally, ROC curves are a valuable visualization tool for the data scientist. Though they\ntake some practice to interpret readily, they separate out performance from operating\nconditions. In doing so they convey the fundamental trade-offs that each model is\nmaking.\n\nA great deal of work in the Machine Learning and Data Mining communities involves\ncomparing classifiers in order to support various claims about learning algorithm superiority. As a result, much has been written about the methodology of classifier comparison. For the interested reader a good place to start is Thomas Dietterich’s (1998)\narticle “Approximate Statistical Tests for Comparing Supervised Classification Learning\nAlgorithms,” and the book Evaluating Learning Algorithms: A Classification Perspective (Japkowicz & Shah, 2011).\n\n232\n\n|\n\nChapter 8: Visualizing Model Performance", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00257", "page_num": 257, "segment": "CHAPTER 9\nEvidence and Probabilities\n\nFundamental concepts: Explicit evidence combination with Bayes’ Rule; Probabilistic\nreasoning via assumptions of conditional independence.\n\nExemplary techniques: Naive Bayes classification; Evidence lift.\n\nSo far we have examined several different methods for using data to help draw conclusions about some unknown quantity of a data instance, such as its classification. Let’s\nnow examine a different way of looking at drawing such conclusions. We could think\nabout the things that we know about a data instance as evidence for or against different\nvalues for the target. The things that we know about the data instance are represented\nas the features of the instance. If we knew the strength of the evidence given by each\nfeature, we could apply principled methods for combining evidence probabilistically to\nreach a conclusion as to the value for the target. We will determine the strength of any\nparticular piece of evidence from the training data.\n\nExample: Targeting Online Consumers With\nAdvertisements\nTo illustrate, let’s consider another business application of classification: targeting online\ndisplay advertisements to consumers, based on what webpages they have visited in the\npast. As consumers, we have become used to getting a vast amount of information and\nservices on the Web seemingly for free. Of course, the “for free” part is very often due\nto the existence or promise of revenue from online advertising, similar to how broadcast\ntelevision is “free.” Let’s consider display advertising---the ads that appear on the top,\nsides, and bottom of pages full of content that we are reading or otherwise consuming.\n\nDisplay advertising is different from search advertising (e.g., the ads that appear with\nthe results of a Google search) in an important way: for most webpages, the user has\nnot typed in a phrase related to exactly what she is looking for. Therefore, the targeting\nof an advertisement to the user needs to be based on other sorts of inference. For several\n\n233", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00258", "page_num": 258, "segment": "chapters now we have been talking about a particular sort of inference: inferring the\nvalue of an instance’s target variable from the values of the instance’s features. Therefore,\nwe could apply the techniques we already have covered to infer whether a particular\nuser would be interested in an advertisement. In this chapter we will introduce a different\nway of looking at the problem, that has wide applicability and is quite easy to apply.\n\nLet’s define our ad targeting problem more precisely. What will be an instance? What\nwill be the target variable? What will be the features? How will we get the training data?\n\nLet’s assume that we are working for a very large content provider (a “publisher”) who\nhas a wide variety of content, sees many online consumers, and has many opportunities\nto show advertisements to these consumers. For example, Yahoo! has a vast number of\ndifferent advertisement-supported web “properties,” which we can think of as different\n“content pieces.” In addition, recently (as of this writing) Yahoo! agreed to purchase the\nblogging site Tumblr, which has 50 billion blog posts across over 100 million blogs. Each\nof these might also be seen as a “content piece” that gives some view into the interests\nof a consumer who reads it. Similarly, Facebook might consider each “Like” that a consumer makes as a piece of evidence regarding the consumer’s tastes, which might help\ntarget ads as well.\n\nFor simplicity, assume we have one advertising campaign for which we would like to\ntarget some subset of the online consumers that visit our sites. This campaign is for the\nupscale hotel chain, Luxhote. The goal of Luxhote is for people to book rooms. We have\nrun this campaign in the past, selecting online consumers randomly. We now want to\nrun a targeted campaign, hopefully getting more bookings per dollar spent on ad impressions.1\n\nTherefore, we will consider a consumer to be an instance. Our target variable will be:\ndid/will the consumer book a Luxhote room within one week after having seen the\nLuxhote advertisement? Through the magic of browser cookies,2 in collaboration with\nLuxhote we can observe which consumers book rooms. For training, we will have a\nbinary value for this target variable for each consumer. In use, we will estimate the\nprobability that a consumer will book a room after having seen an ad, and then, as our\nbudget allows, target some subset of the highest probability consumers.\n\nWe are left with a key question: what will be the features we will use to describe the\nconsumers, such that we might be able to differentiate those that are more or less likely\nto be good customers for Luxhote? For this example, we will consider a consumer to be\ndescribed by the set of content pieces that we have observed her to have viewed (or\nLiked) previously, again as recorded via browser cookies or some other mechanism. We\n\n1. An ad impression is when an ad is displayed somewhere on a page, regardless of whether a user clicks it.\n\n2. A browser exchanges small amounts of information (“cookies”) with the sites that are visited, and saves sitespecific information that can be retrieved later by the same website.\n\n234\n\n|\n\nChapter 9: Evidence and Probabilities", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00259", "page_num": 259, "segment": "have many different kinds of content: finance, sports, entertainment, cooking blogs, etc.\nWe might pick several thousand content pieces that are very popular, or we may consider\nhundreds of millions. We believe that some of these (e.g., finance blogs) are more likely\nto be visited by good prospects for Luxhote, while other content pieces are seen as less\nlikely (e.g., a tractor-pull fan page).\n\nHowever, for this exercise we do not want to rely on our presumptions about such\ncontent, nor do we have the resources to estimate the evidence potential for each content\npiece manually. Furthermore, while humans are quite good at using our knowledge and\ncommon sense to recognize whether evidence is likely to be “for” or “against,” humans\nare notoriously bad at estimating the precise strength of the evidence. We would like\nour historical data to estimate both the direction and the strength of the evidence. We\nnext will describe a very broadly applicable framework both for evaluating the evidence,\nand for combining it to estimate the resulting likelihood of class membership (here, the\nlikelihood that a consumer will book a room after having seen the ad).\n\nIt turns out that there are many other problems that fit the mold of our example: classification/class probability estimation problems where each instance is described by a\nset of pieces of evidence, possibly taken from a very large total collection of possible\nevidence. For example, text document classification fits exactly (which we’ll discuss next\nin Chapter 10). Each document is a collection of words, from a very large total vocabulary. Each word can possibly provide some evidence for or against the classification, and\nwe would like to combine the evidence. The techniques that we introduce next are\nexactly those used in many spam detection systems: an instance is an email message,\nthe target classes are spam or not-spam, and the features are the words and symbols in\nthe email message.\n\nCombining Evidence Probabilistically\n\nMore math than usual ahead\nTo discuss the ideas of combining evidence probabilistically, we need\nto introduce some probability notation. You do not have to have\nlearned (or remember) probability theory---the notions are quite intuitive, and we will not get beyond the basics. The notation allows us\nto be precise. It might look like there’s a lot of math in what follows,\nbut you’ll see that it’s quite straightforward.\n\nWe are interested in quantities such as the probability of a consumer booking a room\nafter being shown an ad. We actually need to be a little more specific: some particular\nconsumer? Or just any consumer? Let’s start with just any consumer: what is the probability that if you show an ad to just any consumer, she will book a room? As this is our\ndesired classification, let’s call this quantity C. We will represent the probability of an\n\nCombining Evidence Probabilistically\n\n|\n\n235", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00260", "page_num": 260, "segment": "event C as p(C). If we say p(C) = 0.0001, that means that if we were to show ads randomly\nto consumers, we would expect about 1 in 10,000 to book rooms.3\n\nNow, we are interested in the probability of C given some evidence E, such as the set of\nwebsites visited by a particular consumer. The notation for this quantity is p(C|E), which\nis read as “the probability of C given E,” or “the probability of C conditioned on E.” This\nis an example of a conditional probability, and the “|” is sometimes called the “conditioning bar.” We would expect that p(C|E) would be different based on different collections of evidence E---in our example, different sets of websites visited.\n\nAs mentioned above, we would like to use some labeled data, such as the data from our\nrandomly targeted campaign, to associate different collections of evidence E with different probabilities. Unfortunately, this introduces a key problem. For any particular\ncollection of evidence E, we probably have not seen enough cases with exactly that same\ncollection of evidence to be able to infer the probability of class membership with any\nconfidence. In fact, we may not have seen this particular collection of evidence at all! In\nour example, if we are considering thousands of different websites, what is the chance\nthat in our training data we have seen a consumer with exactly the same visiting patterns\nas a consumer we will see in the future? It is infinitesimal. Therefore, what we will do\nis to consider the different pieces of evidence separately, and then combine evidence.\nTo discuss this further, we need a few facts about combining probabilities.\n\nJoint Probability and Independence\nLet’s say we have two events, A and B. If we know p(A) and p(B), can we say what is the\nprobability that both A and B occur? Let’s call that p(AB). This is called the joint probability.\n\nThere is one special case when we can: if events A and B are independent. A and B being\nindependent means that knowing about one of them tells you nothing about the likelihood of the other. The typical example used to illustrate independence is rolling a fair\ndie; knowing the value of the first roll tells you nothing about the value of the second.\nIf event A is “roll #1 shows a six” and event B is “roll #2 shows a six”, then p(A) = 1/6\nand p(B) = 1/6, and importantly, even if we know that roll #1 shows a six, still p(B) =\n1/6. In this case, the events are independent, and in the case of independent events,\np(AB) = p(A) · p(B)---we can calculate the probability of the “joint” event AB by multiplying the probabilities of the individual events. In our example, p(AB) = 1/36.\n\nHowever, we cannot in general compute the probabilities of joint events in this way. If\nthis isn’t clear, think about the case of rolling a trick die. In my pocket I have six trick\ndice. Each trick die has one of the numbers from one to six on all faces---all faces show\n\n3. This is not necessarily a reasonable response rate for any particular advertisement, just an illustrative example.\nPurchase rates attributable to online advertisements generally seem very small to those outside the industry.\nIt is important to realize that the cost of placing one ad often is quite small as well.\n\n236\n\n|\n\nChapter 9: Evidence and Probabilities", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00261", "page_num": 261, "segment": "the same number. I pull a die at random from my pocket, and then roll it twice. In this\ncase, p(A) = p(B) = 1/6 (because I could have pulled any of the six dice out with equal\nlikelihood). However, p(AB) = 1/6 as well, because the events are completely dependent!\nIf the first roll is a six, so will be the second (and vice versa).\n\nThe general formula for combining probabilities that takes care of dependencies between events is:\n\nEquation 9-1. Joint probability using conditional probability\n\np(AB) = p(A) · p(B | A)\n\nThis is read as: the probability of A and B is the probability of A times the probability\nof B given A. In other words, given that you know A, what is the probability of B? Take\na minute to make sure that has sunk in.\n\nWe can illustrate with our two dice examples. In the independent case, since knowing\nA tells us nothing about p(B), then p(B|A) = p(B), and we get our formula from above,\nwhere we simply multiply the individual probabilities. In our trick die case, p(B|A) =\n1.0, since if the first roll was a six, then the second roll is guaranteed to be a six. Thus,\np(AB) = p(A) · 1.0 = p(A) = 1/6, just as expected. In general, events may be completely\nindependent, completely dependent, or somewhere in between. In the latter case, knowing something about one event changes the likelihood of the other. In all cases, our\nformula p(AB) = p(A) · p(B|A) combines the probabilities properly.\n\nWe’ve gone through this detail for a very important reason. This formula is the basis for\none of the most famous equations in data science, and in fact in science generally.\n\nBayes’ Rule\nNotice that in p(AB) = p(A)p(B|A) the order of A and B seems rather arbitrary---and it\nis. We could just as well have written:\n\np(AB) = p(B) · p(A | B)\n\nThis means:\n\np(A) · p(B | A) = p(AB) = p(B) · p(A | B)\n\nAnd so:\n\np(A) · p(B | A) = p(B) · p(A | B)\n\nCombining Evidence Probabilistically\n\n|\n\n237", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00262", "page_num": 262, "segment": "If we divide both sides by p(A) we get:\n\np(B | A) =\n\np(A | B) · p(B)\np(A)\n\nNow, let’s consider B to be some hypothesis that we are interested in assessing the likelihood of, and A to be some evidence that we have observed. Renaming with H for\nhypothesis and E for evidence, we get:\n\np(H | E) =\n\np(E | H ) · p(H )\np(E)\n\nThis is the famous Bayes’ Rule, named after the Reverend Thomas Bayes who derived\na special case of the rule back in the 18th century. Bayes’ Rule says that we can compute\nthe probability of our hypothesis H given some evidence E by instead looking at the\nprobability of the evidence given the hypothesis, as well as the unconditional probabilities of the hypothesis and the evidence.\n\nNote: Bayesian methods\nBayes’ Rule, combined with the important fundamental principle of\nthinking carefully about conditional independencies, are the foundation for a vast amount of more advanced data science techniques that\nwe will not cover in this book. These include Bayesian networks, probabilistic topic models, probabilistic relational models, Hidden Markov Models, Markov random fields, and others.\n\nImportantly, the last three quantities may be easier to determine than the quantity of\nultimate interest---namely, p(H|E). To see this, consider a (simplified) example from\nmedical diagnosis. Assume you’re a doctor and a patient arrives with red spots. You\nguess (hypothesize) that the patient has measles. We would like to determine the probability of our hypothesized diagnosis (H = measles), given the evidence (E = red spots).\nIn order to directly estimate p(measles|red spots) we would need to think through all\nthe different reasons a person might exhibit red spots and what proportion of them\nwould be measles. This is likely impossible even for the most broadly knowledgeable\nphysician.\n\nHowever, consider instead the task of estimating this quantity using the righthand side\nof Bayes’ Rule.\n\n• p(E|H) is the probability that one has red spots given that one has measles. An expert\nin infectious diseases may well know this or be able to estimate it relatively accurately.\n\n238\n\n|\n\nChapter 9: Evidence and Probabilities", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00263", "page_num": 263, "segment": "• p(H) is simply the probability that someone has measles, without considering any\n\nevidence; that’s just the prevalence of measles in the population.\n\n• p(E) is the probability of the evidence: what’s the probability that someone has red\nspots---again, simply the prevalence of red spots in the population, which does not\nrequire complicated reasoning about the different underlying causes, just observation and counting.\n\nBayes’ Rule has made estimating p(H|E) much easier. We need three pieces of information, but they’re much easier to estimate than the original value is.\n\np(E) may still be difficult to compute. In many cases, though, it does\nnot have to be computed, because we are interested in comparing the\nprobabilities of different hypotheses given the same evidence. We will\nsee this later.\n\nApplying Bayes’ Rule to Data Science\nIt is possibly quite obvious now that Bayes’ Rule should be critical in data science. Indeed,\na very large portion of data science is based on “Bayesian” methods, which have at their\ncore reasoning based on Bayes’ Rule. Describing Bayesian methods broadly is well beyond the scope of this book. We will introduce the most fundamental ideas, and then\nshow how they apply in the most basic of Bayesian techniques---which is used a great\ndeal. Let’s rewrite Bayes’ Rule yet again, but now returning to classification.\n\nEquation 9-2. Bayes Rule for classification\n\np(C = c | ) =\n\np( | C = c) · p(C = c)\np()\n\nIn Equation 9-2, we have four quantities. On the lefthand side is the quantity we would\nlike to estimate. In the context of a classification problem, this is the probability that the\ntarget variable C takes on the class of interest c after taking the evidence E (the vector\nof feature values) into account. This is called the posterior probability.\n\nBayes’ Rule decomposes the posterior probability into the three quantities that we see\non the righthand side. We would like to be able to compute these quantities from the\ndata:\n\n1. p(C = c) is the “prior” probability of the class, i.e., the probability we would assign\nto the class before seeing any evidence. In Bayesian reasoning generally, this could\ncome from several places. It could be (i) a “subjective” prior, meaning that it is the\n\nApplying Bayes’ Rule to Data Science\n\n|\n\n239", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00264", "page_num": 264, "segment": "belief of a particular decision maker based on all her knowledge, experience, and\nopinions; (ii) a “prior” belief based on some previous application(s) of Bayes’ Rule\nwith other evidence, or (iii) an unconditional probability inferred from data. The\nspecific method we introduce below takes approach (iii), using as the class prior the\n“base rate” of c---the prevalence of c in the population as a whole. This is calculated\neasily from the data as the percentage of all examples that are of class c.\n\n2. p(E |C = c) is the likelihood of seeing the evidence E---the particular features of the\nexample being classified---when the class C = c. One might see this as a “generative”\nquestion: if the world (the “data generating process”) generated an instance of class\nc, how often would it look like E? This likelihood might be calculated from the data\nas the percentage of examples of class c that have feature vector E.\n\n3. Finally, p(E) is the likelihood of the evidence: how common is the feature representation E among all examples? This might be calculated from the data as the\npercentage occurrence of E among all examples.\n\nEstimating these three values from training data, we could calculate an estimate for the\nposterior p(C = c| E) for a particular example in use. This could be used directly as an\nestimate of class probability, possibly in combination with costs and benefits as described in Chapter 7. Alternatively, p(C = c| E) could be used as a score to rank instances\n(e.g., estimating those that are most likely to respond to our advertisement). Or, we\ncould choose as the classification the maximum p(C = c| E) across the different values c.\n\nUnfortunately, we return to the major difficulty we mentioned above, which keeps\nEquation 9-2 from being used directly in data mining. Consider E to be our usual vector\nof attribute values <e1 , e2 , ⋯, ek>, a possibly large, specific collection of conditions.\nApplying Equation 9-2 directly would require knowing the p(E|c) as p(e1 ∧ e2 ∧ ⋯ ∧\nek|c).4 This is very specific and very difficult to measure. We may never see a specific\nexample in the training data that exactly matches a given E in our testing data, and even\nif we do it may be unlikely we’ll see enough of them to estimate a probability with any\nconfidence.\n\nBayesian methods for data science deal with this issue by making assumptions of probabilistic independence. The most broadly used method for dealing with this complication is to make a particularly strong assumption of independence.\n\nConditional Independence and Naive Bayes\nRecall from above the notion of independence: two events are independent if knowing\none does not give you information on the probability of the other. Let’s extend that\nnotion ever so slightly.\n\n4. The ∧ operator means “and.”\n\n240\n\n|\n\nChapter 9: Evidence and Probabilities", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00265", "page_num": 265, "segment": "Conditional independence is the same notion, except using conditional probabilities.\nFor our purposes, we will focus on the class of the example as the condition (since in\nEquation 9-2 we are looking at the probability of the evidence given the class). Conditional independence is directly analogous to the unconditional independence we discussed above. Specifically, without assuming independence, to combine probabilities\nwe need to use Equation 9-1 from above, augmented with the |C condition:\n\np(AB | C) = p(A | C) · p(B | AC)\n\nHowever, as above, if we assume that A and B are conditionally independent given C,5\nwe can now combine the probabilities much more easily:\n\np(AB | C) = p(A | C) · p(B | C)\n\nThis makes a huge difference in our ability to compute the probabilities from the data.\nIn particular, for the conditional probability p(E |C=c) in Equation 9-2, let’s assume that\nthe attributes are conditionally independent, given the class. In other words, in\np(e1∧e2∧⋯∧ek|c), each ei is independent of every other ej given the class c:\n\np( | c) =\n\n=\n\np(e1 ∧ e2 ∧ ⋯ ∧ ek | c)\np(e1 | c) · p(e2 | c) ⋯ p(ek | c)\n\nEach of the p(ei | c) terms can be computed directly from the data, since now we simply\nneed to count up the proportion of the time that we see individual feature ei in the\ninstances of class c, rather than looking for an entire matching feature vector. There are\nlikely to be relatively many occurrences of ei.6 Combining this with Equation 9-2 we get\nthe Naive Bayes equation as shown in Equation 9-3.\n\nEquation 9-3. Naive Bayes equation\n\np(c | ) =\n\np(e1 | c) · p(e2 | c) ⋯ p(ek | c) · p(c)\np()\n\nThis is the basis of the Naive Bayes classifier. It classifies a new example by estimating\nthe probability that the example belongs to each class and reports the class with highest\nprobability.\n\n5. This is a weaker assumption than assuming unconditional independence, by the way.\n\n6. And in the cases where there are not we can use a statistical correction for small counts. The difference is that\n\nwe will not be doing that for all the evidence, as we would have considering the entire E.\n\nApplying Bayes’ Rule to Data Science\n\n|\n\n241", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00266", "page_num": 266, "segment": "If you will allow two paragraphs on a technical detail: at this point you might notice the\np(E) in the denominator of Equation 9-3 and say, whoa there---if I understand you, isn’t\nthat going to be almost as difficult to compute as p(E |C)? It turns out that generally\np(E) never actually has to be calculated, for one of two reasons. First, if we are interested\nin classification, what we mainly care about is: of the different possible classes c, for\nwhich one is p(C| E) the greatest? In this case, E is the same for all, and we can just look\nto see which numerator is larger.\n\nIn cases where we would like the actual probability estimates, we still can get around\ncomputing p(E) in the denominator. This is because the classes often are mutually exclusive and exhaustive, meaning that every instance will belong to one and only one\nclass. In our Luxhote example, a consumer either books a room or does not. Informally,\nif we see evidence E it belongs either to c0 or c1. Mathematically:\n\np() =\n\n=\n\np( ∧ c0) + p( ∧ c1)\np( | c0) · p(c0) + p( | c1) · p(c1)\n\nOur independence assumption allows us to rewrite this as:\n\np() =\n\np(e1 | c0) · p(e2 | c0) ⋯ p(ek | c0) · p(c0)\n+ p(e1 | c1) · p(e2 | c1) ⋯ p(ek | c1) · p(c1)\n\np() = p(e1 | c0) · p(e2 | c0) ⋯ p(ek | c0) · p(c0) + p(e1 | c1) · p(e2 | c1) ⋯ p(ek | c1) · p(c1)\n\nCombining this with Equation 9-3, we get a version of the Naive Bayes equation with\nwhich we can compute the posterior probabilities easily from the data:\n\np(c0 | ) =\n\np(e1 | c0) · p(e2 | c0) ⋯ p(ek | c0) · p(c0)\np(e1 | c0) · p(e2 | c0) ⋯ p(ek | c0) + p(e1 | c1) · p(e2 | c1) ⋯ p(ek | c1)\n\nAlthough it has lots of terms in it, each one is either the evidence “weight” of some\nparticular individual piece of evidence, or a class prior.\n\nAdvantages and Disadvantages of Naive Bayes\nNaive Bayes is a very simple classifier, yet it still takes all the feature evidence into\naccount. It is very efficient in terms of storage space and computation time. Training\nconsists only of storing counts of classes and feature occurrences as each example is\nseen. As mentioned, p(c) can be estimated by counting the proportion of examples of\nclass c among all examples. p(ei|c) can be estimated by the proportion of examples in\nclass c for which feature ei appears.\n\n242\n\n|\n\nChapter 9: Evidence and Probabilities", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00267", "page_num": 267, "segment": "In spite of its simplicity and the strict independence assumptions, the Naive Bayes classifier performs surprisingly well for classification on many real-world tasks. This is\nbecause the violation of the independence assumption tends not to hurt classification\nperformance, for an intuitively satisfying reason. Specifically, consider that two pieces\nof evidence are actually strongly dependent---what does that mean? Roughly, that means\nthat when we see one we’re also likely to see the other. Now, if we treat them as being\nindependent, we’re going to see one and say “there’s evidence for the class” and see the\nother and say “there’s more evidence for the class.” So, to some extent we’ll be doublecounting the evidence. However, as long as the evidence is generally pointing us in the\nright direction, for classification the double-counting won’t tend to hurt us. In fact, it\nwill tend to make the probability estimates more extreme in the correct direction: the\nprobability will be overestimated for the correct class and underestimated for the incorrect class(es). But for classification we’re choosing the class with the greatest probability estimate, so making them more extreme in the correct direction is OK.\n\nThis does become a problem, though, if we’re going to be using the probability estimates\nthemselves---so Naive Bayes should be used with caution for actual decision-making\nwith costs and benefits, as discussed in Chapter 7. Practitioners do use Naive Bayes\nregularly for ranking, where the actual values of the probabilities are not relevant---only\nthe relative values for examples in the different classes.\n\nAnother advantage of Naive Bayes is that it is naturally an “incremental learner.” An\nincremental learner is an induction technique that can update its model one training\nexample at a time. It does not need to reprocess all past training examples when new\ntraining data become available.\n\nIncremental learning is especially advantageous in applications where training labels\nare revealed in the course of the application, and we would like the model to reflect this\nnew information as quickly as possible. For example, consider creating a personalized\njunk email classifier. When I receive a piece of junk email, I can click the “junk” button\nin my browser. Besides removing this email from my Inbox, this also creates a training\ndata point: a positive instance of spam. It would be quite useful if the model that is\nclassifying my email could be updated on the fly, and thereby immediately start classifying other similar emails as being spam. Naive Bayes is the basis of many personalized\nspam detection systems, such as the one in Mozilla’s Thunderbird.\n\nNaive Bayes is included in nearly every data mining toolkit and serves as a common\nbaseline classifier against which more sophisticated methods can be compared. We have\ndiscussed Naive Bayes using binary attributes. The basic idea presented above can be\nextended easily to multi valued categorical attributes, as well as to numeric attributes,\nas you can read about in a textbook on data mining algorithms.\n\nApplying Bayes’ Rule to Data Science\n\n|\n\n243", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00268", "page_num": 268, "segment": "A Model of Evidence “Lift”\n“Cumulative Response and Lift Curves” on page 219 presented the notion of lift as a\nmetric for evaluating a classifier. Intuitively, lift is the amount by which a classifier\nconcentrates the positive examples above the negative examples. Lift measures how\nmuch more prevalent the positive class is in the selected subpopulation over the prevalence in the population as a whole. If the prevalence of hotel bookings in a randomly\ntargeted set of consumers 0.01% and in our selected population it is 0.02%, then the\nclassifier gives us a lift of 2---the selected population has double the booking rate.\n\nWith a slight modification, we can adapt our Naive Bayes equation to model the different\nlifts attributable to the different pieces of evidence, along with a very straightforward\nway of combining them. The slight modification is to assume full feature independence,\nrather than the weaker assumption of conditional independence used for Naive Bayes.\nLet’s call this Naive-Naive Bayes, since it’s making stronger simplifying assumptions\nabout the world. Assuming full feature independence, Equation 9-3 becomes the following for Naive-Naive Bayes:\n\np(c | ) =\n\np(e1 | c) · p(e2 | c) ⋯ p(ek | c) · p(c)\np(e1) · p(e2) ⋯ p(ek)\n\nThe terms in this equation can be rearranged to yield:\n\nEquation 9-4. Probability as a product of evidence lifts\n\np(C = c | ) = p(C = c) · liftc(e1) · liftc(e2) ⋯\n\nwhere liftc(x) is defined as:\n\nliftc(x) =\n\np(x | c)\np(x)\n\nConsider how we’ll use our Bayesian classifier to classifier a new example E =<e1, e2, ⋯,\nek>. Starting at the prior probability, each piece of evidence---each feature ei---raises or\nlowers the probability of the class by a factor equal to that piece of evidence’s lift (which\nmay be less than one).\n\nConceptually, we start off with a number---call it z---set to the prior probability of class\nc. We go through our example, and for each new piece of evidence ei we multiply z by\nliftc(ei). If the lift is greater than one, the probability z is increased; if less than one, z is\ndiminished.\n\n244\n\n|\n\nChapter 9: Evidence and Probabilities", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00269", "page_num": 269, "segment": "In the case of our Luxhote example, z is the probability of booking, and it is initialized\nto 0.0001 (the prior probability, before seeing evidence, that a website visitor will book\na room). Visited a finance site? Multiply the probability of booking by a factor of two.\nVisit a truck-pull site? Multiply the probability by a factor of 0.25. And so on. After\nprocessing all of the ei evidence bits of E, the resulting product (call that zf) is the final\nprobability (belief) that E is a member of class c---in this case, that visitor E will book a\nroom.\n\nConsidered this way, it may become clearer what the independence assumption is doing.\nWe are treating each bit of evidence ei as independent of the others, so we can just\nmultiply z by their individual lifts. But any dependencies among them will result in some\ndistortion of the final value, zf. It will end up either higher or lower than it properly\nshould be. Thus the evidence lifts and their combining are very useful for understanding\nthe data, and for comparing between instances, but the actual final value of the probability should be taken with a large grain of salt.\n\nExample: Evidence Lifts from Facebook “Likes”\nLet’s examine some evidence lifts from real data. To freshen things up a little, let’s consider a brand new domain of application. Researchers Michal Kosinski, David Stillwell,\nand Thore Graepel recently published a paper (Kosinski et al., 2013) in the Proceedings\nof the National Academy of Sciences showing some striking results. What people “Like”\non Facebook 7 is quite predictive of all manner of traits that usually are not directly\napparent:\n\n• How they score on intelligence tests\n\n• How they score on psychometric tests (e.g., how extroverted or conscientious they\n\nare)\n\n• Whether they are (openly) gay\n\n• Whether they drink alcohol or smoke\n\n• Their religion and political views\n\n• And many more.\n\nWe encourage you to read the paper to understand their experimental design. You\nshould be able to understand most of the results now that you have read this book. (For\n\n7. For those unfamiliar with Facebook, it is a social networking site that allows people to share a wide variety\nof information on their interests and activities. Each user has a unique page, and Facebook encourages people\nto connect with other friends on the site. Facebook also has pages devoted to special interests such as TV\nshows, movies, bands, hobbies, and so on. Each such page has a “Like” button, and users can declare themselves to be fans by clicking it. Such “Likes” can usually be seen by other friends.\n\nExample: Evidence Lifts from Facebook “Likes”\n\n|\n\n245", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00270", "page_num": 270, "segment": "example, for evaluating how well they can predict many of the binary traits they report\nthe area under the ROC curve, which you can now interpret properly.)\n\nWhat we would like to do is to look to see what are the Likes that give strong evidence\nlifts for “high IQ,” or more specifically for scoring high on an IQ test. Taking a sample\nof the Facebook population, if we define our target variable as the binary variable\nIQ>130, about 14% of the sample is positive (has IQ>130).\n\nSo let’s examine the Likes that give the highest evidence lifts...8\n\nTable 9-1. Some Facebook page “Likes” and corresponding lifts.\n\nLike\n\nLord Of The Rings\n\nOne Manga\n\nScience\n\nPsychology\n\nThe Big Bang Theory\n\nPaulo Coelho\n\nThe Daily Show\n\nLost\n\nLie to Me\n\nHow I Met Your Mother\n\nDoctor Who\n\nHowl’s Moving Castle\n\nTron\n\nAngry Birds\n\nThe Godfather\n\nLift\n\n1.69\n\n1.57\n\n1.49\n\n1.46\n\n1.43\n\n1.41\n\n1.40\n\n1.39\n\n1.37\n\n1.35\n\n1.34\n\n1.31\n\n1.28\n\n1.25\n\n1.23\n\nLike\n\nWikileaks\n\nBeethoven\n\nNPR\n\nSpirited Away\n\nRunning\n\nRoger Federer\n\nStar Trek\n\nPhilosophy\n\nThe Onion\n\nThe Colbert Report\n\nStar Trek\n\nSheldon Cooper\n\nFight Club\n\nInception\n\nWeeds\n\nLift\n\n1.59\n\n1.52\n\n1.48\n\n1.45\n\n1.41\n\n1.40\n\n1.39\n\n1.38\n\n1.37\n\n1.35\n\n1.32\n\n1.30\n\n1.26\n\n1.25\n\n1.22\n\nSo, recalling Equation 9-4 above, and the independence assumptions made, we can\ncalculate the probability that someone has very high intelligence based on the things\nthey Like. If I Like nothing, then my estimated probability of IQ>130 (let’s call that\nHigh-IQ) is just the base rate in the population: 14%. What if on Facebook I had Liked\none item, Sheldon Cooper. Then using Equation 9-4, my estimated probability would\nincrease by 30% to 0.14 × 1.3 = 18%. If I have three Likes---Sheldon Cooper, Star Trek,\nand Lord of the Rings---then my estimated probability of High-IQ increases to 0.14 ×\n1.3 × 1.39 × 1.69 = 43%.\n\nOf course, there are also Likes that would drag down my probabability of High-IQ. So\nas not to depress you, we won’t list them here.\n\n8. Thanks to Wally Wang for his generous help with generating these results.\n\n246\n\n|\n\nChapter 9: Evidence and Probabilities", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00271", "page_num": 271, "segment": "This example also illustrates how it is important to think carefully about exactly what\nthe results mean in light of the data generating process. This does not really mean that\nliking The Lord of the Rings is a strong indication that I have very high IQ. It means\nclicking “Like” on Facebook’s page called The Lord of the Rings is a strong indication\nthat I have very high IQ. This difference is important: the act of declaring publicly that\nyou Like something is different from simply liking it, and the data we have represent an\ninstance of the former and not the latter.\n\nEvidence in Action: Targeting Consumers with Ads\nIn spite of the math that appears in this chapter, the calculations are quite simple to\nimplement---so simple they can be implemented directly in a spreadsheet. So instead of\npresenting a static example here, we have prepared a spreadsheet with a simple numerical example illustrating Naive Bayes and evidence lift on a toy version of the online adtargeting example. You’ll see how straightforward it is to use these calculations, because\nthey just involve counting things, computing proportions, and multiplying and dividing.\n\nThe spreadsheet can be downloaded here.\n\nThe spreadsheet lays out all the “evidence” (website visits for multiple visitors) and\nshows the intermediate calculations and final probability of a ficticious advertising response. You can experiment with the technique by tweaking the numbers, adding or\ndeleting visitors, and seeing how the estimated probabilities of response and the evidence lifts adjust in response.\n\nSummary\nPrior chapters presented modeling techniques that basically asked the question: “What\nis the best way to distinguish (segment) target values?” Classification trees and linear\nequations both create models this way, trying to minimize loss or entropy, which are\nfunctions of discriminability. These are termed discriminative methods, in that they try\ndirectly to discriminate different targets.\n\nThis chapter introduced a new family of methods that essentially turns the question\naround and asks: “How do different targets generate feature values?” They attempt to\nmodel how the data were generated. In the use phase, when faced with a new example\nto be classified, they apply Bayes’ Rule to their models to answer the question: “Which\nclass most likely generated this example?” Thus, in data science this approach to modeling is called generative, and it forms the basis for a large family of popular methods\n\nSummary\n\n|\n\n247", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00272", "page_num": 272, "segment": "known as Bayesian methods, because they depend critically on Bayes’ Rule. The literature on Bayesian methods is both broad and deep, and you will encounter these methods\noften in data science.\n\nThis chapter focused primarily on a particularly common and simple but very useful\nBayesian method called the Naive Bayes classifier. It is “naive” in the sense that it models\neach feature’s effect on the target independently, so it takes no feature interactions into\naccount. Because of its simplicity it is very fast and efficient, and in spite of its naïveté\nit is surprisingly (almost embarrassingly) effective. In data science it is so simple as to\nbe a common “baseline\" method---one of the first methods to be applied to any new\nproblem.\n\nWe also discussed how Bayesian reasoning using certain independence assumptions\ncan allow us to compute “evidence lifts” to examine large numbers of possible pieces of\nevidence for or against a conclusion. As an example, we showed that “Liking” Fight\nClub, Star Trek, or Sheldon Cooper on Facebook each increases by about 30% our estimation of the probability that you have a high IQ. If you were to Like all three of those,\nit would more than double our estimate that you have a high IQ.\n\n248\n\n|\n\nChapter 9: Evidence and Probabilities", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00273", "page_num": 273, "segment": "CHAPTER 10\nRepresenting and Mining Text\n\nFundamental concepts: The importance of constructing mining-friendly data representations; Representation of text for data mining.\n\nExemplary techniques: Bag of words representation; TFIDF calculation; N-grams; Stemming; Named entity extraction; Topic models.\n\nUp to this point we’ve ignored or side-stepped an important stage of the data mining\nprocess: data preparation. The world does not always present us with data in the feature\nvector representation that most data mining methods take as input. Data are represented\nin ways natural to problems from which they were derived. If we want to apply the many\ndata mining tools that we have at our disposal, we must either engineer the data representation to match the tools, or build new tools to match the data. Top-notch data\nscientists employ both of these strategies. It generally is simpler to first try to engineer\nthe data to match existing tools, since they are well understood and numerous.\n\nIn this chapter, we will focus on one particular sort of data that has become extremely\ncommon as the Internet has become a ubiquitous channel of communication: text data.\nExamining text data allows us to illustrate many real complexities of data engineering,\nand also helps us to understand better a very important type of data. We will see in\nChapter 14 that although in this chapter we focus exclusively on text data, the fundamental principles indeed generalize to other important sorts of data.\n\nWe’ve encountered text once before in this book, in the example involving clustering\nnews stories about Apple Inc. (“Example: Clustering Business News Stories”). There we\ndeliberately avoided a detailed discussion of how the news stories were prepared because\nthe focus was on clustering, and text preparation would have been too much of a digression. This chapter is devoted to the difficulties and opportunities of dealing with\ntext.\n\nIn principle, text is just another form of data, and text processing is just a special case\nof representation engineering. In reality, dealing with text requires dedicated preprocessing steps and sometimes specific expertise on the part of the data science team.\n\n249", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00274", "page_num": 274, "segment": "Entire books and conferences (and companies) are devoted to text mining. In this\nchapter we can only scratch the surface, to give a basic overview of the techniques and\nissues involved in typical business applications.\n\nFirst, let’s discuss why text is so important and why it’s difficult.\n\nWhy Text Is Important\nText is everywhere. Many legacy applications still produce or record text. Medical records, consumer complaint logs, product inquiries, and repair records are still mostly\nintended as communication between people, not computers, so they’re still “coded” as\ntext. Exploiting this vast amount of data requires converting it to a meaningful form.\n\nThe Internet may be the home of “new media,” but much of it is the same form as old\nmedia. It contains a vast amount of text in the form of personal web pages, Twitter feeds,\nemail, Facebook status updates, product descriptions, Reddit comments, blog postings\n---the list goes on. Underlying the search engines (Google and Bing) that we use everyday\nare massive amounts of text-oriented data science. Music and video may account for a\ngreat deal of traffic volume, but when people communicate with each other on the\nInternet it is usually via text. Indeed, the thrust of Web 2.0 was about Internet sites\nallowing users to interact with one another as a community, and to generate much added\ncontent of a site. This user-generated content and interaction usually takes the form of\ntext.\n\nIn business, understanding customer feedback often requires understanding text. This\nisn’t always the case; admittedly, some important consumer attitudes are represented\nexplicitly as data or can be inferred through behavior, for example via five-star ratings,\nclick-through patterns, conversion rates, and so on. We can also pay to have data collected and quantified through focus groups and online surveys. But in many cases if we\nwant to “listen to the customer” we’ll actually have to read what she’s written---in product\nreviews, customer feedback forms, opinion pieces, and email messages.\n\nWhy Text Is Difficult\nText is often referred to as “unstructured” data. This refers to the fact that text does not\nhave the sort of structure that we normally expect for data: tables of records with fields\nhaving fixed meanings (essentially, collections of feature vectors), as well as links between the tables. Text of course has plenty of structure, but it is linguistic structure---\nintended for human consumption, not for computers.\n\nWords can have varying lengths and text fields can have varying numbers of words.\nSometimes word order matters, sometimes not.\n\nAs data, text is relatively dirty. People write ungrammatically, they misspell words, they\nrun words together, they abbreviate unpredictably, and punctuate randomly. Even when\n\n250\n\n|\n\nChapter 10: Representing and Mining Text", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00275", "page_num": 275, "segment": "text is flawlessly expressed it may contain synonyms (multiple words with the same\nmeaning) and homographs (one spelling shared among multiple words with different\nmeanings). Terminology and abbreviations in one domain might be meaningless in\nanother domain---we shouldn’t expect that medical recordkeeping and computer repair\nrecords would share terms in common, and in the worst case they would conflict.\n\nBecause text is intended for communication between people, context is important, much\nmore so than with other forms of data. Consider this movie review excerpt:\n\n“The first part of this movie is far better than the second. The acting is poor and it gets\nout-of-control by the end, with the violence overdone and an incredible ending, but it’s\nstill fun to watch.”\n\nConsider whether the overall sentiment is for or against the film. Is the word incredible positive or negative? It is difficult to evaluate any particular word or phrase here\nwithout taking into account the entire context.\n\nFor these reasons, text must undergo a good amount of preprocessing before it can be\nused as input to a data mining algorithm. Usually the more complex the featurization,\nthe more aspects of the text problem can be included. This chapter can only describe\nsome of the basic methods involved in preparing text for data mining. The next few\nsubsections describe these steps.\n\nRepresentation\nHaving discussed how difficult text can be, let’s go through the basic steps to transform\na body of text into a set of data that can be fed into a data mining algorithm. The general\nstrategy in text mining is to use the simplest (least expensive) technique that works.\nNevertheless, these ideas are the key technology underlying much of web search, like\nGoogle and Bing. A later example will demonstrate basic query retrieval.\n\nFirst, some basic terminology. Most of this is borrowed from the field of Information\nRetrieval (IR). A document is one piece of text, no matter how large or small. A document\ncould be a single sentence or a 100 page report, or anything in between, such as a YouTube comment or a blog posting. Typically, all the text of a document is considered\ntogether and is retrieved as a single item when matched or categorized. A document is\ncomposed of individual tokens or terms. For now, think of a token or term as just a word;\nas we go on we’ll show how they can be different from what are customarily thought of\nas words. A collection of documents is called a corpus.1\n\n1. Latin for “body.” The plural is corpora.\n\nRepresentation\n\n|\n\n251", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00276", "page_num": 276, "segment": "Bag of Words\nIt is important to keep in mind the purpose of the text representation task. In essence,\nwe are taking a set of documents---each of which is a relatively free-form sequence of\nwords---and turning it into our familiar feature-vector form. Each document is one\ninstance but we don’t know in advance what the features will be.\n\nThe approach we introduce first is called “bag of words.” As the name implies, the\napproach is to treat every document as just a collection of individual words. This approach ignores grammar, word order, sentence structure, and (usually) punctuation. It\ntreats every word in a document as a potentially important keyword of the document.\nThe representation is straightforward and inexpensive to generate, and tends to work\nwell for many tasks.\n\nNote: Sets and bags\nThe terms set and bag have specific meanings in mathematics, neither of which we exactly mean here. A set allows only one instance of\neach item, whereas we want to take into account the number of occurrences of words. In mathematics a bag is a multiset, where members are allowed to appear more than once. The bag-of-words representation initially treats documents as bags---multisets---of words,\nthereby ignoring word order and other linguistic structure. However,\nthe representation used for mining the text often is more complex than\njust counting the number of occurrences, as we will describe.\n\nSo if every word is a possible feature, what will be the feature’s value in a given document?\nThere are several approaches to this. In the most basic approach, each word is a token,\nand each document is represented by a one (if the token is present in the document) or\na zero (the token is not present in the document). This approach simply reduces a\ndocument to the set of words contained in it.\n\nTerm Frequency\nThe next step up is to use the word count (frequency) in the document instead of just\na zero or one. This allows us to differentiate between how many times a word is used;\nin some applications, the importance of a term in a document should increase with the\nnumber of times that term occurs. This is called the term frequency representation.\nConsider the three very simple sentences (documents) shown in Table 10-1.\n\nTable 10-1. Three simple documents.\n\nd1 jazz music has a swing rhythm\n\nd2 swing is hard to explain\n\nd3 swing rhythm is a natural rhythm\n\n252\n\n|\n\nChapter 10: Representing and Mining Text", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00277", "page_num": 277, "segment": "Each sentence is considered a separate document. A simple bag-of-words approach\nusing term frequency would produce a table of term counts shown in Table 10-2.\n\nTable 10-2. Term count representation.\n\na\n\nexplain hard has\n\nd1 1\n\nd2 0\n\nd3 1\n\n0\n\n1\n\n0\n\n0\n\n1\n\n0\n\n1\n\n0\n\n0\n\nis\n\n0\n\n1\n\n1\n\njazz music natural\n\nrhythm swing\n\nto\n\n1\n\n0\n\n0\n\n1\n\n0\n\n0\n\n0\n\n0\n\n1\n\n1\n\n0\n\n2\n\n1\n\n1\n\n1\n\n0\n\n1\n\n0\n\nUsually some basic processing is performed on the words before putting them into the\ntable. Consider this more complex sample document:\n\nMicrosoft Corp and Skype Global today announced that they have entered into a definitive agreement under which Microsoft will acquire Skype, the leading Internet communications company, for $8.5 billion in cash from the investor group led by Silver Lake.\nThe agreement has been approved by the boards of directors of both Microsoft and Skype.\n\nTable 10-3 shows a reduction of this document to a term frequency representation.\n\nTable 10-3. Terms after normalization and stemming, ordered by frequency\n\nTerm\n\nskype\n\napprov\n\ndefinit\n\nboard\n\ncompani\n\nCount\n\nTerm\n\nCount\n\nTerm\n\nCount\n\nTerm\n\nCount\n\n3\n\n1\n\n1\n\n1\n\n1\n\nmicrosoft\n\nannounc\n\nlake\n\nled\n\ninvestor\n\n3\n\n1\n\n1\n\n1\n\n1\n\nagreement\n\nacquir\n\ncommunic\n\ndirector\n\nsilver\n\n2\n\n1\n\n1\n\n1\n\n1\n\nglobal\n\nlead\n\ninternet\n\ncorp\n\nbillion\n\n1\n\n1\n\n1\n\n1\n\n1\n\nTo create this table from the sample document, the following steps have been performed:\n\n• First, the case has been normalized: every term is in lowercase. This is so that words\nlike Skype and SKYPE are counted as the same thing. Case variations are so common (consider iPhone, iphone, and IPHONE) that case normalization is usually\nnecessary.\n\n• Second, many words have been stemmed: their suffixes removed, so that verbs like\nannounces, announced and announcing are all reduced to the term announc. Similarly, stemming transforms noun plurals to the singular forms, which is why directors in the text becomes director in the term list.\n\n• Finally, stopwords have been removed. A stopword is a very common word in\nEnglish (or whatever language is being parsed). The words the, and, of, and on are\nconsidered stopwords in English so they are typically removed.\n\nNote that the “$8.5” in the story has been discarded entirely. Should it have been?\nNumbers are commonly regarded as unimportant details for text processing, but the\n\nRepresentation\n\n|\n\n253", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00278", "page_num": 278, "segment": "purpose of the representation should decide this. You can imagine contexts where terms\nlike “4TB” and “1Q13” would be meaningless, and others where they could be critical\nmodifiers.\n\nNote: Careless Stopword Elimination\nA word of caution: stopword elimination is not always a good idea. In\ntitles, for example, common words may be very significant. For example, The Road, Cormac McCarthy’s story of a father and son surviving in a post-apocalyptic world, is very different from John Kerouac’s\nfamous novel On the Road--- though careless stopword removal may\ncause them to be represented identically. Similarly, the recent movie\nthriller Stoker should not be confused with the 1935 film comedy The\nStoker.2\n\nTable 10-3 shows raw counts of terms. Instead of raw counts, some systems perform a\nstep of normalizing the term frequencies with respect to document length. The purpose\nof term frequency is to represent the relevance of a term to a document. Long documents\nusually will have more words---and thus more word occurrences---than shorter ones.\nThis doesn’t mean that the longer document is necessarily more important or relevant\nthan the shorter one. In order to adjust for document length, the raw term frequencies\nare normalized in some way, such as by dividing each by the total number of words in\nthe document.\n\nMeasuring Sparseness: Inverse Document Frequency\nSo term frequency measures how prevalent a term is in a single document. We may also\ncare, when deciding the weight of a term, how common it is in the entire corpus we’re\nmining. There are two opposing considerations.\n\nFirst, a term should not be too rare. For example, say the unusual word prehensile occurs\nin only one document in your corpus. Is it an important term? This may depend on the\napplication. For retrieval, the term may be important since a user may be looking for\nthat exact word. For clustering, there is no point keeping a term that occurs only once:\nit will never be the basis of a meaningful cluster. For this reason, text processing systems\nusually impose a small (arbitrary) lower limit on the number of documents in which a\nterm must occur.\n\nAnother, opposite consideration is that a term should not be too common. A term occurring in every document isn’t useful for classification (it doesn’t distinguish anything)\nand it can’t serve as the basis for a cluster (the entire corpus would cluster together).\n\n2. Both of these examples appeared in recent search results on the film review site of a popular search engine. Not everyone is careful with stopword elimination.\n\n254\n\n|\n\nChapter 10: Representing and Mining Text", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00279", "page_num": 279, "segment": "Overly common terms are typically eliminated. One way to do this is to impose an\narbitrary upper limit on the number (or fraction) of documents in which a word may\noccur.\n\nIn addition to imposing upper and lower limits on term frequency, many systems take\ninto account the distribution of the term over a corpus as well. The fewer documents\nin which a term occurs, the more significant it likely is to be to the documents is does\noccur in. This sparseness of a term t is measured commonly by an equation called inverse\ndocument frequency (IDF), which is shown in Equation 10-1.\n\nEquation 10-1. Inverse Document Frequency (IDF) of a term\n\nIDF(t) = 1 + log (\n\nTotal number of documents\n\nNumber of documents containing t )\n\nIDF may be thought of as the boost a term gets for being rare. Figure 10-1 shows a graph\nof IDF(t) as a function of the number of documents in which t occurs, in a corpus of\n100 documents. As you can see, when a term is very rare (far left) the IDF is quite high.\nIt decreases quickly as t becomes more common in documents, and asymptotes at 1.0.\nMost stopwords, due to their prevalence, will have an IDF near one.\n\nFigure 10-1. IDF of a term t within a corpus of 100 documents.\n\nRepresentation\n\n|\n\n255", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00280", "page_num": 280, "segment": "Combining Them: TFIDF\nA very popular representation for text is the product of Term Frequency (TF) and\nInverse Document Frequency (IDF), commonly referred to as TFIDF. The TFIDF value\nof a term t in a given document d is thus:\n\nTFIDF(t, d ) = TF(t, d ) × IDF(t)\n\nNote that the TFIDF value is specific to a single document (d) whereas IDF depends on\nthe entire corpus. Systems employing the bag-of-words representation typically go\nthrough steps of stemming and stopword elimination before doing term counts. Term\ncounts within the documents form the TF values for each term, and the document\ncounts across the corpus form the IDF values.\n\nEach document thus becomes a feature vector, and the corpus is the set of these feature\nvectors. This set can then be used in a data mining algorithm for classification, clustering, or retrieval.\n\nBecause there are very many potential terms with text representation, feature selection\nis often employed. Systems do this in various ways, such as imposing minimum and\nmaximum thresholds of term counts, and/or using a measure such as information\ngain3 to rank the terms by importance so that low-gain terms can be culled.\n\nThe bag-of-words text representation approach treats every word in a document as an\nindependent potential keyword (feature) of the document, then assigns values to each\ndocument based on frequency and rarity. TFIDF is a very common value representation\nfor terms, but it is not necessarily optimal. If someone describes mining a text corpus\nusing bag of words it just means they’re treating each word individually as a feature.\nTheir values could be binary, term frequency, or TFIDF, with normalization or without.\nData scientists develop intuitions about how best to attack a given text problem, but\nthey’ll typically experiment with different representations to see which produces the\nbest results.\n\nExample: Jazz Musicians\nHaving introduced a few basic concepts, let’s now illustrate them with a concrete example: representing jazz musicians. Specifically, we’re going to look at a small corpus of\n15 prominent jazz musicians and excerpts of their biographies from Wikipedia. Here\nare excerpts from a few jazz musician biographies:\n\n3. See “Example: Attribute Selection with Information Gain” on page 56.\n\n256\n\n|\n\nChapter 10: Representing and Mining Text", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00281", "page_num": 281, "segment": "Charlie Parker\n\nCharles “Charlie” Parker, Jr., was an American jazz saxophonist and composer.\nMiles Davis once said, “You can tell the history of jazz in four words: Louis Armstrong. Charlie Parker.” Parker acquired the nickname “Yardbird” early in his career\nand the shortened form, “Bird,” which continued to be used for the rest of his life,\ninspired the titles of a number of Parker compositions, [...]\n\nDuke Ellington\n\nEdward Kennedy “Duke” Ellington was an American composer, pianist, and bigband leader. Ellington wrote over 1,000 compositions. In the opinion of Bob Blumenthal of The Boston Globe, “in the century since his birth, there has been no\ngreater composer, American or otherwise, than Edward Kennedy Ellington.” A\nmajor figure in the history of jazz, Ellington’s music stretched into various other\ngenres, including blues, gospel, film scores, popular, and classical.[...]\n\nMiles Davis\n\nMiles Dewey Davis III was an American jazz musician, trumpeter, bandleader, and\ncomposer. Widely considered one of the most influential musicians of the 20th\ncentury, Miles Davis was, with his musical groups, at the forefront of several major\ndevelopments in jazz music, including bebop, cool jazz, hard bop, modal jazz, and\njazz fusion.[...]\n\nEven with this fairly small corpus of fifteen documents, the corpus and its vocabulary\nare too large to show here (nearly 2,000 features after stemming and stopword removal)\nso we can only illustrate with a sample. Consider the sample phrase “Famous jazz saxophonist born in Kansas who played bebop and latin.” We could imagine it being typed\nas a query to a search engine. How would it be represented? It is treated and processed\njust like a document, and goes through many of the same steps.\n\nFirst, basic stemming is applied. Stemming methods are not perfect, and can produce\nterms like kansa and famou from “Kansas” and “famous.” Stemming perfection usually\nisn’t important as long as it’s consistent among all the documents. The result is shown\nin Figure 10-2.\n\nExample: Jazz Musicians\n\n|\n\n257", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00282", "page_num": 282, "segment": "Figure 10-2. Representation of the query “Famous jazz saxophonist born in Kansas\nwho played bebop and latin” after stemming.\n\nNext, stopwords (in and and) are removed, and the words are normalized with respect\nto document length. The result is shown in Figure 10-3.\n\nThese values would typically be used as the Term Frequency (TF) feature values if we\nwere to stop here. Instead, we’ll generate the full TFIDF representation by multiplying\neach term’s TF value by its IDF value. As we said, this boosts words that are rare.\n\nJazz and play are very frequent in this corpus of jazz musician biographies so they get\nno boost from IDF. They are almost stopwords in this corpus.\n\nThe terms with the highest TFIDF values (“latin,” “famous,” and “kansas”) are the rarest\nin this corpus so they end up with the highest weights among the terms in the query.\nFinally, the terms are renormalized, producing the final TFIDF weights shown in\nFigure 10-4. This is the feature vector representation of this sample “document” (the\nquery).\n\n258\n\n|\n\nChapter 10: Representing and Mining Text", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00283", "page_num": 283, "segment": "Figure 10-3. Representation of the query “Famous jazz saxophonist born in Kansas\nwho played bebop and latin” after stopword removal and term frequency normalization.\n\nHaving shown how this small “document” would be represented, let’s use it for something. Recall in Chapter 6, we discussed doing nearest-neighbor retrievals by employing\na distance metric, and we showed how similar whiskies could be retrieved. We can do\nthe same thing here. Assume our sample phrase “Famous jazz saxophonist born in Kansas who played bebop and latin” was a search query typed by a user and we were implementing a simple search engine. How might it work? First, we would translate the\nquery to its TFIDF representation, as shown graphically in Figure 10-4. We’ve already\ncomputed TFIDF representations of each of our jazz musician biography documents.\nNow all we need to do is to compute the similarity of our query term to each musician’s\nbiography and choose the closest one!\n\nFor doing this matching, we’ll use the Cosine Similarity function (Equation 6-5) discussed back in the starred section “* Other Distance Functions” on page 158. Cosine\nsimilarity is commonly used in text classification to measure the distance between\ndocuments.\n\nExample: Jazz Musicians\n\n|\n\n259", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00284", "page_num": 284, "segment": "Figure 10-4. Final TFIDF representation of the query “Famous jazz saxophonist born\nin Kansas who played bebop and latin.”\n\nTable 10-4. Similarity of each musician’s text to the query ‘Famous jazz saxophonist\nborn in Kansas who played bebop and latin,’ ordered by decreasing similarity.\n\nMusician\n\nSimilarity\n\n Musician\n\nSimilarity\n\nCharlie Parker\n\nDizzie Gillespie\n\nArt Tatum\n\nClark Terry\n\nDave Brubeck\n\n0.135\n\n0.086\n\n0.050\n\n0.047\n\n0.027\n\nThelonius Monk\n\n0.025\n\nCount Basie\n\nJohn Coltrane\n\nMiles Davis\n\nSun Ra\n\nNina Simone\n\nFats Waller\n\nCharles Mingus\n\n0.019\n\nDuke Ellington\n\n0.119\n\n0.079\n\n0.050\n\n0.030\n\n0.026\n\n0.020\n\n0.017\n\nBenny Goodman 0.016\n\nLouis Armstrong\n\n0.012\n\nAs you can see, the closest matching document is Charlie Parker---who was, in fact, a\nsaxophonist born in Kansas and who played the bebop style of jazz. He sometimes\ncombined other genres, including Latin, a fact that is mentioned in his biography.\n\n260\n\n|\n\nChapter 10: Representing and Mining Text", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00285", "page_num": 285, "segment": "* The Relationship of IDF to Entropy\n\nBack in “Selecting Informative Attributes” on page 49, we introduced the\nentropy measure when we began discussing predictive modeling. The\ncurious reader (with a long memory) may notice that Inverse Document Frequency and entropy are somewhat similar---they both seem to\nmeasure how “mixed” a set is with respect to a property. Is there any\nconnection between the two? Maybe they’re the same? They are not\nidentical, but they are related, and this section will show the relationship. If you’re not curious about this you can skip this section.\n\nFigure 10-5 shows some graphs related to the equations we’re going to talk about. To\nbegin, consider a term t in a document set. What is the probability that a term t occurs\nin a document set? We can estimate it as:\n\np(t) =\n\nNumber of documents containing t\nTotal number of documents\n\nTo simplify things, from here on we’ll refer to this estimate p(t) simply as p. Recall that\nthe definition of IDF of some term t is:\n\nIDF(t) = 1 + log (\n\nTotal number of documents\n\nNumber of documents containing t )\n\nThe 1 is just a constant so let’s discard it. We then notice that IDF(t) is basically log(1/p).\nYou may recall from algebra that log(1/p) is equal to -log(p).\n\nConsider again the document set with respect to a term t. Each document either contains\nt (with probability p) or does not contain it (with probability 1-p). Let’s create a pseudo,\nmirror-image term not_t that, by definition, occurs in every document that does not\ncontain t. What’s the IDF of this new term? It is:\n\nIDF(not _ T ) = log 1 / (1 - p) = - log (1 - p)\n\n* The Relationship of IDF to Entropy\n\n|\n\n261", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00286", "page_num": 286, "segment": "Figure 10-5. Plots of various values related to IDF(t) and IDF(not_t).\n\nSee the upper left graph of Figure 10-5. The two graphs are mirror images of each other,\nas we might expect. Now recall the definition of entropy from Equation 3-1. For a binary\nterm where p2=1-p1, the entropy becomes:\n\nentropy = - p1 log ( p1) - p2 log ( p2)\n\nIn our case, we have a binary term t that either occurs (with probability p) or does not\n(with probability 1-p). So the definition of entropy of a set partitioned by t reduces to:\n\nentropy(t) = - p log (p) - (1 - p) log (1 - p)\n\n262\n\n|\n\nChapter 10: Representing and Mining Text", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00287", "page_num": 287, "segment": "Now, given our definitions of IDF(t) and IDF(not_t), we can start substituting and\nsimplifying (for reference, various of these subexpressions are plotted in the top right\ngraph of Figure 10-5).\n\nentropy(t) =\n\n=\n\n=\n\n- p log (p) - (1 - p) log (1 - p)\np · IDF(t) - (1 - p) - IDF(not _ t)\np · IDF(t) + (1 - p) IDF(not _ t)\n\nNote that this is now in the form of an expected value calculation! We can express entropy\nas the expected value of IDF(t) and IDF(not_t) based on the probability of its occurrence\nin the corpus. Its graph at the bottom left in Figure 10-5 does match the entropy curve\nof Figure 3-3 back in Chapter 3.\n\nBeyond Bag of Words\nThe basic bag of words approach is relatively simple and has much to recommend it. It\nrequires no sophisticated parsing ability or other linguistic analysis. It performs surprisingly well on a variety of tasks, and is usually the first choice of data scientists for a\nnew text mining problem.\n\nStill, there are applications for which bag of words representation isn’t good enough and\nmore sophisticated techniques must be brought to bear. Here we briefly discuss a few\nof them.\n\nN-gram Sequences\nAs presented, the bag-of-words representation treats every individual word as a term,\ndiscarding word order entirely. In some cases, word order is important and you want\nto preserve some information about it in the representation. A next step up in complexity is to include sequences of adjacent words as terms. For example, we could include\npairs of adjacent words so that if a document contained the sentence “The quick brown\nfox jumps.” it would be transformed into the set of its constitutent words {quick, brown,\nfox, jumps}, plus the tokens quick_brown, brown_fox, and fox_jumps.\n\nThis general representation tactic is called n-grams. Adjacent pairs are commonly called\nbi-grams. If you hear a data scientist mention representing text as “bag of n-grams up\nto three” it simply means she’s representing each document using as features its individual words, adjacent word pairs, and adjacent word triples.\n\nN-grams are useful when particular phrases are significant but their component words\nmay not be. In a business news story, the appearance of the tri-gram exceed_ana\nlyst_expectation is more meaningful than simply knowing that the individual words\nanalyst, expectation, and exceed appeared somewhere in a story. An advantage of\n\nBeyond Bag of Words\n\n|\n\n263", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00288", "page_num": 288, "segment": "using n-grams is that they are easy to generate; they require no linguistic knowledge or\ncomplex parsing algorithm.\n\nThe main disadvantage of n-grams is that they greatly increase the size of the feature\nset. There are far more word pairs than individual words, and still more word triples.\nThe number of features generated can quickly get out of hand. Data mining using ngrams almost always needs some special consideration for dealing with massive numbers of features, such as a feature selection stage or special consideration to computational storage space.\n\nNamed Entity Extraction\nSometimes we want still more sophistication in phrase extraction. We want to be able\nto recognize common named entities in documents. Silicon Valley, New York Mets,\nDepartment of the Interior, and Game of Thrones are significant phrases. Their component words mean one thing, and may not be significant, but in sequence they name\nunique entities with interesting identities. The basic bag-of-words (or even n-grams)\nrepresentation may not capture these, and we’d want a preprocessing component that\nknows when word sequences constitute proper names.\n\nMany text-processing toolkits include a named entity extractor of some sort. Usually\nthese can process raw text and extract phrases annotated with terms like person or\norganization. In some cases normalization is done so that, for example, phrases like\n“HP,” “H-P,” and “Hewlett-Packard” all link to some common representation of the\nHewlett-Packard Corporation.\n\nUnlike bag of words and n-grams, which are based on segmenting text on whitespace\nand punctuation, named entity extractors are knowledge intensive. To work well, they\nhave to be trained on a large corpus, or hand coded with extensive knowledge of such\nnames. There is no linguistic principle dictating that the phrase “oakland raiders” should\nrefer to the Oakland Raiders professional football team, rather than, say, a group of\naggressive California investors. This knowledge has to be learned, or coded by hand.\nThe quality of entity recognition can vary, and some extractors may have particular\nareas of expertise, such as industry, government, or popular culture.\n\nTopic Models\nSo far we’ve dealt with models created directly from words (or named entities) appearing\nfrom a document. The resulting model---whatever it may be---refers directly to words.\nLearning such direct models is relatively efficient, but is not always optimal. Because of\nthe complexity of language and documents, sometimes we want an additional layer\nbetween the document and the model. In the context of text we call this the topic layer\n(see Figure 10-6).\n\n264\n\n|\n\nChapter 10: Representing and Mining Text", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00289", "page_num": 289, "segment": "Figure 10-6. Modeling documents with a topic layer.\n\nThe main idea of a topic layer is first to model the set of topics in a corpus separately.\nAs before, each document constitutes a sequence of words, but instead of the words\nbeing used directly by the final classifier, the words map to one or more topics. The\ntopics also are learned from the data (often via unsupervised data mining). The final\nclassifier is defined in terms of these intermediate topics rather than words. One advantage is that in a search engine, for example, a query can use terms that do not exactly\nmatch the specific words of a document; if they map to the correct topic(s), the document\nwill still be considered relevant to the search.\n\nGeneral methods for creating topic models include matrix factorization methods, such\nas Latent Semantic Indexing and Probabilistic Topic Models, such as Latent Dirichlet\nAllocation. The math of these approaches is beyond the scope of this book, but we can\nthink of the topic layer as being a clustering of words. In topic modeling, the terms\nassociated with the topic, and any term weights, are learned by the topic modeling\nprocess. As with clusters, the topics emerge from statistical regularities in the data. As\nsuch, they are not necessarily intelligible, and they are not guaranteed to correspond to\ntopics familiar to people, though in many cases they are.\n\nBeyond Bag of Words\n\n|\n\n265", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00290", "page_num": 290, "segment": "Note: Topics as Latent Information\nTopic models are a type of latent information model, which we’ll discuss a bit more in Chapter 12 (along with a movie recommendation\nexample). You can think of latent information as a type of intermediate, unobserved layer of information inserted between the inputs and\noutputs. The techniques are essentially the same for finding latent\ntopics in text and for finding latent “taste” dimensions of movie viewers. In the case of text, words map to topics (unobserved) and topics\nmap to documents. This makes the entire model more complex and\nmore expensive to learn, but can yield better performance. In addition, the latent information is often interesting and useful in its own\nright (as we will see again in the movie recommendation example in\nChapter 12).\n\nExample: Mining News Stories to Predict Stock Price\nMovement\nTo illustrate some issues in text mining, we introduce a new predictive mining task:\nwe’re going to predict stock price fluctuations based on the text of news stories. Roughly\nspeaking, we are going to “predict the stock market” based on the stories that appear on\nthe news wires. This project contains many common elements of text processing and\nof problem formulation.\n\nThe Task\nEvery trading day there is activity in the stock market. Companies make and announce\ndecisions---mergers, new products, earnings projections, and so forth---and the financial news industry reports on them. Investors read these news stories, possibly change\ntheir beliefs about the prospects of the companies involved, and trade stock accordingly.\nThis results in stock price changes. For example, announcements of acquisitions, earnings, regulatory changes, and so on can all affect the price of a stock, either because it\ndirectly affects the earnings potential or because it affects what traders think other traders are likely to pay for the stock.\n\nThis is a very simplified view of the financial markets, of course, but it’s enough to lay\nout a basic task. We want to predict stock price changes based on financial news. There\nare many ways we could approach this based on the ultimate purpose of the project. If\nwe wanted to make trades based on financial news, ideally we’d like to predict---in advance and with precision---the change in a company’s stock price based on the stream\nof news. In reality there are many complex factors involved in stock price changes, many\nof which are not conveyed in news stories.\n\n266\n\n|\n\nChapter 10: Representing and Mining Text", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00291", "page_num": 291, "segment": "Instead, we’ll mine the news stories for a more modest purpose, that of news recommendation. From this point of view, there is a huge stream of market news coming in\n---some interesting, most not. We’d like predictive text mining to recommend interesting news stories that we should pay attention to. What’s an interesting story? Here we’ll\ndefine it as news that will likely result in a significant change in a stock’s price.\n\nWe have to simplify the problem further to make it more tractable (in fact, this task is\na good example of problem formulation as much as it is of text mining). Here are some\nof the problems and simplifying assumptions:\n\n1. It is difficult to predict the effect of news far in advance. With many stocks, news\narrives fairly often and the market responds quickly. It is unrealistic, for example,\nto predict what price a stock will have a week from now based on a news release\ntoday. Therefore, we’ll try to predict what effect a news story will have on stock\nprice the same day.\n\n2. It is difficult to predict exactly what the stock price will be. Instead, we will be\nsatisfied with the direction of movement: up, down, or no change. In fact, we’ll\nsimplify this further into change and no change. This works well for our example\napplication: recommending a news story if it looks like it will trigger, or indicate, a\nsubsequent change in the stock price.\n\n3. It is difficult to predict small changes in stock price, so instead we’ll predict relatively\nlarge changes. This will make the signal a bit cleaner at the expense of yielding fewer\nevents. We will deliberately ignore the subtlety of small fluctuations.\n\n4. It is difficult to associate a specific piece of news with a price change. In principle,\nany piece of news could affect any stock. If we accepted this idea it would leave us\nwith a huge problem of credit assignment: how do you decide which of today’s\nthousands of stories are relevant? We need to narrow the “causal radius.”\n\nWe will assume that only news stories mentioning a specific stock will affect that\nstock’s price. This is inaccurate, of course---companies are affected by the actions\nof their competitors, customers, and clients, and it’s rare that a news story will\nmention all of them. But for a first pass this is an acceptable simplifying assumption.\n\nWe still have to nail some of this down. Consider issue two. What is a “relatively large”\nchange? We can (somewhat arbitrarily) place a threshold of 5%. If a stock’s price increases by five percent or more, we’ll call it a surge; if it declines by five percent or more,\nwe’ll call it a plunge. What if it changes by some amount in between? We could call any\nvalue in between stable, but that’s cutting it a little close---a 4.9% change and a 5% change\nshouldn’t really be distinct classes. Instead, we’ll designate some “gray zones” to make\nthe classes more separable (see Figure 10-7). Only if a stock’s price stays between 2.5%\nand -2.5% will it be called stable. Otherwise, for the zones between 2.5% to 5% and\n-2.5% to -5%, we’ll refuse to label it.\n\nExample: Mining News Stories to Predict Stock Price Movement\n\n|\n\n267", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00292", "page_num": 292, "segment": "Figure 10-7. Percentage change in price, and corresponding label.\n\nFor the purpose of this example, we’ll create a two-class problem by merging surge and\nplunge into a single class, change. It will be the positive class, and stable (no change)\nwill be the negative class.\n\nThe Data\nThe data we’ll use comprise two separate time series: the stream of news stories (text\ndocuments), and a corresponding stream of daily stock prices. The Internet has many\nsources of financial data, such as Google Finance and Yahoo! Finance. For example, to\nsee what news stories are available about Apple Computer, Inc., see the corresponding\nYahoo! Finance page. Yahoo! aggregates news stories from a variety of sources such as\nReuters, PR Web, and Forbes. Historical stock prices can be acquired from many sources, such as Google Finance.\n\nThe data to be mined are historical data from 1999 for stocks listed on the New York\nStock Exchange and NASDAQ. This data was used in a prior study (Fawcett & Provost,\n1999). We have open and close prices for stocks on the major exchanges, and a large\ncompendium of financial news stories throughout the year---nearly 36,000 stories altogether. Here is a sample news story from the corpus:\n\n 1999-03-30 14:45:00\n WALTHAM, Mass.--(BUSINESS WIRE)--March 30, 1999--Summit Technology,\n Inc. (NASDAQ:BEAM) and Autonomous Technologies Corporation\n (NASDAQ:ATCI) announced today that the Joint Proxy/Prospectus for\n Summit's acquisition of Autonomous has been declared effective by the\n\n268\n\n|\n\nChapter 10: Representing and Mining Text", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00293", "page_num": 293, "segment": "Securities and Exchange Commission. Copies of the document have been\n mailed to stockholders of both companies. \"We are pleased that these\n proxy materials have been declared effective and look forward to the\n shareholder meetings scheduled for April 29,\" said Robert Palmisano,\n Summit's Chief Executive Officer.\n\nAs with many text sources, there is a lot of miscellaneous material since it is intended\nfor human readers and not machine parsing (see “Sidebar: The News Is Messy” on page\n270 for more details). The story includes the date and time, the news source (Reuters),\nstock symbols and link (NASDAQ:BEAM), as well as background material not strictly\ngermane to the news. Each such story is tagged with the stock mentioned.\n\nFigure 10-8. Graph of stock price of Summit Technologies, Inc., (NASDAQ:BEAM) annotated with news story summaries.\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\nSummit Tech announces revenues for the three months ended Dec 31, 1998 were $22.4 million, an increase of 13%.\n\nSummit Tech and Autonomous Technologies Corporation announce that the Joint Proxy/Prospectus for Summit’s acquisition\nof Autonomous has been declared effective by the SEC.\n\nSummit Tech said that its procedure volume reached new levels in the first quarter and that it had concluded its acquisition\nof Autonomous Technologies Corporation.\n\nAnnouncement of annual shareholders meeting.\n\nSummit Tech announces it has filed a registration statement with the SEC to sell 4,000,000 shares of its common stock.\n\nA US FDA panel backs the use of a Summit Tech laser in LASIK procedures to correct nearsightedness with or without\nastigmatism.\n\nSummit up 1-1/8 at 27-3/8.\n\nExample: Mining News Stories to Predict Stock Price Movement\n\n|\n\n269", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00294", "page_num": 294, "segment": "8\n\n9\n\n10\n\n11\n\nSummit Tech said today that its revenues for the three months ended June 30, 1999 increased 14%...\n\nSummit Tech announces the public offering of 3,500,000 shares of its common stock priced at $16/share.\n\nSummit announces an agreement with Sterling Vision, Inc. for the purchase of up to six of Summit’s state of the art, Apex\nPlus Laser Systems.\n\nPreferred Capital Markets, Inc. initiates coverage of Summit Technology Inc. with a Strong Buy rating and a 12-16 month\nprice target of $22.50.\n\nSidebar: The News Is Messy\nThe financial news corpus is actually far messier than this one story implies, for several\nreasons.\n\nFirst, financial news comprises a wide variety of stories, including earnings announcements, analysts’ assessments (“We are reiterating our Buy rating on Apple”), market\ncommentary (“Other stocks featured in this morning’s MarketMovers include Lycos Inc.\nand Staples Inc.”), SEC filings, financial balance sheets, and so on. Companies are mentioned for many different reasons and a single document (“story”) may actually comprise\nmultiple unrelated news blurbs of the day.\n\nSecond, stories come in different formats, some with tabular data, some in a multiparagraph “lead stories of the day” format, and so on. Much of the meaning is imparted\nby context. Our text processing won’t pick this up.\n\nFinally, stock tagging is not perfect. It tends to be overly permissive, such that stories\nare included in the news feed of stocks that were not actually referenced in the story. As\nan extreme example, American blogger Perez Hilton uses the expression “cray cray” to\nmean crazy or disgusting, and some of his blog postings end up in the story feed of Cray\nComputer Corporation.\n\nIn short, the relevance of a stock to a document may not be clear without a careful\nreading. With deep parsing (or at least story segmentation) we could eliminate some of\nthe noise, but with bag of words (or even named entity extraction) we cannot hope to\nremove all of it.\n\nFigure 10-8 shows the kind of data we have to work with. They are basically two linked\ntime series. At the top is a graph of the stock price of Summit Technologies, Inc., a\nmanufacturer of excimer laser systems for use in laser vision correction. Some points\non the graph are annotated with story numbers on the date the story was released. Below\nthe graph are summaries of each story.\n\nData Preprocessing\nAs mentioned, we have two streams of data. Each stock has an opening and closing price\nfor the day, measured at 9:30 am EST and 4:00 pm EST, respectively. From these values\n\n270\n\n|\n\nChapter 10: Representing and Mining Text", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00295", "page_num": 295, "segment": "we can easily compute a percentage change. There is one minor complication. We’re\ntrying to predict stories that produce a substantial change in a stock’s value. Many events\noccur outside of trading hours, and fluctuations near the opening of trading can be\nerratic. For this reason, instead of measuring the opening price at the opening bell (9:30\nam EST) we measure it at 10:00 am, and track the difference between the day’s prices at\n4 pm and 10 am. Divided by the stock’s closing price, this becomes the daily percent\nchange.\n\nThe stories require much more care. The stories are pre-tagged with stocks, which are\nmostly accurate (“Sidebar: The News Is Messy” on page 270 goes into some details on\nwhy this is a difficult text mining problem). Almost all stories have timestamps (those\nwithout are discarded) so we can align them with the correct day and trading window.\nBecause we want a fairly tight association of a story with the stock(s) it might affect, we\nreject any stories mentioning more than two stocks. This gets rid of many stories that\nare just summaries and news aggregations.\n\nThe basic steps outlined in “Bag of Words” on page 252 were applied to reduce each\nstory to a TFIDF representation. In particular, each word was case-normalized and\nstemmed, and stopwords were removed. Finally, we created n-grams up to two, such\nthat every individual term and pair of adjacent terms were used to represent each story.\n\nSubject to this preparation, each story is tagged with a label (change or no change)\nbased on the associated stock(s) price movement, as depicted in Figure 10-7. This results\nin about 16,000 usable tagged stories. For reference, the breakdown of stories was about\n75% no change, 13% surge, and 12% plunge. The surge and plunge stories were merged\nto form change, so 25% of the stories were followed by a significant price change to the\nstocks involved, and 75% were not.\n\nResults\nBefore we dig into results, a short digression.\n\nPrevious chapters (particularly Chapter 7) stressed the importance of thinking carefully\nabout the business problem being solved in order to frame the evaluation. With this\nexample we have not done such careful specification. If the purpose of this task were to\ntrigger stock trades, we might propose an overall trading strategy involving thresholds,\ntime limits, and transaction costs, from which we could produce a complete cost-benefit\nanalysis.4 But the purpose is news recommendation (answering “which stories lead to\nsubstantial stock price changes?”) and we’ve left this pretty open, so we won’t specify\nexact costs and benefits of decisions. For this reason, expected value calculations and\nprofit graphs aren’t really appropriate here.\n\n4. Some researchers have done this, evaluating their systems by simulating stock trades and calculating the\n\nreturn on investment. See, for example, Schumaker & Chen’s (2010) work on AZFinText.\n\nExample: Mining News Stories to Predict Stock Price Movement\n\n|\n\n271", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00296", "page_num": 296, "segment": "Figure 10-9. ROC curves for the stock news classification task.\n\nInstead, let’s look at predictability, just to get a sense of how well this problem can be\nsolved. Figure 10-9 shows the ROC curves of three sample classifiers: Logistic Regression, Naive Bayes, and a Classification Tree. These curves are averaged from ten-fold\ncross-validation, using change as the positive class and no change as the negative class.\nSeveral things are apparent. First, there is a significant “bowing out” of the curves away\nfrom the diagonal (Random) line, and the ROC curve areas (AUCs) are all substantially\nabove 0.5, so there is predictive signal in the news stories. Second, logistic regression\nand Naive Bayes perform similarly, whereas the classification tree (Tree) is considerably\nworse. Finally, there is no obvious region of superiority (or deformity) in the curves.\nBulges or concavities can sometimes reveal characteristics of the problem, or flaws in\nthe data representation, but we see none here.\n\nFigure 10-10 shows the corresponding lift curves of these three classifiers, again averaged from ten-fold cross-validation. Recall that one in four (25%) of the stories in our\npopulation is positive, (i.e., it is followed by a significant change in stock price). Each\ncurve shows the lift in precision5 we would get if we used the model to score and order\nthe news stories. For example, consider the point at x=0.2, where the lifts of Logistic\nRegression and Naive Bayes are both around 2.0. This means that, if you were to score\nall the news stories and take the top 20% (x=0.2), you’d have twice the precision (lift of\ntwo) of finding a positive story in that group than in the population as a whole. Therefore, among the top 20% of the stories as ranked by the model, half are significant.\n\n5. Recall from Chapter 7, precision is the percentage of the cases that are above the classification threshold that\nare actually positive examples, and the lift is how many times more this is than you would expect by chance.\n\n272\n\n|\n\nChapter 10: Representing and Mining Text", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00297", "page_num": 297, "segment": "Figure 10-10. Lift curves for the stock news prediction task.\n\nBefore concluding this example, let’s look at some of the important terms found from\nthis task. The goal of this example was not to create intelligible rules from the data, but\nprior work on the same corpus by Macskassy et al. (2001) did just that. Here is a list of\nterms with high information gain6 taken from their work. Each boldface term is either\na word or a stem followed by suffixes in parentheses:\n\nalert(s,ed), architecture, auction(s,ed,ing,eers), average(s,d), award(s,ed),\nbond(s), brokerage, climb(ed,s,ing), close(d,s), comment(ator,ed,ing,s),\ncommerce(s), corporate, crack(s,ed,ing), cumulative, deal(s), dealing(s),\ndeflect(ed,ing), delays, depart(s,ed), department(s), design(ers,ing),\neconomy, econtent, edesign, eoperate, esource, event(s), exchange(s),\nextens(ion,ive), facilit(y,ies), gain(ed,s,ing), higher, hit(s), imbalance(s),\nindex, issue(s,d), late(ly), law(s,ful), lead(s,ing), legal(ity,ly), lose,\nmajority, merg(ing,ed,es), move(s,d), online, outperform(s,ance,ed),\npartner(s), payments, percent, pharmaceutical(s), price(d), primary,\nrecover(ed,s), redirect(ed,ion), stakeholder(s), stock(s), violat(ing,ion,ors)\n\nMany of these are suggestive of significant announcements of good or bad news for a\ncompany or its stock price. Some of them (econtent, edesign, eoperate) are also suggestive of the “Dotcom Boom” of the late 1990s, from which this corpus is taken, when\nthe e- prefix was in vogue.\n\nThough this example is one of the most complex presented in this book, it is still a fairly\nsimple approach to mining financial news stories. There are many ways this project\n\n6. Recall Chapter 3.\n\nExample: Mining News Stories to Predict Stock Price Movement\n\n|\n\n273", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00298", "page_num": 298, "segment": "could be extended and refined. The bag-of-words representation is primitive for this\ntask; named entity recognition could be used to better extract the names of the companies and people involved. Better still, event parsing should provide real leverage, since\nnews stories usually report events rather than static facts about companies. It is not clear\nfrom individual words who are the subjects and objects of the events, and important\nmodifiers like not, despite, and expect may not be adjacent to the phrases they modify,\nso the bag of words representation is at a disdvantage. Finally, to calculate price changes\nwe considered only daily opening and closing stock prices, rather than hourly or instantaneous (“tick level”) price changes. The market responds quickly to news, and if\nwe wanted to trade on the information we’d need to have fine-grained, reliable timestamps on both stock prices and news stories.\n\nSidebar: Prior Work on Predicting Stock Prices from Financial News\nThe problem of relating financial news stories to market activity has been tackled by\nmany people in the past 15 years or so. Your authors even did some early work on the\ntask (Fawcett & Provost, 1999). Most of the prior work has been published outside the\ndata mining literature, so the data mining community may remain largely unaware of\nthe task and the work. We mention a few articles here for anyone interested in pursuing\nthe topic.\n\nA survey by Mittermayer and Knolmayer (2006) is a good place to start, though it is a\nbit dated by now. It provides a good overview of approaches up to that point.\n\nMost researchers view the problem as predicting the stock market from news. In this\nchapter, we’ve taken an inverse view as that of recommending news stories based on\ntheir future effects. This task was termed information triage by Macskassy et al. (2001).\n\nEarly work looked at the effect of financial news in the mainstream media. Later work\ntakes into account opinions and sentiment from other sources on the Internet, such as\nTwitter updates, blog postings, and search engine trends. A paper by Mao et al. (2011)\nprovides a good analysis and comparison of the effect of these additional sources.\n\nFinally, though it’s not text mining per se, let us mention the paper “Legislating Stock\nPrices” by Cohen, Diether, and Malloy (2012). These researchers examined the relationship of politicians, legislation, and firms affected by the legislation. Obviously, these\nthree groups are interrelated and should affect each other, but surprisingly, the relationship had not been exploited by Wall Street. From publicly available data the researchers discovered a “simple, yet previously undetected impact on firm stock prices”\nthat they report to be able to trade upon profitably. This suggests that there are undiscovered relationships remaining to be mined.\n\n274\n\n|\n\nChapter 10: Representing and Mining Text", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00299", "page_num": 299, "segment": "Summary\nOur problems do not always present us with data in a neat feature vector representation\nthat most data mining methods take as input. Real-world problems often require some\nform of data representation engineering to make them amenable to mining. Generally\nit is simpler to first try to engineer the data to match existing tools. Data in the form of\ntext, images, sound, video, and spatial information usually require special preprocessing\n---and sometimes special knowledge on the part of the data science team.\n\nIn this chapter, we discussed one especially prevalent type of data that requires preprocessing: text. A common way to turn text into a feature vector is to break each document\ninto individual words (its “bag of words” representation), and assign values to each term\nusing the TFIDF formula. This approach is relatively simple, inexpensive and versatile,\nand requires little knowledge of the domain, at least initially. In spite of its simplicity, it\nperforms surprisingly well on a variety of tasks. In fact, we will revisit these ideas on a\ncompletely different, nontext task in Chapter 14.\n\nSummary\n\n|\n\n275", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00300", "page_num": 300, "segment": "", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "none", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00301", "page_num": 301, "segment": "CHAPTER 11\nDecision Analytic Thinking II: Toward\nAnalytical Engineering\n\nFundamental concept: Solving business problems with data science starts with analytical\nengineering: designing an analytical solution, based on the data, tools, and techniques\navailable.\n\nExemplary technique: Expected value as a framework for data science solution design.\n\nUltimately, data science is about extracting information or knowledge from data, based\non principled techniques. However, as we’ve discussed throughout the book, seldom\ndoes the world provide us with important business problems perfectly aligned with these\ntechniques, or with data represented such that the techniques can be applied directly.\nIronically, this fact often is better accepted by the business users (for whom it is often\nobvious) than by entry-level data scientists---because academic programs in statistics,\nmachine learning, and data mining often present students with problems ready for the\napplication of the tools that the programs teach.\n\nReality is much messier. Business problems rarely are classification problems or regression problems or clustering problems. They’re just business problems. Recall the\nmini-cycle in the first stages of the data mining process, where we focus on business\nunderstanding and data understanding. In these stages we must design or engineer a\nsolution to the business problem. As with engineering more broadly, the data science\nteam considers the needs of the business as well as the tools that might be brought to\nbear to solve the problem.\n\nIn this chapter, we will illustrate such analytical engineering with two case studies. In\nthese case studies, we will see the application of the fundamental principles presented\nthroughout the book, as well as some of the specific techniques that we have introduced.\nOne common theme that runs through these case studies is how our expected value\nframework (recall from Chapter 7) helps to decompose each of the business problems\ninto subproblems, such that the subproblems can be attacked with tried-and-true data\n\n277", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00302", "page_num": 302, "segment": "science techniques. Then the expected value framework guides the recombination of\nthe results into a solution to the original problem.\n\nTargeting the Best Prospects for a Charity Mailing\nA classic business problem for applying data science principles and techniques is targeted marketing. Targeted marketing makes for a perfect case study for two reasons.\nFirst, a very large number of businesses have problems that look similar to targeted\nmarketing problems---traditional targeted (database) marketing, customer-specific\ncoupon offers, online ad targeting, and so on. Second, the fundamental structure of the\nproblem occurs in many other problems as well, such as our running example problem\nof churn management.\n\nFor this case study, let’s consider a real example of targeted marketing: targeting the best\nprospects for a charity mailing. Fundraising organizations (including those in universities) need to manage their budgets and the patience of their potential donors. In any\ngiven campaign segment, they would like to solicit from a “good” subset of the donors.\nThis could be a very large subset for an inexpensive, infrequent campaign, or a smaller\nsubset for a focused campaign that includes a not-so-inexpensive incentive package.\n\nThe Expected Value Framework: Decomposing the Business Problem\nand Recomposing the Solution Pieces\nWe would like to “engineer” an analytic solution to the problem, and our fundamental\nconcepts will provide the structure to do so. To frame our data-analytic thinking, we\nbegin by using the data-mining process (Chapter 2) to provide structure to the overall\nanalysis: we start with business and data understanding. More specifically, we need to\nfocus using one of our fundamental principles: what exactly is the business problem\nthat we would like to solve (Chapter 7)?\n\nSo let’s get specific. A data miner might immediately think: we want to model the probability that each prospective customer, a prospective donor in this case, will respond to\nthe offer. However, thinking carefully about the business problem we realize that in this\ncase, the response can vary---some people might donate $100 while others might donate\n$1. We need to take this into account.\n\nWould we like to maximize the total amount of donations? (The amount could be either\nin this particular campaign or over the lifetime of the donor prospects; let’s assume the\nfirst for simplicity.) What if we did that by targeting a massive number of people, and\nthese each give just $1, and our costs are about $1 per person? We would make almost\nno money. So let’s revise our thinking.\n\nFocusing on the business problem that we want to solve may have given us our answer\nright away, because to a business-savvy person it may seem rather obvious: we would\nlike to maximize our donation profit---meaning the net after taking into account the\n\n278\n\n|\n\nChapter 11: Decision Analytic Thinking II: Toward Analytical Engineering", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00303", "page_num": 303, "segment": "costs. However, while we have methods for estimating the probability of response (that’s\na clear application of class probability estimation over a binary outcome), it is not clear\nthat we have methods to estimate profit.\n\nAgain, our fundamental concepts allow us to structure our thinking and engineer a\ndata-analytic solution. Applying another one of our fundamental notions, we can structure this data analysis using the framework of expected value. We can apply the concepts\nintroduced in Chapter 7 to our problem formulation: we can use expected value as a\nframework for structuring our approach to engineering a solution to the problem. Recall\nour formulation of the expected benefit (or cost) of targeting consumer x:\n\nExpected benefit of targeting = p(R | ) · vR + 1 - p(R | ) · vNR\n\nwhere p(R | ) is the probability of response given consumer x, vR is the value we get\nfrom a response, and vNR is the value we get from no response. Since everyone either\nresponds or does not, our estimate of the probability of not responding is just\n(1 - p(R | )). As we discussed in Chapter 7, we can model the probabilities by mining\nhistorical data using one of the many techniques discussed through the book.\n\nHowever, the expected value framework helps us realize that this business problem is\nslightly different from problems we have considered up to this point. In this case, the\nvalue varies from consumer to consumer, and we do not know the value of the donation\nthat any particular consumer will give until after she is targeted! Let’s modify our formulation to make this explicit:\n\nExpected benefit of targeting = p(R | ) · vR() + 1 - p(R | ) · vNR()\n\nwhere vR(x) is the value we get from a response from consumer x and vNR(x) is the value\nwe get if consumer x does not respond. The value of a response, vR(x), would be the\nconsumer’s donation minus the cost of the solicitation. The value of no response, vNR(x),\nin this application would be zero minus the cost of the solicitation. To be complete, we\nalso want to estimate the benefit of not targeting, and then compare the two to make\nthe decision of whether to target or not. The expected benefit of not targeting is simply\nzero---in this application, we do not expect consumers to donate spontaneously without\na solicitation. That may not always be the case, but let’s assume it is here.\n\nWhy exactly does the expected value framework help us? Because we may be able to\nestimate vR(x) and/or vNR(x) from the data as well. Regression modeling estimates such\nvalues. Looking at historical data on consumers who have been targeted, we can use\nregression modeling to estimate how much a consumer will respond. Moreover, the\nexpected value framework gives us even more precise direction: vR(x) is the value we\nwould predict to get if a consumer were to respond --- this would be estimated using a\nmodel trained only on consumers who have responded. This turns out to be a more\n\nTargeting the Best Prospects for a Charity Mailing\n\n|\n\n279", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00304", "page_num": 304, "segment": "useful prediction problem than the problem of estimating the response from a targeted\nconsumer generally, because in this application the vast majority of consumers do not\nrespond at all, and so the regression modeling would need somehow to differentiate\nbetween the cases where the value is zero because of non-response or the value is small\nbecause of the characteristics of the consumer.\n\nStepping back for a moment, this example illustrates why the expected value framework\nis so useful for decomposing business problems: as discussed in Chapter 7, the expected\nvalue is a summation of products of probabilities and values, and data science gives us\nmethods to estimate both probabilities and values. To be clear, we may not need to\nestimate some of these quantities (like vNR(x), which we assume in this example is always\nzero), and estimating them well may be a nontrivial undertaking. The point is that the\nexpected value framework provides a helpful decomposition of possibly complicated\nbusiness problems into subproblems that we understand better how to solve. The\nframework also shows exactly how to put the pieces together. For our example problem\n(chosen for its straightforward derivation), the answer works out to the intuitively satisfying result: mail to those people whose estimated expected donation is greater than\nthe cost associated with mailing! Mathematically, we simply look for those whose expected benefit of targeting is greater than zero, and simplify the inequality algebraically.\nLet dR(x) be the estimated donation if consumer x were to respond, and let c be the\nmailing cost. Then:\n\nExpected benefit of targeting = p(R | ) · vR() + 1 - p(R | ) · vNR()\n\nWe always want this benefit to be greater than zero, so:\n\np(R | ) · (dR() - c) + 1 - p(R | ) · ( - c) >\np(R | ) · dR() - p(R | ) · c - c + p(R | ) · c >\np(R | ) · dR() >\n\n0\n\n0\n\nc\n\nThat is, the expected donation (lefthand side) should be greater than the solicitation\ncost (righthand side).\n\nA Brief Digression on Selection Bias\nThis example brings up an important data science issue whose detailed treatment is\nbeyond the scope of this book, but nevertheless is important to discuss briefly. For\nmodeling the predicted donation, notice that the data may well be biased---meaning\nthat they are not a random sample from the population of all donors. Why? Because the\ndata are from past donations---from the individuals who did respond in the past. This\nis similar to the idea of modeling creditworthiness based on the experience with past\ncredit customers: those are likely the people whom you had deemed to be creditworthy\n\n280\n\n|\n\nChapter 11: Decision Analytic Thinking II: Toward Analytical Engineering", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00305", "page_num": 305, "segment": "in the past! However, you want to apply the model to the general population to find\ngood prospects. Why would those who happened to have been selected in the past be a\ngood sample from which to model the general population? This is an example of selection\nbias---the data were not selected randomly from the population to which you intend to\napply the model, but instead were biased in some way (by who happened to donate, and\nperhaps by those who were targeted using past methods; by who was granted credit in\nthe past).\n\nOne important question for the data scientist is: do you expect the particular selection\nprocedure that biases the data also to have a bearing on the value of the target variable?\nIn modeling creditworthiness, the answer is absolutely yes---the past customers were\nselected precisely because they were predicted to be creditworthy. The donation case is\nnot as straightforward, but it seems reasonable to expect that people who donate larger\nsums do not donate as often. For example, some people may donate $10 each and every\ntime they’re asked. Others may give $100 and then feel they need not donate for a while,\nignoring many subsequent campaigns. The result would be that those who happened\nto donate in some past campaign will be biased towards those who donate less.\n\nFortunately, there are data science techniques to help modelers deal with selection bias.\nThey are beyond the scope of this book, but the interested reader might start by reading\n(Zadrozny & Elkan, 2001; Zadrozny, 2004) for an illustration of dealing with selection\nbias in this exact donation solicitation case study.\n\nOur Churn Example Revisited with Even More\nSophistication\nLet’s return to our example of churn and apply what we’ve learned to examine it dataanalytically. In our prior forays, we did not treat the problem as comprehensively as we\nmight. That was by design, of course, because we had not learned everything we needed\nyet, and the intermediate attempts were illustrative. But now let’s examine the problem\nin more detail, applying the exact same fundamental data science concepts as we just\napplied to the case of soliciting donations.\n\nThe Expected Value Framework: Structuring a More Complicated\nBusiness Problem\nFirst, what exactly is the business problem we would like to solve? Let’s keep our basic\nexample problem setting: we’re having a serious problem with churn in our wireless\nbusiness. Marketing has designed a special retention offer. Our task is to target the offer\nto some appropriate subset of our customer base.\n\nInitially, we had decided that we would try to use our data to determine which customers\nwould be the most likely to defect shortly after their contracts expire. Let’s continue to\nfocus on the set of customers whose contracts are about to expire, because this is where\n\nOur Churn Example Revisited with Even More Sophistication\n\n|\n\n281", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00306", "page_num": 306, "segment": "most of the churn occurs. However, do we really want to target our offer to those with\nthe highest probability of defection?\n\nWe need to go back to our fundamental concept: what exactly is the business problem\nwe want to solve. Why is churn a problem? Because it causes us to lose money. The real\nbusiness problem is losing money. If a customer actually were costly to us rather than\nprofitable, we may not mind losing her. We would like to limit the amount of money\nwe are losing---not simply to keep the most customers. Therefore, as in the donation\nproblem, we want to take the value of the customer into account. Our expected value\nframework helps us to frame that analysis, similar to how it did above. In the case of\nchurn, the value of an individual may be much easier to estimate: these are our customers, and since we have their billing records we can probably forecast their future\nvalue pretty well (contingent on their staying with the company) with a simple extrapolation of their past value. However, in this case we have not completely solved our\nproblem, and framing the analysis using expected value shows why.\n\nLet’s apply our expected value framework to really dig down into the business understanding/data understanding segment of the data mininig process. Is there any problem\nwith treating this case exactly as we did the donation case? As with the donation case\nstudy, we might represent the expected benefit of targeting a customer with the special\noffer as:\n\nExpected benefit of targeting = p(S | ) · vS() + 1 - p(S | ) · vNS()\n\nwhere p(S | ) is the probability that the customer will Stay with the company after\nbeing targeted, vS(x) is the value we get if consumer x stays with the company and vNS(x)\nis the value we get if consumer x does not stay (defects or churns).\n\nCan we use this to target customers with the special offer? All else being equal, targeting\nthose with the highest value seems like it simply targets those with the highest probability\nof staying, rather than the highest probability of leaving! To see this let’s oversimplify\nby assuming that the value if the customer does not stay is zero. Then our expected value\nbecomes:\n\nExpected benefit of targeting = p(S | ) · vs()\n\nThat does not jibe with our prior intuition that we want to target those who have the\nhighest probability of leaving. What’s wrong? Our expected value framework tells us\nexactly---let’s be more careful. We don’t want to just apply what we did in the donation\nproblem, but to think carefully about this problem. We don’t want to target those with\nthe highest value if they were to stay. We want to target those where we would lose the\nmost value if they were to leave. That’s complicated, but our expected value framework\ncan help us to work through the thinking systematically, and as we will see that will cast\n\n282\n\n|\n\nChapter 11: Decision Analytic Thinking II: Toward Analytical Engineering", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00307", "page_num": 307, "segment": "an interesting light on the solution. Recall that in the donation example we said, “To be\ncomplete, we would also want to assess the expected benefit of not targeting, and then\ncompare the two to make the decision of whether to target or not.” We allowed ourselves\nto ignore this in the donation setting because we assumed that consumers were not\ngoing to donate spontaneously without a solicitation. However, in the business understanding phase we need to think through the specifics of each particular business problem.\n\nLet’s think about the “not targeting” case of the churn problem. Is the value zero if we\ndon’t target? No, not necessarily. If we do not target and the customer stays anyway, then\nwe actually achieve higher value because we did not expend the cost of the incentive!\n\nAssessing the Influence of the Incentive\nLet’s dig even deeper, calculating both the benefit of targeting a customer with the incentive and of not targeting her, and making the cost of the incentive explicit. Let’s call\nuS(x) the profit from customer x if she stays, not including the incentive cost; and uNS(x)\nthe profit from customer x if she leaves, not including the incentive cost. Furthmore,\nfor simplicity, let’s assume that we incur the incentive cost c no matter whether the\ncustomer stays or leaves.\n\nFor churn this is not completely realistic, as the incentives usually\ninclude a large cost component that is contingent upon staying, such\nas a new phone. Expanding the analysis to include this small complication is straightforward, and we would draw the same qualitative\nconclusions. Try it.\n\nSo let’s compute separately the expected benefit if we target or if we do not target. In\ndoing so, we need to clarify that there (hopefully) will be different estimated probabilities\nof staying and churning depending on whether we target (i.e., hopefully the incentive\nactually has an effect), which we indicate by conditioning the probability of staying on\nthe two possibilities (target, T, or not target, notT). The expected benefit of targeting is:\n\nE BT () = p(S | , T ) · (us() - c) + 1 - p(S | , T ) · (uNS() - c)\n\nThe expected benefit of not targeting is:\n\nE BnotT () = p(S | , notT ) · (us() - c) + 1 - p(S | , notT ) · (uNS() - c)\n\nSo, now to complete our business problem formulation, we would like to target those\ncustomers for whom we would see the greatest expected benefit from targeting them.\n\nOur Churn Example Revisited with Even More Sophistication\n\n|\n\n283", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00308", "page_num": 308, "segment": "These are specifically those customers where EBT(x) -- EBnotT(x) is the largest. This is a\nsubstantially more complex problem formulation than we have seen before---but the\nexpected value framework structures our thinking so we can think systematically and\nengineer our analysis focusing precisely on the goal.\n\nThe expected value framework also allows us to see what is different about this problem\nstructure than those that we have considered in the past. Specifically, we need to consider\nwhat would happen if we did not target (looking at both EBT and EBnotT), as well as what\nis the actual influence of the incentive (taking the difference of EBT and EBnotT).1\n\nLet’s take another brief mathematical digression to illustrate. Consider the conditions\nunder which this “value of targeting,” VT = EBT(x) - EBnotT(x), would be the largest. Let’s\nexpand the equation for VT, but at the same time simplify by assuming that we get no\nvalue from a customer if she does not stay.\n\nEquation 11-1. VT decomposition\n\nVT =\n\n=\n\n=\n\np(S | , T ) · uS() - p(S | , notT ) · uS() - c\np(S | , T ) - p(S | , notT ) · uS() - c\nΔ(p) · uS() - c\n\nwhere Δ(p) is the difference in the predicted probabilities of staying, depending on\nwhether the customer is targeted or not. Again we see an intuitive result: we want to\ntarget those customers with the greatest change in their probability of staying, moderated by their value if they were to stay! In other words, target those with the greatest\nchange in their expected value as a result of targeting. (The -c is the same for everyone\nin our scenario, and including it here simply assures that the VT is not expected to be\na monetary loss.)\n\nIt’s important not to lose track: this was all work in our Business Understanding phase.\nLet’s turn to the implications for the rest of the data mining process.\n\nFrom an Expected Value Decomposition to a Data Science Solution\nThe prior discussion and specifically the decomposition highlighted in Equation 11-1\nguide us in our data understanding, data formulation, modeling, and evaluation. In\n\n1. This also is an essential starting point for causal analysis: create a so-called counterfactual situation assessing\nthe difference in expected values between two otherwise identical settings. These settings are often called the\n“treated” and “untreated” cases, in analogy to medical inference, where one often wants to assess the causal\ninfluence of the treatment. The many different frameworks for causal analysis, from randomized experimentation, to regression-based causal analysis, to more modern causal modeling approaches, all have this\ndifference in expected values at their core. We will discuss causal data analysis further in Chapter 12.\n\n284\n\n|\n\nChapter 11: Decision Analytic Thinking II: Toward Analytical Engineering", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00309", "page_num": 309, "segment": "particular, from the decomposition we can see precisely what models we will want to\nbuild: models to estimate p(S | , T ) and p(S | , notT ) the probability that a customer\nwill stay if targeted and the probability that a customer will stay anyway, even if not\ntargeted. Unlike our prior data mining solutions, here we want to build two separate\nprobability estimation models. Once these models are built, we can use them to compute\nthe expected value of targeting.\n\nImportantly, the expected value decomposition focuses our Data Understanding efforts.\nWhat data do we need to build these models? In both cases, we need samples of customers who have reached contract expiration. Indeed, we need samples of customers\nwho have gone far enough beyond contract expiration that we are satisfied with concluding they have definitely “stayed” or “left.” For the first model we need a sample of\ncustomers who were targeted with the offer. For the second model, we need a sample\nof customers who were not targeted with the offer. Hopefully this would be a representative sample of the customer base to which the model was applied (see the above discussion of selection bias). Developing our Data Understanding, let’s think more deeply\nabout each of these in turn.\n\nHow can we obtain a sample of such customers who have not been targeted with the\noffer? First, we should assure ourselves that nothing substantial has changed in the\nbusiness environment that would call into question the use of historical data for churn\nprediction (e.g., the introduction of the iPhone only to AT&T customers would have\nbeen such an event for the other phone companies). Assuming there has been no such\nevent, gathering the requisite data should be relatively straightforward: the phone company keeps substantial data on customers for many months, for billing, fraud detection,\nand other purposes. Given that this is a new offer, none of them would have been targeted\nwith it. We would want to double-check that none of our customers was made some\nother offer that would affect the likelihood of churning.\n\nThe situation with modeling p(S | , T ) is quite different, and again highlights how the\nexpected value framework can focus our thinking early, highlighting issues and challenges that we face. What’s the challenge here? This is a new offer. No one has seen it\nyet. We do not have the data to build a model to estimate p(S | , T )!\n\nNonetheless, business exigencies may force us to proceed. We need to reduce churn;\nMarketing has confidence in this offer, and we certainly have some data that might\ninform how we proceed. This is not an uncommon situation in the application of data\nscience to solving a real business problem. The expected value decomposition can lead\nus to a complex formulation that helps us to understand the problem, but for which we\nare not willing or able to address the full complexity. It may be that we simply do not\nhave the resources (data, human, or computing). In our churn example, we do not have\nthe data necessary.\n\nA different scenario might be that we do not believe that the added complexity of the\nfull formulation will add substantially to our effectiveness. For example, we might conOur Churn Example Revisited with Even More Sophistication\n\n|\n\n285", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00310", "page_num": 310, "segment": "clude, “Yes, the formulation of Equation 11-1 helps me understand what I should do, but\nI believe I will do just about as well with a simpler or cheaper formulation.” For example,\nwhat if we were to assume that when given the offer, everyone would Stay with certainty,\np(S | , T ) = 1? This is obviously an oversimplification, but it may allow us to act---\nand in business we need to be ready to act even without ideal information. You could\nverify via Equation 11-1 that the result of applying this assumption would be simply to\ntarget those customers with the largest 1 - p(S | , notT ) · u S ()--- i.e., the customers\nwith the largest expected loss if they were to leave. That makes a lot of sense if we do\nnot have data on the actual differential effect of the offer.\n\nConsider an alternative course of action in a case such as this, where sufficient data are\nnot available on a modeling target. One can instead label the data with a “proxy” for the\ntarget label of interest. For example, perhaps Marketing had come up with a similar, but\nnot identical, offer in the past. If this offer had been made to customers in a similar\nsituation (and recall the selection bias concern discussed above), it may be useful to\nbuild a model using the proxy label. 2\n\nThe expected value decomposition highlights yet another option. What would we need\nto do to model p(S | , T )? We need to obtain data. Specifically, we need to obtain data\nfor customers who are targeted. That means we have to target customers. However, this\nwould incur a cost. What if we target poorly and waste money targeting customers with\nlower probabilities of responding? This situation relates back to our very first fundamental principle of data science: data should be treated as an asset. We need to think\nnot only about taking advantage of the assets that we already have, but also about investing in data assets from which we can generate important returns. Recall from\nChapter 1 the situation Signet Bank faced in “Data and Data Science Capability as a\nStrategic Asset” on page 9. They did not have data on the differential response of customers to the various new sorts of offers they had designed. So they invested in data,\ntaking losses by making offers broadly, and the data assets they acquired is considered\nto be the reason they became the wildly successful Capital One. Our situation may not\nbe so grand, in that we have a single offer, and in making the offer we are not likely to\nlose the sort of money that Signet Bank did when their customers defaulted. Nonetheless, the lesson is the same: if we are willing to invest in data on how people will respond\nto this offer, we may be able to better target the offer to future customers.\n\n2. For some applications, proxy labels might come from completely different events from the event on which\nthe actual target label is based. For example, for building models to predict who will purchase after being\ntargeted with an advertisement, data on actual conversions are scarce. It is surprisingly effective to use visiting\nthe compaign’s brand’s website as a modeling proxy for purchasing (Dalessandro, Hook, Perlich, & Provost,\n2012).\n\n286\n\n|\n\nChapter 11: Decision Analytic Thinking II: Toward Analytical Engineering", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00311", "page_num": 311, "segment": "It’s worth reiterating the importance of deep business understanding.\nDepending on the structure of the offer, we may not lose that much if\nthe offer is not taken, so the simpler formulation above may be quite\nsatisfactory.\n\nNote that this investment in data can be managed carefully, also applying conceptual\ntools developed through the book. Recall the notion of visualizing performance via the\nlearning curve, from Chapter 8. The learning curve helps us to understand the relationship between the amount of data---in this case, the amount of investment in data so\nfar---and the resultant improvement in generalization performance. We can easily extend the notion of generalization performance to include the improvement in performance over a baseline (recall our fundamental concept: think carefully about what you\nwill compare to). That baseline could be our alternative, simple churn model. Thus, we\nwould slowly invest in data, examining whether increasing our data is improving our\nperformance, and whether extrapolating the curve indicates that there are more improvements to come. If this analysis suggests that the investment is not worthwhile, it\ncan be aborted.\n\nImportantly, that does not mean the investment was wasteful. We invested in information: here, information about whether the additional data would pay off for our ultimate\ntask of cost-effective churn reduction.\n\nFurthermore, framing the problem using expected value allows extensions to the formulation to provide a structured way to approach the question of: what is the right offer\nto give. We could expand the formulation to include multiple offers, and judge which\ngives the best value for any particular customer. Or we could parameterize the offers\n(for example with a variable discount amount) and then work to optimize what discount\nwill yield the best expected value. This would likely involve additional investment in\ndata, running experiments to judge different customers’ probabilities of staying or leaving at different offer levels---again similar to what Signet Bank did in becoming Capital\nOne.\n\nSummary\nBy following through the donation and churn examples, we have seen how the expected\nvalue framework can help articulate the true business problem and the role(s) data\nmining will play in its solution.\n\nIt is possible to keep elaborating the business problem into greater and greater detail,\nuncovering additional complexity in the problem (and greater demands on its solution).\nYou may wonder, “Where does this all end? Can’t I keep pushing the analysis on forever?” In principle, yes, but modeling always involves making some simplifying assumpOur Churn Example Revisited with Even More Sophistication\n\n|\n\n287", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00312", "page_num": 312, "segment": "tions to keep the problem tractable. There will always be points in analytical engineering\nat which you should conclude:\n\n• We can’t get data on this event,\n\n• It would be too expensive to model this aspect accurately,\n\n• This event is so improbable we’re just going to ignore it, or\n\n• This formulation seems sufficient for the time being, and we should proceed with\n\nit.\n\nThe point of analytical engineering is not to develop complex solutions by addressing\nevery possible contingency. Rather, the point is to promote thinking about problems\ndata analytically so that the role of data mining is clear, the business constraints, cost,\nand benefits are considered, and any simplifying assumptions are made consciously and\nexplicitly. This increases the chance of project success and reduces the risk of being\nblindsided by problems during deployment.\n\n288\n\n|\n\nChapter 11: Decision Analytic Thinking II: Toward Analytical Engineering", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00313", "page_num": 313, "segment": "CHAPTER 12\nOther Data Science Tasks and Techniques\n\nFundamental concepts: Our fundamental concepts as the basis of many common data\nscience techniques; The importance of familiarity with the building blocks of data science.\n\nExemplary techniques: Association and co-occurrences; Behavior profiling; Link prediction; Data reduction; Latent information mining; Movie recommendation; Bias-variance\ndecomposition of error; Ensembles of models; Causal reasoning from data.\n\nAs discussed in the previous chapter, a useful way to think of a team approaching a\nbusiness problem data analytically is that they are faced with an engineering problem---\nnot mechanical engineering or even software engineering, but analytical engineering.\nThe business problem itself provides the goal as well as constraints on its solution. The\ndata and domain knowledge provide raw materials. And data science provides frameworks for decomposing the problem into subproblems, as well as tools and techniques\nfor solving them. We have discussed some of the most valuable conceptual frameworks\nand some of the most common building blocks for solutions. However, data science is\na vast field, with entire degree programs devoted to it, so we cannot hope to be exhaustive\nin a book like this. Fortunately, the fundamental principles we have discussed undergird\nmost of data science.\n\nAs with other engineering problems, it is often more efficient to cast a new problem\ninto a set of problems for which we already have good tools, rather than trying to build\na custom solution completely from scratch. Analytical engineering is not different: data\nscience provides us with an abundance of tools to solve particular, common tasks. So\nwe have illustrated the fundamental principles with some of the most common tools,\nmethods for finding correlations/finding informative variables, finding similar entities,\nclassification, class-probability estimation, regression, and clustering.\n\nThese are tools for the most common data science tasks, but as described in Chapter 2 there are others as well. Fortunately, the same fundamental concepts that underlie\nthe tasks we have used for illustration also underlie these others. So now that we’ve\n\n289", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00314", "page_num": 314, "segment": "presented the fundamentals, let’s briefly discuss some of the other tasks and techniques\nwe haven’t yet discussed.\n\nCo-occurrences and Associations: Finding Items That Go\nTogether\nCo-occurrence grouping or association discovery attempts to find associations between\nentities based on transactions involving them. Why would we want to find such cooccurrences? There are many applications. Consider a consumer-facing application.\nLet’s say that we run an online retailer. Based on shopping cart data, we might tell a\ncustomer, “Customers who bought the new eWatch also bought the eBracelet Bluetooth\nspeaker companion.” If the associations indeed capture true consumer preferences, this\nmight increase revenue from cross-selling. It also could enhance the consumer experience (in this case, by allowing stereo music listening from their otherwise monaural\neWatch), and thus leverage our data asset to create additional customer loyalty.\n\nConsider an operations application where we ship products to online customers from\nmany distribution centers across the globe. Not every distribution center stocks every\nproduct. Indeed, the smaller, regional distribution centers only stock the more frequently purchased products. We built these regional distribution centers to reduce\nshipping expense, but in practice we see that for many orders we end up either having\nto ship from the main distribution center anyway, or to make multiple deliveries for\nmany orders. The reason is that even when people order popular items, they often\ninclude less-popular items as well. This is a business problem we can try to address by\nmining associations from our data. If there are particular less-popular items that cooccur often with the most-popular items, these also could be stocked in the regional\ndistribution centers, achieving a substantial reduction in our shipping costs.\n\nThe co-occurrence grouping is simply a search through the data for combinations of\nitems whose statistics are “interesting.” There are different ways of framing the task, but\nlet’s think of the co-occurrence as a rule: “If A occurs then B is likely to occur as well.” So\nA might be the sale of an eWatch, and B the sale of the eBracelet.1 The statistics on\n“interesting” generally follow our fundamental principles.\n\nFirst, we need to consider complexity control: there are likely to be a tremendous number of cooccurrences, many of which might simply be due to chance, rather than to a\ngeneralizable pattern. A simple way to control complexity is to place a constraint that\nsuch rules must apply to some minimum percentage of the data---let’s say that we require\nrules to apply to at least 0.01% of all transactions. This is called the support of the association.\n\n1. A and B could be multiple items as well. We will presume that they are single items for the moment. The\n\nFacebook Likes example below generalizes to multiple items.\n\n290\n\n|\n\nChapter 12: Other Data Science Tasks and Techniques", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00315", "page_num": 315, "segment": "We also have the notion of “likely” in the association. If a customer buys the eWatch\nthen she is likely to buy the eBracelet. Again, we may want to require a certain minimum\ndegree of likelihood for the associations we find. We can quantify this notion again using\nthe same notions we have already seen. The probability that B occurs when A occurs\nwe’ve seen before; it is p(B|A), which in association mining is called the confidence or\nstrength of the rule. Let’s call that “strength,” so as not to confuse it with statistical confidence. So we might say we require the strength to be above some threshold, such as\n5% (so that 5% or more of the time, a buyer of A also buys B).\n\nMeasuring Surprise: Lift and Leverage\nFinally, we would like the association to be in some sense “surprising.” There are many\nnotions of surprisingness that have been pursued in data mining, but unfortunately\nmost of them involve matching the discovered knowledge to our prior background\nknowledge, intuition, and common sense. In other words, an association is surprising\nif it contradicts something we already knew or believed. Researchers study how to address this difficult-to-codify knowledge, but dealing with it automatically is not common in practice. Instead, data scientists and business users pore over long lists of asssociations, culling the unsurprising ones.\n\nHowever, there is a weaker but nonetheless intuitive notion of surprisingness that can\nbe computed from the data alone, and which we already have encountered in other\ncontexts: lift --- how much more frequently does this association occur than we would\nexpect by chance? If associations from supermarket shopping cart data revealed that\nbread and milk are often bought together, we might say: “Of course.” Many people buy\nmilk and many people buy bread. So we would expect them to occur together frequently\njust by chance. We would be more surprised if we found associations that occur much\nmore frequently than chance would dictate. Lift is calculated simply by applying basic\nnotions of probability.\n\nEquation 12-1. Lift\n\nLift(A, B) =\n\np(A, B)\np(A) p(B)\n\nIn English, the lift of the co-occurrence of A and B is the probability that we actually\nsee the two together, compared to the probability that we would see the two together if\nthey were unrelated to (independent of) each other. As with other uses of lift we’ve seen,\na lift greater than one is the factor by which seeing A “boosts” the likelihood of seeing\nB as well.\n\nCo-occurrences and Associations: Finding Items That Go Together\n\n|\n\n291", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00316", "page_num": 316, "segment": "This is only one possible way to compute how much more likely than chance a discovered association is. An alternative is to look at the difference of these quantities rather\nthan their ratio. This measure is called leverage.\n\nEquation 12-2. Leverage\n\nLeverage(A, B) = p(B, A) - p(A)p(B)\n\nTake a minute to convince yourself that one of these would be better for associations\nthat are very unlikely to occur by chance, and one better for those rather likely to occur\nby chance.\n\nExample: Beer and Lottery Tickets\nAs we’ve already seen from the “eWatch and eBracelet” example, association discovery\nis often used in market basket analysis to find and analyze co-ocurrences of bought\nitems. Let’s work through a concrete example.\n\nSuppose we operate a small convenience store where people buy groceries, liquor, lottery\ntickets, and so on. Let’s say we analyze all of our transactions over a year’s time. We\ndiscover that people often buy beer and lottery tickets together. However, we know that\nin our store, people buy beer often and people buy lottery tickets often. Let’s say we find\nthat 30% of all transactions involve beer, and 20% of the transactions include both beer\nand lottery tickets! Is this co-occurrence an interesting one? Or is it simply due to the\ncommonality of these two purchases? Association statistics can help us.\n\nFirst, let’s state an association rule representing this belief: “Customers who buy beer\nare also likely to buy lottery tickets”; or more tersely, “beer ⇒ lottery tickets.” Next, let’s\ncalculate the lift of this association. We already know one value we need: p(beer)=0.3.\nLet’s say that lottery tickets also are very popular: p(lottery tickets)=0.4. If these two\nitems were completely unrelated (independent), the chance that they would be bought\ntogether would be the product of these two: p(beer) × p(lottery tickets)=0.12.\n\nWe also have the actual probability (frequency in the data) of people buying the two\nitems together, p(lottery tickets, beer), which we found by combing through the register\nreceipt data looking for all transactions including beer and lottery tickets. As mentioned\nabove, 20% of the transactions included both, and this is our probability: p(lottery tickets, beer) = 0.2. So the lift is 0.2 / 0.12, which is about 1.67. This means that buying\nlottery tickets and beer together is about 1 2/3 times more likely than one would expect\nby chance. We might conclude that there is some relationship there, but much of the\nco-occurrence is due to the fact that these are each very popular items.\n\nWhat about leverage? This is p(lottery tickets, beer) - p(lottery tickets) × p(beer), which\nis 0.2 - 0.12, or 0.08. Whatever is driving the co-occurrence results in an eight\n\n292\n\n|\n\nChapter 12: Other Data Science Tasks and Techniques", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00317", "page_num": 317, "segment": "percentage-point increase in the probability of buying both together over what we would\nexpect simply because they are popular items.\n\nThere are two other significant statistics we should calculate too: the support and the\nstrength. The support of the association is just the prevalence in the data of buying the\ntwo items together, p(lottery tickets, beer), which is 20%. The strength is the conditional\nprobability, p(lottery tickets|beer), which is 67%.\n\nAssociations Among Facebook Likes\nAlthough finding associations is often used with market basket data---and sometimes\nis even called market-basket analysis---the technique is much more general. We can use\nour example from Chapter 9 of “Likes” on Facebook to illustrate. Recall that we have\ndata on the things that were “Liked” by a large collection of users of Facebook (Kosinski,\nStillwell, & Graepel, 2013). By analogy to market basket data, we can consider each of\nthese users to have a “basket” of Likes, by aggregating all the Likes of each user. Now we\ncan ask, do certain Likes tend to co-occur more frequently than we would expect by\nchance? We will use this simply as an interesting example to illustrate association finding, but the process could actually have an important business application. If you are a\nmarketer looking to understand the consumers in a particular market, you might be\ninterested in finding patterns of things people Like. If you are thinking data-analytically,\nyou will apply exactly the sort of thinking we’ve illustrated so far in this chapter: you’ll\nwant to know what things co-occur more frequently than you would expect by chance.\n\nBefore we get to the mining of the data, let’s introduce one more useful idea for association finding. Since we’re using the market basket as an analogy at this point, we should\nconsider broadening our thinking of what might be an item. Why can’t we put just about\nanything we might be interested in finding associations with into our “basket”? For\nexample, we might put a user’s location into the basket, and then we could see associations between Likes and locations. For actual market basket data, these sometimes are\ncalled virtual items, to distinguish from the actual items that people put into their basket\nin the store. For our Facebook data, recall that we might obtain psychometric data on\nmany of the consumers, such as their degree of extroversion or agreableness, or their\nscore on an IQ test. It may be interesting to allow the association search to find associations with these psychometric characteristics as well.\n\nCo-occurrences and Associations: Finding Items That Go Together\n\n|\n\n293", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": null, "chapter_title": null, "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00318", "page_num": 318, "segment": "Note: Supervised Versus Unsupervised?\nWe should keep in mind our distinction between supervised and unsupervised data mining. If we want specifically to understand what\ncorrelates most with agreeableness or with Liking our brand, we should\nformulate this as a supervised problem, with the corresponding target variable. This is what we did when looking at the evidence lifts in\nChapter 9, and at supervised segmentation throughout the book. If we\nwant to explore the data without such a specific goal, then association finding may be more appropriate. See the discussion in Chapter 6\non the differences between supervised and unsupervised mining---\nthere in the context of clustering, but the fundamental concepts apply to association mining as well.\n\nOK, so let’s see what associations we get among Facebook Likes.2 These associations\nwere found using the popular association mining system Magnum Opus.3 Magnum\nOpus allows searching for associations that give the highest lift or highest leverage, while\nfiltering out associations that cover too few cases to be interesting. The list below shows\nsome of the highest lift associations among Facebook Likes with the constraint that they\nhave to cover at least 1% of the users in the dataset. Do these associations make sense?\nDo they give us a picture of the relationships among the users’ tastes? Note that the lifts\nare all above 20, meaning that all of these associations are at least 20 times more likely\nthan we would expect by chance:\n\nFamily Guy & The Daily Show -> The Colbert Report\nSupport=0.010; Strength=0.793; Lift=31.32; Leverage=0.0099\n\nSpirited Away -> Howl's Moving Castle\nSupport=0.011; Strength=0.556; Lift=30.57; Leverage=0.0108\n\nSelena Gomez -> Demi Lovato\nSupport=0.010; Strength=0.419; Lift=27.59; Leverage=0.0100\n\nI really hate slow computers & Random laughter when remembering something ->\n Finding Money In Your Pocket\nSupport=0.010; Strength=0.726; Lift=25.80; Leverage=0.0099\n\nSkittles & Glowsticks -> Being Hyper!\nSupport=0.011; Strength=0.529; Lift=25.53; Leverage=0.0106\n\nLinkin Park & Disturbed & System of a Down & Korn -> Slipknot\nSupport=0.011; Strength=0.862; Lift=25.50; Leverage=0.0107\n\n2. Thanks to Wally Wang for help with this.\n\n3. See this page.\n\n294\n\n|\n\nChapter 12: Other Data Science Tasks and Techniques", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00319", "page_num": 319, "segment": "Lil Wayne & Rihanna -> Drake\nSupport=0.011; Strength=0.619; Lift=25.33; Leverage=0.0104\n\nSkittles & Mountain Dew -> Gatorade\nSupport=0.010; Strength=0.519; Lift=25.23; Leverage=0.0100\n\nSpongeBob SquarePants & Converse -> Patrick Star\nSupport=0.010; Strength=0.654; Lift=24.94; Leverage=0.0097\n\nRihanna & Taylor Swift -> Miley Cyrus\nSupport=0.010; Strength=0.490; Lift=24.90; Leverage=0.0100\n\nDisturbed & Three Days Grace -> Breaking Benjamin\nSupport=0.012; Strength=0.701; Lift=24.64; Leverage=0.0117\n\nEminem & Lil Wayne -> Drake\nSupport=0.014; Strength=0.594; Lift=24.30; Leverage=0.0131\n\nAdam Sandler & System of a Down & Korn -> Slipknot\nSupport=0.010; Strength=0.819; Lift=24.23; Leverage=0.0097\n\nPink Floyd & Slipknot & System of a Down -> Korn\nSupport=0.010; Strength=0.810; Lift=24.05; Leverage=0.0097\n\nMusic & Anime -> Manga\nSupport=0.011; Strength=0.675; Lift=23.99; Leverage=0.0110\n\nMedium IQ & Sour Gummy Worms -> I Love Cookie Dough\nSupport=0.012; Strength=0.568; Lift=23.86; Leverage=0.0118\n\nRihanna & Drake -> Lil Wayne\nSupport=0.011; Strength=0.849; Lift=23.55; Leverage=0.0104\n\nI Love Cookie Dough -> Sour Gummy Worms\nSupport=0.014; Strength=0.569; Lift=23.28; Leverage=0.0130\n\nLaughing until it hurts and you can't breathe! & I really hate slow computers ->\n Finding Money In Your Pocket\nSupport=0.010; Strength=0.651; Lift=23.12; Leverage=0.0098\n\nEvanescence & Three Days Grace -> Breaking Benjamin\nSupport=0.012; Strength=0.656; Lift=23.06; Leverage=0.0117\n\nDisney & Disneyland -> Walt Disney World\nSupport=0.011; Strength=0.615; Lift=22.95; Leverage=0.0103\n\ni finally stop laughing... look back over at you and start all over again ->\n That awkward moment when you glance at someone staring at you.\nSupport=0.011; Strength=0.451; Lift=22.92; Leverage=0.0104\n\nSelena Gomez -> Miley Cyrus\nSupport=0.011; Strength=0.443; Lift=22.54; Leverage=0.0105\n\nCo-occurrences and Associations: Finding Items That Go Together\n\n|\n\n295", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00320", "page_num": 320, "segment": "Reese's & Starburst -> Kelloggs Pop-Tarts\nSupport=0.011; Strength=0.493; Lift=22.52; Leverage=0.0102\n\nSkittles & SpongeBob SquarePants -> Patrick Star\nSupport=0.012; Strength=0.590; Lift=22.49; Leverage=0.0112\n\nDisney & DORY & Toy Story -> Finding Nemo\nSupport=0.011; Strength=0.777; Lift=22.47; Leverage=0.0104\n\nKaty Perry & Taylor Swift -> Miley Cyrus\nSupport=0.011; Strength=0.441; Lift=22.43; Leverage=0.0101\n\nAKON & Black Eyed Peas -> Usher\nSupport=0.010; Strength=0.731; Lift=22.42; Leverage=0.0097\n\nEminem & Drake -> Lil Wayne\nSupport=0.014; Strength=0.807; Lift=22.39; Leverage=0.0131\n\nMost association mining examples use domains (such as Facebook Likes) where readers\nalready have a fair knowledge of the domain. This is because otherwise, since the mining\nis unsupervised, evaluation depends much more critically on domain knowledge validation (recall the discussion in Chapter 6)---we do not have a well-defined target task\nfor an objective evaluation. However, one interesting practical use of association mining\nis to explore data that we do not understand so well. Consider going into a new job.\nExploring the company’s customer transaction data and examining the strong cooccurrences can quickly give broad overview of the taste relationships in the customer\nbase. So, with that in mind, look back at the co-occurrences in the Facebook Likes and\npretend that this was not a domain of popular culture: these and others like them (there\nare huge numbers of such associations) would give you a very broad view of the related\ntastes of the customers.\n\nProfiling: Finding Typical Behavior\nProfiling attempts to characterize the typical behavior of an individual, group, or population. An example profiling question might be: What is the typical credit card usage\nof this customer segment? This could be a simple average of spending, but such a simple\ndescription might not represent the behavior well for our business task. For example,\nfraud detection often uses profiling to characterize normal behavior and then looks for\ninstances that deviate substantially from the normal behavior---especially in ways that\npreviously have been indicative of fraud (Fawcett & Provost, 1997; Bolton & Hand,\n2002). Profiling credit card usage for fraud detection might require a complex description of weekday and weekend averages, international usage, usage across merchant and\nproduct categories, usage from suspicious merchants, and so on. Behavior can be described generally over an entire population, at the level of small groups, or even for each\nindividual. For example, each credit card user might be profiled with respect to his\n\n296\n\n|\n\nChapter 12: Other Data Science Tasks and Techniques", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00321", "page_num": 321, "segment": "international usage, so as not to create many false alarms for an individual who commonly travels abroad.\n\nProfiling combines concepts discussed previously. Profiling can essentially involve\nclustering, if there are subgroups of the population with different behaviors. Many\nprofiling methods seem complicated, but in essence are simply instantiations of the\nfundamental concept introduced in Chapter 4: define a numeric function with some\nparameters, define a goal or objective, and find the parameters that best meet the objective.\n\nSo let’s consider a simple example from business operations management. Businesses\nwould like to use data to help to understand how well their call centers are supporting\ntheir customers.4 One aspect of supporting customers well is to not leave them sitting\non hold for long periods of time. So how might we profile the typical wait time of our\ncustomers who call into the call center? We might calculate the mean and standard\ndeviation of the wait time.\n\nThat seems like exactly what a manager with basic statistical training might do---it turns\nout to be a simple instance of model fitting. Here’s why. Let’s assume that customer wait\ntimes follow a Normal or Gaussian distribution. Saying such things can cause a nonmathematical person to fear what’s to come, but that just means the distribution follows\na bell curve with some particularly nice properties. Importantly, it is a “profile” of the\nwait times that (in this case) has only two important parameters: the mean and the\nstandard deviation. When we calculate the mean and standard deviation, we are finding\nthe “best” profile or model of wait time under the assumption that it is Normally distributed. In this case “best” is the same notion that we discussed for logistic regression,\nfor example, the mean we calculate from the spending gives us the mean of the Gaussian\ndistribution that is most likely to have generated the data (the “maximum likelihood”\nmodel).\n\nThis view illustrates why a data science perspective can help even in simple scenarios:\nit is much clearer now what we are doing when we are calculating averages and standard\ndeviations, even if our memory of the details from statistics classes is hazy. We also need\nto keep in mind our fundamental principles introduced in Chapter 4 and elaborated in\nChapter 7: we need to consider carefully what we desire from our data science results.\nHere we would like to profile the “normal” wait time of our customers. If we plot the\ndata and they do not look like they came from a Gaussian (a symmetric bell curve that\ngoes to zero very quickly in the “tails”), we might want to reconsider simply reporting\nthe mean and standard deviation. We might instead report the median, which is not so\nsensitive to the skew, or possibly even better, fit a different distribution (maybe after\ntalking to a statistically oriented data scientist about what might be appropriate).\n\n4. The interested reader is encouraged to read Brown et al. (2005) for a technical treatment and details on this\n\napplication.\n\nProfiling: Finding Typical Behavior\n\n|\n\n297", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00322", "page_num": 322, "segment": "Figure 12-1. A distribution of wait times for callers into a bank’s call center.\n\nTo illustrate how a data science savvy manager might proceed, let’s look at a distribution\nof wait times for callers into a bank’s call center over a couple of months. Figure 12-1\nshows such a distribution. Importantly, we see how visualizing the distribution should\ncause our data science radar to issue an alert. The distribution is not a symmetric bell\ncurve. We should then worry about simply profiling wait times by reporting the mean\nand standard deviation. For example, the mean (100) does not seem to satisfy our desire\nto profile how long our customers normally wait; it seems too large. Technically, the\nlong “tail” of the distribution skews the mean upward, so it does not represent faithfully\nwhere most of the data really lie. It does not represent faithfully the normal wait time\nof our customers.\n\nTo give more depth to what our data science-savvy manager might do, let’s go a little\nfurther. We will not get into the details here, but a common trick for dealing with data\nthat are skewed in this way is to take the logarithm (log) of the wait times. Figure 12-2\nshows the same distribution as Figure 12-1, except using the logarithms of the wait\ntimes. We now see that after the simple transformation, the wait times look very much\nlike the classic bell curve.\n\n298\n\n|\n\nChapter 12: Other Data Science Tasks and Techniques", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00323", "page_num": 323, "segment": "Figure 12-2. The distribution of wait times for callers into a bank’s call center after a\nquick redefinition of the data.\n\nIndeed, Figure 12-2 also shows an actual Gaussian distribution (the bell curve) fit to the\nbell-shaped distribution, as described above. It fits very well, and thus we have a justification for reporting the mean and standard deviation as summary statistics of the\nprofile of (log) wait times.5\n\nThis simple example extends nicely to more complex situations. Shifting contexts, let’s\nsay we want to profile customer behavior in terms of their spending and their time on\nour website. We believe these to be correlated, but not perfectly, as with the points plotted\nin Figure 12-3. Again, a very common tack is to apply the fundamental notion of\nChapter 4: choose a parameterized numeric function and an objective, and find parameters that maximize the objective. For example, we can choose a two-dimensional\nGaussian, which is essentially a bell oval instead of a bell curve---an oval-shaped blob\nthat is very dense in the center and thins out toward the edges. This is represented by\nthe contour lines in Figure 12-3.\n\n5. A statistically trained data scientist might have noticed immediately the shape of the distribution of the\noriginal data, shown in Figure 12-1. This is a so-called log-normal distribution, which just means that the\nlogs of the quantities in question are normally distributed.\n\nProfiling: Finding Typical Behavior\n\n|\n\n299", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00324", "page_num": 324, "segment": "Figure 12-3. A profile of our customers with respect to their spending and the time they\nspend on our web site, represented as a two-dimensional Gaussian fit to the data.\n\nWe can keep extending the idea to more and more sophisticated profiling. What if we\nbelieve there are different subgroups of customers with different behaviors? We may\nnot be willing to simply fit a Gaussian distribution to the behavior. However, maybe we\nare comfortable assuming that there are k groups of customers, each of whose behavior\nis normally distributed. We can fit a model with multiple Gaussians, called a Gaussian\nMixture Model (GMM). Applying our fundamental concept again, finding the\nmaximum-likelihood parameters identifies the k Gaussians that fit the data best (with\nrespect to this particular objective function). We see an example with k=2 in\nFigure 12-4. The figure shows how the fitting procedure identifies two different groups\nof customers, each modeled by a two-dimensional Gaussian distribution.\n\nFigure 12-4. A profile of our customers with respect to their spending and the time they\nspend on our web site, represented as a Gaussian Mixture Model (GMM), with 2 twodimensional Gaussians fit to the data. The GMM provides a “soft” clustering of our\ncustomers along two these two dimensions.\n\n300\n\n|\n\nChapter 12: Other Data Science Tasks and Techniques", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00325", "page_num": 325, "segment": "Now we have a rather sophisticated profile, which we can understand as a surprisingly\nstraightforward application of our fundamental principles. An interesting side note is\nthat this GMM has produced for us a clustering, but in a different way from the clusterings presented in Chapter 6. This illustrates how fundamental principles, rather than\nspecific tasks or algorithms, form the basis for data science. In this case, clustering can\nbe done in many different ways, just as classification and regression can be.\n\nNote: “Soft” Clustering\nIncidentally, you may notice that the clusters in the GMM overlap with\neach other. The GMM provides what is called a “soft” or probabilistic clustering. Each point does not strictly belong to a single cluster,\nbut instead has a degree or probability of membership in each cluster. In this particular clustering, we can think that a point is more likely\nto have come from some clusters than others. However, there still is a\npossibility, perhaps remote, that the point may have come from any of\nthem.\n\nLink Prediction and Social Recommendation\nSometimes, instead of predicting a property (target value) of a data item, it is more\nuseful to predict connections between data items. A common example of this is predicting that a link should exist between two individuals. Link prediction is common in\nsocial networking systems: Since you and Karen share 10 friends, maybe you’d like to be\nKaren’s friend? Link prediction can also estimate the strength of a link. For example, for\nrecommending movies to customers one can think of a graph between customers and\nthe movies they’ve watched or rated. Within the graph, we search for links that do not\nexist between customers and movies, but that we predict should exist and should be\nstrong. These links form the basis for recommendations.\n\nThere are many approaches to link prediction, and even an entire chapter of this book\nwould not do them justice. However, we can understand a wide variety of approaches\nusing our fundamental concepts of data science. Let’s consider the social network case.\nKnowing what you know now, if you had to predict either the presence or the strength\nof a link between two individuals, how would you go about framing the problem? We\nhave several alternatives. We could presume that links should be between similar individuals. We know then that we need to define a similarity measure that takes into account\nthe important aspects of our application.\n\nCould we define a similarity measure between two individuals that would indicate that\nthey might like to be friends? (Or are already friends, depending on the application.)\nSure. Using the example above directly, we could consider the similarity to be the number of shared friends. Of course, the similarity measure could be more sophisticated:\nwe could weight the friends by the amount of communication, geographical proximity,\n\nLink Prediction and Social Recommendation\n\n|\n\n301", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00326", "page_num": 326, "segment": "or some other factor, and then find or devise a similarity function that takes these\nstrengths into account. We could use this friend strength as one aspect of similarity\nwhile also including others (since after Chapter 6 we are comfortable with multivariate\nsimilarity), such as shared interests, shared demographics, etc. In essence, we could\napply knowledge of “finding similar data items” to people, by considering the different\nways in which we could represent the people as data.\n\nThat is one way to attack the link prediction problem. Let’s consider another, just to\ncontinue to illustrate how the fundamental principles apply to other tasks. Since we\nwant to predict the existence (or strength) of a link, we might well decide to cast the task\nas a predictive modeling problem. So we can apply our framework for thinking about\npredictive modeling problems. As always, we start with business and data understanding. What would we consider to be an instance? At first, we might think: wait a minute\n---here we are looking at the relationship between two instances. Our conceptual framework comes in very handy: let’s stick to our guns, and define an instance for prediction.\nWhat exactly is it that we want to predict? We want to predict the existence of a relationship (or its strength, but let’s just consider the existence here) between two people.\nSo, an instance should be a pair of people!\n\nOnce we have defined an instance to be a pair of people, we can proceed smoothly. Next,\nwhat would be the target variable? Whether the relationship exists, or would be formed\nif recommended. Would this be a supervised task? Yes, we can get training data where\nlinks already do or do not exist, or if we wanted to be more careful we could invest in\nacquiring labels specifically for the recommendation task (we would need to spend a\nbit more time than we have here on defining the exact semantics of the link). What\nwould be the features? These would be features of the pair of people, such as how many\ncommon friends the two individuals have, what is the similarity in their interests, and\nso on. Now that we have cast the problem in the form of a predictive modeling task, we\ncan start to ask what sorts of models we would apply and how we would evaluate them.\nThis is the same conceptual procedure we go through for any predictive modeling task.\n\nData Reduction, Latent Information, and Movie\nRecommendation\nFor some business problems, we would like to take a large set of data and replace it with\na smaller set that preserves much of the important information in the larger set. The\nsmaller dataset may be easier to deal with or to process. Moreover, the smaller dataset\nmay better reveal the information contained within it. For example, a massive dataset\non consumer movie-viewing preferences may be reduced to a much smaller dataset\nrevealing the consumer taste preferences that are latent in the viewing data (for example,\nviewer preferences for movie genre). Such data reduction usually involves sacrificing\nsome information, but what is important is the trade-off between the insight or manageability gained against the information lost. This is often a trade worth making.\n\n302\n\n|\n\nChapter 12: Other Data Science Tasks and Techniques", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00327", "page_num": 327, "segment": "As with link prediction, data reduction is a general task, not a particular technique.\nThere are many techniques, and we can use our fundamental principles to understand\nthem. Let’s discuss a popular technique as an example.\n\nLet’s continue to talk about movie recommendations. In a now famous (at least in data\nscience circles) contest, the movie rental company NetflixTM offered a million dollars to\nthe individual or team that could best predict how consumers would rate movies.\nSpecifically, they set a prediction performance goal on a holdout data set and awarded\nthe prize to the team that first reached this goal.6 Netflix made available historical data\non movie ratings assigned by their customers. The winning team7 produced an extremely complicated technique, but much of the success is attributed to two aspects of\nthe solution: (i) the use of ensembles of models, which we will discuss in “Bias, Variance,\nand Ensemble Methods” on page 306, and (ii) data reduction. The main data reduction\ntechnique that the winners used can be described easily using our fundamental concepts.\n\nThe problem to be solved was essentially a link prediction problem, where specifically\nwe would like to predict the strength of the link between a user and a movie---the\nstrength representing how much the user would like it. As we just discussed, this can\nbe cast as a predictive modeling problem. However, what would the features be for the\nrelationship between a user and a movie?\n\nOne of the most popular approaches for providing recommendations, described in\ndetail in a very nice article by several of the Netflix competition winners (Koren, Bell,\n& Volinsky, 2009), is to base the model on latent dimensions underlying the preferences.\nThe term “latent,” in data science, means “relevant but not observed explicitly in the\ndata.” Chapter 10 discussed topic models, another form of latent model, where the latent\ninformation is the set of topics in the documents. Here the latent dimensions of movie\npreference include possible characterizations like serious versus escapist, comedy versus\ndrama, orientation towards children, or gender orientation. Even if these are not represented explicitly in the data, they may be important for judging whether a particular\nuser will like the movie. The latent dimensions also could include possibly ill-defined\nthings like depth of character development or quirkiness, as well as dimensions never\nexplicitly articulated, since the latent dimensions will emerge from the data.\n\nAgain, we can understand this advanced data science approach as a combination of\nfundamental concepts. The idea of the latent dimension approaches to recommendation\nis to represent each movie as a feature vector using the latent dimensions, and also to\nrepresent each user’s preferences as a feature vector using the latent dimensions. Then\nit is easy to find movies to recommend to any user: compute a similarity score between\nthe user and all the movies; the movies that best match the users’ preferences would be\n\n6. There are some technicalities to the rules of the Netflix Challenge, which you can find on the Wikipedia page.\n\n7. The winning team, Bellkor’s Pragmatic Chaos, had seven members. The history of the contest and the team\n\nevolution is complicated and fascinating. See this Wikipedia page on the Netflix Prize.\n\nData Reduction, Latent Information, and Movie Recommendation\n\n|\n\n303", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00328", "page_num": 328, "segment": "those movies most similar to the user, when both are represented by the same latent\ndimensions.\n\nFigure 12-5. A collection of movies placed in a “taste space” defined by the two strongest\nlatent dimensions mined from the Netflix Challenge data. See the text for a detailed\ndiscussion. A customer would also be placed somewhere in the space, based on the movies she has previously viewed or rated. A similarity-based recommendation approach\nwould suggest the closest movies to the customer as candidate recommendations.\n\nFigure 12-5 shows a two-dimensional latent space actually mined from the Netflix movie\ndata,8 as well as a collection of movies represented in this new space. The interpretation\nof such latent dimensions mined from data must be inferred by the data scientists or\nbusiness users. The most common way is to observe how the dimensions separate the\nmovies, then apply domain knowledge.\n\nIn Figure 12-5, the latent dimension represented by the horizontal axis seems to separate\nthe movies into drama-oriented films on the right and action-oriented films on the left.\n\n8. Thanks to one of the members of the winning team, Chris Volinsky, for his help here.\n\n304\n\n|\n\nChapter 12: Other Data Science Tasks and Techniques", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00329", "page_num": 329, "segment": "At the extremes, on the far right we see films of the heart such as The Sound of Music,\nMoonstruck, and When Harry Met Sally. On the far left we see whatever is the opposite\nof films of the heart (films of the gut?), including films focusing on the stereotypical\nlikes of men and adolescent boys (The Man Show, Porky’s), killing (Texas Chainsaw\nMassacre, Reservoir Dogs), speed (The Fast and the Furious), and monster hunting (Van\nHelsing). The latent dimension represented by the vertical axis seems to separate the\nmovies by intellectual appeal versus emotional appeal, with movies like Being John\nMalkovich, Fear and Loathing in Los Vegas, and Annie Hall at one extreme, and Maid\nin Manhattan, The Fast and the Furious, and You’ve Got Mail at the other. Feel free to\ndisagree with those interpretations of the dimensions---they are completely subjective.\nOne thing is clear, though: The Wizard of Oz captures an unusual balance of whatever\ntastes are represented by the latent dimensions.\n\nTo use this latent space for recommendation, a customer also would be placed somewhere in the space, based on the movies she has rented or rated. The closest movies to\nthe position of the customer would be good candidates for making recommendations.\nNote that for making recommendations, as always we need to keep thinking back to our\nbusiness understanding. For example, different movies have different profit margins,\nso we may want to combine this knowledge with the knowledge of the most similar\nmovies.\n\nBut how do we find the right latent dimensions in the data? We apply the fundamental\nconcept introduced in Chapter 4: we represent the similarity calculation between a user\nand a movie as a mathematical formula using some number d of as-yet-unknown latent\ndimensions. Each dimension would be represented by a set of weights (the coefficients)\non each movie and a set of weights on each customer. A high weight would mean this\ndimension is strongly associated with the movie or the customer. The meaning of the\ndimension would be purely implicit in the weights on the movies and customers. For\nexample, we might look at the movies that are weighted highly on some dimension\nversus low-weighted movies and decide, “the highly rated movies are all ‘quirky.”’ In this\ncase, we could think of the dimension as the degree of quirkiness of the movie, although\nit is important to keep in mind that this interpretation of the dimension was imposed\nby us. The dimension is simply some way in which the movies clustered in the data on\nhow customers rated movies.\n\nRecall that to fit a numeric-function model to data, we find the optimal set of parameters\nfor the numeric function. Initially, the d dimensions are purely a mathematical abstraction; only after the parameters are selected to fit the data can we try to formulate an\ninterpretation of the meaning of the latent dimensions (and sometimes such an effort\nis fruitless). Here, the parameters of the function would be the (unknown) weights for\neach customer and each movie along these dimensions. Intuitively, data mining is determining simultaneously (i) how quirky the movie is, and (ii) how much this viewer\nlikes quirky movies.\n\nData Reduction, Latent Information, and Movie Recommendation\n\n|\n\n305", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00330", "page_num": 330, "segment": "We also need an objective function to determine what is a good fit. We define our\nobjective function for training based on the set of movie ratings we have observed. We\nfind a set of weights that characterizes the users and the movies along these dimensions.\nThere are different objective functions used for the movie-recommendation problem.\nFor example, we can choose the weights that allow us to best predict the observed ratings\nin the training data (subject to regularization, as discussed in Chapter 4). Alternatively,\nwe could choose the dimensions that best explain the variation in the observed ratings.\nThis is often called “matrix factorization,” and the interested reader might start with the\npaper about the Netflix Challenge (Koren, Bell, & Volinsky, 2009).\n\nThe result is that we have for each movie a representation along some reduced set of\ndimensions---maybe how quirky it is, maybe whether it is a “tear-jerker” or a “guy flick,”\nor whatever---the best d latent dimensions that the training finds. We also have a representation of each user in terms of their preferences along these dimensions. We can\nnow look back at Figure 12-5 and the associated discussion. These are the two latent\ndimensions that best fit the data, i.e., the dimensions that result from fitting the data\nwith d=2.\n\nBias, Variance, and Ensemble Methods\nIn the Netflix competition, the winners also took advantage of another common data\nscience technique: they built lots of different recommendation models and combined\nthem into one super model. In data mining parlance, this is referred to as creating an\nensemble model. Ensembles have been observed to improve generalization performance\nin many situations---not just for recommendations, but broadly across classification,\nregression, class probability estimation, and more.\n\nWhy is a collection of models often better than a single model? If we consider each\nmodel as a sort of “expert” on a target prediction task, we can think of an ensemble as\na collection of experts. Instead of just asking one expert, we find a group of experts and\nthen somehow combine their predictions. For example, we could have them vote on a\nclassification, or we could average their numeric predictions. Notice that this is a generalization of the method introduced in Chapter 6 to turn similarity computations into “nearest neighbor” predictive models. To make a k-NN prediction, we find a group\nof similar examples (very simple experts) and then apply some function to combine\ntheir individual predictions. Thus a k-nearest-neighbor model is a simple ensemble\nmethod. Generally, ensemble methods use more complex predictive models as their\n“experts”; for example, they may build a group of classification trees and then report an\naverage (or weighted average) of the predictions.\n\nWhen might we expect ensembles to improve our performance? Certainly, if each expert\nknew exactly the same things, they would all give the same predictions, and the ensemble\nwould provide no advantage. On the other hand, if each expert was knowledgeable in\na slightly different aspect of the problem, they might give complementary predictions,\n\n306\n\n|\n\nChapter 12: Other Data Science Tasks and Techniques", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00331", "page_num": 331, "segment": "and the whole group might provide more information than any individual expert.\nTechnically, we would like the experts to make different sorts of errors---we would like\ntheir errors to be as unrelated as possible, and ideally to be completely independent. In\naveraging the predictions, the errors would then tend to cancel out, the predictions\nindeed would be complementary, and the ensemble would be superior to using any one\nexpert.\n\nEnsemble methods have a long history and are an active area of research in data science. Much has been written about them. The interested reader may want to start with the review article by Dietterich\n(2000).\n\nOne way to understand why ensembles work is to understand that the errors a model\nmakes can be characterized by three factors:\n\n1. Inherent randomness,\n\n2. Bias, and\n\n3. Variance.\n\nThe first, inherent randomness, simply covers cases where a prediction is not “deterministic,” (i.e., we simply do not always get the same value for the target variable every\ntime we see the same set of features). For example, customers described by a certain set\nof characteristics may not always either purchase our product or not. The prediction\nmay simply be inherently probabilistic given the information we have. Thus, a portion\nof the observed “error” in prediction is simply due to this inherent probabilistic nature\nof the problem. We can debate whether a particular data-generating process is truly\nprobabilistic---as opposed to our simply not seeing all the requisite information---but\nthat debate is largely academic,9 because the process may be essentially probabilistic\nbased on the data we have available. Let’s proceed assuming that we’ve reduced the\nrandomness as much as we can, and there simply is some theoretical maximum accuracy\nthat we can achieve for this problem. This accuracy is called the Bayes rate, and it is\ngenerally unknown. For the rest of this section we will consider the Bayes rate as being\n“perfect” accuracy.\n\nBeyond inherent randomness, models make errors for two other reasons. The modeling\nprocedure may be “biased.” What this means can be understood best in reference to\nlearning curves (recall “Learning Curves” on page 130). Specifically, a modeling procedure is biased if no matter how much training data we give it, the learning curve will\n\n9. The debate sometimes can bear fruit. For example, thinking whether we have all the requisite information\n\nmight reveal a new attribute that could be obtained that would increase the possible predictability.\n\nBias, Variance, and Ensemble Methods\n\n|\n\n307", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00332", "page_num": 332, "segment": "never reach perfect accuracy (the Bayes rate). For example, we learn a (linear) logistic\nregression to predict response to an advertising campaign. If the true response really is\nmore complex than the linear model can represent, the model will never achieve perfect\naccuracy.\n\nThe other source of error is due to the fact that we do not have an infinite amount of\ntraining data; we have some finite sample that we want to mine. Modeling procedures\nusually give different models from even slightly different samples. These different models will tend to have different accuracies. How much the accuracy tends to vary across\ndifferent training sets (let’s say, of the same size) is referred to as the modeling procedure’s variance. Procedures with more variance tend to produce models with larger\nerrors, all else being equal.\n\nYou might see now that we would like to have a modeling procedure that has no bias\nand no variance, or at least low bias and low variance. Unfortunately (and intuitively),\nthere typically is a trade-off between the two. Lower variance models tend to have higher\nbias, and vice versa. As a very simple example, we might decide that we want to estimate\nthe response to our advertising campaign simply by ignoring all the customer features\nand simply predict the (average) purchase rate. This will be a very low-variance model,\nbecause we will tend to get about the same average from different datasets of the same\nsize. However, we have no hope to get perfect accuracy if there are customer-specific\ndifferences in propensity to purchase. On the other hand, we might decide to model\ncustomers based on one thousand detailed variables. We may now have the opportunity\nto get much better accuracy, but we would expect there to be much greater variance in\nthe models we obtain based on even slightly different training sets. Thus, we won’t\nnecessarily expect the thousand-variable model to be better; we don’t know exactly\nwhich source of error (bias or variance) will dominate.\n\nYou may be thinking: Of course. As we learned in Chapter 5, the thousand-variable model\nwill overfit. We should apply some sort of complexity control, such as selecting a subset of\nthe variables to use. That is exactly right. More complexity generally gives us lower bias\nbut higher variance. Complexity control generally tries to manage the (usually unknown) trade-off between bias and variance, to find a “sweet spot” where the combination of the errors from each is smallest. So, we could apply variable selection to our\nthousand-variable problem. If there truly are customer-specific differences in purchase\nrate, and we have enough training data, hopefully the variable selection will not throw\naway all variables, which would leave us with just the average over the population.\nHopefully, instead we would get a model with a subset of the variables that allow us to\npredict as well as possible, given the training data available.\n\n308\n\n|\n\nChapter 12: Other Data Science Tasks and Techniques", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00333", "page_num": 333, "segment": "Technically, the accuracies we discuss in this section are the expected\nvalues of the accuracies of the models. We omit that qualification because the discussion otherwise becomes technically baroque. The\nreader interested in understanding bias, variance, and the trade-off\nbetween them might start with the technical but quite readable article by Friedman (1997).\n\nNow we can see why ensemble techniques might work. If we have a modeling method\nwith high variance, averaging over multiple predictions reduces the variance in the\npredictions. Indeed, ensemble methods tend to improve the predictive ability more for\nhigher-variance methods, such as in cases where you would expect more overfitting\n(Perlich, Provost, & Simonoff, 2003). Ensemble methods are often used with tree induction, as classification and regression trees tend to have high variance. In the field\nyou may hear about random forests, bagging, and boosting. These are all ensemble\nmethods popular with trees (the latter two are more general). Check out Wikipedia to\nfind out more about them.\n\nData-Driven Causal Explanation and a Viral Marketing\nExample\nOne important topic that we have only touched on in this book (in Chapter 2 and\nChapter 11) is causal explanation from data. Predictive modeling is extremely useful\nfor many business problems. However, the sort of predictive modeling that we have\ndiscussed so far is based on correlations rather than on knowledge of causation. We\noften want to look more deeply into a phenomenon and ask what influences what. We\nmay want to do this simply to understand our business better, or we may want to use\ndata to improve decisions about how to intervene to cause a desired outcome.\n\nConsider a detailed example. Recently there has been much attention paid to “viral”\nmarketing. One common interpretation of viral marketing is that consumers can be\nhelped to influence each other to purchase a product, and so a marketer can get significant benefit by “seeding” certain consumers (e.g., by giving them the product for free),\nand they then will be “influencers”--- they will cause an increase in the likelihood that\nthe people they know will purchase the product. The holy grail of viral marketing is to\nbe able to create campaigns that spread like an epidemic, but the critical assumption\nbehind “virality” is that consumers actually influence each other. How much do they?\nData scientists work to measure such influence, by observing in the data whether once\na consumer has the product, her social network neighbors indeed have increased likelihood to purchase the product.\n\nUnfortunately, a naive analysis of the data can be tremendously misleading. For important sociological reasons (McPherson, Smith-Lovin, & Cook, 2001), people tend to\n\nData-Driven Causal Explanation and a Viral Marketing Example\n\n|\n\n309", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00334", "page_num": 334, "segment": "cluster in social networks with people who are similar to them. Why is this important?\nThis means that social network neighbors are likely to have similar product preferences,\nand we would expect the neighbors of people who choose or like a product also to choose\nor like the product, even in the absence of any causal influence among the consumers!\nIndeed, based on the careful application of causal analysis, it was shown in the Proceedings of the National Academy of Sciences (Aral, Muchnik, & Sundararajan, 2009)\nthat traditional methods for estimating the influence in viral marketing analysis overestimated the influence by at least 700%!\n\nThere are various methods for careful causal explanation from data, and they can all be\nunderstood within a common data science framework. The point of discussing this here\ntoward the end of the book is that understanding these sophisticated techniques requires\na grasp of the fundamental principles presented so far. Careful causal data analysis requires the understanding of investments in acquiring data, of similarity measurements,\nof expected value calculations, of correlation and finding informative variables, of fitting\nequations to data, and more.\n\nChapter 11 gave a taste of this more sophisticated causal analysis when we returned to\nthe telecommunications churn problem and asked: shouldn’t we be targeting those customers whom the special offer is most likely to influence? This illustrated the key role\nthe expected value framework played, along with several other concepts. There are other\ntechniques for causal understanding that use similarity matching (Chapter 6) to simulate the “counterfactual” that someone might both receive a “treatment” (e.g., an incentive to stay) and not receive the treatment. Still other causal analysis methods fit numeric\nfunctions to data and interpret the coefficients of the functions.10\n\nThe point is that we cannot understand causal data science without first understanding\nthe fundamental principles. Causal data analysis is just one such example; the same\napplies to other more sophisticated methods you may encounter.\n\nSummary\nThere are very many specific techniques used in data science. To achieve a solid understanding of the field, it is important to step back from the specifics and think about\nthe sorts of tasks to which the techniques are applied. In this book, we have focused on\na collection of the most common tasks (finding correlations and informative attributes,\nfinding similar data items, classification, probability estimation, regression, clustering),\nshowing that the concepts of data science provide a firm foundation for understanding\n\n10. It is beyond the scope of this book to explain the conditions under which this can be given a causal interpretation. But if someone presents a regression equation to you with a causal interpretation of the equation’s\nparameters, ask questions about exactly what the coefficients mean and why one can interpret them causally\nuntil you are satisfied. For such analyses, comprehension by decision makers is paramount; insist that you\nunderstand any such results.\n\n310\n\n|\n\nChapter 12: Other Data Science Tasks and Techniques", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00335", "page_num": 335, "segment": "both the tasks and the methods for solving the tasks. In this chapter, we presented several\nother important data science tasks and techniques, and illustrated that they too can be\nunderstood based on the foundation provided by our fundamental concepts.\n\nSpecifically, we discussed: finding interesting co-occurrences or associations among\nitems, such as purchases; profiling typical behavior, such as credit card usage or customer wait time; predicting links between data items, such as potential social connections between people; reducing our data to make it more manageable or to reveal hidden\ninformation, such as latent movie preferences; combining models as if they were experts\nwith different expertise, for example to improve movie recommendations; and drawing\ncausal conclusions from data, such as whether and to what extent the fact that socially\nconnected people buy the same products is actually because they influence each other\n(necessary for viral campaigns), or simply because socially connected people have very\nsimilar tastes (which is well known in sociology). A solid understanding of the basic\nprinciples helps you to understand more complex techniques as instances or combinations of them.\n\nSummary\n\n|\n\n311", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00336", "page_num": 336, "segment": "", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "none", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00337", "page_num": 337, "segment": "CHAPTER 13\nData Science and Business Strategy\n\nFundamental concepts: Our principles as the basis of success for a data-driven business;\nAcquiring and sustaining competitive advantage via data science; The importance of careful curation of data science capability.\n\nIn this chapter we discuss the interaction between data science and business strategy,\nincluding a high-level perspective on choosing problems to be solved with data science.\nWe see that the fundamental concepts of data science allow us to think clearly about\nstrategic issues. We also show how, taken as a whole, the array of concepts is useful for\nthinking about tactical business decisions such as evaluating proposals for data science\nprojects from consultants or internal data science teams. We also discuss in detail the\ncuration of data science capability.\n\nIncreasingly we see stories in the press about how yet another aspect of business has\nbeen addressed with a data science-based solution. As we discussed in Chapter 1, a\nconfluence of factors has led contemporary businesses to be strikingly data rich, as\ncompared to their predecessors. But the availability of data alone does not ensure successful data-driven decision-making. How does a business ensure that it gets the most\nfrom the wealth of data? The answer of course is manifold, but two important factors\nare: (i) the firm’s management must think data-analytically, and (ii) the management\nmust create a culture where data science, and data scientists, will thrive.\n\nThinking Data-Analytically, Redux\nCriterion (i) does not mean that the managers have to be data scientists. However,\nmanagers have to understand the fundamental principles well enough to envision and/\nor appreciate data science opportunities, to supply the appropriate resources to the data\nscience teams, and to be willing to invest in data and experimentation. Furthermore,\nunless the firm has on its management team a seasoned, practical data scientist, often\nthe management must steer the data science team carefully to make sure that the team\nstays on track toward an eventually useful business solution. This is very difficult if the\n\n313", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00338", "page_num": 338, "segment": "managers don’t really understand the principles. Managers need to be able to ask probing questions of a data scientist, who often can get lost in technical details. We need to\naccept that each of us has strengths and weaknesses, and as data science projects span\nso much of a business, a diverse team is essential. Just as we can’t expect a manager\nnecessarily to have deep expertise in data science, we can’t expect a data scientist necessarily to have deep expertise in business solutions. However, an effective data science\nteam involves collaboration between the two, and each needs to have some understanding of the fundamentals of the other’s area of responsibility. Just as it would be a\nSisyphean task to manage a data science team where the team had no understanding of\nthe fundamental concepts of business, it likewise is extremely frustrating at best, and\noften a tremendous waste, for data scientists to struggle under a management that does\nnot understand basic principles of data science.\n\nFor example, it is not uncommon for data scientists to struggle under a management\nthat (sometimes vaguely) sees the potential benefit of predictive modeling, but does not\nhave enough appreciation for the process to invest in proper training data or in proper\nevaluation procedures. Such a company may “succeed” in engineering a model that is\npredictive enough to produce a viable product or service, but will be at a severe disadvantage to a competitor who invests in doing the data science well.\n\nA solid grounding in the fundamentals of data science has much more far-reaching\nstrategic implications. We know of no systematic scientific study, but broad experience\nhas shown that as executives, managers, and investors increase their exposure to data\nscience projects, they see more and more opportunities in turn. We see extreme cases\nin companies like Google and Amazon (there is a vast amount of data science underlying\nweb search, as well as Amazon’s product recommendations and other offerings). Both\nof these companies eventually built subsequent products offering “big data” and datascience related services to other firms. Many, possibly most, data-science oriented startups use Amazon’s cloud storage and processing services for some tasks. Google’s “Prediction API” is increasing in sophistication and utility (we don’t know how broadly used\nit is).\n\nThose are extreme cases, but the basic pattern is seen in almost every data-rich firm.\nOnce the data science capability has been developed for one application, other applications throughout the business become obvious. Louis Pasteur famously wrote, “Fortune favors the prepared mind.” Modern thinking on creativity focuses on the juxtaposition of a new way of thinking with a mind “saturated” with a particular problem.\nWorking through case studies (either in theory or in practice) of data science applications helps prime the mind to see opportunities and connections to new problems that\ncould benefit from data science.\n\nFor example, in the late 1980s and early 1990s, one of the largest phone companies had\napplied predictive modeling---using the techniques we’ve described in this book---to\nthe problem of reducing the cost of repairing problems in the telephone network and\n\n314\n\n|\n\nChapter 13: Data Science and Business Strategy", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00339", "page_num": 339, "segment": "to the design of speech recognition systems. With the increased understanding of the\nuse of data science for helping to solve business problems, the firm subsequently applied\nsimilar ideas to decisions about how to allocate a massive capital investment to best\nimprove its network, and how to reduce fraud in its burgeoning wireless business. The\nprogression continued. Data science projects for reducing fraud discovered that incorporating features based on social-network connections (via who-calls-whom data) into\nfraud prediction models improved the ability to discover fraud substantially. In the early\n2000s, telecommunications firms produced the first solutions using such social connections to improve marketing---and improve marketing it did, showing huge performance lifts over traditional targeted marketing based on socio-demographic, geographic, and prior purchase data. Next, in telecommunications, such social features were\nadded to models for churn prediction, with equally beneficial results. The ideas diffused\nto the online advertising industry, and there was a subsequent flurry of development of\nonline advertising based on the incorporation of data on online social connections (at\nFacebook and at other firms in the online advertising ecosystem).\n\nThis progression was driven both by experienced data scientists moving among business\nproblems as well as by data science savvy managers and entrepreneurs, who saw new\nopportunities for data science advances in the academic and business literature.\n\nAchieving Competitive Advantage with Data Science\nIncreasingly, firms are considering whether and how they can obtain competitive advantage from their data and/or from their data science capability. This is important\nstrategic thinking that should not be superficial, so let’s spend some time digging into\nit.\n\nData and data science capability are (complementary) strategic assets. Under what conditions can a firm achieve competitive advantage from such an asset? First of all, the\nasset has to be valuable to the firm. This seems obvious, but note that the value of an\nasset to a firm depends on the other strategic decisions that the firm has made. Outside\nof the context of data science, in the personal computer industry in the 1990s, Dell\nfamously got substantial competitive advantage early over industry leader Compaq\nfrom using web-based systems to allow customers to configure computers to their personal needs and liking. Compaq could not get the same value from web-based systems.\nOne main reason was that Dell and Compaq had implemented different strategies: Dell\nalready was a direct-to-customer computer retailer, selling via catalogs; web-based systems held tremendous value given this strategy. Compaq sold computers mainly via\nretail outlets; web-based systems were not nearly as valuable given this alternative strategy. When Compaq tried to replicate Dell’s web-based strategy, it faced a severe backlash\nfrom its retailers. The upshot is that the value of the new asset (web-based systems) was\ndependent on each company’s other strategic decisions.\n\nAchieving Competitive Advantage with Data Science\n\n|\n\n315", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00340", "page_num": 340, "segment": "The lesson is that we need to think carefully in the business understanding phase as to\nhow data and data science can provide value in the context of our business strategy, and\nalso whether it would do the same in the context of our competitors’ strategies. This can\nidentify both possible opportunities and possible threats. A direct data science analogy\nof the Dell-Compaq example is Amazon versus Borders. Even very early, Amazon’s data\non customers’ book purchases allowed personalized recommendations to be delivered\nto customers while they were shopping online. Even if Borders were able to exploit its\ndata on who bought what books, its brick-and-mortar retail strategy did not allow the\nsame seamless delivery of data science-based recommendations.\n\nSo, a prerequisite for competetive advantage is that the asset be valuable in the context\nof our strategy. We’ve already begun to talk about the second set of criteria: in order to\ngain competitive advantage, competitors either must not possess the asset, or must not\nbe able to obtain the same value from it. We should think both about the data asset(s)\nand the data science capability. Do we have a unique data asset? If not, do we have an\nasset the utilization of which is better aligned with our strategy than with the stragegy\nof our competitors? Or are we better able to take advantage of the data asset due to our\nbetter data science capability?\n\nThe flip side of asking about achieving competitive advantage with data and data science\nis asking whether we are at a competitive disadvantage. It may be that the answers to\nthe previous questions are affirmative for our competitors and not for us. In what follows\nwe will assume that we are looking to achieve competitive advantage, but the arguments\napply symmetrically if we are trying to achieve parity with a data-savvy competitor.\n\nSustaining Competitive Advantage with Data Science\nThe next question is: even if we can achieve competitive advantage, can we sustain it?\nIf our competitors can easily duplicate our assets and capabilities, our advantage may\nbe short-lived. This is an especially critical question if our competitors have greater\nresources than we do: by adopting our strategy, they may surpass us if they have greater\nresources.\n\nOne strategy for competing based on data science is to plan to always keep one step\nahead of the competition: always be investing in new data assets, and always be developing new techniques and capabilities. Such a strategy can provide for an exciting and\npossibly fast-growing business, but generally few companies are able to execute it. For\nexample, you must have confidence that you have one of the best data science teams,\nsince the effectiveness of data scientists has a huge variance, with the best being much\nmore talented than the average. If you have a great team, you may be willing to bet that\nyou can keep ahead of the competition. We will discuss data science teams more below.\n\nThe alternative to always keeping one step ahead of the competition is to achieve sustainable competitive advantage due to a competitor’s inability to replicate, or their ele316\n\n|\n\nChapter 13: Data Science and Business Strategy", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00341", "page_num": 341, "segment": "vated expense of replicating, the data asset or the data science capability. There are\nseveral avenues to such sustainability.\n\nFormidable Historical Advantage\nHistorical circumstances may have placed our firm in an advantageous position, and it\nmay be too costly for competitors to reach the same position. Amazon again provides\nan outstanding example. In the “Dotcom Boom” of the 1990s, Amazon was able to sell\nbooks below cost, and investors continued to reward the company. This allowed Amazon to amass tremendous data assets (such as massive data on online consumers’ buying\npreferences and online product reviews), which then allowed them to create valuable\ndata-based products (such as recommendations and product ratings). These historical\ncircumstances are gone: it is unlikely today that investors would provide the same level\nof support to a competitor that was trying to replicate Amazon’s data asset by selling\nbooks below cost for years on end (not to mention that Amazon has moved far beyond\nbooks).\n\nThis example also illustrates that the data products themselves can increase the cost to\ncompetitors of replicating the data asset. Consumers value the data-driven recommendations and product reviews/ratings that Amazon provides. This creates switching costs:\ncompetitors would have to provide extra value to Amazon’s customers to entice them\nto shop elsewhere---either with lower prices or with some other valuable product or\nservice that Amazon does not provide. Thus, when the data acquisition is tied directly\nto the value provided by the data, the resulting virtuous cycle creates a catch-22 for\ncompetitors: competitors need customers in order to acquire the necessary data, but\nthey need the data in order to provide equivalent service to attract the customers.\n\nEntrepreneurs and investors might turn this strategic consideration around: what historical circumstances now exist that may not continue indefinitely, and which may allow\nme to gain access to or to build a data asset more cheaply than will be possible in the\nfuture? Or which will allow me to build a data science team that would be more costly\n(or impossible) to build in the future?\n\nUnique Intellectual Property\nOur firm may have unique intellectual property. Data science intellectual property can\ninclude novel techniques for mining the data or for using the results. These might be\npatented, or they might just be trade secrets. In the former case, a competitor either will\nbe unable to (legally) duplicate the solution, or will have an increased expense of doing\nso, either by licensing our technology or by developing new technology to avoid infringing on the patent. In the case of a trade secret, it may be that the competitor simply\ndoes not know how we have implemented our solution. With data science solutions,\nthe actual mechanism is often hidden; with only the result being visible.\n\nSustaining Competitive Advantage with Data Science\n\n|\n\n317", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00342", "page_num": 342, "segment": "Unique Intangible Collateral Assets\nOur competitors may not be able to figure out how to put our solution into practice.\nWith successful data science solutions, the actual source of good performance (for example with effective predictive modeling) may be unclear. The effectiveness of a predictive modeling solution may depend critically on the problem engineering, the attributes created, the combining of different models, and so on. It often is not clear to a\ncompetitor how performance is achieved in practice. Even if our algorithms are published in detail, many implementation details may be critical to get a solution that works\nin the lab to work in production.\n\nFurthermore, success may be based on intangible assets such as a company culture that\nis particularly suitable to the deployment of data science solutions. For example, a culture that embraces business experimentation and the (rigorous) supporting of claims\nwith data will naturally be an easier place for data science solutions to succeed. Alternatively, if developers are encouraged to understand data science, they are less likely to\nscrew up an otherwise top-quality solution. Recall our maxim: Your model is not what\nyour data scientists design, it’s what your engineers implement.\n\nSuperior Data Scientists\nMaybe our data scientists simply are much better than our competitors’. There is a huge\nvariance in the quality and ability of data scientists. Even among well-trained data scientists, it is well accepted within the data science community that certain individuals\nhave the combination of innate creativity, analytical acumen, business sense, and perseverence that enables them to create remarkably better solutions than their peers.\n\nThis extreme difference in ability is illustrated by the year-after-year results in the KDD\nCup data mining competition. Every year, the top professional society for data scientists,\nthe ACM SIGKDD, holds its annual conference (the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining). Each year the conference holds a\ndata mining competition. Some data scientists love to compete, and there are many\ncompetitions. The Netflix competition, discussed in Chapter 12, is one of the most\nfamous, and such competitions have even been turned into a crowd-sourcing business\n(see Kaggle). The KDD Cup is the granddaddy of data mining competitions and has\nbeen held every year since 1997. Why is this relevant? Some of the best data scientists\nin the world participate in these competitions. Depending on the year and the task,\nhundreds or thousands of competitors try their hand at solving the problem. If data\nscience talent were evenly distributed, then one would think it unlikely to see the same\nindividuals repeatedly winning the competitions. But that’s exactly what we see. There\nare individuals who have been on winning teams repeatedly, sometimes multiple years\nin a row and for multiple tasks each year (sometimes the competition has more than\n\n318\n\n|\n\nChapter 13: Data Science and Business Strategy", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00343", "page_num": 343, "segment": "one task).1 The point is that there is substantial variation in the ability even of the best\ndata scientists, and this is illustrated by the “objective” results of the KDD Cup competitions. The upshot is that because of the large variation in ability, the best data scientists can pick and choose the employment opportunities that suit their desires with\nrespect to salary, culture, advancement opportunities, and so on.\n\nThe variation in the quality of data scientists is amplified by the simple fact that topnotch data scientists are in high demand. Anyone can call himself a data scientist, and\nfew companies can really evaluate data scientists well as potential hires. This leads to\nanother catch: you need at least one top-notch data scientist to truly evaluate the quality\nof prospective hires. Thus, if our company has managed to build a strong data science\ncapability, we have a substantial and sustained advantage over competitors who are\nhaving trouble hiring data scientists. Further, top-notch data scientists like to work with\nother top-notch data scientists, which compounds our advantage.\n\nWe also must embrace the fact that data science is in part a craft. Analytical expertise\ntakes time to acquire, and all the great books and video lectures alone will not turn\nsomeone into a master. The craft is learned by experience. The most effective learning\npath resembles that in the classic trades: aspiring data scientists work as apprentices to\nmasters. This could be in a graduate program with a top applications-oriented professor,\nin a postdoctoral program, or in industry working with one of the best industrial data\nscientists. At some point the apprentice is skilled enough to become a “journeyman,”\nand will then work more independently on a team or even lead projects of her own.\nMany high-quality data scientists happily work in this capacity for their careers. Some\nsmall subset become masters themselves, because of a combination of their talent at\nrecognizing the potential of new data science opportunities (more on that in a moment)\nand their mastery of theory and technique. Some of these then take on apprentices.\nUnderstanding this learning path can help to focus on hiring efforts, looking for data\nscientists who have apprenticed with top-notch masters. It also can be used tactically in\na less obvious way: if you can hire one master data scientist, top-notch aspiring data\nscientists may come to apprentice with her.\n\nIn addition to all this, a top-notch data scientist needs to have a strong professional\nnetwork. We don’t mean a network in the sense of what one might find in an online\nprofessional networking system; an effective data scientist needs to have deep connections to other data scientists throughout the data science community. The reason is\nsimply that the field of data science is immense and there are far too many diverse topics\nfor any individual to master. A top-notch data scientist is a master of some area of\ntechnical expertise, and is familiar with many others. (Beware of the “jack-of-all-trades,\nmaster of none.”) However, we do not want the data scientist’s mastery of some area of\n\n1. This is not to say that one should look at the KDD Cup winners as necessarily the best data miners in the\nworld. Many top-notch data scientists have never competed in such a competition; some compete once and\nthen focus their efforts on other things.\n\nSustaining Competitive Advantage with Data Science\n\n|\n\n319", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00344", "page_num": 344, "segment": "technical expertise to turn into the proverbial hammer for which all problems are nails.\nA top-notch data scientist will pull in the necessary expertise for the problem at hand.\nThis is facilitated tremendously by strong and deep professional contacts. Data scientists\ncall on each other to help in steering them to the right solutions. The better a professional\nnetwork is, the better will be the solution. And, the best data scientists have the best\nconnections.\n\nSuperior Data Science Management\nPossibly even more critical to success for data science in business is having good management of the data science team. Good data science managers are especially hard to\nfind. They need to understand the fundamentals of data science well, possibly even being\ncompetent data scientists themselves. Good data science managers also must possess a\nset of other abilities that are rare in a single individual:\n\n• They need to truly understand and appreciate the needs of the business. What’s\nmore, they should be able to anticipate the needs of the business, so that they can\ninteract with their counterparts in other functional areas to develop ideas for new\ndata science products and services.\n\n• They need to be able to communicate well with and be respected by both “techies”\nand “suits”; often this means translating data science jargon (which we have tried\nto minimize in this book) into business jargon, and vice versa.\n\n• They need to coordinate technically complex activities, such as the integration of\nmultiple models or procedures with business constraints and costs. They often need\nto understand the technical architectures of the business, such as the data systems\nor production software systems, in order to ensure that the solutions the team\nproduces are actually useful in practice.\n\n• They need to be able to anticipate outcomes of data science projects. As we have\ndiscussed, data science is more similar to R&D than to any other business activity.\nWhether a particular data science project will produce positive results is highly\nuncertain at the outset, and possibly even well into the project. Elsewhere we discuss\nhow it is important to produce proof-of-concept studies quickly, but neither positive nor negative outcomes of such studies are highly predictive of success or failure\nof the larger project. They just give guidance to investments in the next cycle of the\ndata mining process (recall Chapter 2). If we look to R&D management for clues\nabout data science management, we find that there is only one reliable predictor of\nthe success of a research project, and it is highly predictive: the prior success of the\ninvestigator. We see a similar situation with data science projects. There are individuals who seem to have an intuitive sense of which projects will pay off. We do\nnot know of a careful analysis of why this is the case, but experience shows that it\nis. As with data science competitions, where we see remarkable repeat performances\nby the same individuals, we also see individuals repeatedly envisioning new data\n\n320\n\n|\n\nChapter 13: Data Science and Business Strategy", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00345", "page_num": 345, "segment": "science opportunities and managing them to great success---and this is particularly\nimpressive as many data science managers never see even one project through to\ngreat success.\n\n• They need to do all this within the culture of a particular firm.\n\nFinally, our data science capability may be difficult or expensive for a competitor to\nduplicate because we can hire data scientists and data science managers better. This may\nbe due to our reputation and brand appeal with data scientists---a data scientist may\nprefer to work for a company known as being friendly to data science and data scientists.\nOr our firm may have a more subtle appeal. So let’s examine in a little more detail what\nit takes to attract top-notch data scientists.\n\nAttracting and Nurturing Data Scientists and Their Teams\nAt the beginning of the chapter, we noted that the two most important factors in ensuring\nthat our firm gets the most from its data assets are: (i) the firm’s management must think\ndata-analytically, and (ii) the firm’s management must create a culture where data science, and data scientists, will thrive. As we mentioned above, there can be a huge difference between the effectiveness of a great data scientist and an average data scientist,\nand between a great data science team and an individually great data scientist. But how\ncan one confidently engage top-notch data scientists? How can we create great teams?\n\nThis is a very difficult question to answer in practice. At the time of this writing, the\nsupply of top-notch data scientists is quite thin, resulting in a very competitive market\nfor them. The best companies at hiring data scientists are the IBMs, Microsofts, and\nGoogles of the world, who clearly demonstrate the value they place in data science via\ncompensation, perks, and/or intangibles, such as one particular factor not to be taken\nlightly: data scientists like to be around other top-notch data scientists. One might argue\nthat they need to be around other top-notch data scientists, not only to enjoy their dayto-day work, but also because the field is vast and the collective mind of a group of data\nscientists can bring to bear a much broader array of particular solution techniques.\n\nHowever, just because the market is difficult does not mean all is lost. Many data scientists want to have more individual influence than they would have at a corporate\nbehemoth. Many want more responsibility (and the concomitant experience) with the\nbroader process of producing a data science solution. Some have visions of becoming\nChief Scientist for a firm, and understand that the path to Chief Scientist may be better\npaved with projects in smaller and more varied firms. Some have visions of becoming\nentrepreneurs, and understand that being an early data scientist for a startup can give\nthem invaluable experience. And some simply will enjoy the thrill of taking part in a\nfast-growing venture: working in a company growing at 20% or 50% a year is much\ndifferent from working in a company growing at 5% or 10% a year (or not growing at\nall).\n\nAttracting and Nurturing Data Scientists and Their Teams\n\n|\n\n321", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00346", "page_num": 346, "segment": "In all these cases, the firms that have an advantage in hiring are those that create an\nenvironment for nurturing data science and data scientists. If you do not have a critical\nmass of data scientists, be creative. Encourage your data scientists to become part of\nlocal data science technical communities and global data science academic communities.\n\nA note on publishing\nScience is a social endeavor, and the best data scientists often want to\nstay engaged in the community by publishing their advances. Firms\nsometimes have trouble with this idea, feeling that they are “giving\naway the store” or tipping their hand to competitors by revealing what\nthey are doing. On the other hand, if they do not, they may not be able\nto hire or retain the very best. Publishing also has some advantages for\nthe firm, such as increased publicity, exposure, external validation of\nideas, and so on. There is no clear-cut answer, but the issue needs to\nbe considered carefully. Some firms file patents aggressively on their\ndata science ideas, after which academic publication is natural if the\nidea is truly novel and important.\n\nA firm’s data science presence can be bolstered by engaging academic data scientists.\nThere are several ways of doing this. For those academics interested in practical applications of their work, it may be possible to fund their research programs. Both of your\nauthors, when working in industry, funded academic programs and essentially extended\nthe data science team that was focusing on their problems and interacting. The best\narrangement (by our experience) is a combination of data, money, and an interesting\nbusiness problem; if the project ends up being a portion of the Ph.D. thesis of a student\nin a top-notch program, the benefit to the firm can far outweigh the cost. Funding a\nPh.D. student might cost a firm in the ballpark of $50K/year, which is a fraction of the\nfully loaded cost of a top data scientist. A key is to have enough understanding of data\nscience to select the right professor---one with the appropriate expertise for the problem\nat hand.\n\nAnother tactic that can be very cost-effective is to take on one or more top-notch data\nscientists as scientific advisors. If the relationship is structured such that the advisors\ntruly interact on the solutions to problems, firms that do not have the resources or the\nclout to hire the very best data scientists can substantially increase the quality of the\neventual solutions. Such advisors can be data scientists at partner firms, data scientists\nfrom firms who share investors or board members, or academics who have some consulting time.\n\nA different tack altogether is to hire a third party to conduct the data science. There are\nvarious third-party data science providers, ranging from massive firms specializing in\nbusiness analytics (such as IBM), to data-science-specific consulting firms (such as Elder\n\n322\n\n|\n\nChapter 13: Data Science and Business Strategy", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00347", "page_num": 347, "segment": "Research), to boutique data science firms who take on a very small number of clients\nto help them develop their data science capabilities (such as Data Scientists, LLC).2 You\ncan find a large list of data-science service companies, as well as a wide variety of other\ndata science resources, at KDnuggets. A caveat about engaging data science consulting\nfirms is that their interests are not always well aligned with their customers’ interests;\nthis is obvious to seasoned users of consultants, but not to everyone.\n\nSavvy managers employ all of these resources tactically. A chief scientist or empowered\nmanager often can assemble for a project a substantially more powerful and diverse\nteam than most companies can hire.\n\nExamine Data Science Case Studies\nBeyond building a solid data science team, how can a manager ensure that her firm is\nbest positioned to take advantage of opportunities for applying data science? Make sure\nthat there is an understanding of and appreciation for the fundamental principles of\ndata science. Empowered employees across the firm often see novel applications.\n\nAfter gaining command of the fundamental principles of data science, the best way to\nposition oneself for success is to work through many examples of the application of data\nscience to business problems. Read case studies that actually walk through the data\nmining process. Formulate your own case studies. Actually mining data is helpful, but\neven more important is working through the connection between the business problem\nand the possible data science solutions. The more, different problems you work through,\nthe better you will be at naturally seeing and capitalizing on opportunities for bringing\nto bear the information and knowledge “stored” in the data---often the same problem\nformulation from one problem can be applied by analogy to another, with only minor\nchanges.\n\nIt is important to keep in mind that the examples we have presented in this book were\nchosen or designed for illustration. In reality, the business and data science team should\nbe prepared for all manner of mess and contraints, and must be flexible in dealing with\nthem. Sometimes there is a wealth of data and data science techniques available to be\nbrought to bear. Other times the situation seems more like the critical scene from the\nmovie Apollo 13. In the movie, a malfunction and explosion in the command module\nleave the astronauts stranded a quarter of a million miles from Earth, with the CO2 levels\nrising too rapidly for them to survive the return trip. In a nutshell, because of the constraints placed by what the astronauts have on hand, the engineers have to figure out\nhow to use a large cubic filter in place of a narrower cylindrical filter (to literally put a\nsquare peg in a round hole). In the key scene, the head engineer dumps out onto a table\nall the “stuff ” that’s there in the command module, and tells his team: “OK, people ...\n\n2. Disclaimer: The authors have a relationship with Data Scientists, LLC.\n\nExamine Data Science Case Studies\n\n|\n\n323", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00348", "page_num": 348, "segment": "we got to find a way to make this fit into the hole for this, using nothing but that.” Real\ndata science problems often seem more like the Apollo 13 situation than a textbook\nsituation.\n\nFor example, Perlich et al. (2013) describe a study of just such a case. For targeting\nconsumers with online display advertisements, obtaining an adequate supply of the ideal\ntraining data would have been prohibitively expensive. However, data were available at\nmuch lower cost from various other distributions and for other target variables. Their\nvery effective solution cobbled together models built from these surrogate data, and\n“transferred” these models for use on the desired task. The use of these surrogate data\nallowed them to operate with a substantially reduced investment in data from the ideal\n(and expensive) training distribution.\n\nBe Ready to Accept Creative Ideas from Any Source\n\nOnce different role players understand fundamental principles of data science, creative\nideas for new solutions can come from any direction---such as from executives examining potential new lines of business, from directors dealing with profit and loss responsibility, from managers looking critically at a business process, and from line employees with detailed knowledge of exactly how a particular business process functions.\nData scientists should be encouraged to interact with employees throughout the business, and part of their performance evaluation should be based on how well they produce\nideas for improving the business with data science. Incidentally, doing so can pay off in\nunintended ways: the data processing skills possessed by data scientists often can be\napplied in ways that are not so sophisticated but nevertheless can help other employees\nwithout those skills. Often a manager may have no idea that particular data can even be\nobtained---data that might help the manager directly, without sophisticated data science.\n\nBe Ready to Evaluate Proposals for Data Science Projects\nIdeas for improving business decisions through data science can come from any direction. Managers, investors, and employees should be able to formulate such ideas clearly,\nand decision makers should be prepared to evaluate them. Essentially, we need to be\nable to formulate solid proposals and to evaluate proposals.\n\nThe data mining process, described in Chapter 2, provides a framework to direct this.\nEach stage in the process reveals questions that should be asked both in formulating\nproposals for projects and in evaluating them:\n\n• Is the business problem well specified? Does the data science solution solve the\n\nproblem?\n\n• Is it clear how we would evaluate a solution?\n\n324\n\n|\n\nChapter 13: Data Science and Business Strategy", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00349", "page_num": 349, "segment": "• Would we be able see evidence of success before making a huge investment in deployment?\n\n• Does the firm have the data assets it needs? For example, for supervised modeling,\nare there actually labeled training data? Is the firm ready to invest in the assets it\ndoes not have yet?\n\nAppendix A provides a starting list of questions for evaluating data science proposals,\norganized by the data mining process. Let’s walk through an illustrative example. (In\nAppendix B you will find another example proposal to evaluate, focusing on our running\nchurn problem.)\n\nExample Data Mining Proposal\nYour company has an installed user base of 900,000 current users of your Whiz-bang®\nwidget. You now have developed Whiz-bang® 2.0, which has substantially lower operating costs than the original. Ideally, you would like to convert (“migrate”) your entire\nuser base over to version 2.0; however, using 2.0 requires that users master the new\ninterface, and there is a serious risk that in attempting to do so, the customers will\nbecome frustrated and not convert, become less satistified with the company, or in the\nworst case, switch to your competitor’s popular Boppo® widget. Marketing has designed\na brand-new migration incentive plan, which will cost $250 per selected customer. There\nis no guarantee that a customer will choose to migrate even if she takes this incentive.\n\nAn external firm, Big Red Consulting, is proposing a plan to target customers carefully\nfor Whiz-bang® 2.0, and given your demonstrated fluency with the fundamentals of data\nscience, you are called in to help assess Big Red’s proposal. Do Big Red’s choices seem\ncorrect?\n\nTargeted Whiz-bang Customer Migration---prepared by Big Red Consulting, Inc.\n\nWe will develop a predictive model using modern data-mining technology. As discussed\nin our last meeting, we assume a budget of $5,000,000 for this phase of customer migration; adjusting the plan for other budgets is straightforward. Thus we can target 20,000\ncustomers under this budget. Here is how we will select those customers:\n\nWe will use data to build a model of whether or not a customer will migrate given the\nincentive. The dataset will comprise a set of attributes of customers, such as the number\nand type of prior customer service interactions, level of usage of the widget, location of\nthe customer, estimated technical sophistication, tenure with the firm, and other loyalty\nindicators, such as number of other firm products and services in use. The target will be\nwhether or not the customer will migrate to the new widget if he/she is given the incentive.\nUsing these data, we will build a linear regression to estimate the target variable. The\nmodel will be evaluated based on its accuracy on these data; in particular, we want to\nensure that the accuracy is substantially greater than if we targeted randomly.\n\nTo use the model: for each customer we will apply the regression model to estimate the\ntarget variable. If the estimate is greater than 0.5, we will predict that the customer will\nmigrate; otherwise, we will say the customer will not migrate. We then will select at ranBe Ready to Evaluate Proposals for Data Science Projects\n\n|\n\n325", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00350", "page_num": 350, "segment": "dom 20,000 customers from those predicted to migrate, and these 20,000 will be the\nrecommended targets.\n\nFlaws in the Big Red Proposal\nWe can use our understanding of the fundamental principles and other basic concepts\nof data science to identify flaws in the proposal. Appendix A provides a starting guide\nfor reviewing such proposals, with some of the main questions to ask. However, this\nbook as a whole really can be seen as a proposal review guide. Here are some of the most\negregious flaws in Big Data’s proposal:\n\nBusiness Understanding\n\n• The target variable definition is imprecise. For example, over what time period must\n\nthe migration occur? (Chapter 3)\n\n• The formulation of the data mining problem could be better-aligned with the business problem. For example, what if certain customers (or everyone) were likely to\nmigrate anyway (without the incentive)? Then we would be wasting the cost of the\nincentive in targeting them. (Chapter 2, Chapter 11)\n\nData Understanding/Data Preparation\n\n• There aren’t any labeled training data! This is a brand-new incentive. We should\ninvest some of our budget in obtaining labels for some examples. This can be done\nby targeting a (randomly) selected subset of customers with the incentive. One also\nmight propose a more sophisticated approach (Chapter 2, Chapter 3, Chapter 11).\n\n• If we are worried about wasting the incentive on customers who are likely to migrate\nwithout it, we also should observe a “control group” over the period where we are\nobtaining training data. This should be easy, since everyone we don’t target to gather\nlabels would be a “control” subject. We can build a separate model for migrate or\nnot given no incentive, and combine the models in an expected value framework.\n(Chapter 11)\n\nModeling\n\n• Linear regression is not a good choice for modeling a categorical target variable.\nRather one should use a classification method, such as tree induction, logistic regression, k-NN, and so on. Even better, why not try a bunch of methods and evaluate\nthem experimentally to see which performs best? (Chapter 2, Chapter 3, Chapter 4, Chapter 5, Chapter 6, Chapter 7, Chapter 8)\n\n326\n\n|\n\nChapter 13: Data Science and Business Strategy", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00351", "page_num": 351, "segment": "Evaluation\n\n• The evaluation shouldn’t be on the training data. Some sort of holdout approach\nshould be used (e.g., cross-validation and/or a staged approach as discussed above).\n(Chapter 5)\n\n• Is there going to be any domain-knowledge validation of the model? What if it is\ncapturing some weirdness of the data collection process? (Chapter 7, Chapter 11,\nChapter 14)\n\nDeployment\n\n• The idea of randomly selecting customers with regression scores greater than 0.5\nis not well considered. First, it is not clear that a regression score of 0.5 really corresponds to a probability of migration of 0.5. Second, 0.5 is rather arbitrary in any\ncase. Third, since our model is providing a ranking (e.g., by likelihood of migration,\nor by expected value if we use the more complex formulation), we should use the\nranking to guide our targeting: choose the top-ranked candidates, as the budget\nwill allow. (Chapter 2, Chapter 3, Chapter 7, Chapter 8, Chapter 11)\n\nOf course, this is just one example with a particular set of flaws. A different set of concepts may need to be brought to bear for a different proposal that is flawed in other\nways.\n\nA Firm’s Data Science Maturity\nFor a firm to realistically plan data science endeavors it should assess, frankly and rationally, its own maturity in terms of data science capability. It is beyond the scope of\nthis book to provide a self-assessment guide, but a few words on the topic are important.\n\nFirms vary widely in their data science capabilities along many dimensions. One dimension that is very important for strategic planning is the firm’s “maturity,” specifically,\nhow systematic and well founded are the processes used to guide the firm’s data science\nprojects.3\n\nAt one end of the maturity spectrum, a firm’s data science processes are completely ad\nhoc. In many firms, the employees engaged in data science and business analytics endeavors have no formal training in these areas, and the managers involved have little\nunderstanding of the fundamental principles of data science and data analytic thinking.\n\n3. The reader interested in this notion of the maturity of a firm’s capabilities is encouraged to read about the\n\nCapability Maturity Model for software engineering, which is the inspiration for this discussion.\n\nA Firm’s Data Science Maturity\n\n|\n\n327", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00352", "page_num": 352, "segment": "A note on “immature” firms\nBeing “immature” does not mean that a firm is destined to failure. It\nmeans that success is highly variable and is much more dependent on\nluck than in a mature firm. Project success will depend upon the heroic efforts by individuals who happen to have a natural acuity for dataanalytic thinking. An immature firm may implement not-sosophisticated data science solutions at a large scale, or may implement sophisticated solutions at a small scale. Rarely, though, will an\nimmature firm implement sophisticated data science solutions at a\nlarge scale.\n\nA firm with a medium level of maturity employs well-trained data scientists, as well as\nbusiness managers and other stakeholders who understand the fundamental principles\nof data science. Both sides can think clearly about how to solve business problems with\ndata science, and both sides participate in the design and implementation of solutions\nthat directly address the problems of the business.\n\nAt the high end of maturity are firms who continually work to improve their data science\nprocesses (and not just the solutions). Executives at such firms continually challenge the\ndata science team to instill processes that will align their solutions better with the business problems. At the same time they realize that pragmatic trade-offs may favor the\nchoice of a suboptimal solution that can be realized today over a theoretically much\nbetter solution that won’t be ready until next year. Data scientists at such a firm should\nhave the confidence that when they propose an investments to improve data science\nprocesses, their suggestions will be met with open and informed minds. That’s not to\nsay that every such request will be approved, but that the proposal will be evaluated on\nits own merits in the context of the business.\n\nNote: Data science is neither operations nor engineering.\nThere is some danger in making an analogy to the Capability Maturity Model from software engineering---danger that the analogy will be\ntaken too literally. Trying to apply the same sort of processes that work\nfor software engineering, or worse for manufacturing or operations,\nwill fail for data science. Moreover, misguided attempts to do so will\nsend a firm’s best data scientists out the door before the management\neven knows what happened. The key is to understand data science\nprocesses and how to data science well, and work to establish consistency and support. Remember that data science is more like R&D than\nlike engineering or manufacturing. As a concrete example, management should consistently make available the resources needed for solid evaluation of data science projects early and often. Sometimes this\ninvolves investing in data that would not otherwise have been obtained. Often this involves assigning engineering resources to sup328\n\n|\n\nChapter 13: Data Science and Business Strategy", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00353", "page_num": 353, "segment": "port the data science team. The data science team should in return\nwork to provide management with evaluations that are as well aligned\nwith the actual business problem(s) as possible.\n\nAs a concrete example, consider yet again our telecom churn problem and how firms\nof varying maturity might address it:\n\n• An immature firm will have (hopefully) analytically adept employees implementing\nad hoc solutions based on their intuitions about how to manage churn. These may\nwork well or they may not. In an immature firm, it will be difficult for management\nto evaluate these choices against alternatives, or to determine when they’ve implemented a nearly optimal solution.\n\n• A firm of medium maturity will have implemented a well-defined framework for\ntesting different alternative solutions. They will test under conditions that mimic\nas closely as possible the actual business setting---for example, running the latest\nproduction data through a testbed platform that compares how different methods\n“would have done,” and considering carefully the costs and benefits involved.\n\n• A very mature organization may have deployed the exact same methods as the\nmedium-maturity firm for identifying the customers with the highest probability\nof leaving, or even the highest expected loss if they were to churn. They would also\nbe working to implement the processes, and gather the data, necessary to judge also\nthe effect of the incentives and thereby work towards finding those individuals for\nwhich the incentives will produce the largest expected increase in value (over not\ngiving the incentive). Such a firm may also be working to integrate such a procedure\ninto an experimentation and/or optimization framework for assessing different offers or different parameters (like the level of discount) to a given offer.\n\nA frank self-assessment of data science maturity is difficult, but it is essential to getting\nthe best out of one’s current capabilities, and to improving one’s capabilities.\n\nA Firm’s Data Science Maturity\n\n|\n\n329", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00354", "page_num": 354, "segment": "", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "none", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00355", "page_num": 355, "segment": "CHAPTER 14\nConclusion\n\nIf you can’t explain it simply, you don’t understand\nit well enough.\n\n---Albert Einstein\n\nThe practice of data science can best be described as a combination of analytical engineering and exploration. The business presents a problem we would like to solve. Rarely\nis the business problem directly one of our basic data mining tasks. We decompose the\nproblem into subtasks that we think we can solve, usually starting with existing tools.\nFor some of these tasks we may not know how well we can solve them, so we have to\nmine the data and conduct evaluation to see. If that does not succeed, we may need to\ntry something completely different. In the process we may discover knowledge that will\nhelp us to solve the problem we had set out to solve, or we may discover something\nunexpected that leads us to other important successes.\n\nNeither the analytical engineering nor the exploration should be omitted when considering the application of data science methods to solve a business problem. Omitting\nthe engineering aspect usually makes it much less likely that the results of mining data\nwill actually solve the business problem. Omitting the understanding of process as one\nof exploration and discovery often keeps an organization from putting the right management, incentives, and investments in place for the project to succeed.\n\nThe Fundamental Concepts of Data Science\nBoth the analytical engineering and the exploration and discovery are made more systematic and thereby more likely to succeed by the understanding and embracing of the\nfundamental concepts of data science. In this book we have introduced a collection of\nthe most important fundamental concepts. Some of these concepts we made into headliners for the chapters, and others were introduced more naturally through the discus331", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00356", "page_num": 356, "segment": "sions (and not necessarily labeled as fundamental concepts). These concepts span the\nprocess from envisioning how data science can improve business decisions, to applying\ndata science techniques, to deploying the results to improve decision-making. The concepts also undergird a large array of business analytics.\n\nWe can group our fundamental concepts roughly into three types:\n\n1. General concepts about how data science fits in the organization and the competitive landscape, including ways to attract, structure, and nurture data science teams,\nways for thinking about how data science leads to competitive advantage, ways that\ncompetitive advantage can be sustained, and tactical principles for doing well with\ndata science projects.\n\n2. General ways of thinking data-analytically, which help us to gather appropriate data\nand consider appropriate methods. The concepts include the data mining process,\nthe collection of different high-level data science tasks, as well as principles such as\nthe following.\n\n• Data should be considered an asset, and therefore we should think carefully about\n\nwhat investments we should make to get the best leverage from our asset\n\n• The expected value framework can help us to structure business problems so we\ncan see the component data mining problems as well as the connective tissue of\ncosts, benefits, and constraints imposed by the business environment\n\n• Generalization and overfitting: if we look too hard at the data, we will find patterns;\n\nwe want patterns that generalize to data we have not yet seen\n\n• Applying data science to a well-structured problem versus exploratory data mining\n\nrequire different levels of effort in different stages of the data mining process\n\n3. General concepts for actually extracting knowledge from data, which undergird the\nvast array of data science techniques. These include concepts such as the following.\n\n• Identifying informative attributes---those that correlate with or give us information about an unknown quantity of interest\n\n• Fitting a numeric function model to data by choosing an objective and finding a\n\nset of parameters based on that objective\n\n• Controlling complexity is necessary to find a good trade-off between generalization\n\nand overfitting\n\n• Calculating similarity between objects described by data\n\n332\n\n|\n\nChapter 14: Conclusion", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00357", "page_num": 357, "segment": "Once we think about data science in terms of its fundamental concepts, we see the same\nconcepts underlying many different data science strategies, tasks, algorithms, and processes. As we have illustrated throughout the book, these principles not only allow us\nto understand the theory and practice of data science much more deeply, they also allow\nus to understand the methods and techniques of data science very broadly, because these\nmethods and techniques are quite often simply particular instantiations of one or more\nof the fundamental principles.\n\nAt a high level we saw how structuring business problems using the expected value\nframework allows us to decompose problems into data science tasks that we understand\nbetter how to solve, and this applies across many different sorts of business problems.\n\nFor extracting knowledge from data, we saw that our fundamental concept of determining the similarity of two objects described by data is used directly, for example to\nfind customers similar to our best customers. It is used for classification and for regression, via nearest-neighbor methods. It is the basis for clustering, the unsupervised\ngrouping of data objects. It is the basis for finding documents most related to a search\nquery. And it is the basis for more than one common method for making recommendations, for example by casting both customers and movies into the same “taste space,”\nand then finding movies most similar to a particular customer.\n\nWhen it comes to measurement, we see the notion of lift---determining how much more\nlikely a pattern is than would be expected by chance---appearing broadly across data\nscience, when evaluating very different sorts of patterns. One evaluates algorithms for\ntargeting advertisements by computing the lift one gets for the targeted population. One\ncalculates lift for judging the weight of evidence for or against a conclusion. One calculates lift to help judge whether a repeated co-occurrence is interesting, as opposed to\nsimply being a natural consequence of popularity.\n\nUnderstanding the fundamental concepts also facilitates communication between business stakeholders and data scientists, not only because of the shared vocabulary, but\nbecause both sides actually understand better. Instead of missing important aspects of\na discussion completely, we can dig in and ask questions that will reveal critical aspects\nthat otherwise would not have been uncovered.\n\nFor example, let’s say your venture firm is considering investing in a data science-based\ncompany producing a personalized online news service. You ask how exactly they are\npersonalizing the news. They say they use support vector machines. Let’s even pretend\nthat we had not talked about support vector machines in this book. You should feel\nconfident enough in your knowledge of data science now that you should not simply\nsay “Oh, OK.” You should be able to confidently ask: “What’s that exactly?” If they really\ndo know what they are talking about, they should give you some explanation based upon\nour fundamental principles (as we did in Chapter 4). You also are now prepared to ask,\n“What exactly are the training data you intend to use?” Not only might that impress\ndata scientists on their team, but it actually is an important question to be asked to see\n\nThe Fundamental Concepts of Data Science\n\n|\n\n333", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00358", "page_num": 358, "segment": "whether they are doing something credible, or just using “data science” as a smokescreen\nto hide behind. You can go on to think about whether you really believe building any\npredictive model from these data---regardless of what sort of model it is---would be\nlikely to solve the business problem they’re attacking. You should be ready to ask whether\nyou really think they will have reliable training labels for such a task. And so on.\n\nApplying Our Fundamental Concepts to a New Problem: Mining\nMobile Device Data\nAs we’ve emphasized repeatedly, once we think about data science as a collection of\nconcepts, principles, and general methods, we will have much more success both understanding data science activities broadly, and also applying data science to new business problems. Let’s consider a fresh example.\n\nRecently (as of this writing), there has been a marked shift in consumer online activity\nfrom traditional computers to a wide variety of mobile devices. Companies, many still\nworking to understand how to reach consumers on their desktop computers, now are\nscrambling to understand how to reach consumers on their mobile devices: smart\nphones, tablets, and even increasingly mobile laptop computers, as WiFi access becomes\nubiquitous. We won’t talk about most of the complexity of that problem, but from our\nperspective, the data-analytic thinker might notice that mobile devices provide a new\nsort of data from which little leverage has yet been obtained. In particular, mobile devices\nare associated with data on their location.\n\nFor example, in the mobile advertising ecosystem, depending on my privacy settings,\nmy mobile device may broadcast my exact GPS location to those entities who would\nlike to target me with advertisements, daily deals, and other offers. Figure 14-1 shows\na scatterplot of a small sample of locations that a potential advertiser might see, sampled\nfrom the mobile advertising ecosystem. Even if I do not broadcast my GPS location, my\ndevice broadcasts the IP address of the network it currently is using, which often conveys\nlocation information.\n\n334\n\n|\n\nChapter 14: Conclusion", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00359", "page_num": 359, "segment": "Figure 14-1. A scatterplot of a sample of GPS locations captured from mobile devices.\n\nAs an interesting side point, this is just a scatterplot of the latitude and\nlongitudes broadcast by mobile devices; there is no map! It gives a\nstriking picture of population density across the world. And it makes\nus wonder what’s going on with mobile devices in Antarctica.\n\nHow might we use such data? Let’s apply our fundamental concepts. If we want to get\nbeyond exploratory data analysis (as we’ve started with the visualization in\nFigure 14-1), we need to think in terms of some concrete business problem. A particular\nfirm might have certain problems to solve, and be focused on one or two. An entrepreneur or investor might scan across different possible problems she sees that businesses\nor consumers currently have. Let’s pick one related to these data.\n\nAdvertisers face the problem that in this new world, we see a variety of different devices\nand a particular consumer’s behavior may be fragmented across several. In the desktop\nworld, once the advertisers identify a good prospect, perhaps via a cookie in a particular\nconsumer’s browser or a device ID, they can then begin to take action accordingly; for\nexample, by presenting targeted ads. In the mobile ecosystem, this consumer’s activity\n\nThe Fundamental Concepts of Data Science\n\n|\n\n335", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00360", "page_num": 360, "segment": "is fragmented across devices. Even if a good prospect is found on one device, how can\nshe be targeted on her other devices?\n\nOne possibility is to use the location data to winnow the space of possible other devices\nthat could belong to this prospect. Figure 14-1 suggests that a huge portion of the space\nof possible alternatives would be eliminated if we could profile the location visitation\nbehavior of a mobile device. Presumably, my location behavior on my smart phone will\nbe fairly similar to my location behavior on my laptop, especially if I am considering\nthe WiFi locations that I use.1 So I may want to draw on what I know about assessing\nthe similarity of data items (Chapter 6).\n\nWhen working through our data-understanding phase, we need to decide how exactly\nwe will represent devices and their locations. Once we are able to step back from the\ndetails of algorithms and applications, and think instead about the fundamentals, we\nmight notice that the ideas discussed in the example of problem formulation for text\nmining (Chapter 10) would apply very well here---even though this example has nothing\nto do with text. When mining data on documents, we often ignore much of the structure\nof the text, such as its sequence. For many problems we can simply treat each document\nas a collection of words from a potentially large vocabulary. The same thinking will\napply here. Obviously there is considerable structure to the locations one visits, such as\nthe sequence in which they are visited, but for data mining a simplest-first strategy is\noften best. Let’s just consider each device to be a “bag of locations,” in analogy to the\nbag-of-words representation discussed in Chapter 10.\n\nIf we are looking to try to find other instances of the same user, we might also profitably\napply the ideas of TFIDF for text to our locations. WiFi locations that are very popular\n(like the Starbucks on the corner of Washington Square Park) are unlikely to be so\ninformative in a similarity calculation focused on finding the same user on different\ndevices. Such a location would get a low IDF score (think of the “D” as being for “Device”\nrather than “Document”). At the other end of the spectrum, for many people their\napartment WiFi networks would have few different devices, and thereby be quite discriminative. TFIDF on location would magnify the importance of these locations in a\nsimilarity calculation. In between these two in discriminability might be an office WiFi\nnetwork, which might get a middle-of-the-road IDF score.\n\nNow, if our device profile is a TFIDF representation based on our bag of locations, as\nwith using similarity over the TFIDF formulation for our search query for the jazz\nmusician example in Chapter 10, we might look for the devices most similar to the one\nthat we had identified as a good prospect. Let’s say that my laptop was the device identified as a good prospect. My laptop is observed on my apartment WiFi network and on\nmy work WiFi network. The only other devices that are observed there are my phone,\nmy tablet, and possibly the mobile devices of my wife and a few friends and colleagues\n\n1. Which incidentally can be anonymized if I am concerned about invasions of privacy. More on that later.\n\n336\n\n|\n\nChapter 14: Conclusion", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00361", "page_num": 361, "segment": "(but note that these will get low TF scores at one or the other location, as compared to\nmy devices). Thus, it is likely that my phone and tablet will be strongly similar---possibly\nmost similar---to the one identified as a prospect. If the advertiser had identified my\nlaptop as a good prospect for a particular ad, then this formulation would also identify\nmy phone and tablet as good prospects for the same ad.\n\nThis example isn’t meant to be a definitive solution to the problem of finding corresponding users on different mobile devices;2 it shows how having a conceptual toolkit\ncan be helpful in thinking about a brand-new problem. Once these ideas are conceptualized, data scientists would dig in to figure out what really works and how to flesh\nout and extend the ideas, applying many of the concepts we have discussed (such as\nhow to evaluate alternative implementation options).\n\nChanging the Way We Think about Solutions to Business Problems\nThe example also provides a concrete illustration of yet another important fundamental\nconcept (we haven’t exhausted them even after this many pages of a detailed book). It\nis quite common that in the business understanding/data understanding subcycle of\nthe data mining process, our notion of what is the problem changes to fit what we actually\ncan do with the data. Often the change is subtle, but it is very important to (try to) notice\nwhen it happens. Why? Because all stakeholders are not involved with the data science\nproblem formulation. If we forget that we have changed the problem, especially if the\nchange is subtle, we may run into resistance down the line. And it may be resistance\ndue purely to misunderstanding! What’s worse, it may be perceived as due to stubbornness, which might lead to hard feelings that threaten the success of the project.\n\nLet’s look back at the mobile targeting example. The astute reader might have said: Wait\na minute. We started by saying that we were going to find the same users on different\ndevices. What we’ve done is to find very similar users in terms of their location information. I may be willing to agree that the set of these similar users is very likely to contain\nthe same user---more likely than any alternative I can think of---but that’s not the same\nas finding the same user on different devices. This reader would be correct. In working\nthrough our problem formulation the problem changed slightly. We now have made\nthe identification of the same user probabilistic: there may be a very high probability\nthat the subset of devices with very similar location profiles will contain other instances\nof the same user, but it is not guaranteed. This needs to be clear in our minds, and\nclarified to stakeholders.\n\nIt turns out that for targeting advertisements or offers, this change probably will be\nacceptable to all stakeholders. Recalling our cost/benefit framework for evaluating data\nmining solutions (Chapter 7), it’s pretty clear that for many offers targeting some false\n\n2. It is however the essence of a real-world solution to the problem implemented by one of the most advanced\n\nmobile advertising companies.\n\nThe Fundamental Concepts of Data Science\n\n|\n\n337", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00362", "page_num": 362, "segment": "positives will be of relatively low cost as compared to the benefit of hitting more true\npositives. What’s more, for many offers targeters may actually be happy to “miss,” if each\nmiss constitutes hitting other people with similar interests. And my wife and close\nfriends and colleagues are pretty good hits for many of my tastes and interests!3\n\nWhat Data Can’t Do: Humans in the Loop, Revisited\nThis book has focused on how, why, and when we can get business value from data\nscience by enhancing data-driven decision-making. It is important also to consider the\nlimits of data science and data-driven decision-making.\n\nThere are things computers are good at and things people are good at, but often these\naren’t the same things. For example, humans are much better at identifying---from everything out in the world---small sets of relevant aspects of the world from which to\ngather data in support of a particular task. Computers are much better at sifting through\na massive collection of data, including a huge number of (possibly) relevant variables,\nand quantifying the variables’ relevance to predicting a target.\n\nNew York Times Op-Ed columnist David Brooks has written an\nexcellent essay entitled “What Data Can’t Do” (Brooks, 2013). You\nshould read this if you are considering the magical application of\ndata science to solve your problems.\n\nData science involves the judicious integration of human knowledge and computerbased techniques to achieve what neither of them could achieve alone. (And beware of\nany tool vendor who suggests otherwise!) The data mining process introduced in\nChapter 2 helps direct the combination of humans and computers. The structure imposed by the process emphasizes the interaction of humans early, to ensure that the\napplication of data science methods are focused on the right tasks. Examining the data\nmining process also reveals that task selection and specification is not the only place\nwhere human interaction is critical. As discussed in Chapter 2, one of the places where\nhuman creativity, knowledge, and common sense adds value is in selecting the right\ndata to mine---which is far too often overlooked in discussions of data mining, especially\nconsidering its importance.\n\n3. In an article in the Proceedings of the National Academy of Sciences, Crandall et al. (2010) show that geographic\nco-ocurrences between individuals are very strongly predictive of the individuals being friends: “The knowledge that two people were proximate at just a few distinct locations at roughly the same times can indicate a\nhigh conditional probability that they are directly linked in the underlying social network.” This means that\neven “misses” due to location similarity may still contain some of the advantage of social network targeting\n---which has been shown to be extremely effective for marketing (Hill et al., 2006).\n\n338\n\n|\n\nChapter 14: Conclusion", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00363", "page_num": 363, "segment": "Human interaction is also critical in the evaluation stage of the process. The combination of the right data and data science techniques excels at finding models that optimize\nsome objective criterion. Only humans can tell what is the best objective criterion to\noptimize for a particular problem. This involves substantial subjective human judgment,\nbecause often the true criterion to be optimized cannot be measured, so the humans\nhave to pick the best proxy or proxies possible---and keep these decisions in mind as\nsources of risk when the models are deployed. And then we need careful, and sometimes\ncreative, attention to whether the resultant models or patterns actually do help solve the\nproblem.\n\nWe also need to keep in mind that the data to which we will apply data science techniques\nare the product of some process that involved human decisions. We should not fall prey\nto thinking that the data represent objective truth.4 Data incorporate the beliefs, purposes, biases, and pragmatics of those who designed the data collection systems. The\nmeaning of data is colored by our own beliefs.\n\nConsider the following simple example. Many years ago, your authors worked together\nas data scientists at one of the largest telephone companies. There was a terrible problem\nwith fraud in the wireless business, and we applied data science methods to massive data\non cell phone usage, social calling patterns, locations visited, etc. (Fawcett & Provost,\n1996, 1997). A seemingly well-performing component of a model for detecting fraud\nindicated that “calling from cellsite number zero provides substantially increased risk\nof fraud.” This was verified through careful holdout evaluation. Fortunately (in this\ninstance), we followed good data science practice and in the evaluation phase worked\nto ensure domain-knowledge validation of the model. We had trouble understanding\nthis particular model component. There were many cellsites that indicated elevated\nprobability of fraud,5 but cellsite zero stood out. Furthermore, the other cellsites made\nsense because when you looked up their locations, there at least was a good story---for\nexample, the cellsite was in a high-crime area. Looking up cellsite zero resulted in nothing at all. It wasn’t in the cellsite lists. We went to the top data guru to divine the answer.\nIndeed, there was no cellsite zero. But the data clearly have many fraudulent calls from\ncellsite zero!\n\n4. The philosophically minded should read W. V. O. Quine’s (1951) classic essay, “Two Dogmas of Empiricism,”\nin which he presents a biting criticism of the common notion that there is a dichotomy between the empirical\nand the analytical.\n\n5. Technically, the models were most useful if there was a significant change in behavior to more calling from\n\nthese cellsites. If you are interested, the papers describe this in detail.\n\nWhat Data Can’t Do: Humans in the Loop, Revisited\n\n|\n\n339", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00364", "page_num": 364, "segment": "To make a quite long story short, our understanding of the data was wrong. Briefly,\nwhen fraud was resolved on a customer’s account, often a substantial amount of time\npassed between their bill being printed, sent out, received by the customer, opened, read,\nand acted upon. During this time, fraudulent activity continued. Now that fraud had\nbeen detected, these calls should not appear on the customer’s next bill, so they were\nremoved from the billing system. They were not discarded however, but (fortunately\nfor the data mining efforts) were kept in a different database. Unfortunately, whoever\ndesigned that database decided that it was not important to keep certain fields. One of\nthese was the cellsite. Thus, when the data science effort asked for data on all the fraudulent calls, in order to build training and test sets, these calls were included. As they\ndid not have a cellsite, another design decision (conscious or not) led the fields to be\nfilled with zeros. Thus, many of the fraudulent calls seemed to be from cellsite zero!\n\nThis is a “leak,” as introduced in Chapter 2. You might think that should have been easy\nto spot. It wasn’t, for several reasons. Consider how many phone calls are made by tens\nof millions of customers over many months, and for each call there was a very large\nnumber of possible descriptive attributes. There was no possibility to manually examine\nthe data. Further, the calls were grouped by customer, so there wasn’t a bulk of cellsitezero calls; they were interspersed with each customer’s other calls. Finally, and possibly\nmost importantly, as part of the data preparation the data were scrubbed to improve the\nquality of the target variable. Some calls credited as fraud to an account were not actually\nfraudulent. Many of these, in turn, could be identified by seeing that the customer called\nthem in a prior, nonfraud period. The result was that calls from cellsite zero had an\nelevated probability of fraud, but were not a perfect predictor of fraud (which would\nhave been a red flag).\n\nThe point of this mini-case study is to illustrate that “what the data is” is an interpretation\nthat we place. This interpretation often changes through the process of data mining,\nand we need to embrace this malleability. Our fraud detection example showed a change\nin the interpretation of a data item. We often also change our understanding of how the\ndata were sampled as we uncover biases in the data collection process. For example, if\nwe want to model consumer behavior in order to design or deliver a marketing campaign, it is essential to understand exactly what was the consumer base from which the\ndata were sampled. This again sounds obvious in theory, but in practice it may involve\nin-depth analysis of the systems and businesses from which the data came.\n\nFinally we need to be discerning in the sorts of problems for which data science, even\nwith the integration of humans, is likely to add value. We must ask: are there really\nsufficient data pertaining to the decision at hand? Very high-level strategic decisions\nmay be placed in a unique context. Data analyses, as well as theoretical simulations, may\nprovide insight, but often for the highest-level decisions the decision makers must rely\non their experience, knowledge, and intuition. This applies certainly to strategic decisions such as whether to acquire a particular company: data analysis can support the\n\n340\n\n|\n\nChapter 14: Conclusion", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00365", "page_num": 365, "segment": "decision, but ultimately each situation is unique and the judgment of an experienced\nstrategist will be necessary.\n\nThis idea of unique situations should be carried through. At an extreme we might think\nof Steve Jobs’ famous statement: “It’s really hard to design products by focus groups. A\nlot of times, people don’t know what they want until you show it to them... That doesn’t\nmean we don’t listen to customers, but it’s hard for them to tell you what they want when\nthey’ve never seen anything remotely like it.” As we look to the future we may hope that\nwith the increasing ability to do careful, automated experimentation we may move from\nasking people what they would like or would find useful to observing what they like or\nfind useful. To do this well, we need to follow our fundamental principle: consider data\nas an asset, in which we may need to invest. Our Capital One case from Chapter 1 is an\nexample of creating many products and investing in data and data science to determine\nboth which ones people would want, and for each product which people would be\nappropriate (i.e., profitable) customers.\n\nPrivacy, Ethics, and Mining Data About Individuals\nMining data, especially data about individuals, raises important ethical issues that\nshould not be ignored. There recently has been considerable discussion in the press and\nwithin government agencies about privacy and data (especially online data), but the\nissues are much broader. Most consumer-facing large companies collect or purchase\ndetailed data on all of us. These data are used directly to make decisions regarding many\nof the business applications we have discussed through the book: should we be granted\ncredit? If so, what should be our credit line? Should we be targeted with an offer? What\ncontent would we like to be shown on the website? What products should be recommended to us? Are we likely to defect to a competitor? Is there fraud on our account?\n\nThe tension between privacy and improving business decisions is intense because there\nseems to be a direct relationship between increased use of personal data and increased\neffectiveness of the associated business decisions. For example, a study by researchers\nat the University of Toronto and MIT showed that after particularly stringent privacy\nprotection was enacted in Europe, online advertising became significantly less effective.\nIn particular, “the difference in stated purchase intent between those who were exposed\nto ads and those who were not dropped by approximately 65%. There was no such\nchange for countries outside Europe” (Goldfarb & Tucker, 2011).6 This is not a phenomenon restricted to online advertising: adding fine-grained social network data (e.g.,\nwho communicates with whom) to more traditional data on individuals substantially\nincreases the effectiveness of fraud detection (Fawcett & Provost, 1997) and targeted\nmarketing (Hill et al., 2006). Generally, the more fine-grained data you collect on in6. See Mayer and Narayanan’s web site for a criticism of this and other research claims about the value of\n\nbehaviorally targeted online advertising.\n\nPrivacy, Ethics, and Mining Data About Individuals\n\n|\n\n341", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00366", "page_num": 366, "segment": "dividuals, the better you can predict things about them that are important for business\ndecision-making. This seeming direct relationship between reduced privacy and increased business performance elicits strong feelings from both the privacy and the\nbusiness perspectives (sometimes within the same person).\n\nIt is far beyond the scope of this book to resolve this problem, and the issues are extremely complicated (for example, what sort of “anonymization” would be sufficient?)\nand diverse. Possibly the biggest impediment to the reasoned consideration of privacyfriendly data science designs is the difficulty with even defining what privacy is. Daniel\nSolove is a world authority on privacy. His article “A Taxonomy of Privacy” (2006) starts:\n\nPrivacy is a concept in disarray. Nobody can articulate what it means. As one commentator has observed, privacy suffers from “an embarrassment of meanings.”\n\nSolove’s article goes on to spend over 80 pages giving a taxonomy of privacy. Helen\nNissenbaum is another world authority on privacy, who has concentrated recently\nspecifically on the relationship of privacy and massive databases (and the mining thereof). Her book on this topic, Privacy in Context, is over 300 pages (and well worth reading). We bring this up to emphasize that privacy concerns are not some easy-tounderstand or easy-to-deal-with issues that can be quickly dispatched, or even written\nabout well as a section or chapter of a data science book. If you are either a data scientist\nor a business stakeholder in data science projects, you should care about privacy concerns, and you will need to invest serious time in thinking carefully about them.\n\nTo help frame our thinking about data science and privacy, we have posted an online\nappendix (see our page here) explaining some of the various concepts and issues, and\npointing to additional reading material where we all can further expand our thinking.\n\nIs There More to Data Science?\nAlthough this book is fairly thick, we have tried hard to pick the most relevant fundamental concepts to help the data scientist and the business stakeholder to understand\ndata science and to communicate well. Of course, we have not covered all the fundamental concepts of data science, and any given data scientist may dispute whether we\nhave included exactly the right ones. But all should agree that these are some of the most\nimportant concepts and that they underlie a vast amount of the science.\n\nThere are all manner of advanced topics and closely related topics that build upon the\nfundamentals presented here. We will not try to list them---if you’re interested, peruse\nthe programs of recent top-notch data mining research conferences, such as the ACM\nSIGKDD International Conference on Data Mining and Knowledge Discovery, or the\nIEEE International Conference on Data Mining. Both of these conferences have topnotch Industry Tracks as well, focusing on applications of data science to business and\ngovernment problems.\n\n342\n\n|\n\nChapter 14: Conclusion", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00367", "page_num": 367, "segment": "Let us give just one more concrete example of the sort of topic one might find when\nexploring further. Recall our first principle of data science: data (and data science capability) should be regarded as assets, and should be candidates for investment. Through\nthe book we have discussed in increasing complexity the notion of investing in data. If\nwe apply our general framework of considering the costs and benefits in data science\nprojects explicitly, it leads us to new thinking about investing in data.\n\nFinal Example: From Crowd-Sourcing to Cloud-Sourcing\nThe connectivity between businesses and “consumers” brought about by the Internet\nhas changed the economics of labor. Web-based systems like Amazon’s Mechanical Turk\nand oDesk (among others) facilitate a type of crowd-sourcing that might be called “cloud\nlabor”---harnessing via the Internet a vast pool of independent contractors. One sort of\ncloud labor that is particularly relevant to data science is “micro-outsourcing”: the outsourcing of large numbers of very small, well-defined tasks. Micro-outsourcing is particularly relevant to data science, because it changes the economics, as well as the practicalities, of investing in data.7\n\nAs one example, recall the requirements for applying supervised modeling. We need to\nhave specified a target variable precisely, and we need to actually have values for the\ntarget variable (“labels”) for a set of training data. Sometimes we can specify the target\nvariable precisely, but we find we do not have any labeled data. In certain cases, we can\nuse micro-outsourcing systems such as Mechanical Turk to label data.\n\nFor example, advertisers would like to keep their advertisements off of objectionable\nweb pages, like those that contain hate speech. However, with billions of pages to put\ntheir ads on, how can they know which ones are objectionable? It would be far too costly\nto have employees look at them all. We might immediately recognize this as a possible\ncandidate for text classification (Chapter 10): we can get the text of the page, represent\nit as feature vectors as we have discussed, and build a hate-speech classifier. Unfortunately, we have no representative sample of hate speech pages to use as training data.\nHowever, if this problem is important enough8 then we should consider investing in\nlabeled training data to see whether we can build a model to identify pages containing\nhate speech.\n\n7. The interested reader can go to Google Scholar and query on “data mining mechanical turk” or more broadly\non “human computation” to find papers on the topic, and to follow the forward citation links (“Cited by”) to\nfind even more.\n\n8. In fact, the problem of ads appearing on objectionable pages was reported to be a $2 billion problem (Winterberry Group, 2010).\n\nFinal Example: From Crowd-Sourcing to Cloud-Sourcing\n\n|\n\n343", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00368", "page_num": 368, "segment": "Cloud labor changes the economics of investing in data in our example of getting labeled\ntraining data. We can engage very inexpensive labor via the Internet to invest in data in\nvarious ways. For example, we can have workers on Amazon Mechanical Turk label\npages as objectionable or not, providing us with target labels, much more cheaply than\nhiring even student workers.\n\nThe rate of completion, when done by a trained intern, was 250 websites per hour, at a\ncost of $15/hr. When posted on Amazon Mechanical Turk, the labeling rate went up to\n2,500 websites per hour and the overall cost remained the same. (Ipeirotis et al., 2010)\n\nThe problem is that you get what you pay for, and low cost sometimes means low quality.\nThere has been a surge of research over the past half decade on the problems of maintaining quality while taking advantage of cloud labor. Note that page labeling is just one\nexample of enhancing data science with cloud labor. Even in this case study there are\nother options, such as using cloud labor to search for positive examples of hate speech,\ninstead of labeling pages that we give them (Attenberg & Provost, 2010), or cloud laborers can be challenged in a game-like system to find cases where the current model\nmakes mistakes---to “beat the machine” (Attenberg et al., 2011).\n\nFinal Words\nYour authors have been working on applying data science to real business problems for\nmore than two decades. You would think that it would all become second nature. It is\nstriking how useful it still can be even for us to have this set of explicit fundamental\nconcepts in hand. So many times when you reach a seeming impasse in thinking, pulling\nout the the fundamental concepts makes the way clear. “Well, let’s go back to our business\nand data understanding...what exactly is the problem we are trying to solve” can resolve\nmany problems, whether we then decide to work through the implications of the expected value framework, or to think more carefully about how the data are gathered, or\nabout whether the costs and benefits are specified well, or about further investing in\ndata, or to consider whether the target variable has been defined appropriately for the\nproblem to be solved, etc. Knowing what are the different sorts of data science tasks\nhelps to keep the data scientist from treating all business problems as nails for the particular hammers that he knows well. Thinking carefully about what is important to the\nbusiness problem, when considering evaluation and “baselines” for comparison, brings\ninteractions with stakeholders to life. (Compare that with the chilling effect of reporting\nsome statistic like mean-squared error when it is meaningless to the problem at hand.)\nThis facilitation of data-analytic thinking applies not just to the data scientists, but to\neveryone involved.\n\nIf you are a business stakeholder rather than a data scientist, don’t let so-called data\nscientists bamboozle you with jargon: the concepts of this book plus knowledge of your\nown business and data systems should allow you to understand 80% or more of the data\nscience at a reasonable enough level to be productive for your business. After having\n\n344\n\n|\n\nChapter 14: Conclusion", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00369", "page_num": 369, "segment": "read this book, if you don’t understand what a data scientist is talking about, be wary.\nThere are of course many, many more complex concepts in data science, but a good data\nscientist should be able to describe the fundamentals of the problem and its solution at\nthe level and in the terms of this book.\n\nIf you are a data scientist, take this as our challenge: think deeply about exactly why your\nwork is relevant to helping the business and be able to present it as such.\n\nFinal Words\n\n|\n\n345", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00370", "page_num": 370, "segment": "", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "none", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00371", "page_num": 371, "segment": "APPENDIX A\nProposal Review Guide\n\nEffective data analytic thinking should allow you to assess potential data mining projects\nsystematically. The material in this book should give you the necessary background to\nassess proposed data mining projects, and to uncover potential flaws in proposals. This\nskill can be applied both as a self-assessment for your own proposals and as an aid in\nevaluating proposals from internal data science teams or external consultants.\n\nWhat follows contains a set of questions that one should have in mind when considering\na data mining project. The questions are framed by the data mining process discussed\nin detail in Chapter 2, and used as a conceptual framework throughout the book. After\nreading this book, you should be able to apply these conceptually to a new business\nproblem. The list that follows is not meant to be exhaustive (in general, the book isn’t\nmeant to be exhaustive). However, the list contains a selection of some of the most\nimportant questions to ask.\n\nThroughout the book we have concentrated on data science projects where the focus is\nto mine some regularities, patterns, or models from the data. The proposal review guide\nreflects this. There may be data science projects in an organization where these regularities are not so explicitly defined. For example, many data visualization projects initially do not have crisply defined objectives for modeling. Nevertheless, the data mining\nprocess can help to structure data-analytic thinking about such projects---they simply\nresemble unsupervised data mining more than supervised data mining.\n\nBusiness and Data Understanding\n\n• What exactly is the business problem to be solved?\n\n• Is the data science solution formulated appropriately to solve this business problem?\n\nNB: sometimes we have to make judicious approximations.\n\n• What business entity does an instance/example correspond to?\n\n347", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00372", "page_num": 372, "segment": "• Is the problem a supervised or unsupervised problem?\n\n--- If supervised,\n\n--- Is a target variable defined?\n\n--- If so, is it defined precisely?\n\n--- Think about the values it can take.\n\n• Are the attributes defined precisely?\n\n--- Think about the values they can take.\n\n• For supervised problems: will modeling this target variable improve the stated\nbusiness problem? An important subproblem? If the latter, is the rest of the business\nproblem addressed?\n\n• Does framing the problem in terms of expected value help to structure the subtasks\n\nthat need to be solved?\n\n• If unsupervised, is there an “exploratory data analysis” path well defined? (That is,\n\nwhere is the analysis going?)\n\nData Preparation\n\n• Will it be practical to get values for attributes and create feature vectors, and put\n\nthem into a single table?\n\n• If not, is an alternative data format defined clearly and precisely? Is this taken into\naccount in the later stages of the project? (Many of the later methods/techniques\nassume the dataset is in feature vector format.)\n\n• If the modeling will be supervised, is the target variable well defined? Is it clear how\nto get values for the target variable (for training and testing) and put them into the\ntable?\n\n• How exactly will the values for the target variable be acquired? Are there any costs\n\ninvolved? If so, are the costs taken into account in the proposal?\n\n• Are the data being drawn from the similar population to which the model will be\napplied? If there are discrepancies, are the selection biases noted clearly? Is there a\nplan for how to compensate for them?\n\nModeling\n\n• Is the choice of model appropriate for the choice of target variable?\n\n--- Classification, class probability estimation, ranking, regression, clustering, etc.\n\n348\n\n| Appendix A: Proposal Review Guide", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00373", "page_num": 373, "segment": "• Does the model/modeling technique meet the other requirements of the task?\n\n--- Generalization performance, comprehensibility, speed of learning, speed of application, amount of data required, type of data, missing values?\n\n--- Is the choice of modeling technique compatible with prior knowledge of problem (e.g., is a linear model being proposed for a definitely nonlinear problem)?\n\n• Should various models be tried and compared (in evaluation)?\n\n• For clustering, is there a similarity metric defined? Does it make sense for the business problem?\n\nEvaluation and Deployment\n\n• Is there a plan for domain-knowledge validation?\n\n--- Will domain experts or stakeholders want to vet the model before deployment?\n\nIf so, will the model be in a form they can understand?\n\n• Is the evaluation setup and metric appropriate for the business task? Recall the\n\noriginal formulation.\n\n--- Are business costs and benefits taken into account?\n\n--- For classification, how is a classification threshold chosen?\n\n--- Are probability estimates used directly?\n\n--- Is ranking more appropriate (e.g., for a fixed budget)?\n\n--- For regression, how will you evaluate the quality of numeric predictions? Why\n\nis this the right way in the context of the problem?\n\n• Does the evaluation use holdout data?\n\n--- Cross-validation is one technique.\n\n• Against what baselines will the results be compared?\n\n--- Why do these make sense in the context of the actual problem to be solved?\n\n--- Is there a plan to evaluate the baseline methods objectively as well?\n\n• For clustering, how will the clustering be understood?\n\n• Will deployment as planned actually (best) address the stated business problem?\n\n• If the project expense has to be justified to stakeholders, what is the plan to measure\n\nthe final (deployed) business impact?\n\nEvaluation and Deployment\n\n|\n\n349", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00374", "page_num": 374, "segment": "", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "none", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00375", "page_num": 375, "segment": "APPENDIX B\nAnother Sample Proposal\n\nAppendix A presented a set of guidelines and questions useful for evaluating data science\nproposals. Chapter 13 contained a sample proposal (“Example Data Mining Proposal” on page 325) for a “customer migration” campaign and a critique of its weaknesses\n(“Flaws in the Big Red Proposal” on page 326).\n\nWe’ve used the telecommunications churn problem as a running example throughout\nthis book. Here we present a second sample proposal and critique, this one based on\nthe churn problem.\n\nScenario and Proposal\nYou’ve landed a great job with Green Giant Consulting (GGC), managing an analytical\nteam that is just building up its data science skill set. GGC is proposing a data science\nproject with TelCo, the nation’s second-largest provider of wireless communication\nservices, to help address their problem of customer churn. Your team of analysts has\nproduced the following proposal, and you are reviewing it prior to presenting the proposed plan to TelCo. Do you find any flaws with the plan? Do you have any suggestions\nfor how to improve it?\n\nChurn Reduction via Targeted Incentives --- A GGC Proposal\nWe propose that TelCo test its ability to control its customer churn via an analysis of\nchurn prediction. The key idea is that TelCo can use data on customer behavior to predict\nwhen customers will leave, and then can target these customers with special incentives\nto remain with TelCo. We propose the following modeling problem, which can be carried\nout using data already in TelCo’s possession.\n\nWe will model the probability that a customer will (or will not) leave within 90 days of\ncontract expiration, with the understanding that there is a separate problem of retaining\ncustomers who are continuing their service month-to-month, long after contract expiration. We believe that predicting churn in this 90-day window is an appropriate starting\npoint, and the lessons learned may apply to other churn-prediction cases as well. The\n\n351", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00376", "page_num": 376, "segment": "model will be built on a database of historical cases of customers who have left the company. Churn probability will be predicted based on data 45 days prior to contract expiration, in order for TelCo to have sufficient lead time to affect customer behavior with\nan incentive offer. We will model churn probability by building an ensemble of trees\n(random forest) model, which is known to have high accuracy for a wide variety of estimation problems.\n\nWe estimate that we will be able to identify 70% of the customers who will leave within\nthe 90-day time window. We will verify this by running the model on the database to\nverify that indeed the model can reach this level of accuracy. Through interactions with\nTelCo stakeholders, we understand that it is very important that the V.P. of Customer\nRetention sign off on any new customer retention procedures, and she has indicated that\nshe will base her decision on her own assessment that the procedure used for identifying\ncustomers makes sense and on the opinions about the procedure from selected firm\nexperts in customer retention. Therefore, we will give the V.P. and the experts access to\nthe model, so that they can verify that it will operate effectively and appropriately. We\npropose that every week, the model be run to estimate the probabilities of churn of the\ncustomers whose contracts expire in 45 days (give or take a week). The customers will be\nranked based on these probabilities, and the top N will be selected to receive the current\nincentive, with N based on the cost of the incentive and the weekly retention budget.\n\nFlaws in the GGC Proposal\nWe can use our understanding of the fundamental principles and other basic concepts\nof data science to identify flaws in the proposal. Appendix A provides a starting “guide”\nfor reviewing such proposals, with some of the main questions to ask. However, this\nbook as a whole really can be seen as a proposal review guide. Here are some of the most\negregious flaws in Green Giant’s proposal:\n\n1. The proposal currently only mentions modeling based on “customers who have left\nthe company.” For training (and testing) we will also want to have customers who\ndid not leave the company, in order for the modeling to find discriminative information. (Chapter 2, Chapter 3, Chapter 4, Chapter 7)\n\n2. Why rank customers by the highest probability of churn? Why not rank them by\nexpected loss, using a standard expected value computation? (Chapter 7, Chapter 11)\n\n3. Even better, should we not try to model those customers who are most likely to be\n\ninfluenced (positively) by the incentive? (Chapter 11, Chapter 12)\n\n4. If we’re going to proceed as in (3), we have the problem of not having the training\ndata we need. We’ll have to invest in obtaining training data. (Chapter 3, Chapter 11)\n\nNote that the current proposal may well be just a first step toward the business goal, but\nthis would need to be spelled out explicitly: see if we can estimate the probabilities well.\nIf we can, then it makes sense to proceed. If not, we may need to rethink our investment\nin this project.\n\n352\n\n| Appendix B: Another Sample Proposal", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00377", "page_num": 377, "segment": "5. The proposal says nothing about assessing generalization performance (i.e., doing\na holdout evaluation). It sounds like they are going to test on the training set (“...\nrunning the model on the database...”). (Chapter 5)\n\n6. The proposal does not define (nor even mention) what attributes are going to be\nused! Is this just an omission? Is this because the team hasn’t even thought about\nit? What is the plan? (Chapter 2, Chapter 3)\n\n7. How does the team estimate that the model will be able to identify 70% of the\ncustomers who will leave? There is no mention that any pilot study already has been\nconducted, nor learning curves having been produced on data samples, nor any\nother support for this claim. It seems like a guess. (Chapter 2, Chapter 5, Chapter 7)\n\n8. Furthermore, without discussing the error rate or the notion of false positives and\nfalse negatives, it’s not clear what “identify 70% of the customers who will leave”\nreally means. If I say nothing about the false-positive rate, I can identify 100% of\nthem simply by saying everyone will leave. So talking about true-positive rate only\nmakes sense if you also talk about false-positive rate. (Chapter 7, Chapter 8)\n\n9. Why choose one particular model? With modern toolkits, we can easily compare\n\nvarious models on the same data. (Chapter 4, Chapter 7, Chapter 8)\n\n10. The V.P. of Customer Retention must sign off on the procedure, and has indicated\nthat she will examine the procedure to see if it makes sense (domain knowledge\nvalidation). However, ensembles of trees are black-box models. The proposal says\nnothing about how she is going to understand how the procedure is making its\ndecisions. Given her desire, it would be better to sacrifice some accuracy to build a\nmore comprehensible model. Once she is “on board” it may be possible to use lesscomprehensible techniques to achieve higher accuracies. (Chapter 3, Chapter 7,\nChapter 12)\n\nScenario and Proposal\n\n|\n\n353", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00378", "page_num": 378, "segment": "", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "none", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00379", "page_num": 379, "segment": "Glossary\n\nNote: This glossary is an extension to one compiled by Ron Kohavi and Foster Provost\n(1998), used with kind permission of Springer Science and Business Media.\n\na priori\n\nAssociation mining\n\nA priori is a term borrowed from philosophy meaning “prior to experience.” In data\nscience, an a priori belief is one that is\nbrought to the problem as background\nknowledge, as opposed to a belief that is\nformed after examining data. For example,\nyou might say, “There is no a priori reason\nto believe that this relationship is linear.”\nAfter examining data you might decide that\ntwo variables have a linear relationship\n(and so linear regression should work fairly\nwell), but there was no reason to believe,\nfrom prior knowledge, that they should be\nso related. The opposite of a priori is a posteriori.\n\nAccuracy (error rate)\n\nThe rate of correct (incorrect) predictions\nmade by the model over a dataset (cf. coverage). Accuracy is usually estimated using\nan independent (holdout) dataset that was\nnot used at any time during the learning\nprocess. More complex accuracy estimation techniques, such as cross-validation\nand the bootstrap, are commonly used, especially with datasets containing a small\nnumber of instances.\n\nTechniques that find conjunctive implication rules of the form “X and Y → A and B”\n(associations) that satisfy given criteria.\n\nAttribute (field, variable, feature)\n\nA quantity describing an instance. An attribute has a domain defined by the attribute type, which denotes the values that\ncan be taken by an attribute. The following\ndomain types are common:\n\n• Categorical (symbolic): A\n\nfinite\nnumber of discrete values. The type\nnominal denotes that there is no ordering between the values, such as last\nnames and colors. The type ordinal\ndenotes that there is an ordering, such\nas in an attribute taking on the values\nlow, medium, or high.\n\n• Continuous (quantitative): Commonly, subset of real numbers, where\nthere is a measurable difference between the possible values. Integers are\nusually treated as continuous in practical problems.\n\nWe do not differentiate in this book, but\noften the distinction is made that a feature\nis the specification of an attribute and its\n\n355", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00380", "page_num": 380, "segment": "Class (label)\n\nvalue. For example, color is an attribute.\n“Color is blue” is a feature of an example.\nMany transformations to the attribute set\nleave the feature set unchanged (for example, regrouping attribute values or transforming multivalued attributes to binary\nattributes). In this book we follow the practice of many authors and practitioners, and\nuse feature as a synonym for attribute.\n\nClass (label)\n\nOne of a small, mutually exclusive set of labels used as possible values for the target\nvariable in a classification problem. Labeled\ndata has one class label assigned to each example. For example, in a dollar bill classification problem the classes could be legitimate and counterfeit. In a stock assessment\ntask the classes might be will gain substantially, will lose substantially, and _will\nmaintain its value.\n\nClassifier\n\nA mapping from unlabeled instances to\n(discrete) classes. Classifiers have a form\n(e.g., classification tree) plus an interpretation procedure (including how to handle\nunknown values, etc.). Most classifiers also\ncan provide probability estimates (or other\nlikelihood scores), which can be thresholded to yield a discrete class decision thereby\ntaking into account a cost/benefit or utility\nfunction.\n\nConfusion matrix\n\nA matrix showing the predicted and actual\nclassifications. A confusion matrix is of size\nl × l, where l is the number of different label\nvalues. A variety of classifier evaluation\nmetrics are defined based on the contents\nof the confusion matrix, including accuracy, true positive rate, false positive rate, true\nnegative rate, false negative rate, precision,\nrecall, sensitivity, specificity, positive predictive value, and negative predictive value.\n\nCoverage\n\nThe proportion of a dataset for which a\nclassifier makes a prediction. If a classifier\ndoes not classify all the instances, it may be\nimportant to know its performance on the\n\n356\n\n| Glossary\n\nset of cases for which it is confident enough\nto make a prediction.\n\nCost (utility/loss/payoff)\n\nA measurement of the cost to the performance task (and/or benefit) of making a\nprediction ŷ when the actual label is y. The\nuse of accuracy to evaluate a model assumes\nuniform costs of errors and uniform benefits of correct classifications.\n\nCross-validation\n\nA method for estimating the accuracy (or\nerror) of an inducer by dividing the data\ninto k mutually exclusive subsets (the\n“folds”) of approximately equal size. The\ninducer is trained and tested k times. Each\ntime it is trained on the dataset minus one\nof the folds and tested on that fold. The accuracy estimate is the average accuracy for\nthe k folds or the accuracy on the combined\n(“pooled”) testing folds.\n\nData cleaning/cleansing\n\nThe process of improving the quality of the\ndata by modifying its form or content, for\nexample by removing or correcting data\nvalues that are incorrect. This step usually\nprecedes the modeling step, although a pass\nthrough the data mining process may indicate that further cleaning is desired and may\nsuggest ways to improve the quality of the\ndata.\n\nData mining\n\nThe term data mining is somewhat overloaded. It sometimes refers to the whole data mining process and sometimes to the\nspecific application of modeling techniques\nto data in order to build models or find\nother patterns/regularities.\n\nDataset\n\nA schema and a set of instances matching\nthe schema. Generally, no ordering on instances is assumed. Most data mining work\nuses a single fixed-format table or collection of feature vectors.\n\nDimension\n\nAn attribute or several attributes that together describe a property. For example, a", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00381", "page_num": 381, "segment": "geographical dimension might consist of\nthree attributes: country, state, city. A time\ndimension might include 5 attributes: year,\nmonth, day, hour, minute.\n\nError rate\n\nSee Accuracy (error rate).\n\nExample\n\nSee Instance (example, case, record).\n\nFeature\n\nSee Attribute (field, variable, feature).\n\nFeature vector (record, tuple)\n\nA list of features describing an instance.\n\nField\n\nSee Attribute.\n\ni.i.d. sample\n\nA set of independent and identically distributed instances.\n\nInduction\n\nInduction is the process of creating a general model (such as a classification tree or\nan equation) from a set of data. Induction\nmay be contrasted with deduction: deduction starts with a general rule or model and\none or more facts, and creates other specific\nfacts from them. Induction goes in the other direction: induction takes a collection of\nfacts and creates a general rule or model. In\nthe context of this book, model induction\nis synonymous with learning or mining a\nmodel, and the rules or models are generally statistical in nature.\n\nInstance (example, case, record)\n\nA single object of the world from which a\nmodel will be learned, or on which a model\nwill be used (e.g., for prediction). In most\ndata science work, instances are described\nby feature vectors; some work uses more\ncomplex representations (e.g., containing\nrelations between instances or between\nparts of instances).\n\nKDD\n\noriginally was an abbreviation for Knowledge Discovery from Databases. It is now\nused to cover broadly the discovery of\n\nModel deployment\n\nknowledge from data, and often is used\nsynonymously with data mining.\n\nKnowledge discovery\n\nThe nontrivial process of identifying valid,\nnovel, potentially useful, and ultimately understandable patterns in data. This is the\ndefinition used in “Advances in Knowledge\nDiscovery and Data Mining,” by Fayyad,\nPiatetsky-Shapiro, & Smyth (1996).\n\nLoss\n\nSee Cost (utility/loss/payoff).\n\nMachine learning\n\nIn data science, machine learning is most\ncommonly used to mean the application of\ninduction algorithms to data. The term\noften used synonymously with the modeling stage the data mining process. Machine\nLearning is the field of scientific study that\nconcentrates on induction algorithms and\non other algorithms that can be said to\nlearn.\n\nMissing value\n\nThe situation where the value for an attribute is not known or does not exist.\nThere are several possible reasons for a value to be missing, such as: it was not measured; there was an instrument malfunction;\nthe attribute does not apply, or the attribute’s value cannot be known. Some algorithms have problems dealing with missing values.\n\nModel\n\nA structure and corresponding interpretation that summarizes or partially summarizes a set of data, for description or prediction. Most inductive algorithms generate models that can then be used as classifiers, as regressors, as patterns for human\nconsumption, and/or as input to subsequent stages of the data mining process.\n\nModel deployment\n\nThe use of a learned model to solve a realworld problem. Deployment often is used\nspecifically to contrast with the “use” of a\nmodel in the Evaluation stage of the data\nmining process. In the latter, deployment\n\nGlossary\n\n|\n\n357", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00382", "page_num": 382, "segment": "OLAP (MOLAP, ROLAP)\n\nusually is simulated on data where the true\nanswer is known.\n\nSpecificity\n\nTrue negative rate (see Confusion matrix).\n\nOLAP (MOLAP, ROLAP)\n\nSupervised learning\n\nwith MOLAP\n\nOnline Analytical Processing. Usually synonymous\n(multidimensional OLAP). OLAP engines facilitate the exploration of data along several\n(predetermined) dimensions. OLAP commonly uses intermediate data structures to\nstore precalculated results on multidimensional data, allowing fast computations.\nROLAP (relational OLAP) refers to performing OLAP using relational databases.\n\nRecord\n\nSee Feature vector (record, tuple).\n\nSchema\n\nA description of a dataset’s attributes and\ntheir properties.\n\nSensitivity\n\nTrue positive rate (see Confusion matrix).\n\nTechniques used to learn the relationship\nbetween independent attributes and a designated dependent attribute (the label).\nMost induction algorithms fall into the supervised learning category.\n\nTuple\n\nSee Feature vector (record, tuple).\n\nUnsupervised learning\n\nLearning techniques that group instances\nwithout a pre-specified target attribute.\nClustering algorithms are usually unsupervised.\n\nUtility\n\nSee Cost (utility/loss/payoff).\n\n358\n\n| Glossary", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00383", "page_num": 383, "segment": "Bibliography\n\nAamodt, A., & Plaza, E. (1994). Case-based reasoning: Foundational issues, methodological variations, and system approaches. Artificial Intelligence Communications,\n7(1), 39--59. Available: http://www.iiia.csic.es/People/enric/AICom.html.\n\nAdams, N. M., & Hand, D. J. (1999). Comparing classifiers when the misallocations\n\ncosts are uncertain. Pattern Recognition, 32, 1139--1147.\n\nAha, D. W. (Ed.). (1997). Lazy learning. Kluwer Academic Publishers, Norwell, MA,\n\nUSA.\n\nAha, D. W., Kibler, D., & Albert, M. K. (1991). Instance-based learning algorithms.\n\nMachine Learning, 6, 37--66.\n\nAggarwal, C., & Yu, P. (2008). Privacy-preserving Data Mining: Models and Algorithms. Springer, USA.\n\nAral, S., Muchnik, L., & Sundararajan, A. (2009). Distinguishing influence-based contagion from homophily-driven diffusion in dynamic networks. Proceedings of the\nNational Academy of Sciences, 106(51), 21544-21549.\n\nArthur, D., & Vassilvitskii, S. (2007). K-means++: the advantages of careful seeding. In\nProceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 1027--1035.\n\nAttenberg, J., Ipeirotis, P., & Provost, F. (2011). Beat the machine: Challenging workers\nto find the unknown unknowns. In Workshops at the Twenty-Fifth AAAI Conference on Artificial Intelligence.\n\nAttenberg, J., & Provost, F. (2010). Why label when you can search?: Alternatives to\nactive learning for applying human resources to build classification models under\nextreme class imbalance. In Proceedings of the 16th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, pp. 423--432. ACM.\n\n359", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00384", "page_num": 384, "segment": "Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. http://\narchive.ics.uci.edu/ml. Irvine, CA: University of California, School of Information\nand Computer Science.\n\nBolton, R., & Hand, D. (2002). Statistical Fraud Detection: A Review. Statistical Science, 17(3), 235-255.\n\nBreiman, L., Friedman, J., Olshen, R., & Stone, C. (1984). Classification and regression\n\ntrees. Wadsworth International Group, Belmont, CA.\n\nBrooks, D. (2013). What Data Can’t Do. New York Times, Feb. 18.\n\nBrown, L., Gans, N., Mandelbaum, A., Sakov, A., Shen, H., Zeltyn, S., & Zhao, L. (2005).\nStatistical analysis of a telephone call center: A queueing-science perspective. Journal of the American Statistical Association, 100(469), 36-50.\n\nBrynjolfsson, E., & Smith, M. (2000). Frictionless commerce? A comparison of internet\n\nand conventional retailers. Management Science, 46, 563--585.\n\nBrynjolfsson, E., Hitt, L. M., & Kim, H. H. (2011). Strength in numbers: How does datadriven decision making affect firm performance? Tech. rep., available at SSRN:\nhttp://ssrn.com/abstract=1819486 or http://dx.doi.org/10.2139/ssrn.1819486.\n\nBusiness Insider (2012). The Digital 100: The world’s most valuable private tech companies. http:///2012-digital-100.\n\nCiccarelli, F. D., Doerks, T., Von Mering, C., Creevey, C. J., Snel, B., & Bork, P. (2006).\nToward automatic reconstruction of a highly resolved tree of life. Science, 311\n(5765), 1283--1287.\n\nClearwater, S., & Stern, E. (1991). A rule-learning program in high energy physics event\n\nclassification. Comp Physics Comm, 67, 159--182.\n\nClemons, E., & Thatcher, M. (1998). Capital One: Exploiting and Information-based\nStrategy. In Proceedings of the 31st Hawaii International Conference on System Sciences.\n\nCohen, L., Diether, K., & Malloy, C. (2012). Legislating Stock Prices. Harvard Business\n\nSchool Working Paper, No. 13--010.\n\nCover, T., & Hart, P. (1967). Nearest neighbor pattern classification. Information Theory,\n\nIEEE Transactions on, 13(1), 21--27.\n\nCrandall, D., Backstrom, L., Cosley, D., Suri, S., Huttenlocher, D., & Kleinberg, J. (2010).\nInferring social ties from geographic coincidences. Proceedings of the National\nAcademy of Sciences, 107(52), 22436-22441.\n\nDeza, E., & Deza, M. (2006). Dictionary of distances. Elsevier Science.\n\n360\n\n| Bibliography", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00385", "page_num": 385, "segment": "Dietterich, T. G. (1998). Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation, 10, 1895--1923.\n\nDietterich, T. G. (2000). Ensemble methods in machine learning. Multiple Classifier\n\nSystems, 1-15.\n\nDuhigg, C. (2012). How Companies Learn Your Secrets. New York Times, Feb. 19.\n\nElmagarmid, A., Ipeirotis, P., & Verykios, V. (2007). Duplicate record detection: A survey. Knowledge and Data Engineering, IEEE Transactions on, 19(1), 1--16.\n\nEvans, R., & Fisher, D. (2002). Using decision tree induction to minimize process delays\nin the printing industry. In Klosgen, W., & Zytkow, J. (Eds.), Handbook of Data\nMining and Knowledge Discovery, pp. 874--881. Oxford University Press.\n\nEzawa, K., Singh, M., & Norton, S. (1996). Learning goal oriented Bayesian networks\nfor telecommunications risk management. In Saitta, L. (Ed.), Proceedings of the\nThirteenth International Conference on Machine Learning, pp. 139--147. San Francisco, CA. Morgan Kaufmann.\n\nFawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27(8),\n\n861--874.\n\nFawcett, T., & Provost, F. (1996). Combining data mining and machine learning for\neffective user profiling. In Simoudis, Han, & Fayyad (Eds.), Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, pp. 8--13.\nMenlo Park, CA. AAAI Press.\n\nFawcett, T., & Provost, F. (1997). Adaptive fraud detection. Data Mining and Knowledge\n\nDiscovery, 1 (3), 291--316.\n\nFayyad, U., Piatetsky-shapiro, G., & Smyth, P. (1996). From data mining to knowledge\n\ndiscovery in databases. AI Magazine, 17, 37--54.\n\nFrank, A., & Asuncion, A. (2010). UCI machine learning repository.\n\nFriedman, J. (1997). On bias, variance, 0/1-loss, and the curse-of-dimensionality. Data\n\nMining and Knowledge Discovery, 1(1), 55-77.\n\nGandy, O. H. (2009). Coming to Terms with Chance: Engaging Rational Discrimination\n\nand Cumulative Disadvantage. Ashgate Publishing Company.\n\nGoldfarb, A. & Tucker, C. (2011). Online advertising, behavioral targeting, and privacy.\n\nCommunications of the ACM 54(5), 25-27.\n\nHaimowitz, I., & Schwartz, H. (1997). Clustering and prediction for credit line optimization. In Fawcett, Haimowitz, Provost, & Stolfo (Eds.), AI Approaches to Fraud\nDetection and Risk Management, pp. 29--33. AAAI Press. Available as Technical\nReport WS-97-07.\n\nBibliography\n\n|\n\n361", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00386", "page_num": 386, "segment": "Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P. & Witten, I. (2009). The\n\nWEKA data mining software: An update. SIGKDD Explorations, 11 (1).\n\nHand, D. J. (2008). Statistics: A Very Short Introduction. Oxford University Press.\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning:\nData Mining, Inference, and Prediction (Second Edition edition). Springer.\n\nHays, C. L. (2004). What they know about you. The New York Times.\n\nHernández, M. A., & Stolfo, S. J. (1995). The merge/purge problem for large databases.\n\nSIGMOD Rec., 24, 127--138.\n\nHill, S., Provost, F., & Volinsky, C. (2006). Network-based marketing: Identifying likely\n\nadopters via consumer networks. Statistical Science, 21 (2), 256--276.\n\nHolte, R. C. (1993). Very simple classification rules perform well on most commonly\n\nused datasets. Machine Learning, 11, 63--91.\n\nIpeirotis, P., Provost, F., & Wang, J. (2010). Quality management on Amazon Mechanical\nTurk. In Proceedings of the 2010 ACM SIGKDD Workshop on Human Computation, pp. 64-67. ACM.\n\nJackson, M. (1989). Michael Jackson’s Malt Whisky Companion: a Connoisseur’s Guide\n\nto the Malt Whiskies of Scotland. Dorling Kindersley, London.\n\nJapkowicz, N., & Stephen, S. (2002). The class imbalance problem: A systematic study.\n\nIntelligent Data Analysis, 6 (5), 429--450.\n\nJapkowicz, N., & Shah, M. (2011). Evaluating Learning Algorithms: A Classification\n\nPerspective. Cambridge University Press.\n\nJensen, D. D., & Cohen, P. R. (2000). Multiple comparisons in induction algorithms.\n\nMachine Learning, 38(3), 309--338.\n\nKass, G. V. (1980). An exploratory technique for investigating large quantities of categorical data. Applied Statistics, 29(2), 119--127.\n\nKaufman, S., Rosset, S., Perlich, C., & Stitelman, O. (2012). Leakage in data mining:\nFormulation, detection, and avoidance. ACM Transactions on Knowledge Discovery from Data (TKDD), 6(4), 15.\n\nKohavi, R., Brodley, C., Frasca, B., Mason, L., & Zheng, Z. (2000). KDD-cup 2000 organizers’ report: Peeling the onion. ACM SIGKDD Explorations. 2(2).\n\nKohavi, R., Deng, A., Frasca, B., Longbotham, R., Walker, T., & Xu, Y. (2012). Trustworthy online controlled experiments: Five puzzling outcomes explained. In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 786--794. ACM.\n\n362\n\n| Bibliography", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00387", "page_num": 387, "segment": "Kohavi, R., & Longbotham, R. (2007). Online experiments: Lessons learned. Computer, 40 (9), 103--105.\n\nKohavi, R., Longbotham, R., Sommerfield, D., & Henne, R. (2009). Controlled experiments on the web: Survey and practical guide. Data Mining and Knowledge Discovery, 18(1), 140-181.\n\nKohavi, R., & Parekh, R. (2003). Ten supplementary analyses to improve e-commerce\n\nweb sites. In Proceedings of the Fifth WEBKDD workshop.\n\nKohavi, R., & Provost, F. (1998). Glossary of terms. Machine Learning, 30(2-3), 271-274.\n\nKolodner, J. (1993). Case-Based Reasoning. Morgan Kaufmann, San Mateo.\n\nKoren, Y., Bell, R., & Volinsky, C. (2009). Matrix factorization techniques for recommender systems. Computer, 42 (8), 30-37.\n\nKosinski, M., Stillwell, D., & Graepel, T. (2013). Private traits and attributes are predictable from digital records of human behavior. Proceedings of the National Academy of Sciences, doi: 10.1073/pnas.1218772110.\n\nLapointe, F.-J., & Legendre, P. (1994). A classification of pure malt Scotch whiskies.\n\nApplied Statistics, 43 (1), 237--257.\n\nLeigh, D. (1995). Neural networks for credit scoring. In Goonatilake, S., & Treleaven, P.\n(Eds.), Intelligent Systems for Finance and Business, pp. 61--69. John Wiley and Sons\nLtd., West Sussex, England.\n\nLetunic, & Bork (2006). Interactive tree of life (iTOL): an online tool for phylogenetic\n\ntree display and annotation. Bioinformatics, 23 (1).\n\nLin, J.-H., & Vitter, J. S. (1994). A theory for memory-based learning. Machine Learning, 17, 143--167.\n\nLloyd, S. P. (1982). Least square quantization in PCM. IEEE Transactions on Information\n\nTheory, 28 (2), 129--137.\n\nMacKay, D. (2003). Information Theory, Inference and Learning Algorithms, Chapter\n\n20. An Example Inference Task: Clustering. Cambridge University Press.\n\nMacQueen, J. B. (1967). Some methods for classification and analysis of multivariate\nobservations. In Proceedings of 5th Berkeley Symposium on Mathematical Statistics\nand Probability, pp. 281--297. University of California Press.\n\nMalin, B. & Sweeney, L. (2004). How (not) to protect genomic data privacy in a distributed network: Using trail re-identification to evaluate and design anonymity\nprotection systems. Journal of Biomedical Informatics, 37(3), 179-192.\n\nBibliography\n\n|\n\n363", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00388", "page_num": 388, "segment": "Martens, D., & Provost, F. (2011). Pseudo-social network targeting from consumer\ntransaction data. Working paper CeDER-11-05, New York University -- Stern\nSchool of Business.\n\nMcDowell, G. (2008). Cracking the Coding Interview: 150 Programming Questions and\n\nSolutions. CareerCup LLC.\n\nMcNamee, M. (2001). Credit Card Revolutionary. Stanford Business 69 (3).\n\nMcPherson, M., Smith-Lovin, L., & Cook, J. M. (2001). Birds of a feather: Homophily\n\nin social networks. Annual Review of Sociology, 27:415-444.\n\nMittermayer, M., & Knolmayer, G. (2006). Text mining systems for market response to\nnews: A survey. Working Paper No.184, Institute of Information Systems, University of Bern.\n\nMuoio, A. (1997). They have a better idea ... do you? Fast Company, 10.\n\nNissenbaum, H. (2010). Privacy in context. Stanford University Press.\n\nPapadopoulos, A. N., & Manolopoulos, Y. (2005). Nearest Neighbor Search: A Database\n\nPerspective. Springer.\n\nPennisi, E. (2003). A tree of life. Available online only: http:///site/\n\nfeature/data/tol/.\n\nPerlich, C., Provost, F., & Simonoff, J. (2003). Tree Induction vs. Logistic Regression: A\nLearning-Curve Analysis. Journal of Machine Learning Research, 4, 211-255.\n\nPerlich, C., Dalessandro, B., Stitelman, O., Raeder, T., & Provost, F. (2013). Machine\nlearning for targeted display advertising: Transfer learning in action. Machine\nLearning\n(in press; published online: 30 May 2013. DOI 10.1007/\ns10994-013-5375-2).\n\nPoundstone, W. (2012). Are You Smart Enough to Work at Google?: Trick Questions,\nZen-like Riddles, Insanely Difficult Puzzles, and Other Devious Interviewing Techniques You Need to Know to Get a Job Anywhere in the New Economy. Little, Brown\nand Company.\n\nProvost, F., & Fawcett, T. (1997). Analysis and visualization of classifier performance:\nComparison under imprecise class and cost distributions. In Proceedings of the\nThird International Conference on Knowledge Discovery and Data Mining\n(KDD-97), pp. 43--48 Menlo Park, CA. AAAI Press.\n\nProvost, F., & Fawcett, T. (2001). Robust classification for imprecise environments.\n\nMachine learning, 42(3), 203--231.\n\nProvost, F., Fawcett, T., & Kohavi, R. (1998). The case against accuracy estimation for\ncomparing induction algorithms. In Shavlik, J. (Ed.), Proceedings of ICML-98, pp.\n445--453 San Francisco, CA. Morgan Kaufmann.\n\n364\n\n| Bibliography", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00389", "page_num": 389, "segment": "Pyle, D. (1999). Data Preparation for Data Mining. Morgan Kaufmann.\n\nQuine, W.V.O. (1951). Two dogmas of empiricism, The Philosophical Review 60: 20-43.\nReprinted in his 1953 From a Logical Point of View. Harvard University Press.\n\nQuinlan, J. R. (1993). C4.5: Programs for machine learning. Morgan Kaufmann.\n\nQuinlan, J. (1986). Induction of decision trees. Machine Learning, 1 (1), 81--106.\n\nRaeder, T., Dalessandro, B., Stitelman, O., Perlich, C., & Provost, F. (2012). Design principles of massive, robust prediction systems. In Proceedings of the 18th ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining.\n\nRosset, S., & Zhu, J. (2007). Piecewise linear regularized solution paths. The Annals of\n\nStatistics, 35(3), 1012--1030.\n\nSchumaker, R., & Chen, H. (2010). A Discrete Stock Price Prediction Engine Based on\n\nFinancial News Keywords. IEEE Computer, 43(1), 51--56.\n\nSengupta, S. (2012). Facebook’s prospects may rest on trove of data.\n\nShakhnarovich, G., Darrell, T., & Indyk, P.(Eds., 2005). Nearest-Neighbor Methods in\nLearning and Vision. Neural Information Processing Series. The MIT Press, Cambridge, Massachusetts, USA.\n\nShannon, C. E. (1948). A mathematical theory of communication. Bell System Technical\n\nJournal, 27, 379--423.\n\nShearer, C. (2000). The CRISP-DM model: The new blueprint for data mining. Journal\n\nof Data Warehousing, 5(4), 13--22.\n\nShmueli, G. (2010). To explain or to predict?. Statistical Science, 25(3), 289--310.\n\nSilver, N. (2012). The Signal and the Noise. The Penguin Press HC.\n\nSolove, D. (2006). A taxonomy of privacy. University of Pennsylvania Law Review,\n\n154(3), 477-564.\n\nStein, R. M. (2005). The relationship between default prediction and lending profits:\nIntegrating ROC analysis and loan pricing. Journal of Banking and Finance, 29,\n1213--1236.\n\nSugden, A. M., Jasny, B. R., Culotta, E., & Pennisi, E. (2003). Charting the evolutionary\n\nhistory of life. Science, 300(5626).\n\nSwets, J. (1988). Measuring the accuracy of diagnostic systems. Science, 240, 1285--1293.\n\nSwets, J. A. (1996). Signal Detection Theory and ROC Analysis in Psychology and Diagnostics: Collected Papers. Lawrence Erlbaum Associates, Mahwah, NJ.\n\nSwets, J. A., Dawes, R. M., & Monahan, J. (2000). Better decisions through science.\n\nScientific American, 283, 82--87.\n\nBibliography\n\n|\n\n365", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00390", "page_num": 390, "segment": "Tambe, P. (2013). Big Data Investment, Skills, and Firm Value. Working Paper, NYU\nStern. Available: http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2294077.\n\nWEKA (2001). Weka machine learning software. Available: http://www.cs.waikato.ac.nz/~ml/index.html.\n\nWikipedia (2012). Determining the number of clusters in a data set. Wikipedia, the free\nencyclopedia. http://en.wikipedia.org/wiki/Determining_the_number_of_clus\nters_in_a_data_set [Online; accessed 14-February-2013].\n\nWilcoxon, F. (1945). Individual comparisons by ranking methods. Biometrics Bulletin,\n1(6), 80--83. Available: http://sci2s.ugr.es/keel/pdf/algorithm/articulo/wilcoxon1945.pdf.\n\nWinterberry Group (2010). Beyond the grey areas: Transparency, brand safety and the\nfuture of online advertising. White Paper, Winterberry Group LLC. http:///ourinsights/wp\n\nWishart, D. (2006). Whisky Classified. Pavilion.\n\nWitten, I., & Frank, E. (2000). Data mining: Practical machine learning tools and techniques with Java implementations. Morgan Kaufmann, San Francisco. Software\navailable from http://www.cs.waikato.ac.nz/~ml/weka/.\n\nZadrozny, B. (2004). Learning and evaluating classifiers under sample selection bias. In\nProceedings of the Twenty-first International Conference on Machine Learning, pp.\n903-910.\n\nZadrozny, B., & Elkan, C. (2001). Learning and making decisions when costs and probabilities are both unknown. In Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 204--213. ACM.\n\n366\n\n| Bibliography", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00391", "page_num": 391, "segment": "Index\n\nSymbols\n2-D Gaussian distributions, 299\n“and” operator, 240\n\nA\nA Taxonomy of Privacy (Solove), 342\nAberfeldy single malt scotch, 178\nAberlour single malt whiskey, 145\nabsolute errors, 95\naccuracy (term), 189\naccuracy results, 128\nACM SIGKDD, 318, 342\nad impressions, 234\nadding variables to functions, 123\nadvertising, 233\nagency, 40\nalarms, 188\nalgorithms\n\nclustering, 169\ndata mining, 20\nk-means, 171\nmodeling, 135\n\nAmazon, 1, 7, 9, 11, 142\nBorders vs., 316\ncloud storage, 314\ndata science services provided by, 314\nhistorical advantages of, 317\n\nanalysis\n\ncounterfactual, 23\nlearning curves and, 132\nanalytic engineering, 277--287\nchurn example, 281--287\nexpected value decomposition and, 284--287\nincentives, assessing influence of, 283--284\nproviding structure for business problem/\n\nsolutions with, 278--280\n\nselection bias, 280--281\ntargeting best prospects with, 278--281\n\nanalytic skills, software skills vs., 35\nanalytic solutions, 14\nanalytic techniques, 35--41, 187--208\n\napplying to business questions, 40--41\nbaseline performance and, 204--207\nclassification accuracy, 189--194\nconfusion matrix, 189--190\ndata warehousing, 38\ndatabase queries, 37--38\nexpected values, 194--204\ngeneralization methods for, 193--194\nmachine learning and, 39--40\nOLAP, 38\nregression analysis, 39\nstatistics, 35--37\nanalytic technologies, 29\nanalytic tools, 113\nAngry Birds, 246\n\nWe’d like to hear your suggestions for improving our indexes. Send email to index@oreilly.com.\n\n367", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00392", "page_num": 392, "segment": "Annie Hall (film), 305\nApollo 13 (film), 323\nApple Computer, 174--177, 268\napplications, 1, 187\narea under ROC curves (AUC), 219, 225, 226\nArmstrong, Louis, 259\nassessing overfitting, 113\nassociation discovery, 290--296\n\namong Facebook Likes, 293--296\nbeer and lottery example, 292--293\neWatch/eBracelet example, 290--291\nMagnum Opus system for, 294\nmarket basket analysis, 293--296\nsurprisingness, 291--292\n\nAT&T, 285\nattribute selection, 43, 49--67, 56--62, 332\nattributes, 46\nfinding, 43\nheterogeneous, 155, 157\nvariable features vs., 46\n\nAudubon Society Field Guide to North American Mushrooms, 57\n\nautomatic decision-making, 7\naverage customers, profitable customers vs., 40\n\nB\nbag of words approach, 252\nbags, 252\nbase rates, 97, 115, 190\nbaseline classifiers, 243\nbaseline methods, of data science, 248\nBasie, Count, 259\nBayes rate, 307\nBayes, Thomas, 238\nBayesian methods, 238, 248\nBayes’ Rule, 237--245\nbeer and lottery example, 292--293\nBeethoven, Ludwig van, 246\nbeginning cross-validation, 127\nbehavior description, 22\nBeing John Malkovich (film), 305\nBellkors Pragmatic Chaos (Netflix Challenge\n\nteam), 303\n\nbenefit improvement, calculating, 203\nbenefits\n\nand underlying profit calculation, 214\ndata-driven decision-making, 5\nestimating, 199\nin budgeting, 210\n\n368\n\n|\n\nIndex\n\nnearest-neighbor methods, 156\n\nbi-grams, 263\nbias errors, ensemble methods and, 306--309\nBig Data\n\ndata science and, 7--8\nevolution of, 8--9\non Amazon and Google, 314\n\nbig data technologies, 8\n\nstate of, 8\nutilizing, 8\n\nBig Red proposal example, 325--327\nBing, 250, 251\nBlack-Sholes model, 44\nblog postings, 250\nblog posts, 234\nBorders (book retailer), 316\nbreast cancer example, 102--105\nBrooks, David, 338\nbrowser cookies, 234\nBrubeck, Dave, 259\nBruichladdich single malt scotch, 178\nBrynjolfsson, Erik, 5, 8\nbudget, 210\nbudget constraints, 213\nbuilding modeling labs, 127\nbuilding models, 25, 28, 127\nBunnahabhain single malt whiskey, 145, 168\nbusiness news stories example, 174--177\nbusiness problems\n\nchanging definition of, to fit available data,\n\n337--338\n\ndata exploration vs., 182--184\nengineering problems vs., 289\nevaluating in a proposal, 324\nexpected value framework, structuring with,\n\n281--283\n\nexploratory data mining vs., 332\nunique context of, 340\nusing expected values to provide framework\n\nfor, 278--280\nbusiness strategy, 313--329\n\naccepting creative ideas, 324\ncase studies, examining, 323\ncompetitive advantages, 315--316, 316--321\ndata scientists, evaluating, 318--320\nevaluating proposals, 324--327\nhistorical advantages and, 317\nintangible collateral assets and, 318\nintellectual property and, 317", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00393", "page_num": 393, "segment": "managing data scientists effectively, 320--321\nmaturity of the data science, 327--329\nthinking data-analytically for, 313--315\n\nregression and, 21\nsupervised data mining and, 25\n\nclassification accuracy\n\nC\nCaesars Entertainment, 11\ncall center example, 297--299\nCapability Maturity Model, 328\nCapital One, 11, 286\nCase-Based Reasoning, 151\ncases\n\ncreating, 32\nranking vs. classifying, 209--231\n\ncasual modeling, 23\ncausal analysis, 284\ncausal explanation, 309\ncausal radius, 267\ncausation, correlation vs., 177\ncellular churn example\n\nunbalanced classes in, 190\nunequal costs and benefits in, 193\nCensus Bureau Economic Survey, 36\ncentroid locations, 172\ncentroid-based clustering, 174\ncentroids, 169--174, 174--177\ncharacteristics, 41\ncharacterizing customers, 41\nchurn, 4, 14, 191\n\nand expected value, 197\nfinding variables, 15\nperformance analytics for modeling, 223--\n\n231\n\nchurn prediction, 315\nCiccarelli, Francesca, 167\nclass confusion, 189\nclass labels, 101--102\nclass membership, estimating likelihood of, 235\nclass priors, 201, 214, 219, 222\nclass probability, 2, 21, 96--105, 306\nclasses\n\nexhaustive, 242\nmutually exclusive, 242\nprobability of evidence given, 241\nseparating, 123\nclassification, 2, 20, 141\nBayes’ Rule for, 239\nbuilding models for, 28\nensemble methods and, 306\nneighbors and, 147\n\nconfusion matrix, 189--190\nevaluating, with expected values, 196--198\nmeasurability of, 189\nunbalanced classes, 190--192\nunequal costs/benefit ratios, 193--193\n\nclassification function, 85\nclassification modeling, 193\nclassification tasks, 21\nclassification trees, 63\n\nas sets of rules, 71--71\nensemble methods and, 309\nin KDD Cup churn problem, 224--231\ninducing, 67\nlogistic regression and, 129\npredictive models and, 63\nvisualizing, 67--69\nclassifier accuracy, 189\nclassifiers\n\nand ROC graphs, 216--217\nbaseline, 243\nconfusion matrix produced by, 210--211\nconservative, 216\ncumulative response curves of, 220--221\ndiscrete (binary), 217\ninability to obtain accurate probability estimates from, 210\n\nlift of, 220\nlinear, 84\nNaive Bayes, 241\noperating conditions of, 219\nperformance de-coupled from conditions\n\nfor, 218\npermissive, 217\nplus thresholds, 210\nrandom, 213\nscores given to instances by, 210\nclassifying cases, ranking vs., 209--211\nclimatology, 205\nclipping dendrograms, 165\ncloud labor, 344\nclumps of instances, 119\ncluster centers, 169\ncluster distortion, 172\nclustering, 21, 163--182, 249\n\nalgorithm, 169\nbusiness news stories example, 174--177\n\nIndex\n\n|\n\n369", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00394", "page_num": 394, "segment": "centroid-based, 174\ncreating, 165\ndata preparation for, 174--175\nhierarchical, 164--169\nindicating, 164\ninterpreting results of, 177--179\nnearest neighbors and, 169--174\nprofiling and, 297\nsoft, 301\nsupervised learning and, 179--182\nwhiskey example, 163--165\n\nclusters, 141, 178\nco-occurrence grouping, 21--22, 290--296\nbeer and lottery example, 292--293\neWatch/eBracelet example, 290--291\nmarket basket analysis, 293--296\nsurprisingness, 291--292\n\nCoelho, Paul, 246\ncognition, 40\nColtrane, John, 259\ncombining functions, 147, 161--163\ncommon tasks, 19--23, 19\ncommunication, between scientists and business\n\npeople, 320, 333\n\ncompany culture, as intangible asset, 318\ncomparisons, multiple, 139--139\ncomplex functions, 118, 123\ncomplexity, 131\ncomplexity control, 133--138, 136\nensemble method and, 308\nnearest-neighbor reasoning and, 151--153\n\ncomplications, 50\ncomprehensibility, of models, 31\ncomputing errors, 95\ncomputing likelihood, 101\nconditional independence\nand Bayes’ Rule, 238\nunconditional vs., 241\nconditional probability, 236\nconditioning bar, 236\nconfidence, in association mining, 291\nconfusion matrix\n\nand points in ROC space, 217\nevaluating models with, 189--190\nexpected value corresponding to, 212\nproduced by classifiers, 210--211\ntrue positive and false negative rates for, 215\n\nconstraints\n\nbudget, 213\n\n370\n\n|\n\nIndex\n\nworkforce, 214\n\nconsumer movie-viewing preferences example,\n\n302\n\nconsumer voice, 9\nconsumers, describing, 234--235\ncontent pieces, online consumer targeting based\n\non, 234\n\ncontext, importance of, 251\ncontrol group, evaluating data models with, 326\nconverting data, 30\ncookies, browser, 234\ncorpus, 251\ncorrelations, 20, 37\n\ncausation vs., 177\ngeneral-purpose meaning, 37\nspecific technical meaning, 37\n\ncosine distance, 159, 160\ncosine similarity, 159\nCosine Similarity function, 259\ncost matrix, 212\ncost-benefit matrix, 199, 200, 203\ncosts\n\nand underlying profit calculation, 214\nestimating, 199\nin budgeting, 210\nof data, 28\n\ncounterfactual analysis, 23\nCray Computer Corporation, 270\ncredit-card transactions, 29, 296\ncreditworthiness model, as example of selection\n\nbias, 280\nCRISP cycle, 34\n\napproaches and, 34\nstrategy and, 34\nCRISP-DM, 14, 26\nCross Industry Standard Process for Data Mining (CRISP), 14, 26--34, 26\nbusiness understanding, 27--28\ndata preparation, 29--30\ndata understanding, 28--29\ndeployment, 32--34\nevaluation, 31--32\nmodeling, 31\nsoftware development cycle vs., 34--35\n\ncross-validation, 126, 140\n\nbeginning, 127\ndatasets and, 126\nnested, 135\noverfitting and, 126--129", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00395", "page_num": 395, "segment": "cumulative response curves, 219--222\ncurse of dimensionality, 155\ncustomer churn example\n\nanalytic engineering example, 281--287\nand data firm maturity, 329\ncustomer churn, predicting, 4\n\nwith cross-validation, 129--129\nwith tree induction, 73--78\n\ncustomer retention, 4\ncustomers, characterizing, 41\n\nData Mining (field), 40\ndata mining algorithms, 20\ndata mining proposal example, 325--327\ndata preparation, 30, 249\ndata preprocessing, 270--271\ndata processing technologies, 7\ndata processing, data science vs., 7--8\ndata reduction, 22--23, 302--306\ndata requirements, 29\ndata science, 1--17, 313--329, 331--345\n\nD\ndata\n\nas a strategic asset, 11\nconverting, 30\ncost, 28\nholdout, 113\ninvestment in, 286\nlabeled, 48\nobjective truth vs., 339\nobtaining, 286\ntraining, 45, 48\ndata analysis, 4, 20\ndata exploration, 182--184\ndata landscape, 166\ndata mining, 19--42\n\nand Bayes’ Rule, 240\napplying, 40--41, 48\nas strategic component, 12\nCRISP codification of, 26--34\ndata science and, 2, 14--15\ndomain knowledge and, 156\nearly stages, 25\nfundamental ideas, 62\nimplementing techniques, 8\nimportant distinctions, 25\nmatching analytic techniques to problems,\n\n35--41\n\nprocess of, 26--34\nresults of, 25--26, 32\nskills, 35\nsoftware development cycle vs., 34--35\nstages, 14\nstructuring projects, 19\nsupervised vs. unsupervised methods of, 24--\n\n25\nsystems, 33\ntasks, fitting business problems to, 19--23, 19\ntechniques, 33\n\nand adding value to applications, 187\nas craft, 319\nas strategic asset, 9--12\nbaseline methods of, 248\nbehavior predictions based on past actions, 3\nBig Data and, 7--8\ncase studies, examining, 323\nclassification modeling for issues in, 193\ncloud labor and, 343--344\ncustomer churn, predicting, 4\ndata mining about individuals, 341--342\ndata mining and, 2, 14--15\ndata processing vs., 7--8\ndata science engineers, 34\ndata-analytic thinking in, 12--13\ndata-driven business vs., 7\ndata-driven decision-making, 4--7\nengineering, 4--7\nengineering and, 15\nevolving uses for, 8--9\nfitting problem to available data, 337--338\nfundamental principles, 2\nhistory, 39\nhuman interaction and, 338--341\nhuman knowledge and, 338--341\nHurricane Frances example, 3\nlearning path for, 319\nlimits of, 338--341\nmining mobile device data example, 334--\n\n337\n\nopportunities for, 1--3\nprinciples, 4, 19\nprivacy and ethics of, 341--342\nprocesses, 4\nsoftware development vs., 328\nstructure, 39\ntechniques, 4\ntechnology vs. theory of, 15--16\nunderstanding, 2, 7\n\nIndex\n\n|\n\n371", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00396", "page_num": 396, "segment": "data science maturity, of firms, 327--329\ndata scientists\n\nacademic, 322\nas scientific advisors, 322\nattracting/nurturing, 321--323\nevaluating, 318--320\nmanaging, 320--321\nData Scientists, LLC, 323\ndata sources, 206\ndata understanding, 28--29\n\nexpected value decomposition and, 284--287\nexpected value framework and, 281--283\n\ndata warehousing, 38\ndata-analytic thinking, 12--13\n\nand unbalanced classes, 190\nfor business strategies, 313--315\n\ndata-driven business\ndata science vs., 7\nunderstanding, 7\n\ndifferential descriptions, 182\nDigital 100 companies, 12\nDillman, Linda, 6\ndimensionality, of nearest-neighbor reasoning,\n\n155--156\n\ndirected marketing example, 278--281\ndiscoveries, 6\ndiscrete (binary) classifiers, 217\ndiscrete classifiers, 215\ndiscretized numeric variables, 56\ndiscriminants, linear, 85\ndiscriminative modeling methods, generative\n\nvs., 247\n\ndisorder, measuring, 51\ndisplay advertising, 233\ndistance functions, for nearest-neighbor reasoning, 158--161\n\ndistance, measuring, 143\ndistribution\n\ndata-driven causal explanations, 309--310\ndata-driven decision-making, 4--7\n\nGaussian, 95\nNormal, 95\n\nbenefits, 5\ndiscoveries, 6\nrepetition, 6\n\ndatabase queries, as analytic technique, 37--38\ndatabase tables, 47\ndataset entropy, 58\ndatasets, 47\n\nanalyzing, 44\nattributes of, 119\ncross-validation and, 126\nlimited, 126\n\nDavis, Miles, 257, 259\nDeanston single malt scotch, 178\ndecision boundaries, 69, 83\ndecision lines, 69\ndecision nodes, 63\ndecision stumps, 206\ndecision surfaces, 69\ndecision trees, 63\ndecision-making, automatic, 7\ndeduction, induction vs., 47\nDell, 174, 315\ndemand, local, 3\ndendrograms, 164, 165\ndependent variables, 47\ndescriptive attributes, 15\ndescriptive modeling, 46\nDictionary of Distances (Deza & Deza), 158\n\n372\n\n|\n\nIndex\n\ndistribution of properties, 56\nDoctor Who (television show), 246\ndocument (term), 251\ndomain knowledge\n\ndata mining processes and, 156\nnearest-neighbor reasoning and, 155--156\n\ndomain knowledge validation, 296\ndomains, in association discovery, 296\nDotcom Boom, 273, 317\ndouble counting, 203\ndraws, statistical, 102\n\nE\nedit distance, 160, 161\nEinstein, Albert, 331\nElder Research, 322\nEllington, Duke, 257, 259\nemail, 250\nengineering, 15, 28\nengineering problems, business problems vs.,\n\n289\n\nensemble method, 306--309\nentropy, 49--56, 51, 58, 78\n\nand Inverse Document Frequency, 261\nchange in, 52\nequation for, 51\ngraphs, 58", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00397", "page_num": 397, "segment": "equations\n\ncosine distance, 159\nentropy, 51\nEuclidean distance, 144\ngeneral linear model, 85\ninformation gain (IG), 53\nJaccard distance, 159\nL2 norm, 158\nlog-odds linear function, 99\nlogistic function, 100\nmajority scoring function, 161\nmajority vote classification, 161\nManhattan distance, 158\nsimilarity-moderated classification, 162\nsimilarity-moderated regression, 162\nsimilarity-moderated scoring, 162\n\nerror costs, 219\nerror rates, 189, 198\nerrors\n\nabsolute, 95\ncomputing, 95\nfalse negative vs. false positive, 189\nsquared, 94\n\nestimating generalization performance, 126\nestimation, frequency based, 72\nethics of data mining, 341--342\nEuclid, 143\nEuclidean distance, 144\nevaluating models, 187--208\n\nbaseline performance and, 204--207\nclassification accuracy, 189--194\nconfusion matrix, 189--190\nexpected values, 194--204\ngeneralization methods for, 193--194\nprocedure, 327\n\nevaluating training data, 113\nevaluation\n\nin vivo, 32\npurpose, 31\n\nevaluation framework, 32\nevents\n\ncalculating probability of, 236--236\nindependent, 236--237\n\nevidence\n\ncomputing probability from, 238, 239\ndetermining strength of, 235\nlikelihood of, 240\nstrongly dependent, 243\n\nevidence lift\n\nFacebook “Likes” example, 245--247\nmodeling, with Naive Bayes, 244--245\n\neWatch/eBracelet example, 290--291\nexamining clusters, 178\nexamples, 46\n\nanalytic engineering, 278--287\nassociations, 293--296\nbeer and lottery association, 292--293\nbiases in data, 339\nBig Red proposal, 325--327\nbreast cancer, 102--105\nbusiness news stories, 174--177\ncall center metrics, 297--299\ncellular churn, 190, 193\ncentroid-based clustering, 169--174\ncloud labor, 343--344\nclustering, 163--182\nconsumer movie-viewing preferences, 302\ncooccurrence/association, 290--291, 292--293\ncross-validation, 126--129\ncustomer churn, 4, 73--78, 126--129, 329\ndata mining proposal evaluation, 325--327\ndata-driven causal explanations, 309--310\ndetecting credit-card fraud, 296\ndirected marketing, 278--281\nevaluating proposals, 351--353\nevidence lift, 245--247\neWatch/eBracelet, 290--291\nFacebook “Likes”, 245--247, 293--296\nGreen Giant Consulting, 351--353\nHurricane Frances, 3\ninformation gain, attribute selection with,\n\n56--62\n\niris overfitting, 88, 119--123\nJazz musicians, 256--260\njunk email classifier, 243\nmarket basket analysis, 293--296\nmining linear discriminants from data, 88--\n\n108\n\nmining mobile device data, 334--337\nmining news stories, 266--274\nmushroom, 56--62\nNaive Bayes, 247\nnearest-neighbor reasoning, 144--146\noverfitting linear functions, 119--123\noverfitting, performance degradation and,\n\n124--126\nPEC, 233--235\n\nIndex\n\n|\n\n373", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00398", "page_num": 398, "segment": "profiling, 296, 297--299\nstock price movement, 266--274\nsupervised learning to generate cluster descriptions, 179--182\n\ntargeted ad, 233--235, 247, 341\ntext representation tasks, 256--260, 266--274\ntree induction vs. logistic regression, 102--\n\n105\n\nviral marketing, 309--310\nwhiskey analytics, 144--146\nwhiskey clustering, 163--165\nWhiz-bang widget, 325--327\nwireless fraud, 339\nexhaustive classes, 242\nexpected profit, 212--214\n\nand relative levels of costs and benefits, 214\ncalculation of, 198\nfor classifiers, 193\nuncertainty of, 215\n\nexpected value\n\ncalculation of, 263\ngeneral form, 194\nin aggregate, 197\nnegative, 210\n\nexpected value framework, 332\n\nproviding structure for business problem/\n\nsolutions with, 278--280\n\nstructuring complicated business problems\n\nwith, 281--283\nexpected values, 194--204\n\ncost-benefit matrix and, 198--204\ndecomposition of, moving to data science\n\nsolution with, 284--287\n\nerror rates and, 198\nframing classifier evaluation with, 196--198\nframing classifier use with, 195--196\n\nexplanatory variables, 47\nexploratory data mining vs. defined problems,\n\n332\n\nextract patterns, 14\n\nF\nFacebook, 11, 250, 315\n\nonline consumer targeting by, 234\n“Likes“ example, 245--247\n\nFairbanks, Richard, 9\nfalse alarm rate, 216, 217\nfalse negative rate, 203\nfalse negatives, 189, 190, 193, 200\n\n374\n\n|\n\nIndex\n\nfalse positive rate, 203, 216--219\nfalse positives, 189, 190, 193, 199\nfeature vectors, 46\nfeatures, 46, 47\nFederer, Roger, 246\nFettercairn single malt scotch, 178\nFight Club, 246\nfinancial markets, 266\nfirmographic data, 21\nfirst-layer models, 107\nfitting, 101, 113--115, 126, 131, 140, 225--226\nfolds, 127, 129\nfraud detection, 29, 214, 315\nfree Web services, 233\nfrequency, 254\nfrequency-based estimates, 72, 73\nfunctions\n\nadding variables to, 123\nclassification, 85\ncombining, 147\ncomplex, 118, 123\nkernel, 106\nlinkage, 166\nlog-odds, 99\nlogistic, 100\nloss, 94--95\nobjective, 108\nfundamental ideas, 62\nfundamental principles, 2\n\nG\nGaussian distribution, 95, 297\nGaussian Mixture Model (GMM), 300\nGE Capital, 184\ngeneralization, 116, 332\nmean of, 126, 140\noverfitting and, 111--112\nvariance of, 126, 140\n\ngeneralization performance, 113, 126\ngeneralizations, incorrect, 124\ngenerative modeling methods, discriminative\n\nvs., 247\n\ngenerative questions, 240\ngeometric interpretation, nearest-neighbor reasoning and, 151--153\n\nGillespie, Dizzie, 259\nGini Coefficient, 219\nGlen Albyn single malt scotch, 180\nGlen Grant single malt scotch, 180", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00399", "page_num": 399, "segment": "Glen Mhor single malt scotch, 178\nGlen Spey single malt scotch, 178\nGlenfiddich single malt scotch, 178\nGlenglassaugh single malt whiskey, 168\nGlengoyne single malt scotch, 180\nGlenlossie single malt scotch, 180\nGlentauchers single malt scotch, 178\nGlenugie single malt scotch, 178\ngoals, 87\nGoethe, Johann Wolfgang von, 1\nGoodman, Benny, 259\nGoogle, 250, 251, 321\n\nPrediction API, 314\nsearch advertising on, 233\n\nGoogle Finance, 268\nGoogle Scholar, 343\nGraepel, Thore, 245--245\ngraphical user interface (GUI), 37\ngraphs\n\nentropy, 58\nfitting, 126, 140\n\nGreen Giant Consulting example, 351--353\nGUI, 37\n\nH\nHaimowitz, Ira, 184\nHarrahs casinos, 7, 11\nhashing methods, 157\nheterogeneous attributes, 155\nHewlett-Packard, 141, 174, 264\nhierarchical clustering, 164--169\nHilton, Perez, 270\nhinge loss, 93, 94\nhistory, 39\nhit rate, 216, 220\nholdout data, 113\ncreating, 113\noverfitting and, 113--115\n\nholdout evaluations, of overfitting, 126\nholdout testing, 126\nhomogenous regions, 83\nhomographs, 251\nHow I Met Your Mother (television show), 246\nHowls Moving Castle, 246\nhuman interaction and data science, 338--341\nHurricane Frances example, 3\nhyperplanes, 69, 85\nhypotheses, computing probability of, 238\nhypothesis generation, 37\n\nhypothesis tests, 133\n\nI\nIBM, 141, 178, 321, 322\nIEEE International Conference on Data Mining,\n\n342\n\nimmature data firms, 328\nimpurity, 50\nin vivo evaluation, 32\nin-sample accuracy, 114\nInception (film), 246\nincorrect generalizations, 124\nincremental learning, 243\nindependence\n\nand evidence lift, 245\nin probability, 236--237\nunconditional vs. conditional, 241\n\nindependent events, probability of, 236--237\nindependent variables, 47\nindices, 173\ninduction, deduction vs., 47\ninferring missing values, 30\ninfluence, 23\ninformation\n\njudging, 48\nmeasuring, 52\n\ninformation gain (IG), 51, 78, 273\n\napplying, 56--62\nattribute selection with, 56--62\ndefining, 52\nequation for, 53\nusing, 57\n\nInformation Retrieval (IR), 251\ninformation triage, 274\ninformative attributes, finding, 44, 62\ninformative meaning, 43\ninformative variables, selecting, 49\ninstance scoring, 188\ninstances, 46\n\nclumping, 119\ncomparing, with evidence lift, 245\nfor targeting online consumers, 234\n\nintangible collateral assets, 318\nintellectual property, 317\nintelligence test score, 246--247\nintelligent methods, 44\nintelligibility, 180\nInternet, 250\n\nIndex\n\n|\n\n375", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00400", "page_num": 400, "segment": "inverse document frequency (IDF), 254--255\n\nlatent information, 302--306\n\nand entropy, 261--275\nin TFIDF, 256\nterm frequency, combining with, 256\ninvestments in data, evaluating, 204--207\niPhone, 176, 285\nIQ, evidence lifts for, 246--247\niris example\n\nfor overfitting linear functions, 119--123\nmining linear discriminants from data, 88--\n\n108\niTunes, 22, 177\n\nJ\nJaccard distance (equation), 159\nJackson, Michael, 145\nJazz musicians example, 256--260\nJobs, Steve, 175, 341\njoint probability, 236--237\njudging information, 48\njudgments, 142\njunk email classifier example, 243\njustifying decisions, 154\n\nK\nk-means algorithm, 169, 171\nKDD Cup, 318\nkernel function, 106\nkernels, polynomial, 106\nKerouac, Jack, 254\nKnowledge Discovery and Data Mining (KDD),\n\n40\nanalytic techniques for, 39--40\ndata mining competition of 2009, 223--231\n\nknowledge extraction, 333\nKosinski, Michal, 245--245\n\nL\nL2 norm (equation), 158\nlabeled data, 48\nlabels, 24\nLadyburn single malt scotch, 178\nLaphroaig single malt scotch, 178\nLapointe, François-Joseph, 145, 168, 178\nLatent Dirichlet Allocation, 265\n\n376\n\n|\n\nIndex\n\nconsumer movie-viewing preferences example, 302\n\nweighted scoring, 305\nlatent information model, 266\nLatent Semantic Indexing, 265\nlearning\n\nincremental, 243\nmachine, 39--40\nparameter, 81\nsupervised, 24, 179--182\nunsupervised, 24\nlearning curves, 126, 140\nanalytical use, 132\nfitting graphs and, 131\nlogistic regression, 131\noverfitting vs., 130--132\ntree induction, 131\n\nleast squares regression, 95, 96\nLegendre, Pierre, 145, 168, 178\nLevenshtein distance, 160\nleverage, 291--292\nLie to Me (television show), 246\nlift, 244, 291--292, 333\nlift curves, 219--222, 228--229\nlikelihood, computing, 101\nlikely responders, 195\nLikes, Facebook, 234\nlimited datasets, 126\nlinear boundaries, 122\nlinear classifiers, 83, 84\n\nlinear discriminant functions and, 85--87\nobjective functions, optimizing, 87\nparametric modeling and, 83\nsupport vector machines, 91--93\n\nlinear discriminants, 85\nfunctions for, 85--87\nmining, from data, 88--93\nscoring/ranking instances of, 90\nsupport vector machines and, 91--93\nlinear estimation, logistic regression and, 98\nlinear models, 82\nlinear regression, standard, 95\nlinguistic structure, 250\nlink prediction, 22, 301--302\nlinkage functions, 166\nLinkwood single malt scotch, 180\nlocal demand, 3", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00401", "page_num": 401, "segment": "location visitation behavior of mobile devices,\n\n336\n\nlog-normal distribution, 299\nlog-odds, 98\nlog-odds linear function, 99\nlogistic function, 100\nlogistic regression, 87, 96--105, 119\nbreast cancer example, 102--105\nclassification trees and, 129\nin KDD Cup churn problem, 224--231\nlearning curves for, 131\nlinear estimation and, 98\nmathematics of, 99--102\ntree induction vs., 102--105\nunderstanding, 97\nLord Of The Rings, 246\nloss functions, 94--95\nLost (television series), 246\n\nM\nmachine learning\n\nanalytic techniques for, 39--40\nmethods, 39\nMagnum Opus, 294\nmajority classifiers, 205\nmajority scoring function (equation), 161\nmajority vote classification (equation), 161\nmajority voting, 149\nManhattan distance (equation), 158\nMann-Whitney-Wilcoxon measure, 219\nmargin-maximizing boundary, 92\nmargins, 91\nmarket basket analysis, 293--296\nMassachusetts Institute of Technology (MIT), 5,\n\n341\n\nmathematical functions, overfitting in, 118--119\nmatrix factorization, 306\nmaximizing objective functions, 136\nmaximizing the margin, 92\nmaximum likelihood model, 297\nMcCarthy, Cormac, 254\nMcKinsey and Company, 13\nmean generalization, 126, 140\nMechanical Turk, 343\nMedicare fraud, detecting, 29\nMichael Jackson’s Malt Whisky Companion\n\n(Jackson), 145\n\nmicro-outsourcing, 343\nMicrosoft, 253, 321\n\nMingus, Charles, 259\nmissing values, 30\nmobile devices\n\nlocation of, finding, 334\nmining data from, 334--337\n\nmodel accuracy, 114\nmodel building, test data and, 134\nmodel evaluation and classification, 190\nmodel induction, 47\nmodel intelligibility, 154\nmodel performance, visualizing, 209--231\n\narea under ROC curves, 219\ncumulative response curves, 219--222\nlift curves, 219--222\nprofit curves, 212--214\nranking vs. classifying cases, 209--231\n\nmodel types, 44\n\nBlack-Sholes option pricing, 44\ndescriptive, 46\npredictive, 45\n\nmodelers, 118\nmodeling algorithms, 135, 326\nmodeling labs, 127\nmodels\n\ncomprehensibility, 31\ncreating, 47\nfirst-layer, 107\nfitting to data, 82, 332\nlinear, 82\nparameterizing, 81\nparameters, 81\nproblems, 72\nproducing, 127\nsecond-layer, 107\nstructure, 81\ntable, 112\nunderstanding types of, 67\nworsening, 124\n\nmodifiers (of words), 274\nMonk, Thelonius, 259\nMoonstruck (film), 305\nMorris, Nigel, 9\nmultiple comparisons, 139--139\nmultisets, 252\nmushroom example, 56--62\nmutually exclusive classes, 242\n\nN\nn-gram sequences, 263\n\nIndex\n\n|\n\n377", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00402", "page_num": 402, "segment": "Naive Bayes, 240--242\n\nadvantages/disadvantages of, 242--243\nconditional independence and, 240--245\nin KDD Cup churn problem, 224--231\nmodeling evidence lift with, 244--245\nperformance of, 243\ntargeted ad example of, 247\n\nNaive-Naive Bayes, 244--245\nnamed entity extraction, 264--264\nNASDAQ, 268\nNational Public Radio (NPR), 246\nnearest neighbors\n\ncentroids and, 169--174\nclustering and, 169--174\nensemble method as, 306\n\nnearest-neighbor methods\n\nbenefits of, 156\nin KDD Cup churn problem, 224--231\n\nnearest-neighbor reasoning, 144--163\n\ncalculating scores from neighbors, 161--163\nclassification, 147--148\ncombining functions, 161--163\ncomplexity control and, 151--153\ncomputational efficiency of, 156\ndetermining sample size, 149\ndimensionality of, 155--156\ndistance functions for, 158--161\ndomain knowledge and, 155--156\nfor predictive modeling, 146\ngeometric interpretation and, 151--153\nheterogeneous attributes and, 157\ninfluence of neighbors, determining, 149--\n\n151\n\nintelligibility of, 154--155\noverfitting and, 151--153\nperformance of, 156\nprobability estimation, 148\nregression, 148\nwhiskey analytics, 144--146\n\nnegative profit, 212\nnegatives, 188\nneighbor retrieval, speeding up, 157\nneighbors\n\nclassification and, 147\nretrieving, 149\nusing, 149\n\nnested cross-validation, 135\nNetflix, 7, 142, 303\nNetflix Challenge, 302--306, 318\n\n378\n\n|\n\nIndex\n\nneural networks, 106, 107\n\nparametric modeling and, 105--108\nusing, 107\n\nNew York Stock Exchange, 268\nNew York University (NYU), 8\nNissenbaum, Helen, 342\nnon-linear support vector machines, 91, 106\nNormal distribution, 95, 297\nnormalization, 253\nNorth Port single malt scotch, 180\nnot likely responders, 195\nnot-spam (target class), 235\nnumbers, 253\nnumeric variables, 56\nnumerical predictions, 25\n\nO\nOakland Raiders, 264\nobjective functions, 108\nadvantages, 96\ncreating, 87\ndrawbacks, 96\nmaximizing, 136\noptimizing, 87\n\nobjectives, 87\nodds, 97, 98\noDesk, 343\nOn the Road (Kerouac), 254\nOn-line Analytical Processing (OLAP), 38\non-line processing, 38\nOne Manga, 246\nOrange (French Telecom company), 223\noutliers, 166\nover the wall transfers, 34\noverfitting, 15, 73, 111--139, 332\n\nand tree induction, 116--118, 133\nassessing, 113\navoiding, 113, 119, 133--138\ncomplexity control, 133--138\ncross-validation example, 126--129\nensemble method and, 308\nfitting graphs and, 113--115\ngeneral methodology for avoiding, 134--136\ngeneralization and, 111--112\nholdout data and, 113--115\nholdout evaluations of, 126\nin mathematical functions, 118--119\nlearning curves vs., 130--132\nlinear functions, 119--123", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00403", "page_num": 403, "segment": "nearest-neighbor reasoning and, 151--153\nparameter optimization and, 136--138\nperformance degradation and, 124--126\ntechniques for avoiding, 126\n\nP\nparabola, 105, 123\nparameter learning, 81\nparameterized models, 81\nparameterized numeric functions, 299\nparametric modeling, 81\n\nclass probability estimation, 96--105\nlinear classifiers, 83\nlinear regression and, 94--96\nlogistic regression, 96--105\nneural networks and, 105--108\nnon-linear functions for, 105--108\nsupport vector machines and, 105--108\n\nParker, Charlie, 257, 259\nPasteur, Louis, 314\npatents, as intellectual property, 317\npatterns\n\nextract, 14\nfinding, 25\npenalties, 137\nperformance analytics, for modeling churn,\n\n223--231\n\nperformance degradation, 124--126\nperformance, of nearest-neighbor reasoning,\n\n156\n\nphrase extraction, 264\npilot studies, 353\nplunge (stock prices), 267\npolynomial kernels, 106\npositives, 188\nposterior probability, 239--240\nPrecision metric, 203\nprediction, 6, 45\nPrediction API (Google), 314\npredictive learning methods, 180\npredictive modeling, 43--44, 81\nalternative methods, 81\nbasic concepts, 78\ncausal explanations and, 309\nclassification trees and, 67--71\ncustomer churn, predicting with tree induction, 73--78\n\nfocus, 48\ninduction and, 44--48\n\nlink prediction, 301--302\nnearest-neighbor reasoning for, 146\nparametric modeling and, 81\nprobability estimating and, 71--73\nsocial recommendations and, 301--302\nsupervised segmentation, 48--79\n\npredictors, 47\npreparation, 30\nprinciples, 4, 23\nprior beliefs, probability based on, 239\nprior churn, 14\nprior probability, class, 239\nprivacy and data mining, 341--342\nPrivacy in Context (Nissenbaum), 342\nprivacy protection, 341\nprobabilistic evidence combination (PEC), 233--\n\n248\nBayes’ Rule and, 237--245\nprobability theory for, 235--237\ntargeted ad example, 233--235\n\nProbabilistic Topic Models, 265\nprobability, 101--102\n\nand nearest-neighbor reasoning, 148\nbasic rule of, 201\nbuilding models for estimation of, 28\nconditional, 236\njoint, 236--237\nof errors, 198\nof evidence, 239\nof independent events, 236--237\nposterior, 239--240\nprior, 239\nunconditional, 238, 239\n\nprobability estimation trees, 64, 72\nprobability notation, 235--236\nprobability theory, 235--237\nprocesses, 4\nprofiling, 22, 296--301\n\nconsumer movie-viewing preferences example, 302\n\nwhen the distribution is not symmetric, 298\n\nprofit curves, 212--214, 229--230\nprofit, negative, 212\nprofitability, 40\nprofitable customers, average customers vs., 40\nproposals, evaluating, 324--327, 351--353\nproxy labels, 286\npsychometric data, 293\npublishing, 322\n\nIndex\n\n|\n\n379", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00404", "page_num": 404, "segment": "purity, 49--56\nPythagorean Theorem, 143\n\nQ\nqueries, 37\n\nabilities, 38\nformulating, 37\ntools, 38\nquerying, 37\nQuine, W. V. O., 339\n\nR\nRa, Sun, 259\nranking cases, classifying vs., 209--231\nranking variables, 48\nreasoning, 141\nRecall metric, 203\nReceiver Operating Characteristics (ROC)\n\ngraphs, 214--219\narea under ROC curves (AUC), 219\nin KDD Cup churn problem, 227--227\n\nrecommendations, 142\nReddit, 250\nregional distribution centers, grouping/associations and, 290\nregression, 20, 21, 141\n\nbuilding models for, 28\nclassification and, 21\nensemble methods and, 306\nleast squares, 95\nlogistic, 119\nridge, 138\nsupervised data mining and, 25\nsupervised segmentation and, 56\n\nregression modeling, 193\nregression trees, 64, 309\nregularization, 136, 140\nremoving missing values, 30\nrepetition, 6\nrequirements, 29\nresponders, likely vs. not likely, 195\nretrieving, 141\nretrieving neighbors, 149\nReuters news agency, 174\nridge regression, 138\nroot-mean-squared error, 194\n\n380\n\n|\n\nIndex\n\nS\nSaint Magdalene single malt scotch, 180\nScapa single malt scotch, 178\nSchwartz, Henry, 184\nscoring, 21\nsearch advertising, display vs., 233\nsearch engines, 250\nsecond-layer models, 107\nsegmentation\n\ncreating the best, 56\nsupervised, 163\nunsupervised, 182\n\nselecting\n\nattributes, 43\ninformative variables, 49\nvariables, 43\n\nselection bias, 280--281\nsemantic similarity, syntactic vs., 177\nseparating classes, 123\nsequential backward elimination, 135\nsequential forward selection (SFS), 135\nservice usage, 21\nsets, 252\nShannon, Claude, 51\nSheldon Cooper (fictional character), 246\nsign consistency, in cost-benefit matrix, 203\nSignet Bank, 9, 286\nSilver Lake, 253\nSilver, Nate, 205\nsimilarity, 141--182\napplying, 146\ncalculating, 332\nclustering, 163--177\ncosine, 159\ndata exploration vs. business problems and,\n\n182--184\n\ndistance and, 142--144\nheterogeneous attributes and, 157\nlink recommendation and, 301\nmeasuring, 143\nnearest-neighbor reasoning, 144--163\n\nsimilarity matching, 21\nsimilarity-moderated classification (equation),\n\n162\n\nsimilarity-moderated regression (equation), 162\nsimilarity-moderated scoring (equation), 162\nSimone, Nina, 259\nskew, 190\nSkype Global, 253", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00405", "page_num": 405, "segment": "smoothing, 73\nsocial recommendations, 301--302\nsoft clustering, 301\nsoftware development, 34\nsoftware engineering, data science vs., 328\nsoftware skills, analytic skills vs., 35\nSolove, Daniel, 342\nsolution paths, changing, 29\nspam (target class), 235\nspam detection systems, 235\nspecified class value, 26\nspecified target value, 26\nspeech recognition systems, 315\nspeeding up neighbor retrieval, 157\nSpirited Away, 246\nspreadsheet, implementation of Naive Bayes\n\nwith, 247\n\nspurious correlations, 124\nSQL, 37\nsquared errors, 94\nstable stock prices, 267\nstandard linear regression, 95\nStar Trek, 246\nStarbucks, 336\nstatistical draws, 102\nstatistics\n\ncalculating conditionally, 35\nfield of study, 36\nsummary, 35\nuses, 35\n\nstemming, 253, 257\nStillwell, David, 245\nstock market, 266\nstock price movement example, 266--274\nStoker (movie thriller), 254\nstopwords, 253, 254\nstrategic considerations, 9\nstrategy, 34\nstrength, in association mining, 291, 293\nstrongly dependent evidence, 243\nstructure, 39\nStructured Query Language (SQL), 37\nstructured thinking, 14\nstructuring, 28\nsubjective priors, 239\nsubtasks, 20\nsummary statistics, 35, 36\nSummit Technology, Inc., 269\nSun Ra, 259\n\nsupervised data, 43--44, 78\nsupervised data mining\nclassification, 25\nconditions, 24\nregression, 25\nsubclasses, 25\nunsupervised vs., 24--25\n\nsupervised learning\n\ngenerating cluster descriptions with, 179--\n\n182\n\nmethods of, 180\nterm, 24\n\nsupervised segmentation, 43--44, 48--67, 163\n\nattribute selection, 49--62\ncreating, 62\nentropy, 49--56\ninducing, 64\nperforming, 44\npurity of datasets, 49--56\nregression problems and, 56\ntree induction of, 64--67\ntree-structured models for, 62--64\n\nsupport vector machines, 87, 119\n\nlinear discriminants and, 91--93, 91\nnon-linear, 91, 106\nobjective function, 91\nparametric modeling and, 105--108\n\nsupport, in association mining, 293\nsurge (stock prices), 267\nsurprisingness, 291--292\nsynonyms, 251\nsyntactic similarity, semantic vs., 177\n\nT\ntable models, 112, 114\ntables, 47\nTambe, Prasanna, 8\nTamdhu single malt scotch, 180\nTarget, 6\ntarget variables, 47, 149\nestimating value, 56\nevaluating, 326\n\ntargeted ad example, 233--235\n\nof Naive Bayes, 247\nprivacy protection in Europe and, 341\ntargeting best prospects example, 278--281\ntasks/techniques, 4, 289--311\nassociations, 290--296\nbias, 306--309\n\nIndex\n\n|\n\n381", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00406", "page_num": 406, "segment": "classification, 21\nco-occurrence, 290--296\ndata reduction, 302--306\ndata-driven causal explanations, 309--310\nensemble method, 306--309\nlatent information, 302--306\nlink prediction, 301--302\nmarket basket analysis, 293--296\noverlap in, 39\nprinciples underlying, 23\nprofiling, 296--301\nsocial recommendations, 301--302\nvariance, 306--309\nviral marketing example, 309--310\n\nTatum, Art, 259\ntechnology\n\nanalytic, 29\napplying, 35\nbig-data, 8\ntheory in data science vs., 15--16\n\nterm frequency (TF), 252--254\n\ndefined, 252\nin TFIDF, 256\ninverse document frequency, combining\n\nwith, 256\nvalues for, 258\n\nterms\n\nin documents, 251\nsupervised learning, 24\nunsupervised learning, 24\nweights of, 265\n\nTerry, Clark, 259\ntest data, model building and, 134\ntest sets, 114\ntesting, holdout, 126\ntext, 249\n\nas unstructured data, 250--251\ndata, 249\nfields, varying number of words in, 250\nimportance of, 250\nJazz musicians example, 256--260\nrelative dirtiness of, 250\ntext processing, 249\ntext representation task, 251--256\n\ntext representation task, 251--256\nbag of words approach to, 252\ndata preparation, 268--270\ndata preprocessing, 270--271\ndefining, 266--268\n\n382\n\n|\n\nIndex\n\ninverse document frequency, 254--255\nJazz musicians example, 256--260\nlocation mining as, 336\nmeasuring prevalence in, 252--254\nmeasuring sparseness in, 254--255\nmining news stories example, 266--274\nn-gram sequence approach to, 263\nnamed entity extraction, 264--264\nresults, interpreting, 271--274\nstock price movement example, 266--274\nterm frequency, 252--254\nTFIDF value and, 256\ntopic models for, 264--266\nTFIDF scores (TFIDF values), 174\n\napplied to locations, 336\ntext representation task and, 256\n\nThe Big Bang Theory, 246\nThe Colbert Report, 246\nThe Daily Show, 246\nThe Godfather, 246\nThe New York Times, 3, 338\nThe Onion, 246\nThe Road (McCarthy), 254\nThe Signal and the Noise (Silver), 205\nThe Sound of Music (film), 305\nThe Stoker (film comedy), 254\nThe Wizard of Oz (film), 305\nThomson Reuters Text Research Collection\n\n(TRC2), 174\n\nthresholds\n\nand classifiers, 210--211\nand performance curves, 212\n\ntime series (data), 268\nTobermory single malt scotch, 178\ntokens, 251\ntools, analytic, 113\ntopic layer, 264\ntopic models for text representation, 264--266\ntrade secrets, 317\ntraining data, 45, 48, 113\nevaluating, 113, 326\nlimits on, 308\nusing, 126, 131, 140\n\ntraining sets, 114\ntransfers, over the wall, 34\ntree induction, 44\n\nensemble methods and, 309\nlearning curves for, 131\nlimiting, 133", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00407", "page_num": 407, "segment": "logistic regression vs., 102--105\nof supervised segmentation, 64--67\noverfitting and, 116--118, 133--134\nproblems with, 133\n\nTree of Life (Sugden et al; Pennisi), 166\ntree-structured models\nclassification, 63\ncreating, 64\ndecision, 63\nfor supervised segmentation, 62--64\ngoals, 64\nprobability estimation, 64, 72\npruning, 134\nregression, 64\nrestricting, 118\n\ntri-grams, 263\nTron, 246\ntrue negative rate, 203\ntrue negatives, 200\ntrue positive rate, 203, 216--217, 221\ntrue positives, 200\nTullibardine single malt whiskey, 168\nTumblr, online consumer targeting by, 234\nTwitter, 250\nTwo Dogmas of Empiricism (Quine), 339\n\nU\nUCI Dataset Repository, 88--93\nunconditional independence, conditional vs.,\n\n241\n\nunconditional probability\n\nof hypothesis and evidence, 238\nprior probability based on, 239\n\nunique context, of strategic decisions, 340\nUniversity of California at Irvine, 57, 103\nUniversity of Montréal, 145\nUniversity of Toronto, 341\nunstructured data, 250\nunstructured data, text as, 250--251\nunsupervised learning, 24\nunsupervised methods of data mining, supervised vs., 24--25\n\nunsupervised problems, 184\nunsupervised segmentation, 182\nuser-generated content, 250\n\nV\nvalue (worth), adding, to applications, 187\n\nvalue estimation, 21\nvariables\n\ndependent, 47\nexplanatory, 47\nfinding, 15, 43\nindependent, 47\ninformative, 49\nnumeric, 56\nranking, 48\nrelationship between, 46\nselecting, 43\ntarget, 47, 56, 149\n\nvariance, 56\n\nerrors, ensemble methods and, 306--309\ngeneralization, 126, 140\n\nviral marketing example, 309--310\nvisualizations, calculations vs., 209\nVolinsky, Chris, 304\n\nW\nWal-Mart, 1, 3, 6\nWaller, Fats, 259\nWang, Wally, 246, 294\nWashington Square Park, 336\nweather forecasting, 205\nWeb 2.0, 250\nweb pages, personal, 250\nweb properties, as content pieces, 234\nWeb services, free, 233\nWeeds (television series), 246\nweighted scoring, 150, 305\nweighted voting, 149\nWhat Data Cant Do (Brooks), 338\nwhiskey example\n\nclustering and, 163--165\nfor nearest-neighbors, 144--146\nsupervised learning to generate cluster descriptions, 179--182\n\nWhiz-bang example, 325--327\nWikileaks, 246\nwireless fraud example, 339\nWisconsin Breast Cancer Dataset, 103\nwords\n\nlengths of, 250\nmodifiers of, 274\nsequences of, 263\nworkforce constraint, 214\nworksheets, 47\nworsening models, 124\n\nIndex\n\n|\n\n383", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00408", "page_num": 408, "segment": "Y\nYahoo! Finance, 268\nYahoo!, online consumer targeting by, 234\n\nZ\nzero-one loss, 94\n\n384\n\n|\n\nIndex", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"segment_id": "00409", "page_num": 409, "segment": "About the Authors\nFoster Provost is Professor and NEC Faculty Fellow at the NYU Stern School of Business\nwhere he teaches in the Business Analytics, Data Science, and MBA programs. His\naward-winning research is read and cited broadly. Prior to joining NYU, he worked as\na research data scientist for five years for what’s now Verizon. Over the past decade,\nProfessor Provost has co-founded several successful data-science-driven companies.\n\nTom Fawcett holds a Ph.D. in machine learning and has worked in industry R&D for\nmore than two decades (GTE Laboratories, NYNEX/Verizon Labs, HP Labs, etc.). His\npublished work has become standard reading in data science both on methodology (e.g.,\nevaluating data mining results) and on applications (e.g., fraud detection and spam\nfiltering).\n\nColophon\nThe cover font is Adobe ITC Garamond. The text font is Adobe Minion Pro and the\nheading font is Adobe Myriad Condensed.", "source_file": "Data-Science-for-Business.pdf", "extraction_method": "pdfminer", "ocr_used": false, "has_table": false, "chapter_num": "9", "chapter_title": ", and at supervised segmentation throughout the book. If we", "section_num": "0.99", "section_title": "- 0.43 × 0.39 + 0.57 × 0.79"}
{"total_cards": 409, "chapters_detected": 1, "sections_detected": 1, "structure_coverage": 0.812, "toc_found": true, "stage": "structure_detect", "version": "2.0.0", "created_at": "2025-11-05T11:25:37Z", "segment_id": "__audit__"}
{"segment_id": "__footer__", "created_at": "2025-11-05T11:25:37Z", "version": "2.0.0", "product": "archivist magika", "manifest_sha256": "eb5dbfe1d3febb308f37c65027d2b37792168d22da24ff6f975676f7fc3038ff", "card_count": 409}
===DATASET_END===

# ğŸ“š Archivist Magika MVP v2.0

**Semantic Search RAG Pipeline for Technical Books**

Transform PDF books into searchable knowledge bases with advanced metadata extraction, OCR support, and hybrid search capabilities.

---

## âœ¨ Features

### Core Pipeline
- **ğŸ” Robust PDF Extraction** - Multi-extractor chain with retry/fallback (pdfminer â†’ pdfplumber â†’ PyPDF2 â†’ OCR)
- **ğŸ“Š Table Extraction** - Structured table extraction with markdown export
- **ğŸ‘ï¸ OCR Support** - Tesseract OCR for scanned pages with confidence scoring
- **ğŸ“– Structure Detection** - Automatic chapter/section detection and TOC generation
- **ğŸ“ Summarization** - Extractive summaries (L1/L2)
- **ğŸ¤– LM-Enhanced Metadata** - Extended fields via Ollama (topics, entities, complexity, etc.)
- **ğŸ”„ Deduplication** - Exact + fuzzy duplicate detection
- **âœ‚ï¸ Smart Chunking** - Context-aware chunking with structure preservation
- **ğŸ¯ Vector Embeddings** - BGE-M3 embeddings with FAISS indexing

### Search Capabilities
- **ğŸ” Semantic Search** - Dense retrieval via FAISS
- **ğŸ“‹ Keyword Search** - BM25-based sparse retrieval
- **ğŸ”€ Hybrid Search** - Combined semantic + keyword (weighted)
- **ğŸ¨ Query Expansion** - Synonym-based query augmentation
- **ğŸ”§ Comprehensive Filters** - Filter by chapters, topics, complexity, tools, companies, etc.
- **ğŸ“ Context Expansion** - Retrieve surrounding chunks for extended context

### Quality & Robustness
- **âœ… Quality Tracking** - Metrics tracking across all pipeline stages
- **ğŸ” Retry Logic** - Exponential backoff for transient failures
- **ğŸ›¡ï¸ Graceful Degradation** - Partial results better than nothing
- **ğŸ“Š Threshold Checking** - Automated quality validation

---

## ğŸš€ Quick Start

### Prerequisites

**System Requirements:**
- Python 3.8+
- Tesseract OCR (for scanned PDFs)
- Poppler utils (for PDF to image conversion)
- Ollama (for LM-enhanced metadata)

**Install System Dependencies:**

```bash
# Ubuntu/Debian
sudo apt-get install tesseract-ocr poppler-utils

# macOS
brew install tesseract poppler

# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3.2:3b
```

### Installation

```bash
# Clone repository
git clone https://github.com/yourusername/archivist-magika.git
cd archivist-magika

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements_v2.0.txt

# Verify installation
python -c "import faiss; import sentence_transformers; print('âœ“ All dependencies installed')"
```

### Basic Usage

**1. Process a single PDF:**

```bash
python run_mvp.py -i data/sources/pdf/accelerate.pdf
```

This will:
- Extract text/tables with OCR fallback
- Detect chapters and sections
- Generate summaries
- Add LM-enhanced metadata
- Create chunks with embeddings
- Build FAISS index

**2. Search the index:**

```bash
python rag/search.py -i accelerate -q "deployment frequency metrics"
```

**3. Search with filters:**

```bash
python rag/search.py -i accelerate -q "architecture patterns" \
  --chapter "Chapter 5" \
  --complexity intermediate \
  --has-diagram
```

---

## ğŸ“– Usage Examples

### Pipeline Operations

**Process a directory of PDFs:**

```bash
python run_mvp.py -i data/sources/pdf/ --batch
```

**Resume from specific stage:**

```bash
# If you already have structural datasets, start from structure detection
python run_mvp.py -i data/datasets/structural/book.dataset.jsonl \
  --start structure_detect
```

**Run partial pipeline:**

```bash
# Only extract structure, don't embed
python run_mvp.py -i book.pdf --start structural --end finalize
```

**Dry run (plan only):**

```bash
python run_mvp.py -i book.pdf --dry-run
```

### Search Operations

**Semantic search:**

```bash
python rag/search.py -i accelerate -q "continuous delivery practices" -k 10
```

**Hybrid search (semantic + keyword):**

```bash
python rag/search.py -i accelerate -q "DevOps metrics" --hybrid
```

**Search with context expansion:**

```bash
python rag/search.py -i accelerate -q "deployment pipeline" \
  --expand-context 2
```

**Advanced filtering:**

```bash
python rag/search.py -i accelerate \
  -q "microservices architecture" \
  --domain devops \
  --content-type case_study \
  --has-code \
  --topic "architecture" --topic "scalability"
```

### Quality Tracking

**Generate quality report:**

```bash
python run_mvp.py --quality-report
```

**Check specific metrics:**

```bash
python tools/quality_tracker.py report -s structural
```

**Compare sources:**

```bash
python tools/quality_tracker.py compare -s extended \
  --sources book1 book2 book3
```

---

## ğŸ—ï¸ Architecture

### Pipeline Stages

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PDF Input                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 1: Structural Extraction (am_structural_robust.py)       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â€¢ Multi-extractor chain: pdfminer â†’ pdfplumber â†’ PyPDF2 â†’ OCR â”‚
â”‚  â€¢ Table extraction (pdfplumber)                                â”‚
â”‚  â€¢ Smart blank page detection                                   â”‚
â”‚  â€¢ Retry logic with exponential backoff                         â”‚
â”‚  Output: structural/*.dataset.jsonl                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 2: Structure Detection (am_structure_detect.py)          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â€¢ Chapter detection (regex + heuristics)                       â”‚
â”‚  â€¢ Section detection (numbered, Title Case)                     â”‚
â”‚  â€¢ TOC generation                                               â”‚
â”‚  â€¢ Section path updates                                         â”‚
â”‚  Output: structured/*.dataset.jsonl                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 3: Summarization (am_summarize.py)                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â€¢ L1 summary (300 chars)                                       â”‚
â”‚  â€¢ L2 summary (900 chars) - optional                            â”‚
â”‚  â€¢ Extractive summarization                                     â”‚
â”‚  Output: summarized/*.dataset.jsonl                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 4: Extended Fields (am_extended.py)                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â€¢ Deduplication (exact + fuzzy)                                â”‚
â”‚  â€¢ LM-enhanced metadata via Ollama:                             â”‚
â”‚    - content_type, domain, complexity                           â”‚
â”‚    - entities (companies, people, technologies, etc)            â”‚
â”‚    - actionable (best practices, antipatterns)                  â”‚
â”‚    - topics, key concepts, tools                                â”‚
â”‚  Output: extended/*.dataset.jsonl                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 5: Finalization (am_finalize.py)                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â€¢ Schema validation                                            â”‚
â”‚  â€¢ Extended fields validation                                   â”‚
â”‚  â€¢ Policy checks                                                â”‚
â”‚  â€¢ Manifest SHA256 recalculation                                â”‚
â”‚  Output: final/*.dataset.jsonl                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 6: Chunking (am_chunk.py)                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â€¢ Smart chunking with overlap                                  â”‚
â”‚  â€¢ Structure context: [BOOK | CHAPTER | SECTION]               â”‚
â”‚  â€¢ Metadata preservation                                        â”‚
â”‚  â€¢ Table preservation                                           â”‚
â”‚  Output: chunks/*.jsonl                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 7: Embedding (am_embed.py)                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â€¢ BGE-M3 embeddings (1024 dim)                                 â”‚
â”‚  â€¢ FAISS index creation                                         â”‚
â”‚  â€¢ Metadata storage                                             â”‚
â”‚  Output: indexes/faiss/*.index + metadata/*.pkl                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Search (rag/search.py)                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  â€¢ Semantic search (FAISS)                                      â”‚
â”‚  â€¢ Keyword search (BM25)                                        â”‚
â”‚  â€¢ Hybrid search                                                â”‚
â”‚  â€¢ Comprehensive filtering                                      â”‚
â”‚  â€¢ Context expansion                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Extended Fields Schema

```json
{
  "extended_fields": {
    "content_type": "case_study",
    "domain": "devops",
    "complexity": "intermediate",
    
    "entities": {
      "companies": ["Netflix", "Amazon"],
      "people": ["Gene Kim"],
      "products": ["Jenkins", "Docker"],
      "technologies": ["Kubernetes", "Terraform"],
      "frameworks": ["DevOps", "Agile"],
      "methodologies": ["CI/CD", "Infrastructure as Code"]
    },
    
    "technical": {
      "has_code": true,
      "has_formulas": false,
      "has_diagram": true,
      "programming_languages": ["Python", "Go"]
    },
    
    "actionable": {
      "has_best_practices": true,
      "has_antipatterns": true,
      "has_instructions": true
    },
    
    "business": {
      "has_metrics": true,
      "metrics": ["deployment frequency", "lead time"],
      "has_case_study": true,
      "case_study_company": "Netflix"
    },
    
    "content_analysis": {
      "topics": ["architecture", "deployment", "monitoring"],
      "key_concepts": ["continuous delivery", "microservices"],
      "problem_statement": "...",
      "solution_approach": "..."
    },
    
    "tools_mentioned": ["Jenkins", "Kubernetes", "Terraform"]
  }
}
```

---

## ğŸ“Š Quality Metrics

### Per-Stage Thresholds

**Structural:**
- Min success ratio: 95%
- Max empty pages: 10%
- Min avg page length: 500 chars

**Extended:**
- Max duplicates: 5%
- Min extended fields coverage: 70%
- Min topics per page: 1

**Chunking:**
- Min chunk length: 100 chars
- Max chunk length: 2000 chars

**Embedding:**
- Min embedding success: 99%

---

## ğŸ”§ Configuration

Edit `config/am_config_v2.0.yaml`:

```yaml
pipeline:
  structural:
    ocr:
      enabled: true
      languages: ['eng']
      dpi: 300
    tables:
      enabled: true
      min_rows: 2
  
  extended:
    ollama:
      base_url: "http://localhost:11434"
      model: "llama3.2:3b"
      timeout: 60
  
  chunk:
    chunk_size: 512
    overlap: 50
  
  embed:
    model: "BAAI/bge-m3"
    batch_size: 32
```

---

## ğŸ§ª Testing

**Run all tests:**

```bash
python tests/test_basic.py
```

**Run specific test class:**

```bash
python -m unittest tests.test_basic.TestDatasetIO
```

---

## ğŸ“ Project Structure

```
archivist-magika/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ am_common.py              # Core utilities
â”‚   â”œâ”€â”€ am_config.yaml            # Configuration
â”‚   â””â”€â”€ am_logging.py             # Logging setup
â”‚
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ am_structural_robust.py   # Stage 1: Extraction
â”‚   â”œâ”€â”€ am_structure_detect.py    # Stage 2: Structure
â”‚   â”œâ”€â”€ am_summarize.py           # Stage 3: Summaries
â”‚   â”œâ”€â”€ am_extended.py            # Stage 4: Extended fields
â”‚   â”œâ”€â”€ am_finalize.py            # Stage 5: Validation
â”‚   â”œâ”€â”€ am_chunk.py               # Stage 6: Chunking
â”‚   â””â”€â”€ am_embed.py               # Stage 7: Embeddings
â”‚
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ search.py                 # Search engine
â”‚   â””â”€â”€ index_manager.py          # Index management
â”‚
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ quality_tracker.py        # Quality metrics
â”‚   â””â”€â”€ validate.py               # Validation tools
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_basic.py             # Unit tests
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sources/pdf/              # Input PDFs
â”‚   â”œâ”€â”€ datasets/                 # Processed datasets
â”‚   â”œâ”€â”€ indexes/                  # FAISS indexes
â”‚   â””â”€â”€ quality/                  # Quality reports
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ am_config_v2.0.yaml       # Configuration
â”‚
â”œâ”€â”€ run_mvp.py                    # Main orchestrator
â”œâ”€â”€ requirements_v2.0.txt         # Dependencies
â””â”€â”€ README.md                     # This file
```

---

## ğŸ› Troubleshooting

### Common Issues

**1. OCR not working:**

```bash
# Check Tesseract installation
tesseract --version

# Install language data
sudo apt-get install tesseract-ocr-eng
```

**2. FAISS import error:**

```bash
# Use CPU version if GPU not available
pip uninstall faiss-gpu
pip install faiss-cpu
```

**3. Ollama connection failed:**

```bash
# Start Ollama service
ollama serve

# Pull model
ollama pull llama3.2:3b
```

**4. Out of memory during embedding:**

```yaml
# Reduce batch size in config
embed:
  batch_size: 8  # Default: 32
```

**5. Poor search results:**

```bash
# Check index exists
python rag/index_manager.py list

# Rebuild index if corrupted
python run_mvp.py -i book.pdf --start chunk --end embed
```

### Performance Tips

**For large PDFs (500+ pages):**
- Disable OCR if not needed: `--no-ocr`
- Increase batch size for embedding
- Use SSD for data directory

**For many PDFs:**
- Use batch mode with `--batch`
- Enable quality checking to catch issues early
- Monitor disk space (indexes can be large)

---

## ğŸ“ˆ Performance Benchmarks

**Single PDF Processing (300 pages):**
- Structural: ~2 minutes
- Structure detect: ~30 seconds
- Extended fields: ~10 minutes (with LM)
- Chunking + Embedding: ~2 minutes

**Search Performance:**
- Semantic search: <100ms for 10K chunks
- Hybrid search: <150ms for 10K chunks

**Memory Usage:**
- Embedding model: ~2GB RAM
- FAISS index: ~10MB per 10K chunks
- LM (Ollama): ~4GB RAM

---

## ğŸ—ºï¸ Roadmap

### v2.1 (Next Release)
- [ ] Multi-format support (DOCX, HTML, Markdown)
- [ ] Advanced reranking with cross-encoders
- [ ] Query reformulation
- [ ] Result clustering

### v2.5 (Future)
- [ ] REST API
- [ ] Web UI
- [ ] Real-time updates
- [ ] Multi-modal search (text + images)

### v3.0 (Long-term)
- [ ] Graph-based RAG
- [ ] Multi-hop reasoning
- [ ] Production deployment (Docker/K8s)
- [ ] Distributed processing

---

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new features
4. Submit a pull request

---

## ğŸ“„ License

MIT License - see LICENSE file for details

---

## ğŸ™ Acknowledgments

- **BGE-M3** - Embedding model by BAAI
- **FAISS** - Vector search by Meta AI
- **pdfplumber** - PDF processing library
- **Tesseract OCR** - Google's OCR engine
- **Ollama** - Local LLM platform

---

## ğŸ“§ Contact

For questions or support:
- Open an issue on GitHub
- Email: your.email@example.com

---

**Version:** 2.0.0  
**Last Updated:** 2025-01-28

Made with â¤ï¸ for knowledge seekers